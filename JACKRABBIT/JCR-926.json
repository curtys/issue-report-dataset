{
    "comments": [
        {
            "author": "Jukka Zitting",
            "body": "Attached (DataStore.patch) is a first draft of what such a data store could look like. The main interface is simply:\n\n    public interface DataStore {\n\n        DataRecord getRecord(DataIdentifier identifier) throws IOException;\n\n        DataRecord addRecord(InputStream stream) throws IOException;\n\n    }\n\nThe patch contains a simple file-based implementation of this interface. See the javadocs in the patch for more details.\n\nI will proceed to propose a way to integrate this concept in the existing Jackrabbit core if there's consensus that this approach is worth pursuing.",
            "date": "2007-05-16T11:13:30.371+0000",
            "id": 0
        },
        {
            "author": "Jukka Zitting",
            "body": "Attached a prototype patch that integrates the data store concept in Jackrabbit. The integration is very ugly at this stage and the attached code is definitely not meant for inclusion as-is.\n\nFor very rough performance testing I created a simple test application that creates a versionable folder with 100 files in it, each containing about 270kB of application/octet-stream data. Once populated, the entire folder was checked into the version store. The numbers, averaged over a couple of test runs, are: \n\nCurrent svn trunk:\n\n    100 added files: 5625 milliseconds\n    checkin everything: 11094 milliseconds\n\nWith this patch:\n\n    100 added files: 2750 milliseconds\n    checkin everything: 1906 milliseconds\n",
            "date": "2007-06-06T10:29:42.295+0000",
            "id": 1
        },
        {
            "author": "Jukka Zitting",
            "body": "Note that the checkin performance boost will likely be even more impressive with bundle persistence. For this test I used the \"old style\" database persistence that is still the default configuration.",
            "date": "2007-06-06T10:35:25.666+0000",
            "id": 2
        },
        {
            "author": "Thomas Mueller",
            "body": "Hi,\n\nI wrote a small benchmark application to better understand the problem\n'Upload of a large file will block other concurrent actions' as described here:\n\nhttp://issues.apache.org/jira/browse/JCR-314\nhttp://www.mail-archive.com/users@jackrabbit.apache.org/msg02503.html\n\nHowever, in the current Jackrabbit, it looks like this problem is solved.\nHere is what I tried:\n- For 10 seconds, a new thread is added each second\n- 2 threads write large files, the others just write simple nodes\n- I made tests with small (8 KB) up to large (16 MB) files\n\nTo compare the results, my application has a mode\nwhere the file is not sent to Jackrabbit, instead it is written to disk \n(RandomAccessFile). I wanted to find out how long the 'simple' \nthreads are blocked by one thread writing a large files. The results are:\n\n- Storing the file outside the repository is about 30% faster\n  (but the reason might be the write buffer size or so).\n- When a thread writes a large object, the other threads are _not_ blocked badly.\n  At least not more than if the file is stored on the same disk.\n  \nIf you want to 'not block others' when writing large objects, the only solution\nI found is to store large objects on another hard drive. I have tested this as \nwell, and it completely solves the problem.\n\nThomas\n",
            "date": "2007-06-19T12:40:32.168+0000",
            "id": 3
        },
        {
            "author": "Marcel Reutegger",
            "body": "This may be due to caching on the LocalItemStateManager level. If you create a new session each time you read from the workspace or do random reads on a larger workspace the reading sessions will be blocked while the binary is written.\n\nI've committed a test case that illustrates the problem:\n\nhttp://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/test/java/org/apache/jackrabbit/core/ReadWhileSaveTest.java\n\nUsing the default locking strategy in SharedItemState manager the read thread is able to read 127 times on my laptop. When using the fine-grained locking the number goes up to 372.",
            "date": "2007-06-20T12:54:32.527+0000",
            "id": 4
        },
        {
            "author": "Marcel Reutegger",
            "body": "I haven't used the test case with the DataStore patch, but I expect that it will be similar to the number of reads for the fine-grained locking.",
            "date": "2007-06-20T12:56:07.144+0000",
            "id": 5
        },
        {
            "author": "Jukka Zitting",
            "body": "I made a few modifications to ReadWhileSaveTest to better illustrate the problem. See the attached patch that instead of saving a number of 10MB files saves a single 300MB file. It also keeps track of how many times the root node is traversed while the 300MB file is being persisted.\n\nThe raw output of a test run is below:\n\n    Wed Jun 20 23:41:17 EEST 2007 - setProperty() - 0\n    Wed Jun 20 23:41:39 EEST 2007 - begin save() - 195\n    Wed Jun 20 23:42:05 EEST 2007 - end save() - 197\n    numReads: 198\n\nEssentially:\n\n    * setProperty(): 22 seconds, during which 195 root node traversals happened\n    * save():  26 seconds, during which 2 root node traversals happened\n\nThe two traversals reported for save() most likely happened between the println() and save() statements.\n\nObservations:\n\n1) Currently we create one extra copy of the binary stream, the write performance would essentially be doubled simply by removing that extra copy. The stream passed to setProperty should be given directly to the DataStore implementation so no extra copies are needed.\n\n2) More alarmingly, this seems to indicate that the fine grained locking from JCR-314 does not work as well as it should, i.e. a save() still blocks readers. Note that I explicitly added a save() call after the \"stuff\" node is added to make sure that the write should not affect nodes that are being read. I ran the test against latest svn trunk, revision 549230. \n",
            "date": "2007-06-20T21:11:33.229+0000",
            "id": 6
        },
        {
            "author": "Pablo Rios",
            "body": "I ran ReadWhileSaveTest test with the patch applied using the FineGrainedISMLocking many times, and the results are completely different from the former one, showing in each case that this locking strategy clearly improves concurrency.\n\nThese are the output of two test runs:\n\nWed Jun 20 19:03:54 PDT 2007 - setProperty() - 1\nWed Jun 20 19:04:14 PDT 2007 - begin save() - 186\nWed Jun 20 19:04:36 PDT 2007 - end save() - 402\nnumReads: 403\n\nWed Jun 20 19:18:31 PDT 2007 - setProperty() - 1\nWed Jun 20 19:18:49 PDT 2007 - begin save() - 175\nWed Jun 20 19:19:09 PDT 2007 - end save() - 373\nnumReads: 373\n\nIn all the runs I got similar results.\n\nPablo",
            "date": "2007-06-21T02:34:39.607+0000",
            "id": 7
        },
        {
            "author": "Jukka Zitting",
            "body": "Hmm, you're correct. I assumed that FineGrainedISMLocking was already the default in latest svn trunk but it isn't. After enabling it I see similar results as you do:\n\n    Thu Jun 21 11:44:29 EEST 2007 - setProperty() - 0\n    Thu Jun 21 11:44:50 EEST 2007 - begin save() - 196\n    Thu Jun 21 11:45:13 EEST 2007 - end save() - 405\n    numReads: 406\n",
            "date": "2007-06-21T08:50:54.805+0000",
            "id": 8
        },
        {
            "author": "Jukka Zitting",
            "body": "For the record, the same test run with DataStore2.patch applied:\n\n    Thu Jun 21 13:41:43 EEST 2007 - setProperty() - 0\n    Thu Jun 21 13:41:54 EEST 2007 - begin save() - 81\n    Thu Jun 21 13:41:57 EEST 2007 - end save() - 113\n    numReads: 113\n\nThe total time is down from 44 to 14 seconds. :-)",
            "date": "2007-06-21T10:48:35.062+0000",
            "id": 9
        },
        {
            "author": "Jukka Zitting",
            "body": "The above comment is probably not comparable with previous numbers, the setProperty() time should not change considerably change with the DataStore patch (in fact it should take a bit longer due to the SHA-1 calculation). To avoid things like disk caches to interfere with the test I increased the size of the test file to 3GB (I only have 1GB RAM).\n\nWith DataStore2.patch and FineGrainedISMLocking the result is:\n\n    Thu Jun 21 13:51:09 EEST 2007 - setProperty() - 1\n    Thu Jun 21 13:55:17 EEST 2007 - begin save() - 2338\n    Thu Jun 21 13:55:18 EEST 2007 - end save() - 2352\n    numReads: 2353\n\nsetProperty() = 248 seconds, save() = 1 second\n\nWithout DataStore2.patch but with FineGrainedISMLocking the result is:\n\n    Thu Jun 21 14:08:33 EEST 2007 - setProperty() - 0\n    Thu Jun 21 14:12:58 EEST 2007 - begin save() - 2419\n    Thu Jun 21 14:17:03 EEST 2007 - end save() - 4766\n    numReads: 4816\n\nsetProperty() = 265 seconds, save() = 245 seconds\n\nI guess the stream copy algorithm in FileDataStore is slightly faster than the one in BLOBFileValue, otherwise the numbers are pretty much as expected.",
            "date": "2007-06-21T11:28:18.956+0000",
            "id": 10
        },
        {
            "author": "Thomas Mueller",
            "body": "This is a patch to clean up InternalValue.internalValue():\n\nBefore replacing the BLOBFileValue class with the new 'Global Data Store' implementation, I wanted to clean up a few things in InternalValue. The method InternalValue.internalValue() was called a lot in the Jackrabbit code. It returns a java.lang.Object which was then cast to the required class. This has a few disadvantages:\n\n- Unnecessary casts in many places\n- Hard to change the internal working of InternalValue, specially replace BLOBFileValue\n- A few times, 'instanceof' was used, making it hard to change BLOBFileValue\n- For developers new to the Jackrabbit code (like me), it's not always easy to understand what is going on in the code\n- NodeIndexer used the java.lang.Object directly, assuming the implementation will always use Boolean, Long, Double, BLOBFileValue and so on objects.\n\nIn this patch, I added specific getter methods to InternalValue, like done in the Value interface. Additionally, there are getPath (for PropertyType.PATH), getQName (for PropertyType.NAME), and getUUID (for PropertyType.REFERENCE).\n\nI had to make a few assertions, some of them were not 100% clear from the code, so could you please review them:\n\n- The 'value' of InternalValue is never 'null'. \n    ValueConstraint was checking for 'null', but as far as I see \n  it is never really possible to have a 'null' value.\n- The type of QName.JCR_FROZENUUID is STRING (Object.toString() was used before).\n- The type of QName.JCR_MIMETYPE is STRING\n- The type of QName.JCR_ENCODING is STRING\n- Currently, for types PropertyType.BINARY, the object is always \n  a BLOBFileValue (there was no other constructor for PropertyType.BINARY)\n\nNodeIndexer still has a few unnecessary type casts (addBinaryValue, addBinaryValue,...) but the methods are protected, and I was afraid to change them right now. I hope those can be changed soon to avoid a few unnecessary casts and conversions (Long, Double).\n\nThere are no functional changes yet in this patch (as far as I see). But I think this patch is required, otherwise subsequent changes will be much harder.\n\nI didn't remove InternalValue.internalValue so far, but set it to 'deprecated'. I hope it can be removed in the near future.\n \nThomas\n",
            "date": "2007-06-22T09:14:53.075+0000",
            "id": 11
        },
        {
            "author": "Stefan Guggisberg",
            "body": "> I had to make a few assertions, some of them were not 100% clear from the code, so could you please review them:\n> \n> - The 'value' of InternalValue is never 'null'.\n>     ValueConstraint was checking for 'null', but as far as I see\n>   it is never really possible to have a 'null' value.\n> - The type of QName.JCR_FROZENUUID is STRING (Object.toString() was used before).\n> - The type of QName.JCR_MIMETYPE is STRING\n> - The type of QName.JCR_ENCODING is STRING\n> - Currently, for types PropertyType.BINARY, the object is always\n>   a BLOBFileValue (there was no other constructor for PropertyType.BINARY)\n\nall the above assumptions are correct.",
            "date": "2007-06-25T08:01:49.555+0000",
            "id": 12
        },
        {
            "author": "Jukka Zitting",
            "body": "Looks good, thanks! Committed internalValue.patch as-is in revision 550493.\n\nThis change also makes it easier to switch to an InternalValue class hierarchy (as in InternalLongValue, etc.) later on if we want.\n",
            "date": "2007-06-25T13:30:16.587+0000",
            "id": 13
        },
        {
            "author": "Thomas Mueller",
            "body": "Hi,\n\nThis is a refactoring patch for GlobalDataStore. The patch introduces DataStore (almost) wherever it is required, but the behavior is not yet changed (the data store is disabled). This patch may break backwards compatibility.\n\nNodeImpl.internalCopyPropertyFrom: Never used, removed.\n\nItemStateBinding.readState and writeState: Never used, removed.\n\nDeprecated class org.apache.jackrabbit.core.state.PMContext and org.apache.jackrabbit.core.state.util.Serializer: Removed. Adding a parameter would break backwards compatibility anyway.\n\nThe parameter 'DataStore store' was added to many constructors and methods. I don't like it. Would there be a better way to do it? Idea: create a new class 'RepositoryContext' with getNodeTypeRegistry(), maybe getNamespaceResolver(), getNamespaceRegistry(), and getDataStore(). Pass this object where appropriate.\n\nSometimes BLOBs are used only for a short time. I renamed the method create(InputStream in) to createTemporary.\n\nBLOBFileValue is now an abstract class. The original implementation was renamed to 'BLOBFileValueOld'. This is only a temporary class (until DataStore is done). There is also BLOBFileValueMemory for very small binary properties (a few hundres bytes), but currently not used.\n\nThe DataStore parameter is still missing in InternalValue.valueOf (this method is never called for BINARY types), this will be changed.\n\nInternalValue: BOOLEAN_TRUE and BOOLEAN_FALSE is fixed now. \n\n\n\nA few notes about the FileDataStore implementation:\n\nI didn't change Jukka's implementation so far, but I have a few ideas:\n\nCurrently all files are stored in the same directory. However this is a problem for Windows XP (and may be other file systems). I would limit the number of files in the data store root directory to 1024. Afterwards, create subdirectories data1024-2047, data2048-3071,... with 1024 files each. When required, FileDataStore reads the directory list. If faster, one index file per directory could be created. \n\nThe file name is currently the SHA-1 digest. I suggest to use SHA-256 (unless it is a lot slower or not available on some systems). Yes you can call me paranoid. SHA-1 could be broken in a few years.\n\nAs the file name, I would use: <id>-<digest>.data. As the DataIdentifier, use <id>-<digest>. This would speed up finding files when reading, as (id / 1024) is the directory (direct lookup). Also this would allow to bundle data files in tar files. Tar file support would be priority 2. I would only bundle very small (< 4 KB) files in tar files anyway. Priority 3 would be compression (for text data mainly).\n\nThere is no garbage collection at this time. This still needs to be implemented.\n\nThomas\n",
            "date": "2007-06-28T10:44:09.276+0000",
            "id": 14
        },
        {
            "author": "Thomas Mueller",
            "body": "I forgot to click on 'Grant license to ASF for inclusion in ASF works'.",
            "date": "2007-06-28T10:46:31.232+0000",
            "id": 15
        },
        {
            "author": "Thomas Mueller",
            "body": "With this patch, both the old style 'BLOBStore' and the new style 'DataStore' implementations co-exist. The BLOBStore is used by default. To use the DataStore, set the System Property org.jackrabbit.useDataStore to 'true' as in:\n\njava -Dorg.jackrabbit.useDataStore=true ...\n\nSo this patch can be used to test the GlobalDataStore feature. All unit tests pass.\n\nThere are still a few things missing: There is no garbage collection yet. Almost each blob creates two new subdirectories (this works, but is a bit slower, and means lots of directories; can be avoided maybe).\n\nThe abstract class BLOBFileValue is now called BLOBValue (because, it is now not always a file). The old BLOBFileValue is now again named BLOBFileValue.\n\nNew class Base64ReaderInputStream for BufferedStringValue to avoid creating a file when converting long Base64 strings to BINARY data. Actually the higher performance is just a side effect; the main reason to implement this was becuase the old constructor is based on a file resource and can't be used with the DataStore.\n",
            "date": "2007-07-02T15:01:21.956+0000",
            "id": 16
        },
        {
            "author": "Pablo Rios",
            "body": "Which is the revision the last version of the binary data store patch should be applied to ?\n\n(I would like to have both the old style BLOBStore and the new style DataStore implementations co-exist and the clean up of the InternalValue.internalValue() method)\n\nThe simplest steps that I found to apply the dataStore3.patch was:\n- checkout revision 552445 (revision of the files modified by this patch)\n- delete org.apache.jackrabbit.core.data package (already exists in revision 552445)\n- make a copy of the file BLOBFileValue.java as BLOBValue.java (can't find the file to patch at line 2659)\n- apply dataStore3.patch\n\nBoth with the data store feature disabled (org.jackrabbit.useDataStore=false) and with this feature enabled (org.jackrabbit.useDataStore=true) all TCK tests passed using maven (mvn test).\nFor some reasons I don't know yet, running these tests with the data store enabled within Eclipse IDE, I occasionally got 46 errors and 10 failures.\n\nI would like to start contributing testing this feature.\n\n",
            "date": "2007-07-05T22:48:13.145+0000",
            "id": 17
        },
        {
            "author": "Pablo Rios",
            "body": "I have a couple of questions about the data store.\n\nGiven the append-only nature of this feature, when a node with a binary property is removed binary content remains in the data store. When do you expect to have a garbage collection process of binary content (files) in the file system ?\n\nAre you planning to provide a database-backed implementation of the data store ?\n\nRegards,\nPablo",
            "date": "2007-07-05T23:29:06.479+0000",
            "id": 18
        },
        {
            "author": "Thomas Mueller",
            "body": "This patch contains a background garbage collection implementation. The algorithm is:\n\n- Remember when the scan started\n- Start an EventListener\n- Tell the DataStore to update the last modification date of files that are read\n  (usually only storing files, or adding a link to an existing file\n  updates the modification date, but now during the GC also reading does)\n- Recursively iterate through all nodes\n- If the node contains binary properties, start reading them,\n  but close the input stream immediately. \n  This updates the modification date\n- If new nodes are added, the EventListener does the same\n  (recurse through all added nodes).\n  Actually it would only be required to scan the moved nodes,\n  but not sure how to do that.\n- The application needs to call 'scan' for each workspace\n  (this is not done yet, not sure how to get the list of workspaces).\n- When the scan is done, wait one second. This is for the EventListener\n  to catch up. How long do we have to wait for the observation listeners?\n  Is there a way to 'force' Jackrabbit to call the observation listeners?\n- Then, delete all data records that where not modified since GC scan started.\n\nTo test the garbage collection, there is also a simple application (BlobGCTest.java). This is not yet a unit test, it is a standalone application. It creates a few nodes:\n/node1\n/node2\n/node2/nodeWithBlob\n/node2/nodeWithTemporaryBlob\n\nThen it deletes nodeWithTemporaryBlob. The file is still in the data store afterwards. Then the garbage collection is started. While the scan is running, after node1 was scanned but before node2, the /node2/nodeWithBlob is moved to /node1/nodeWithBlob. Usually, the garbage collection wouldn't notice this (as the scan was past node1 already). But because of the EventListener, it scans the moved node as well (at the very end usually). The output is:\n\nscanning...\nscanned: /node1\nmoved /node2/nodeWithBlob to /node1\nscanned: /node2\nidentifiers:\n  17ec4a160f44f9467b4204aa20e5981d9508c4df\n  74b5b1b26f806661292b9add2e78f671cf06f432\nstop scanning...\nscanned: /node1/nodeWithBlob\ndeleting...\nidentifiers:\n  17ec4a160f44f9467b4204aa20e5981d9508c4df\n\nThis is a patch for revision 553213 (actually the revision number is in the patch as well).\n\nTo delete files early in the garbage collection scan, we could do this:\n        \nA) If garbage collection was run before, see if there a file with the list of UUIDs ('uuids.txt').\n        \nB) If yes, and if the checksum is ok, read all those nodes first (if not so many).\n    This updates the modified date of all old files that are still in use.\n    Afterwards, delete all files with an older modified date than the last scan!\n    Newer files, and files that are read have a newer modification date.\n\nC) Delete the 'uuids.txt' file (in any case).\n        \nD) Iterate (recurse) through all nodes and properties like now.\n    If a node has a binary property, store the UUID of the node in the file ('uuids.txt').\n    Also store the time when the scan started.\n        \nE) Checksum and close the file.\n        \nF) Like now, delete files with an older modification date than this scan.\n\nWe can't use node path for this, UUIDs are required as nodes could be moved around.\n\n",
            "date": "2007-07-06T09:44:26.354+0000",
            "id": 19
        },
        {
            "author": "Esteban Franqueiro",
            "body": "Thomas, your patch references org.apache.jackrabbit.benchmark.RandomInputStream, wich is not included in the patch, and I couldn't find it anywhere.\nRegards ",
            "date": "2007-07-06T16:07:48.137+0000",
            "id": 20
        },
        {
            "author": "Thomas Mueller",
            "body": "The garbage collection implementation has a few disadvantages, for example you can't stop and restart it. I suggest to get the nodes directly from the persistence manager.  To do this, I suggest to add a new method to AbstractBundlePersistenceManager:\n\nprotected synchronized NodeIdIterator getAllNodeIds(NodeId startWith, int maxCount)\n\nstartWith can be null, in which case there is no lower limit.\nIf maxCount is 0 then all node ids are returned.\n\nLike this you can stop and restart it:\nDay 1: getAllNodeIds(null, 1000);\n ... iterate though all node ids\n ... remember the last node id of this batch, for example 0x12345678\n\nDay 2: getAllNodeIds(0x12345678, 1000);\n ... iterate though all node ids\n ... remember the last node id of this batch,...\n\nNew nodes are not a problem, as the modified date of a node is updated in this case.\nAbstractBundlePersistenceManager.getAllNodeIds could be used for other features later on (for example, fast repository cloning, backup).\n\nThomas",
            "date": "2007-07-10T11:06:33.431+0000",
            "id": 21
        },
        {
            "author": "Thomas Mueller",
            "body": "What needs to be done to commit this to Jackrabbit?\nMy patches get bigger and more complicated...",
            "date": "2007-07-19T08:28:30.631+0000",
            "id": 22
        },
        {
            "author": "Jukka Zitting",
            "body": "I'm sorry I haven't had the cycles lately to properly review the patches. Would it be possible to split them to smaller semi-independent pieces? For example we could commit the data store implementation independent of the integration to rest of the Jackrabbit core. ",
            "date": "2007-07-19T08:50:11.771+0000",
            "id": 23
        },
        {
            "author": "Thomas Mueller",
            "body": "Hi,\n\nTo help integrate the changes more quickly, I suggest to split the big patches into multiple smaller ones. Here is the addition of the garbage collector. This patch does not affect currently used code. \n\nThomas",
            "date": "2007-08-06T12:42:22.390+0000",
            "id": 24
        },
        {
            "author": "Thomas Mueller",
            "body": "Hi,\n\nIs there anything I can do to help speed up integrating the global data store changes?\nIt's a bit frustrating for me if I have to wait and can't do anything.\n\nThomas",
            "date": "2007-08-08T08:33:05.321+0000",
            "id": 25
        },
        {
            "author": "Thomas Mueller",
            "body": "Revision 570033: garbage collection implementation for the global data store",
            "date": "2007-08-27T08:31:15.301+0000",
            "id": 26
        },
        {
            "author": "Thomas Mueller",
            "body": "Revision 570040: garbage collection implementation for the global data store",
            "date": "2007-08-27T08:49:46.845+0000",
            "id": 27
        },
        {
            "author": "Thomas Mueller",
            "body": "Revision 570336: BLOBFileValue and InternalValue refactoring, improved GarbageCollector",
            "date": "2007-08-28T07:00:55.038+0000",
            "id": 28
        },
        {
            "author": "Claus K\u00f6ll",
            "body": "Does the GlobalDataStore also prevent the BundleDBPersistenceManager to load the binary property automatically \nwhen you get a node ? I have posted this a few weeks ago in the mailinglist an jukka told me that this problem will hopefully\nbe fixed with the GlobalDataStore.\n\n",
            "date": "2007-08-28T08:37:16.836+0000",
            "id": 29
        },
        {
            "author": "Thomas Mueller",
            "body": "> Does the GlobalDataStore also prevent the BundleDBPersistenceManager \n> to load the binary property automatically when you get a node ?\n\nYes. If 'Global Data Store' is enabled, larger binary properties are be stored there and loaded from there. Only the DataIdentifier (a String) and small binaries (up to 1 KB or so, needs to be tested) will be stored in the persistence manager.\n",
            "date": "2007-08-28T08:43:17.233+0000",
            "id": 30
        },
        {
            "author": "Claus K\u00f6ll",
            "body": "thanks for the quick answer thomas.\n\nok and where will the \"datastore\" be stored on the filesystem.\ncan i configure it because i think for data backup and recory process it\nwill be very interesting to save the files on a huge, fast filesystem. (SAN)",
            "date": "2007-08-28T08:53:28.009+0000",
            "id": 31
        },
        {
            "author": "Thomas Mueller",
            "body": "> where will the \"datastore\" be stored on the filesystem. \nThis will be a configuration option for the FileDataStore\n",
            "date": "2007-08-28T09:24:25.386+0000",
            "id": 32
        },
        {
            "author": "Thomas Mueller",
            "body": "Revision 570407: add DataStore to constructors",
            "date": "2007-08-28T13:08:19.574+0000",
            "id": 33
        },
        {
            "author": "Thomas Mueller",
            "body": "Revision 570439: re-added useful methods",
            "date": "2007-08-28T14:17:14.861+0000",
            "id": 34
        },
        {
            "author": "Thomas Mueller",
            "body": "Revision 570439: renamed BLOBFileValue to BLOBValue, new abstract class BLOBFileValue",
            "date": "2007-08-29T08:50:09.003+0000",
            "id": 35
        },
        {
            "author": "Thomas Mueller",
            "body": "Revision 571094: global data store: new in-memory, data store, and temp file BLOB\nThe data store can now be tested, however it is disabled by default. \nTo enable, set the system property \"org.jackrabbit.useDataStore\" to \"true\"\nbefore starting the application: java -Dorg.jackrabbit.useDataStore=true ...\n(this does not work, not sure why: mvn -Dorg.jackrabbit.useDataStore=true ...)",
            "date": "2007-08-30T09:42:10.949+0000",
            "id": 36
        },
        {
            "author": "Claus K\u00f6ll",
            "body": "hi thomas,\nfirst ... great work !\ncan you explain how to configure the datastore or is this feature not yet implemented ?\ni mean will the datastore be configureable in the workspace.xml  .. i think,so each workspace can have its own\ndatastore to define different backup solutions ..\nthanks\nclaus\n",
            "date": "2007-08-30T09:58:51.476+0000",
            "id": 37
        },
        {
            "author": "Jukka Zitting",
            "body": "Nice work!\n\n> (this does not work, not sure why: mvn -Dorg.jackrabbit.useDataStore=true ...)\n\nMaven probably forks a separate JVM instance for running the test suite.",
            "date": "2007-08-30T10:04:03.807+0000",
            "id": 38
        },
        {
            "author": "Thomas Mueller",
            "body": "Hi,\n\n> great work !\nMost of it was Jukkas work.\n\n> configure the datastore\nThat's the next priority. Currently, you need to use system properties.\n\n> each workspace can have its own datastore\nNo, there is only one data store per repository. Technically it would be possible to store the files in the different workspaces, but in my view a lot of the benefit would be lost.\n\n> different backup solutions\nBackup (online and incremental) is important. It is very easy to backup the data store: just copy all files. They are never modified, and only renamed from temp file to live file. Deleted only when no longer used (by the garbage collector). But I know only a few use cases: 'backup everything', 'backup incrementally', 'backup with data compression', 'backup with encryption', and 'restore'. What other backup use cases / solutions do you have in mind?",
            "date": "2007-08-30T10:27:12.053+0000",
            "id": 39
        },
        {
            "author": "Claus K\u00f6ll",
            "body": "ok great work to both of you :-)\nI think only one datastore is not a good way.\nwe have jackrabbit running in a model 3 architecture with one repository and for each application one workspace.\nthe problem is not the backup solution, we would prefere backup incremential by third party solutions.\nat the moment we define for each workspace different persistencemanagers to different db servers.\nwe want for each workspace \"application\" define different SAN storage places, because we must discount the\nstorage volume for each application. if we have only one datastore it is not possible to know how much\nspace each application consumes.\nhope for a feature to define that.\n\none thing at the end ... what do you mean with \n> Deleted only when no longer used (by the garbage collector). \nthe files in the datastore are the permanent files or not ?\nthanks\nclaus",
            "date": "2007-08-30T11:43:38.879+0000",
            "id": 40
        },
        {
            "author": "Thomas Mueller",
            "body": "Hi,\n\n> one repository and for each application one workspace.\n\nWhy not one repository for each application? Like this you can limit the heap memory as well.\n\n> the files in the datastore are the permanent files\n\nIf things get deleted, the space must eventually be reclaimed (unless you work for a hard drive company).\n\nThomas",
            "date": "2007-08-30T11:57:47.567+0000",
            "id": 41
        },
        {
            "author": "Claus K\u00f6ll",
            "body": "the heap is not the problem ... we have a lot ;-)\ni think to use different workspaces is for us the better way ..\nwhat are the benefits you would lost with a datastore per workspace ?\ni see the datastore as enhancement for the persistencemanager because the other properties will be stored through the persistencemanager \nper workspace and now we will put everything from all workspaces into one datastore ?\ngreets \nclaus",
            "date": "2007-08-30T13:39:12.118+0000",
            "id": 42
        },
        {
            "author": "Jukka Zitting",
            "body": "A central idea of the *Global* Data Store is that its global to the repository, especially to drive down the costs of versioning and other cross-workspace operations.\n\nIt would in principle be feasible to allow a workspace-specific data store to be configured, but that would make handling of cross-workspace operations considerably more complex. IMHO the benefits of workspace-local data stores wouldn't be worth the added complexity.\n\nOn a longer timescale I also believe Jackrabbit should be moving even more to centralized repository-global resource handling as that would for example help a lot in making things like versioning operations transactional.\n\nAs for features like per-workspace quota or backups, I think those would be best achieved by implementing the features in Jackrabbit instead of relying on the underlying storage mechanism.",
            "date": "2007-08-30T13:59:26.654+0000",
            "id": 43
        },
        {
            "author": "Thomas Mueller",
            "body": "As far as I understand, one (important) use case is to use one workspace for 'authoring' and another for 'production'. The workspaces contain mostly the same data (maybe 90% is the same). Having a data store for each workspace would mean having to copy all large files. Having one data store saves you 50% of the space (for large objects). Also you can move data from one workspace to the other very quickly (because the files don't have to be copied, only the identifiers). Also cloning of a workspace is very fast for the same reasons.\n\n> i think to use different workspaces is for us the better way .. \nDo you know about blob store? If not you should try it out, because it sounds like this would be exactly what you need. The blob store already available.\n",
            "date": "2007-08-30T14:05:36.905+0000",
            "id": 44
        },
        {
            "author": "Claus K\u00f6ll",
            "body": "ok if you have a use case as you described i think a global datastore is the best way to make cross-workspace operations\nmore easy. you have only one file and no copies af same files.\nif the road goes to a centralized repository a global datastore makes of course sense.\n\nin my case (and i think other has also a similarly use case) a per workspace datastore makes things easier\ni am working for a government and the office employee get a lot of paper every day. they scan it and put it into jackrabbit.\nnow we must keep the documents based on the law up to 5-7 years with fast read access in jackrabbit. after that time we can archive it (slow access) and therefore\nwe want to store this documents not on a SAN storage (because its expensive) rather save it to a cheaper storage system (tape drive system)\nwe have planed to make this with moving the data from one workspace (SAN) to a other one (tape drive system)\nwith the global datastore is this not possible i think\n\nhow would you solve such scenarios ?\ngreets\nclaus\n\n",
            "date": "2007-08-31T06:14:58.775+0000",
            "id": 45
        },
        {
            "author": "Thomas Mueller",
            "body": "Hi,\n\nTheoretically the data store could be split to different directories / hard drives. Content that is accessed more often could be moved to a faster disk, and less used data could eventually be moved to slower / cheaper disk. That would be an extension of the 'memory hierarchy' (see also http://en.wikipedia.org/wiki/Memory_hierarchy). Of course this wouldn't limit the space used per workspace, but would improve system performance if done right. Maybe we need to do that anyway in the near future to better support solid state disk. \n\nWorkspace usage: I'm not sure what solution would be the best for your use case. Maybe this would better be discussed in a separate thread. See also http://wiki.apache.org/jackrabbit/DavidsModel#head-ca639e0ee110b80e8277a50f9b9de092b5d86427",
            "date": "2007-08-31T07:34:01.019+0000",
            "id": 46
        },
        {
            "author": "Thomas Mueller",
            "body": "Revision 571399: Follow Jackrabbit code style. Better do that now before it is too late, before re-formatting makes comparing version difficult",
            "date": "2007-08-31T08:28:17.750+0000",
            "id": 47
        },
        {
            "author": "Thomas Mueller",
            "body": "Revision 573209: Configuration is now supported. Still the system property 'org.jackrabbit.useDataStore' is required to enable this feature, but now the data store class (and for the FileDataStore, the path) can be configured:\n\n<Repository>\n    <DataStore class=\"org.apache.jackrabbit.core.data.FileDataStore\">\n        <param name=\"path\" value=\"${rep.home}/repository\"/>\n    </DataStore>\n    ....\n</Repository>\n\nThe DataStore API was changed a bit to support this. The DataStore configuration is optional, if missing the system almost works as now. Almost, because the BLOBValue class is no longer used. The system property org.jackrabbit.useDataStore will be removed when this is tested. Also, the system property org.jackrabbit.minBlobFileSize will be integrated into DataStore. My idea is that each data store implementation (file system, database, S3?) can have a different 'minimum size' depending on the overhead to store / load a value.\n\nBy the way, the FileDataStore overhead (mainly calculating the SHA-1 digest) is quite low, smaller than 10%:\nWriting and reading 5 files 100 KB each, average over 5 runs:\nFileDataStore: 1390 ms, FileOutputStream: 1287 ms\n",
            "date": "2007-09-06T10:26:22.665+0000",
            "id": 48
        },
        {
            "author": "Thomas Mueller",
            "body": "Revision 574543: The FileDataStore now supports the configuration option 'minRecordLength' (default: 100). \n\nMissing unit tests have been added, the test code coverage is now OK. A bug has been fixed (when importing, a temp file was created but never stored in the data store).\n\nThe setting 'org.jackrabbit.useDataStore' can now be enabled in my view, I will do this in a day or two unless there is a problem. Then, the new classes BLOBInResource, BLOBInTempFile, BLOBInMemory are used instead of BLOBValue. The file data store is still only used when configured in repository.xml. Temp files are still created in some cases (for example importing).",
            "date": "2007-09-11T10:40:34.608+0000",
            "id": 49
        },
        {
            "author": "Thomas Mueller",
            "body": "Revision 15482: Remove unused methods\n",
            "date": "2007-09-12T09:06:59.053+0000",
            "id": 50
        },
        {
            "author": "Claus K\u00f6ll",
            "body": "hi thomas\n\nwhat do you think about migration steps for existing workspaces ?\n\ngreets\nclaus\n",
            "date": "2007-09-12T09:11:06.077+0000",
            "id": 51
        },
        {
            "author": "Thomas Mueller",
            "body": "Compatibility: the plan is to make this feature fully backward compatible. When the system property 'org.jackrabbit.useDataStore' is enabled, and when FileDataStore is configured in repository.xml, old repositories can still be used. Only new binary objects are stored in the data store, and only when this is configured in repository.xml.\n\nMigration to data store: binary data that is currently in the blob store (or in the persistence manager) can be moved to the data store by cloning a workspace, or export / import.\n\nMigration away from the data store: currently, you need to export the data with the data store enabled / configured, and then import into a new repository without data store.",
            "date": "2007-09-12T09:29:22.901+0000",
            "id": 52
        },
        {
            "author": "Thomas Mueller",
            "body": "Migration from one data store type to another: Currently not required as there is only the file data store. A tool will be added when needed. The API required is already there: DataStore.getAllIdentifiers().",
            "date": "2007-09-12T09:38:31.499+0000",
            "id": 53
        },
        {
            "author": "Pankaj Gupta",
            "body": "Currently clustering requires all blobs to be stored in the database to ensure transaction semantics. But in our application we would prefer to store blobs externally for performance reasons. I am assuming that with the global data store we would be able to that since blobs in the transient space would be stored separately.  Please let me know if this is correct as this will resolve a big issue that we are currently facing with Jackrabbit.\n\nThanks,\nPankaj",
            "date": "2007-09-12T21:04:17.498+0000",
            "id": 54
        },
        {
            "author": "Thomas Mueller",
            "body": "> clustering - store blobs externally for performance reasons\n> with the global data store we would be able to that \n\nYes, clustering is supported. Entries are added as early as possible, and deleted only when they are not reachable (garbage collection). There is no 'update' operation, only 'add new entry'. Data is added before the transaction is committed. Additions are globally atomic, cluster nodes can share the same data store. Even different repositories can share the same store, as long as garbage collection is done correctly.\n\nAdvantages: no duplicate entries (saves space; speeds up versioning, workspace cloning, node copy operations). Improved concurrency.\n\nDisadvantage: garbage collection is required. There are ideas how to efficiently do that (without having to scan through the whole repository).",
            "date": "2007-09-13T06:49:57.300+0000",
            "id": 55
        },
        {
            "author": "Thomas Mueller",
            "body": "Revision 575304: enabling the data store\n\nTo use the FileDataStore, add this to your repository.xml after the <Repository> start tag:\n\n    <DataStore class=\"org.apache.jackrabbit.core.data.FileDataStore\"/>\n\nThe files will be stored under repository/datastore by default. For more information about the configuration, see FileDataStore.java",
            "date": "2007-09-13T13:23:38.070+0000",
            "id": 56
        },
        {
            "author": "Jukka Zitting",
            "body": "Nice! Time to resolve this issue and file new issues for future fixes/enhancements?",
            "date": "2007-09-13T14:25:48.150+0000",
            "id": 57
        }
    ],
    "component": "jackrabbit-core",
    "description": "There are three main problems with the way Jackrabbit currently handles large binary values:\n\n1) Persisting a large binary value blocks access to the persistence layer for extended amounts of time (see JCR-314)\n2) At least two copies of binary streams are made when saving them through the JCR API: one in the transient space, and one when persisting the value\n3) Versioining and copy operations on nodes or subtrees that contain large binary values can quickly end up consuming excessive amounts of storage space.\n\nTo solve these issues (and to get other nice benefits), I propose that we implement a global \"data store\" concept in the repository. A data store is an append-only set of binary values that uses short identifiers to identify and access the stored binary values. The data store would trivially fit the requirements of transient space and transaction handling due to the append-only nature. An explicit mark-and-sweep garbage collection process could be added to avoid concerns about storing garbage values.\n\nSee the recent NGP value record discussion, especially [1], for more background on this idea.\n\n[1] http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200705.mbox/%3c510143ac0705120919k37d48dc1jc7474b23c9f02cbd@mail.gmail.com%3e\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "JCR-926",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Major",
    "product": "JACKRABBIT",
    "project": "JACKRABBIT",
    "summary": "Global data store for binaries",
    "systemSpecification": true,
    "version": ""
}