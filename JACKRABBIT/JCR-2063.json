{
    "comments": [
        {
            "author": "Thomas Mueller",
            "body": "Comitted in revision 767427 (trunk)",
            "date": "2009-04-22T09:16:37.011+0000",
            "id": 0
        },
        {
            "author": "Jukka Zitting",
            "body": "Merged to the 1.5 branch in revision 767555.",
            "date": "2009-04-22T14:54:13.226+0000",
            "id": 1
        },
        {
            "author": "Thomas Mueller",
            "body": "A workaround for implementations where this is not fixed is:\n\ngc.mark();\ntry {\n    // sleep to ensure the last modified time is updated\n    // even for file system with a lower time resolution\n    Thread.sleep(5000);\n} catch (Exception e) {\n    // can not ignore, otherwise data that is in use may be deleted\n    throw new RepositoryException(\"Interrupted\");\n}\ngc.mark();\n\n",
            "date": "2010-02-16T13:56:35.339+0000",
            "id": 2
        }
    ],
    "component": "jackrabbit-core",
    "description": "It looks like the FileDataStore garbage collection (both regular scan and persistence manager scan) can delete files that are still needed.\n\nCurrently it looks like the reason is the last access time resolution of the operating system. This is 2 seconds for FAT and Mac OS X, NTFS 100 ns, and 1 second for other file systems. That means file that are scanned at the very beginning are sometimes deleted, because they have a later last modified time then when the scan was started.",
    "hasPatch": false,
    "hasScreenshot": false,
    "id": "JCR-2063",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "JACKRABBIT",
    "project": "JACKRABBIT",
    "summary": "FileDataStore: garbage collection can delete files that are still needed",
    "systemSpecification": true,
    "version": ""
}