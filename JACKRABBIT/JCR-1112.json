{
    "comments": [
        {
            "author": "Przemo Pakulski",
            "body": "Attached simple patch which allows to set the interval programmatically, and change the default interval to 10 seconds.",
            "date": "2007-09-04T16:37:04.231+0000",
            "id": 0
        },
        {
            "author": "Jukka Zitting",
            "body": "+1 I'd wouldn't mind pushing the default interval up to a minute or even higher. I don't think there's much benefit in too aggressive cache balancing.",
            "date": "2007-09-04T18:19:21.094+0000",
            "id": 1
        },
        {
            "author": "Jukka Zitting",
            "body": "BTW, please use spaces instead of tabs for indentation.",
            "date": "2007-09-04T18:21:47.274+0000",
            "id": 2
        },
        {
            "author": "Thomas Mueller",
            "body": "For certain use cases, 10 seconds delay may result in OutOfMemoryException. I'm not against applying this patch, but we need to be sure it doesn't render the CacheManager ineffective.\n\n> under some load \n\nIs there a way to reproduce this problem using a simple test application? If not, I like to write one. To do that, I need some more information: How many sessions, how much memory is available / used (java -Xmx...), what is the algorithm, is XA used, versioning, how do the nodes look like, how does the data look like, virtual machine, operating system? \n\n> up to 10-15% percent of CPU time (profiler metrics) \n\nHow was this measured?\n\nThanks,\nThomas\n",
            "date": "2007-09-05T08:20:34.703+0000",
            "id": 3
        },
        {
            "author": "Raffaele Sena",
            "body": "I can reproduce this problem or something related to this by simply importing an XML file with a few thousand nodes. The more nodes I have in the repository and the more time the system spend in rebalancing the cache, pretty much at every access to the repository.\n\nI was experimenting with a system with a simple hierarchical structure like:\n\n/users\n  user1\n  user2\n  user3\n\nfor each user I had a large data structure stored as XML (like an imported XML file)\n\nwith such a system, even with only a few users, accessing to the node /users/<userX> takes seconds (and the more users or complex structure I have the longer it takes) and all I see in the logs is resizeAll() messages\n\n",
            "date": "2007-09-06T21:02:43.427+0000",
            "id": 4
        },
        {
            "author": "Thomas Mueller",
            "body": "> and all I see in the logs is resizeAll() messages\n\nDon't blame the messenger. Seeing the messages doesn't mean this is the problem. The messages are disabled in the trunk.\n\nDo you have some profiling data?",
            "date": "2007-09-07T04:13:50.120+0000",
            "id": 5
        },
        {
            "author": "Thomas Mueller",
            "body": "In my view, the root cause (why does recalculation take so long, are there hundreds of caches?) should be understood and fixed. Just changing the interval would hide another problem, and may cause out of memory error.\n\nTo solve this problem the root cause of the problem needs to be reproducible (but maybe the solution will be different then).",
            "date": "2007-09-18T07:40:38.061+0000",
            "id": 6
        },
        {
            "author": "Martijn Hendriks",
            "body": "Hi,\n\nI think that there are some issues with the current CacheManager that could be improved:\n\n- The MLRUItemStateCache.touch method triggers the CacheManager.cacheAccessed method, which may call resizeAll. When the system is heavily loaded, many threads may unnecessarily be blocked by the synchronized block in CacheManager.cacheAccessed. The chances for this increase as SLEEP decreases and the time needed for resizeAll increases. This could easily be improved .\n\n- The resizeAll method is expensive (for MLRUItemStateCaches, which are used everywhere) because it calls MLRUItemStateCache.getMemoryUsed, which recalculates the size of each entry in the cache (linear complexity in the size of the cache...). Since the NodeState/PropertyState.calculateMemoryFootprint seem to give approximate values anyway, wouldn't it be an idea to keep track of the approximate cache size in the MLRUItemStateCache itself? Furthermore, getMemoryUsed even blocks read-access to the cache. A large shared cache such as the one of the SharedItemStateManager suffers significantly from this, I think.\n\nThe minimum time between rebalancing seems small but, as Thomas noted, there are certain use-cases where this is needed. Isn't there a way to detect such extreme cache blowups in another way? When, for instance, a MLRUItemStateCache keeps track of its own approximate size, the time derivative of this size could be used to prevent blowups.\n\nBest wishes,\n\nMartijn",
            "date": "2007-09-19T09:55:55.109+0000",
            "id": 7
        },
        {
            "author": "Thomas Mueller",
            "body": "Hi,\n\nThe method getMemoryUsed could be improved by keeping the current value and only add / subtract when there are changes. Still from time to time a full recalculation (like now) is required because the size of the objects in the cache could change. If only one in 20 calls trigger a full recalculation, it would result in a speed up of about 10 times.\n\nFor me, the CacheManager is a workaround; if possible the number of caches should be reduced to one.\n\nThomas",
            "date": "2007-09-25T09:55:37.263+0000",
            "id": 8
        },
        {
            "author": "Jukka Zitting",
            "body": "> For me, the CacheManager is a workaround; if possible the number of caches should be reduced to one.\n\n+1!",
            "date": "2007-09-25T10:00:49.885+0000",
            "id": 9
        },
        {
            "author": "Jukka Zitting",
            "body": "Resolving as fixed for 1.4 based Przemo's changes in revision 592950.",
            "date": "2007-11-22T00:47:29.643+0000",
            "id": 10
        },
        {
            "author": "Jukka Zitting",
            "body": "Merged to the 1.3 branch in revision 631606.",
            "date": "2008-02-29T14:17:31.521+0000",
            "id": 11
        }
    ],
    "component": "jackrabbit-core",
    "description": "Currently interval between recaluclation of cahce size is hard coded to 1000 ms. Resizing/recalculation of cache size is quite expensive method (especially getMemoryUsed on MLRUItemStateCache is time consuming)\n\nDepending on the configuration, we realized that under some load up to 10-15% percent of CPU time (profiler metrics) could be spend doing such recalculations. It does not seem to be needed to resize cache every second. Best this interval should be configurable in external config. file with other cache settings (like memory sizes).",
    "hasPatch": false,
    "hasScreenshot": false,
    "id": "JCR-1112",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Minor",
    "product": "JACKRABBIT",
    "project": "JACKRABBIT",
    "summary": "CacheManager interval between recalculation of cache sizes should be configurable",
    "systemSpecification": true,
    "version": ""
}