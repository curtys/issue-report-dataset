{
    "comments": [
        {
            "author": "Jukka Zitting",
            "body": "See the attached plots for a few example results I got from running the current basic performance tests over last night.",
            "date": "2010-08-04T08:36:15.457+0000",
            "id": 0
        },
        {
            "author": "Jukka Zitting",
            "body": "A few more plots.",
            "date": "2010-08-04T08:37:03.058+0000",
            "id": 1
        },
        {
            "author": "Jukka Zitting",
            "body": "Resolving this as fixed since we now have a basic performance test suite in place. We can use followup issues to add new test cases or to fix performance issues highlighted by existing tests.",
            "date": "2010-08-04T08:38:44.900+0000",
            "id": 2
        },
        {
            "author": "Alexander Klimetschek",
            "body": "Looks cool! (Well, the test suite, not necessarily the graphs for versions >= 2.0 ;-))\n\nInteresting would also be the 90th percentile value of all runs of a certain test, instead of the mean.",
            "date": "2010-08-04T10:04:29.547+0000",
            "id": 3
        },
        {
            "author": "Jukka Zitting",
            "body": "Currently the jcr-benchmark suite currently uses the commons-math class SummaryStatistics which doesn't do percentiles, but it should be pretty straightforward to switch to using DescriptiveStatistics instead. The above plots show the mean value and standard deviations. But switching to something like the median plus 10th and 90th percentiles might give a more accurate picture. Alternatively we could just  do a scatter plot of the entire data set.\n\nAbout the 2.x figures, the increase in session startup times is probably due to the extra JCR 2.0 features (access control?) we're starting up. Note that the performance drop starts already in 1.6 where we started introducing new JCR 2.0 features. In any case starting up a single session still takes only about 0.1ms on my computer, which seems OK to me.\n\nThe switch to using the FileDataStore by default in Jackrabbit 2.0 seems to explain the changes in file handling performance. We're pretty good with both reading and writing large files, but the handling of smaller files suffers somewhat from the way the FileDataStore uses intermediate directories. Perhaps we should consider putting all smaller binaries inside one big file instead.",
            "date": "2010-08-04T10:52:43.504+0000",
            "id": 4
        },
        {
            "author": "Jukka Zitting",
            "body": "Attached a pretty impressive simple search (100 queries on 10k nodes, each returning 100 results) performance graph. It could be that the post-1.4 figures are a result of the entire data set residing in the bundle cache.",
            "date": "2010-08-09T15:16:30.369+0000",
            "id": 5
        },
        {
            "author": "Thomas Mueller",
            "body": "I created JCR-2781 to speed up the FileDataStore.",
            "date": "2010-10-13T07:45:42.305+0000",
            "id": 6
        }
    ],
    "component": "jackrabbit-core",
    "description": "I'd like to set up a multi-version performance test suite inside jackrabbit-core/src/test/performance, similar to the compatibility test suite we added in JCR-2631. This performance test suite would produce comparable performance numbers for a number of simple benchmark tests across different Jackrabbit versions, including the latest snapshot.\n\n",
    "hasPatch": false,
    "hasScreenshot": true,
    "id": "JCR-2695",
    "issuetypeClassified": "OTHER",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "JACKRABBIT",
    "project": "JACKRABBIT",
    "summary": "Jackrabbit performance test suite",
    "systemSpecification": true,
    "version": ""
}