{
    "comments": [
        {
            "author": "Thomas Mueller",
            "body": "Do you have a reproducible test case where it uses a lot of memory or even runs out of memory?\n\nAs far as I see, the persistence manager only ever selects one row at a time (using a where condition), except for the consistency check, where only the node id is selected. Therefore, I don't see where it could use a lot of memory.\n\nPlease re-open if you have a test case where it's actually a problem, or if you can point to the code where many complete rows are selected.",
            "date": "2011-02-16T07:46:42.738+0000",
            "id": 0
        },
        {
            "author": "Christopher Elkins",
            "body": "I do have a reproducible case in my application, but it is not something I can easily reduce to a test case suitable for attachment here.\n\nThe actual number of rows returned is irrelevant. From the aforelinked PDF,\n\n\"Since the buffers are allocated when the SQL is parsed, the size of the buffers depends not on the actual size of the row data returned by the query, but on the maximum size possible for the row data. After the SQL is parsed, the type of every column is known and from that information the driver can compute the maximum amount of memory required to store each column. The driver also has the fetchSize, the number of rows to retrieve on each fetch. With the size of each column and the number of rows, the driver can compute the absolute maximum size of the data returned in a single fetch. That is the size of the buffers.\"\n\nThe specific context is the same as that of JCR-2832, adding a new node to a cluster. However, any statement that returns a BLOB column given the current fetch size of 10,000 is going to create large buffers.",
            "date": "2011-02-16T17:14:56.942+0000",
            "id": 1
        },
        {
            "author": "Thomas Mueller",
            "body": "Wow, I would never have thought the Oracle driver actually allocates the memory... sounds like a really bad idea to do nowadays, but well this is Oracle. Anyway, if you do have a failing test case, then of course the fetch size needs to be changed.",
            "date": "2011-02-17T08:45:53.005+0000",
            "id": 2
        },
        {
            "author": "Danilo Ghirardelli",
            "body": "I have the same problem, 10000 in fetch size is causing out of memory, even jackrabbit version 2.2.8.\nIn my case it seems to be triggered by clustering configuration, because probably that will return more than a single row, and in this case oracle will allocate all the necessary ram for about 10000 rows.\n\nYou can try with Oracle 10 (XE edition also causes the problem), driver 10.2.0.4 (or 10.2.0.5), default java xms size (32 bit version, not 64, on Windows platform) and clustering configured. The simple existence of clustering causes the crash in my case, you may want to save a few nodes to force something in the clustering tables.\n\nAnyway, that size of fetch size is dangerous. If it was set only to limit postgresql allocation and you are sure that the average query will return a single record, you should hardcode it to 10... I tried that the application will start correctly with a value of 10 and 100 but not with 1000 or above.",
            "date": "2011-09-01T14:12:16.436+0000",
            "id": 3
        },
        {
            "author": "Stephen Chester",
            "body": "I am also experiencing this issue, and am working on a patch that will add the ability to specify the fetch size to the configuration options of the relevant classes, with the default being 10,000.",
            "date": "2011-09-01T23:35:01.212+0000",
            "id": 4
        },
        {
            "author": "Manuel Molaschi",
            "body": "We have a different problem with the same cause... We experience a big performance loss on an app built on Magnolia CMS and starting on Tomcat (configured with xmx at 1024m)\nAfter a long investigation we assumed that this behaviour was caused by jvm spending a lot of time in GC (cpu usage was always very high)\n\nThese are the startup time on different jr versions / fetch size\n\nwith jr 2.2.1: 129 sec\nwith jr 2.2.8 (fetch size 10000): > 1h\nwith jr 2.2.8 (patch applied on ConnectionHelper to comment out stmt.setFetchSize): 135 sec",
            "date": "2011-09-05T14:07:39.303+0000",
            "id": 5
        },
        {
            "author": "Danilo Ghirardelli",
            "body": "I attached a very minimalistic patch that will set the fetchSize to 10 in the OracleConnectionHelper, that is used for all oracle connections (bundles, journal and datastore).\n\nBut still there are a few points that may be considered:\n- as stated by the original poster, if the fetchSize problem was Postgresql specific, it would be better to create a PostgresConnectionHelper and isolate the different fetchSize there.\n- generally speaking, if you are sure that the ResultSet is not going to be very big, you should really trim down that fetchSize (or don't set it so jdbc drivers would use their defaults), otherwise a simple change in some other driver behaviour might cause out of memory or slowness.\n- this issue is currently unassigned, so nobody would apply any patch... :-)\n",
            "date": "2011-09-08T10:56:38.632+0000",
            "id": 6
        },
        {
            "author": "Claus K\u00f6ll",
            "body": "Soon I'll take a look at this issue",
            "date": "2011-09-15T05:37:07.745+0000",
            "id": 7
        },
        {
            "author": "Danilo Ghirardelli",
            "body": "The linked issue JCR-3090 addressed the same issue in a different way: every call to the \"reallyExec\" method (where the setFetchSize is set) has the maxRows value set to 0, so the patch for that issue is simply indirectly disabling the fetch size hint. This would probably solve the problem of this issue but will probably re-open the problem stated in JCR-2832.\n\nBy the way, just like Manuel, I also used Magnolia CMS (with clustering) on Tomcat to reproduce the problem.",
            "date": "2011-10-04T08:30:06.060+0000",
            "id": 8
        },
        {
            "author": "Claus K\u00f6ll",
            "body": "Created a patch that creates for a PostgreSQLDB a own ConnectionHelper that sets the fetchSize to 10000 to not break any code. On other DB's the fetchSize will be 100.",
            "date": "2011-10-11T11:09:01.015+0000",
            "id": 9
        },
        {
            "author": "Danilo Ghirardelli",
            "body": "I'm sorry if I wasn't clear, but the value of 100 just worked for my application (clustered Magnolia CMS), and it was the maximum fetchSize value that allowed the application to start. Anyway, at 100 the memory occupation was quite high, so maybe that value was good in my case but may crash in other cases. Would it be possible to avoid setting a fetchSize at all (except for postgresql)? Performance were always good and there was no problem until it was set for the first time in JCR-2832...",
            "date": "2011-10-11T11:53:00.140+0000",
            "id": 10
        },
        {
            "author": "Claus K\u00f6ll",
            "body": "Hi Danilo,\n\nOk i see. I think we can change the fetchSize to 0 per default. This should work as it means the hint should be ignored.\n",
            "date": "2011-10-11T12:41:04.949+0000",
            "id": 11
        },
        {
            "author": "Claus K\u00f6ll",
            "body": "Modified (fetch Size 0 per default) patch applied in rev 1182667.\nPostreSQL has a fetchSize of 10000 as before. If it makes no problems we can leave it so high.",
            "date": "2011-10-13T05:59:02.167+0000",
            "id": 12
        },
        {
            "author": "Jukka Zitting",
            "body": "Merged to the 2.2 branch in revision 1202191.",
            "date": "2011-11-15T13:50:31.075+0000",
            "id": 13
        }
    ],
    "component": "jackrabbit-core, sql",
    "description": "Since Release 10g, Oracle JDBC drivers use the fetch size to allocate buffers for caching row data.\ncf. http://www.oracle.com/technetwork/database/enterprise-edition/memory.pdf\n\nr1060431 hard-codes the fetch size for all ResultSet-returning statements to 10,000. This value has significant, potentially deleterious, effects on the heap space required for even moderately-sized repositories. For example, the BUNDLE table (from 'oracle.ddl') has two columns -- NODE_ID raw(16) and BUNDLE_DATA blob -- which require 16 b and 4 kb of buffer space, respectively. This requires a buffer of more than 40 mb [(16+4096) * 10000 = 41120000].\n\nIf the issue described in JCR-2832 is truly specific to PostgreSQL, I think its resolution should be moved to a PostgreSQL-specific ConnectionHelper subclass. Failing that, there should be a way to override this hard-coded value in OracleConnectionHelper.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "JCR-2892",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "JACKRABBIT",
    "project": "JACKRABBIT",
    "summary": "Large fetch sizes have potentially deleterious effects on VM memory requirements when using Oracle",
    "systemSpecification": true,
    "version": "2.2.2"
}