{
    "comments": [
        {
            "author": "Michael D\u00fcrig",
            "body": "As promised some numbers. All measurements are done using ReadPerformanceTest.java [1]. \n\nBatch size: 24340, 12170, 6085, 3043, 1521, 761, 380, 190, 95, 48, 24, 12, 6, 3, 1\nms per request: 20.2, 24.2, 17.4, 16.3, 7.3, 3.0, 2.5, 2.1, 2.0, 1.3, 1.3, 1.1, 1.0, 1.0, 1.1\n\nThe performance impact of large batches is clearly visible here. Without refresh operations [2] the picture remains similar but less pronounced:\n\nBatch size: 24340, 12170, 6085, 3043, 1521, 761, 380, 190, 95, 48, 24, 12, 6, 3, 1\nms per request: 5.1, 17.1, 16.3, 12.0, 6.0, 2.6, 2.7, 2.0, 2.0, 1.4, 1.4, 1.2, 1.0, 1.1, 1.3\n\n\n[1] http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-jcr2spi/src/test/java/org/apache/jackrabbit/jcr2spi/benchmark/ReadPerformanceTest.java?revision=910523&view=markup&pathrev=910523\n\n[2] See upcoming patch\n\n",
            "date": "2010-02-16T15:28:14.985+0000",
            "id": 0
        },
        {
            "author": "Michael D\u00fcrig",
            "body": "Here's the patch mentioned in [2] above. \n\nIndex: src/test/java/org/apache/jackrabbit/jcr2spi/benchmark/ReadPerformanceTest.java\n===================================================================\n--- src/test/java/org/apache/jackrabbit/jcr2spi/benchmark/ReadPerformanceTest.java\n+++ src/test/java/org/apache/jackrabbit/jcr2spi/benchmark/ReadPerformanceTest.java\n@@ -136,7 +136,7 @@\n         final List<Item> items = new ArrayList<Item>();\n \n         for (int k = 0; k < count; k ++) {\n-            switch (rnd.nextInt(4)) {\n+            switch (rnd.nextInt(3)) {\n                 case 0: { // getItem\n                     callables.add(new Callable<Long>() {\n                         public Long call() throws Exception {\n",
            "date": "2010-02-16T15:29:30.991+0000",
            "id": 1
        },
        {
            "author": "Michael D\u00fcrig",
            "body": "POC of the cache implementation as described above. \n\nThe patch is functionally complete. The implementation is hard coded however and not yet exposed to the respective APIs. See fixme tags in the code. ",
            "date": "2010-02-16T15:35:12.259+0000",
            "id": 2
        },
        {
            "author": "Michael D\u00fcrig",
            "body": "Some more numbers demonstrating the effect with JCR-2498-poc.patch applied. The 'new/old time' row gives the quotients of the request times with the patch applied vs. without the patch applied. The 'new/old rts' row gives the quotients of the network round trips with the patch applied vs. without the patch applied. \n\nThe first measurement includes all operations (getItem, getNode, getProperty and refresh) as above. \n\nBatch size: 24340, 12170, 6085, 3043, 1521, 761, 380, 190, 95, 48, 24, 12, 6, 3, 1\nnew/old time: 0.1, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.5, 0.7, 0.6, 1, 1, 1.1, 0.8\nnew/old rts: 2.1, 2.8, 1.8, 2.4, 1.8, 1.4, 1.3, 1.2, 1, 1.1, 1, 1, 0.9, 1, 0.9\n\nMost obvious is the vast performance increase (up to factor 10) for reading items. However this comes along with an increase of the number of network round trips. Three things should be noted here: 1. For realistic batch sizes the increase of the number of network round trips is not so significant. 2. The increase of the number of network round trips are caused by the refresh operations. In the test scenario the number of refresh operations is unrealistically high (every fourth operation is a refresh). 3. The items in the batches of the test case are not realistically distributed across the items of the repository. That is, the items are randomly chosen from the repository. In practice however, the items in a batch would be related to each other by some locality criteria. I assume that this would further mitigate the observed effect. \n\nFor completeness sake here the same measurement as above but without refresh operations: \n\nBatch size: 24340, 12170, 6085, 3043, 1521, 761, 380, 190, 95, 48, 24, 12, 6, 3, 1\nnew/old time: 0.2, 0, 0, 0.1, 0.1, 0.2, 0.4, 0.4, 0.6, 0.6, 0.7, 1, 1, 1, 1.1\nnew/old rts: 1, 1, 0.9, 0.9, 0.8, 0.9, 0.9, 0.9, 0.9, 1, 1, 1, 1, 1, 1\n",
            "date": "2010-02-16T15:53:57.640+0000",
            "id": 3
        },
        {
            "author": "angela",
            "body": "although i didn't look at the poc-patch in detail....based on our f2f discussion: looks reasonable to me :)\n\n",
            "date": "2010-02-16T17:32:03.164+0000",
            "id": 4
        },
        {
            "author": "Michael D\u00fcrig",
            "body": "Applied a cleaned up/improved version of the patch in revision 915810  \n",
            "date": "2010-02-24T14:49:28.554+0000",
            "id": 5
        }
    ],
    "component": "jackrabbit-jcr2spi, jackrabbit-spi",
    "description": "Currently all ItemInfos returned by RepositoryService#getItemInfos are placed into the hierarchy right away. For big batch sizes this is prohibitively expensive. The overhead is so great (*), that it quickly outweighs the overhead of network round trips. Moreover, SPI implementations usually choose the batch in a way determined by the backing persistence store and not by the requirements of the consuming application on the JCR side. That is, many of the items in the batch might never be actually needed. \n\nI suggest to implement a cache for ItemInfo batches. Conceptually such a cache would live inside jcr2spi right above the SPI API. The actual implementation would be provided by SPI implementations. This approach allows for fine tuning cache/batch sizes to a given persistence store and network environment. This would also better separate different concerns: the purpose of the existing item cache is to optimize for the requirement of the consumer of the JCR API ('the application'). The new ItemInfo cache is to optimize for the specific network environment and backing persistence store. \n\n(*) Numbers follow ",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "JCR-2498",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "JACKRABBIT",
    "project": "JACKRABBIT",
    "summary": "Implement caching mechanism for ItemInfo batches",
    "systemSpecification": true,
    "version": ""
}