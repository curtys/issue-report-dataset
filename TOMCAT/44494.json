{
    "comments": [
        {
            "author": null,
            "body": "Bug Confirmed - regression in 6.0.x as well.\n\nSEVERE: Servlet.service() for servlet jsp threw exception\njava.lang.ArrayIndexOutOfBoundsException: 8192\n\tat org.apache.tomcat.util.buf.CharChunk.substract(CharChunk.java:388)\n\tat org.apache.catalina.connector.InputBuffer.read(InputBuffer.java:368)\n\tat org.apache.catalina.connector.CoyoteReader.read(CoyoteReader.java:93)\n\tat org.apache.jsp.upload_jsp._jspService(upload_jsp.java:83)\n\tat org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:803)\n\tat org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:374)\n\tat org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:337)\n\tat org.apache.jasper.servlet.JspServlet.service(JspServlet.java:266)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:803)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n\tat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n\tat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175)\n\tat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)\n\tat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n\tat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n\tat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)\n\tat org.apache.coyote.http11.Http11NioProcessor.process(Http11NioProcessor.java:879)\n\tat org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process(Http11NioProtocol.java:719)\n\tat org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:2080)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)\n\tat java.lang.Thread.run(Thread.java:595)\n",
            "date": "20080226T21:00:48",
            "id": 0
        },
        {
            "author": null,
            "body": "This has been fixed in the trunk, proposals to fix it in 6.0.x and 5.5.x have been added.",
            "date": "20080227T12:20:50",
            "id": 1
        },
        {
            "author": null,
            "body": "URL to fix\nhttp://svn.apache.org/viewvc?view=rev&revision=631700",
            "date": "20080227T12:22:56",
            "id": 2
        },
        {
            "author": null,
            "body": "In some cases, the Fix r631700 seems O.K.\nBut there are still some troubles. \n\n1. The truncation still occurs,\n   in the case of non default maxHttpHeaderSize values like \"49152\"\n2. The data is broken after BufferedReader's mark(large readAheadLimit) method\n   and reset() method are called, though the size is O.K.\n3. Garbage remains in the buffer of ReadConvertor that extends InputStreamReader,\n   when the broken data was POSTed or the application does not\n   read the data completely, especially with the multibyte characters. \n   The data of the next request will be broken.\n\nI applied r631700 to Tomcat6.0.16 and Tomcat5.5.26, and detected them. \n\nI attach patches.\n\n",
            "date": "20080319T05:03:27",
            "id": 3
        },
        {
            "author": null,
            "body": "Created attachment 21683\npatch for trunk repository\n\nthis patch may fix some buffering bugs.\npatch for trunk repos. http://svn.apache.org/repos/asf/tomcat/trunk",
            "date": "20080319T05:10:13",
            "id": 4
        },
        {
            "author": null,
            "body": "Created attachment 21684\npatch for tomcat6.x repository\n\nthis patch may fix some buffering bugs.\npatch for Tomcat6.x repos. http://svn.apache.org/repos/asf/tomcat/tc6.0.x/trunk",
            "date": "20080319T05:12:19",
            "id": 5
        },
        {
            "author": null,
            "body": "Created attachment 21685\npatch for tomcat5.5\n\nthis patch may fix some buffering bugs.\npatch for Tomcat5.5 connector repos. http://svn.apache.org/repos/asf/tomcat/connectors/trunk",
            "date": "20080319T05:13:54",
            "id": 6
        },
        {
            "author": null,
            "body": "Created attachment 21686\npatch for tomcat5.5\n\nthis patch may fix some buffering bugs.\npatch for Tomcat5.5 container repos. http://svn.apache.org/repos/asf/tomcat/container/tc5.5.x",
            "date": "20080319T05:15:26",
            "id": 7
        },
        {
            "author": null,
            "body": "Although they may be not perfect possibly,\nI tested these patches on some patterns that includes above-mentioned.",
            "date": "20080319T05:53:43",
            "id": 8
        },
        {
            "author": null,
            "body": "Not bad, but I have some problem believing that no longer using bb.getLength() as the value for limit (or not using IntermediateInputStream.available() inside convert, which is the same) doesn't cause any problems.",
            "date": "20080319T08:38:17",
            "id": 9
        },
        {
            "author": null,
            "body": "I tested the patch on 5.5 and it didn't work, I will look deeper into it\n\n",
            "date": "20080319T08:58:09",
            "id": 10
        },
        {
            "author": null,
            "body": "I also tested the trunk patch, and couldn't get it to work properly",
            "date": "20080319T09:07:05",
            "id": 11
        },
        {
            "author": null,
            "body": "Just use the 6.0 branch for testing, no need to test all branches, the code is identical in all of them. The key difference I see is that the limit used in the loop is not the same, and you should probably be testing that.",
            "date": "20080319T09:55:37",
            "id": 12
        },
        {
            "author": null,
            "body": "(In reply to comment #13)\n> Just use the 6.0 branch for testing, no need to test all branches, the code is\n> identical in all of them. The key difference I see is that the limit used in\n> the loop is not the same, and you should probably be testing that.\n> \n\ntrue, there is a question here, that could/would solve the problem\nwhy is Reader.markSupported() return true, isn't this misleading.\nif we are streaming a few megabytes of data, tomcat would have to cache all that data in order to support mark properly.\n\nmarkSupported()==false, would mean we don't need to support mark() and reset(), and IMHO corresponds more to the streaming implementation we have\n\n",
            "date": "20080319T12:25:03",
            "id": 13
        },
        {
            "author": null,
            "body": "Remy and Filip, thank you for immediate checks. \n\nYes, the logic is same in all of them.\n\n(In reply to comment #11, #12)\n> I tested the patch on 5.5 and it didn't work, I will look deeper into it\n> I also tested the trunk patch, and couldn't get it to work properly\n\nI'm sorry, I might have made a mistake in something.\nI will check them more.\n\n(In reply to comment #14)\n> if we are streaming a few megabytes of data, tomcat would have to cache all\n> that data in order to support mark properly.\n\nIf it says more in detail,\nTomcat would have to cache data only after the mark() method calling, and\nTomcat would have to cache characters up to only the size\nthat the application specified with the method.\n\nOf course, we can choose to support these methods or not to support.\nI think it's wonderful if Tomcat can support that.\nAnd I'm believing that the implementation of Tomcat has enough possibility.\n\n",
            "date": "20080320T18:26:37",
            "id": 14
        },
        {
            "author": null,
            "body": "(In reply to comment #10)\n\n> Not bad, but I have some problem believing that no longer using bb.getLength()\n> as the value for limit (or not using IntermediateInputStream.available() inside\n> convert, which is the same) doesn't cause any problems.\n\nIs your problem solved by followings?\n\nbc.getLength() is called // bc in IntermediateInputStream means bb.\nat IntermediateInputStream.available()\nat StreamDecoder$CharsetSD.inReady()\nat StreamDecoder$CharsetSD.implRead(char[], int, int)\nat StreamDecoder$CharsetSD(StreamDecoder).read(char[], int, int)\nat ReadConvertor(InputStreamReader).read(char[], int, int)\nat ReadConvertor.read(char[], int, int)\nat B2CConverter.convert(CharChunk, int)\nat B2CConverter.convert(ByteChunk, CharChunk, int)\nat B2CConverter.convert(ByteChunk, CharChunk, int)\nat InputBuffer.realReadChars(char[], int, int)\nat CharChunk.substract(char[], int, int)\nat InputBuffer.read(char[], int, int)\nat CoyoteReader.read\n\nI saw that with debugger.\n\nAnd, IntermediateInputStream.read ( means bb.subStruct() )\nwill return -1 if the requested byte stream is in the end.\n\nAs one of the possibilities that the problem occurs,\nthe ReadConvertor object that extends InputStreamReader\nis recycled and reused after the underlying input stream returns -1,\nthough it works with no trouble in my environment.\n\nIf it is not guaranteed, we might have to create a new ReadConvertor object\nat each request without recycling used object.\n",
            "date": "20080320T19:57:35",
            "id": 15
        },
        {
            "author": null,
            "body": "(In reply to comment #15)\n> Remy and Filip, thank you for immediate checks. \n> \n> Yes, the logic is same in all of them.\n> \n> (In reply to comment #11, #12)\n> > I tested the patch on 5.5 and it didn't work, I will look deeper into it\n> > I also tested the trunk patch, and couldn't get it to work properly\n> \n> I'm sorry, I might have made a mistake in something.\n> I will check them more.\n> \n> (In reply to comment #14)\n> > if we are streaming a few megabytes of data, tomcat would have to cache all\n> > that data in order to support mark properly.\n> \n> If it says more in detail,\n> Tomcat would have to cache data only after the mark() method calling, and\n> Tomcat would have to cache characters up to only the size\n> that the application specified with the method.\nyes, only after mark has been called, but if you set a readAheadLimit of 10MB, one has to cache that.\n> \n> Of course, we can choose to support these methods or not to support.\n> I think it's wonderful if Tomcat can support that.\n\nyes, it's not a question about supporting or not supporting, it's where in the code you support that. with the bytechunk/charchunk the byte arrays referenced left and right, it makes it so much harder. I'll create a little POC to show how simpl it it is\n> And I'm believing that the implementation of Tomcat has enough possibility.\n> \n\ngoes against the KISS principle :)",
            "date": "20080321T08:48:11",
            "id": 16
        },
        {
            "author": null,
            "body": "I posted to the dev lists about this, basically, one can just take advantage of java.io.BufferedReader to do the caching\n\n    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {\n        HttpServletRequest req = (HttpServletRequest)request;\n        HttpServletRequestWrapper wrapper = new HttpServletRequestWrapper(req) {\n            BufferedReader reader = null;\n            public BufferedReader getReader() throws IOException {\n                if (reader==null) {\n                    BufferedReader bf = super.getReader();\n                    reader = new BufferedReader(bf);\n                }\n                return reader;\n            }\n        };\n        chain.doFilter(req, response);\n    }\n\n",
            "date": "20080321T10:24:06",
            "id": 17
        },
        {
            "author": null,
            "body": "Did you really need to get three emails posted on the mailing list to express this idea ?\n\nPersonally, I would be interested to know exactly which cases are not working at the moment after applying:\n  Index: java/org/apache/catalina/connector/InputBuffer.java\n===================================================================\n--- java/org/apache/catalina/connector/InputBuffer.java\t(revision 633279)\n+++ java/org/apache/catalina/connector/InputBuffer.java\t(working copy)\n@@ -355,7 +355,7 @@\n         }\n \n         state = CHAR_STATE;\n-        conv.convert(bb, cb, len);\n+        conv.convert(bb, cb, bb.getLength());\n         bb.setOffset(bb.getEnd());\n \n         return cb.getLength();\n\nThis includes no mark usage and with marks (which can probably be tested using readLine).Could I get some test cases ?",
            "date": "20080321T11:59:45",
            "id": 18
        },
        {
            "author": null,
            "body": "Created attachment 21700\ntest case for Reader buffering\n\nwar for the test",
            "date": "20080322T09:04:52",
            "id": 19
        },
        {
            "author": null,
            "body": "(In reply to comment #16)\n\nI tested some cases with http://svn.apache.org/repos/asf/tomcat/tc6.0.x/trunk\nat r639909 (that includes 639891) of course without my patch.\n\nI attached a WAR,\nthat is not a practicable application, but for test.\n\nIt includes multibyte UTF-8 characters,\nI'm worry about it might not work correctly in your environment.\n\n/index.jsp : using 3 bytes characters\n\n\nThe results I tested:\n read(): expected 24587\n    Sometimes, more/less length occurs.\n read(char[1]): expected 8203\n    Sometimes, more/less length occurs.\n readLine(): expected 8203\n    Sometimes, more/less length occurs,\n    or sometimes following exception occurs.\n\n java.io.IOException\n  at org.apache.catalina.connector.InputBuffer.reset(InputBuffer.java:456)\n  at org.apache.catalina.connector.CoyoteReader.reset(CoyoteReader.java:134)\n  at org.apache.catalina.connector.CoyoteReader.readLine(CoyoteReader.java:191)\n  at org.apache.jsp.readLine_jsp._jspService(readLine_jsp.java:60)\n ...\n\nThat's very rare when the browser is on the same machine with tomcat.\nBut if the browser is on another machine that occurs more easily. \n\nI just guess those behavior may depend on the internal buffer state\nand the buffer of SocketInputStream might be related.\n(the next data arrived or not?)\n\n\nThese tests are passed with my patch in my environment.\nDoes the attached patch (at least for tomcat6.x) not work?\n\n\n(In reply to comment #18)\nI have not tried yet. \nI think it is needed at least that the read(char[], int, int) method of\nunderlying-wrapped BufferedReader work correctly.\nWhich revision is suitable for the Filter?\n\nAll of Tomcat6.0.16, Tomcat6.0.16+r631700(see Comment #4 1.),\nand r639891(above-mentioned) seems working incorrectly.\nAre they problems of my environment?\n",
            "date": "20080322T09:54:44",
            "id": 20
        },
        {
            "author": null,
            "body": "Created attachment 21705\nNew test application that can check the contents of requested body\n\nThe attached WAR on Comment #20 works only with current IE.\nThis new one works with IE7, FireFox, and Netscape7.1\nand this one can check whether the contents of the requested body are appropriate or not, too.",
            "date": "20080324T02:51:07",
            "id": 21
        },
        {
            "author": null,
            "body": "Ok, first of all thanks a lot for the test case (I wouldn't be able to write a good multibyte test to save my life).\n\nAs with the original bug, the problem is with the limit being incorrect (reading too much causes causes the ByteBuffer to refill itself magically, while the calling code still hasn't seen anything back). In the loop, the limit int represents bytes, but is decremented with the amount of chars read (which works with single byte charsets, obviously ...).\n\nThe new loop would be (replacing the old convert method):\n\n    public void convert(ByteChunk bb, CharChunk cb, int limit)\n    throws IOException {\n        iis.setByteChunk(bb);\n        try {\n            // read from the reader\n            int l = 0;\n            while( limit > 0 ) { // conv.ready() ) {\n                int size = limit < BUFFER_SIZE ? limit : BUFFER_SIZE;\n                l = bb.getLength();\n                int cnt=conv.read( result, 0, size );\n                if( cnt <= 0 ) {\n                    // End of stream ! - we may be in a bad state\n                    if( debug>0)\n                        log( \"EOF\" );\n                    //\t\t    reset();\n                    return;\n                }\n                if( debug > 1 )\n                    log(\"Converted: \" + new String( result, 0, cnt ));\n\n                cb.setLimit(cb.getStart() + cnt);\n                cb.append( result, 0, cnt );\n                limit = limit - (l - bb.getLength());\n            }\n        } catch( IOException ex) {\n            if( debug>0)\n                log( \"Reseting the converter \" + ex.toString() );\n            reset();\n            throw ex;\n        }\n    }\n",
            "date": "20080325T18:27:12",
            "id": 22
        },
        {
            "author": null,
            "body": "when I ran the test:\n\nit fails consistently with tomcat/6.0.x/trunk\n\nafter updating tomcat/trunk to the latest(same fix as in 6.0.x)it also fails consistently.\n\nIf I revert to revision 640451 in tomcat/trunk, the test seems to work fine\n\n\nHow do I make this error happen and how do I spot the error?\nDo I need to configure any buffer sizes?\n\nFilip",
            "date": "20080325T19:04:21",
            "id": 23
        },
        {
            "author": null,
            "body": "The first step is to reproduce the problem (which is easy for me with TC 6.0 trunk and the latest version of the test case). After fixing the update of the limit (because substracting cnt is obviously wrong if limit is inited to bb.length()), then my traces in no longer misbehave and I haven't been able to reproduce any problem. This makes perfect sense to me since with 1 byte charsets (such as regular ascii) cnt is equal to the amount of bytes consumed, and character input was working fine for me after applying my one line fix.",
            "date": "20080325T19:34:24",
            "id": 24
        },
        {
            "author": null,
            "body": "(In reply to comment #23)\n\n> cb.setLimit(cb.getStart() + cnt);\n> cb.append( result, 0, cnt );\n\ncb.append() doesn't modify cb.start\nand cb.start is set zero by InputBuffer#realReadChars if markPos == -1.\nSo \"cb.getStart() + cnt\" is same to or less than bb.length(),\nand it will be bb.length()/3 if all characters are 3 bytes.\n(the surplus bytes may exist in the buffer of ReadConvertor)\n\nThis will break the buffer of CharChunk.\n\nIf \"cb.getEnd() + cnt\" is used as alternative,\nit will overwrite the limit that is set by CoyoteReader#mark method.\nI guess that may cause another problem.\n(though mark/reset methods have not worked correctly before that)\n\nYuichiro\n",
            "date": "20080325T21:11:57",
            "id": 25
        },
        {
            "author": null,
            "body": "Created attachment 21717\nWAR includes other test cases\n\nAll of the test cases described in comment #4 is covered, if no bugs are in this WAR. \nFor the case 1, define maxHttpHeaderSize=\"xxxxx\" as the attribute of Connector tag in server.xml",
            "date": "20080326T03:38:11",
            "id": 26
        },
        {
            "author": null,
            "body": "(In reply to comment #26)\n> > cb.setLimit(cb.getStart() + cnt);\n> > cb.append( result, 0, cnt );\n> \n> cb.append() doesn't modify cb.start\n> and cb.start is set zero by InputBuffer#realReadChars if markPos == -1.\n> So \"cb.getStart() + cnt\" is same to or less than bb.length(),\n> and it will be bb.length()/3 if all characters are 3 bytes.\n> (the surplus bytes may exist in the buffer of ReadConvertor)\n\nThat's not the fix, that's something bad that I left in (the purpose was to make sure that cnt could be appended, so it should be getEnd + cnt, if getEnd + cnt > getLimit). I do not see why changing the limit to something greater could cause problems with marking.\n\nAs far as I am concerned, this bug is fixed. If you think it's not, feel free to spend time on it and propose alternate fixes.",
            "date": "20080326T05:44:40",
            "id": 27
        },
        {
            "author": null,
            "body": "I have tested Remy's patch (less the one bad line) and it does fix the various multi-byte read test cases.\n\nThe patch has been applied to trunk and proposed for 6.0.x and 5.5.x.\n\nmark/reset is still broken.",
            "date": "20080330T14:19:03",
            "id": 28
        },
        {
            "author": null,
            "body": "The multi-byte read patch has been applied to 6.0.x and will be in the next release.",
            "date": "20080405T10:49:43",
            "id": 29
        },
        {
            "author": null,
            "body": "I found a problem with the buffer resizing in conjunction with mark/reset.\n\nIndex: java/org/apache/tomcat/util/buf/CharChunk.java\n===================================================================\n--- java/org/apache/tomcat/util/buf/CharChunk.java\t(revision 645466)\n+++ java/org/apache/tomcat/util/buf/CharChunk.java\t(working copy)\n@@ -480,7 +480,7 @@\n \t    tmp=new char[newSize];\n \t}\n \t\n-\tSystem.arraycopy(buff, start, tmp, start, end-start);\n+\tSystem.arraycopy(buff, 0, tmp, 0, end);\n \tbuff = tmp;\n \ttmp = null;\n     }\n\nAlternately, a buffered reader could be used (since the decoding code is now fixed, there should be no problem).\n",
            "date": "20080407T08:10:29",
            "id": 30
        },
        {
            "author": null,
            "body": "The mark/reset test case still fails. I'm looking into it but it is taking me time to figure out exactly where the error is.",
            "date": "20080407T14:27:43",
            "id": 31
        },
        {
            "author": null,
            "body": "I have a fix for this now. Thsi fix and Remy's fix above have been proposed for 6.0.x and 5.5.x",
            "date": "20080407T15:50:41",
            "id": 32
        },
        {
            "author": null,
            "body": "I tested recent trunk and also 6.x with the latest STATUS file patches for this issue applied.\n\nWhen we change maxHttpHeaderSize from the default to something bigger, we still have a problem with POST bodies bigger than 8K. Using smaller maxHttpHedaerSize than the default 8KB seems to be no problem, bigger sizes like 16KB or 48KB show the problem.\n\nWe simply use a text file of some size between 16KB and 64KB, send it via POST using e.g. \"curl --data-binary @FILE\" or \"ab -p FILE\" and try to read via a simple JSP:\n\n<html>\nSize: <%= request.getContentLength() %><br>\n<%\nout.println(\"Start reading<br>\");\ntry {\n    String line = null;\n    int n = 0;\n    int m = 0;\n    java.io.BufferedReader br = request.getReader();\n    while ((line = br.readLine()) != null) {\n        n=n+1;\n        m=m+line.length()+1;\n        out.println(line);\n    }\n    out.println(\"Nothing more available, lines=\" + n + \", bytes=\" + m);\n} catch(Exception ex) {\n    out.println(ex);ex.printStackTrace();\n}\nout.println(\"<br>Done\");\n%>\n</html>\n\nIt's primitive and the byte count shown below is wrong if the file has DOS line endings, but the failure is shown clearly, because when truncation appears, you'll be shown less than 8KB of data.",
            "date": "20080410T01:59:00",
            "id": 33
        },
        {
            "author": null,
            "body": "You've got to be able to reproduce this using the test webapp provided in this report (the line length is configurable, and there's a readLine test). I don't like a test using ab.",
            "date": "20080410T08:05:13",
            "id": 34
        },
        {
            "author": null,
            "body": "I'll try that.\n\nFor the sake of completeness, here's the exception stack:\n\njava.io.IOException\n        at org.apache.catalina.connector.InputBuffer.reset(InputBuffer.java:475)\n        at org.apache.catalina.connector.CoyoteReader.reset(CoyoteReader.java:134)\n        at org.apache.catalina.connector.CoyoteReader.readLine(CoyoteReader.java:191)\n        at org.apache.jsp.maxpost_jsp._jspService(maxpost_jsp.java:65)\n        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:717)\n        at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:369)\n        at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:337)\n        at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:266)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:717)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n        at org.apache.catalina.valves.AccessLogValve.invoke(AccessLogValve.java:568)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447)\n        at java.lang.Thread.run(Thread.java:595)\n\nThe same test (curl/ab) works for 6.0.14 and for default maxHttpHeaderSize. Will report about the war soon.",
            "date": "20080410T08:15:03",
            "id": 35
        },
        {
            "author": null,
            "body": "Using the war and maxHttpHeaderSize=\"16384\" I did the following test case:\n\n   Characters size: 17000_______________\n   Use MultiByte Character: ( ) yes (*) no\n     _____________________________________________________________________________________________________________\n\n   [test case:]\n   (*) only read [readLine()/read()/read(char[1])]\n   ( ) garbage in buffer\n     * real read size: 4096________________\n\n   ( ) mark/reset\n     * read size before mark(): 0___________________\n     * readAheadLimit size: 8192________________\n     * read size after mark()/before reset(): 8192________________\n     _____________________________________________________________________________________________________________\n\n   submit\n     _____________________________________________________________________________________________________________\n\n\nAnd on the next page I choose the first send (#readLine test):\n\n   request#getReader()#readLine test\n   abcdefghijklmnopqrst send\n   request#getReader()#read() test\n   abcdefghijklmnopqrst send\n   request#getReader()#read(char[1]) test\n   abcdefghijklmnopqrst send\n\nThe result is:\n\nrequest#getReader test.\n\n   readLine.jsp is called.\n     _____________________________________________________________________________________________________________\n\n   Content-Type:multipart/form-data; boundary=xnyLAaB03X\n   Character Encoding:UTF-8\n   Content-Length:17115\n   expected:17076\n   real read:4827\n   isSame:false\n\n",
            "date": "20080410T08:24:21",
            "id": 36
        },
        {
            "author": null,
            "body": "Since most people are deeper in the code than I, here's what i get if I instrument o.a.c.connector.InputBuffer with a couple of log statements and post the file with increased maxHttpHeaderSize. So markPos get set to -1 in realWriteChars:\n\n10.04.2008 20:25:17 org.apache.catalina.connector.InputBuffer mark\nINFO: Set markPos in mark from -1 to 0\n10.04.2008 20:25:17 org.apache.catalina.connector.InputBuffer realReadChars\nINFO: Found markPos in realReadChars 0\n10.04.2008 20:25:17 org.apache.catalina.connector.InputBuffer realWriteChars\nINFO: Reset markPos in realWriteChars from 0 to -1\n10.04.2008 20:25:17 org.apache.catalina.connector.InputBuffer reset\nINFO: Found =-1 in reset for CHAR_STATE\n10.04.2008 20:25:17 org.apache.catalina.connector.InputBuffer reset\nINFO: Reset markPos in reset/CHAR_STATE to -1\njava.io.IOException: markPos=-1\n",
            "date": "20080410T11:25:43",
            "id": 37
        },
        {
            "author": null,
            "body": "And the stack in realWriteChars is:\n\n        at org.apache.catalina.connector.InputBuffer.realWriteChars(InputBuffer.\njava:335)\n        at org.apache.tomcat.util.buf.CharChunk.flushBuffer(CharChunk.java:440)\n        at org.apache.tomcat.util.buf.CharChunk.append(CharChunk.java:295)\n        at org.apache.tomcat.util.buf.B2CConverter.convert(B2CConverter.java:100\n)\n        at org.apache.catalina.connector.InputBuffer.realReadChars(InputBuffer.j\nava:371)\n        at org.apache.tomcat.util.buf.CharChunk.substract(CharChunk.java:416)\n        at org.apache.catalina.connector.InputBuffer.read(InputBuffer.java:405)\n        at org.apache.catalina.connector.CoyoteReader.read(CoyoteReader.java:105\n)\n        at org.apache.catalina.connector.CoyoteReader.readLine(CoyoteReader.java\n:154)\n        at org.apache.jsp.maxpost_jsp._jspService(maxpost_jsp.java:64)\n        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:717)\n        at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper\n.java:369)\n        at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:3\n37)\n        at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:266)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:717)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl\nicationFilterChain.java:290)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF\nilterChain.java:206)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV\nalve.java:233)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV\nalve.java:175)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j\nava:128)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j\nava:102)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal\nve.java:109)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav\na:286)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java\n:844)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.proce\nss(Http11Protocol.java:583)\n        at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:44\n7)\n        at java.lang.Thread.run(Thread.java:595)\n\n",
            "date": "20080410T11:41:32",
            "id": 38
        },
        {
            "author": null,
            "body": "I think I might have found the problem,\n\nI've been unable to reproduce the error using the NIO/APR (APR below) connector with setting\n    <Connector port=\"8080\" \n               protocol=\"org.apache.coyote.http11.Http11NioProtocol\" \n               connectionTimeout=\"20000\" \n               redirectPort=\"8443\" \n               maxHttpHeaderSize=\"16384\" \n               maxKeepAliveRequests=\"100\"/>\n\n\nI reproduce the error everytime using the regular connector\n    <Connector port=\"8081\" \n               protocol=\"org.apache.coyote.http11.Http11Protocol\" \n               connectionTimeout=\"20000\" \n               redirectPort=\"8443\" \n               maxHttpHeaderSize=\"16384\" \n               maxKeepAliveRequests=\"100\"/>\n\nand I am unable to reproduce it using the APR connector\n    <Connector port=\"8082\" \n               protocol=\"org.apache.coyote.http11.Http11AprProtocol\" \n               connectionTimeout=\"20000\" \n               redirectPort=\"8443\" \n               maxHttpHeaderSize=\"16384\" \n               maxKeepAliveRequests=\"100\"/>\n\nso I started debugging it, \nThe problem is in org.apache.tomcat.util.CharChunk\nin two subsequent reads, where the inputbuffer is 16384, it tries to use the CharChunk to store all the data. The reason it works with the APR/NIO connector is cause those connectors never pull out more than 8192 bytes from the socket buffer. \n\nhowever, the regular BIO connector will read as much as it can, and then the B2CConverter tries to append the character array to the CharChunk,\nbut the CharChunk refuses to grow beyond the limit.\n\nThe fix is simple\nIndex: java/org/apache/tomcat/util/buf/CharChunk.java\n===================================================================\n--- java/org/apache/tomcat/util/buf/CharChunk.java      (revision 646950)\n+++ java/org/apache/tomcat/util/buf/CharChunk.java      (working copy)\n@@ -454,7 +454,8 @@\n        // Can't grow above the limit\n        if( limit > 0 &&\n            desiredSize > limit) {\n-           desiredSize=limit;\n+           limit = desiredSize;\n+           //desiredSize=limit;\n        }\n\n        if( buff==null ) {\n\nhowever, we could probably shrink the CharChunk back on a recycle, but it wont grow beyond maxHttpHeaderSize, so I am not sure we need to\n\n",
            "date": "20080410T14:56:29",
            "id": 39
        },
        {
            "author": null,
            "body": "Hi Filip,\n\nI didn't yet read your comment, because we both worked in parallel (mid-air collission). I'm posting my finding nevertheless, so we can look for the best solution.\n\nThis gets funny: while looking at the code I realized, that the 5.5.x/6.0.x patch that introduced the problem (r569969 resp. r572498) actually removed 3 lines of code, that I added in r371804 (resp. Bill added my patch).\n\nhttps://issues.apache.org/bugzilla/show_bug.cgi?id=38346\n\nIf I add those lines back, then I can not reproduce the problem with or without maxHttpHeaderSize. I tried my POST test and also tried some fo the numerous possibilities with the test war. So I think the following patch for trunk is worth some tests:\n\nhttp://people.apache.org/~rjung/patches/maxhttpheadersize.patch",
            "date": "20080410T15:15:06",
            "id": 40
        },
        {
            "author": null,
            "body": "If we don't want the simple fix of growing the CharChunk, (since CharChunk will always be <=maxHttpHeaderSize, a character is at least one byte)\n\nthen the code would have to back out during the conversion, but I think it's not worth the time, this code is fragile, and has proven to break easily during larger fixes and refactoring of APIs.\n\n",
            "date": "20080410T15:20:00",
            "id": 41
        },
        {
            "author": null,
            "body": "hi Rainer, both patches do the exact same thing, one explicit one implicit, I'd vote +1 for either or\n\nFilip",
            "date": "20080410T15:21:07",
            "id": 42
        },
        {
            "author": null,
            "body": "Next mid-air, BZ is obviously not IM :)\n\nWhen I use the test war, the garbage test still seems to fail, even with\ndefault maxHttpHeaderSize. The behaviour is the same with my patch, with\nFilip's, with both of them applied and with unchanged trunk.\n\nIt might be, that I don't understand how to use this test case. I choose the\ngarbage radio button with default values, submit and the choose send. The\nresulting page says \"correct:true\", but then asks me to Retry the request. If I\ndo that, I get a correct:false.",
            "date": "20080410T15:29:58",
            "id": 43
        },
        {
            "author": null,
            "body": "Hi,\n\nThe behavior of the test case with default values is as follows. \n\n1. It posts 8192 multibytes characters.\n2. The posted data is read up to 4096 characters.\n   (And other information like the multipart-boundary characters\n    and the Content-Disposition is read, too)\n3. It checks whether the read data is correct or not.\n4. They are repeated by retrying.\n\nIf \"correct:false\" is output by retrying,\nperhaps there were garbage bytes of the multibyte character\nin the buffer of the ReadConvertor object, after the previous request.\n\nWhen the end of the read byte sequence is not complete\nas the multibyte character, InputStreamReader seems to keep imperfect byte\nsequence internally until the under-lying stream returns -1. \n\nAnd the imperfect byte sequence will be used as the start bytes\nof the next request.\n\nPlease see the recycle method part of the attached patch.\n\nYuichiro\n",
            "date": "20080410T21:26:03",
            "id": 44
        },
        {
            "author": null,
            "body": "OK. That light up ahead might actually be the end of the tunnel rather than yet another train.\n\nI believe that all necessary patches to fix all issues identified by the test case have been:\n- applied to trunk\n- proposed for 6.0.x\n- proposed for 5.5.x\n\nPlease comment here if you think something has been missed.\n\nMany thanks to Suzuki Yuichiro whose test case has proved invaluable in fixing this issue.",
            "date": "20080411T14:06:49",
            "id": 45
        },
        {
            "author": null,
            "body": "I tried current 6.0.x. It looks working fine at all test cases.\nThank you very much for the fix.\n\nBut there is a few comments.\n1. readAheadLimit value that specified by the mark method has no meanings.\n   After the mark method is called once, tomcat seems to buffer all of the body.\n   The JavaDoc of BufferedReader allows tomcat to use arbitrary size\n   (over readAheadLimit) buffer, but that may cause OutOfMemory\n   however the application developer specified a moderate size for readAheadLimit.\n\n2. At the recycle phase, ReadConverter convert all byte sequence\n   that was not read by application,  but the converted character sequence\n   is not used.\n   In recycling, it's more efficient that the under-lying input stream\n   returns -1 immediately. (Though it might not be the most efficient solution.)\n\nThey may be not bugs, but improvement level.\n\nBest Regards.\n",
            "date": "20080418T21:46:52",
            "id": 46
        },
        {
            "author": null,
            "body": "Created attachment 21832\npatch that make readAheadLimit value effective\n\npatch for tomcat6.0.x\n\nThis will make the readAheadLimit value effective.",
            "date": "20080418T21:51:32",
            "id": 47
        },
        {
            "author": null,
            "body": "Oops, I forgot to reset InputBuffer.readAheadLimit in the recycle method\nof the patch, though the result may be O.K. with or without it.\nIf you use the patch, please add it for the better code.\n",
            "date": "20080420T17:43:44",
            "id": 48
        },
        {
            "author": null,
            "body": "Created attachment 21872\nPatch for TC 5.5 Part 1 - connectors (CharChunk, B2CConverter)\n\nBackport of patches applied to TC 6.0 for fixing BZ 44494.",
            "date": "20080428T11:47:29",
            "id": 49
        },
        {
            "author": null,
            "body": "Created attachment 21873\n Patch for TC 5.5 Part 2 - container (InputBuffer)   \n\nBackport of patches applied to TC 6.0 for fixing BZ 44494.",
            "date": "20080428T11:48:39",
            "id": 50
        },
        {
            "author": null,
            "body": "I ported the patches for 44494 applied until now to TC 6.0 back to TC 5.5.\n\nI reviewed all differences for the classes in o.a.tomcat.util.buf and o.a.c.connector for any side effects, since the classes drifted further apart between TC 5.5 and TC 6.0 than necessary. Some cleanups have been done in the older branch 5.5 and not in the newer ones.\n\nThe patch looks good to me (w.r.t. the test war attached here).\n\nAs soon as svn will be back to read/write, I'll add the patches to the TC 5.5 STATUS file.\n\nFurther tests and comments welcome. The patched classes are compatible with 5.5 trunk and 5.5.26.",
            "date": "20080428T11:53:54",
            "id": 51
        },
        {
            "author": null,
            "body": "Since this bug effectively means tomcat 6.0.16 is unable to reliably handle requests, perhaps it should be publicised more widely, and the 6.0.16 release pulled (or replaced with 6.0.16.1)?",
            "date": "20080513T04:31:05",
            "id": 52
        },
        {
            "author": null,
            "body": "I hate to make a post like this but as it has not been decided, is there a plan for releasing this as it has caught a number of teams off guard? I agree with the poster from 53 that this is a major issue as requests can't be trusted. Please update the homepage with this issue and pull 6.0.16 and 5.5.26 from public release at the very least.\n\nThanks for the fix and I look forward to the changed release.\n\n(In reply to comment #53)\n> Since this bug effectively means tomcat 6.0.16 is unable to reliably handle\n> requests, perhaps it should be publicised more widely, and the 6.0.16 release\n> pulled (or replaced with 6.0.16.1)?\n> \n\n",
            "date": "20080701T11:49:57",
            "id": 53
        },
        {
            "author": null,
            "body": "*** Bug 45350 has been marked as a duplicate of this bug. ***",
            "date": "20080709T14:32:20",
            "id": 54
        },
        {
            "author": null,
            "body": "*** Bug 45349 has been marked as a duplicate of this bug. ***",
            "date": "20080709T14:32:46",
            "id": 55
        },
        {
            "author": null,
            "body": "This has now been fixed in 5.5.x and will be included in 5.5.27 onwards.",
            "date": "20080814T01:31:33",
            "id": 56
        },
        {
            "author": null,
            "body": "Just for the record, the 5.5.x fixes to the connector module were sufficient to fix this issue for 4.1.x as well.",
            "date": "20080815T00:23:15",
            "id": 57
        },
        {
            "author": null,
            "body": "*** Bug 45215 has been marked as a duplicate of this bug. ***",
            "date": "20080924T13:25:29",
            "id": 58
        }
    ],
    "component": "Connector:Coyote",
    "description": "After updating to Tomcat 5.5.26 from Tomcat 5.5.23 we appear to be having trouble with requests, that are greater than 8k, being truncated with the CoyoteRequest.getReader() method.\n\nThe following steps caused this issue to appear:\n1. I constructed a post request from a jsp page which exceeded 8k in size.\n2. In the corresponding servlet I call request.getReader() and pipe the returned BufferedReader into a string.\n3. Inspection of the string reveals that the content is  being truncated after 8k.\n4. If I replace the request.getReader() method call with request.getInputStream() no truncation occurs. Also, after reverting back to Tomcat 5.5.23 and using the request.getReader() method no truncation occurs.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "44494",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "P2 normal",
    "product": "Tomcat 5",
    "project": "TOMCAT",
    "summary": "Requests greater than 8k being truncated.",
    "systemSpecification": true,
    "version": "Nightly Build"
}