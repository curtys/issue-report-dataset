{
    "comments": [
        {
            "author": null,
            "body": "Async data send means that there is not time guarantee for when the session is \ndelivered. The session should not get lost without any error trace in the logs.\nI am still debating whether to remove this feature all together, but I left it \nin for people to play with. I have not found a case where async is useful, but \nI am sure there is which is why it is still there. Most of the time people \nwant to be ensured that the session gets replicated, that is when pooled mode \ncomes in.\n\nalso, from experience, using mod_jk in high load can result in \"lost\" \nsessions, cause it sometimes messes up the request and looses the session id.\nfrom my experience, pen (siag.nu/pen) works better as a load balancer\n\nI strongly suggest to retry the same test with replicationMode=\"pooled\" and \nsee if you get better results. \nPooled means that replication is synchronous, but on concurrent channels.\n\nFilip",
            "date": "20040402T15:57:54",
            "id": 0
        },
        {
            "author": null,
            "body": "I respect your sugestion to not use asynchronous, although it looked to me like\nthe right way to do it.\n\nJust for your information: The messages really get lost, even after we stop load\nthe missing messages don't get replicated. So it's not just a problem of\nmessages getting replicated too late.\n\nThere are definitely only debug log stetments all the time, except for a few\ninfo messages giving mean values for replication data size. No other non debug\nlog statements on any cluster node. Also from what I see I'm pretty sure, that\nthe replication data is written to the Socket.\n\nConcerning mod_jk: For this test case we used each session only once. So the\ncorrectness of the response through mod_jk somehow didn't matter. We could\neasily reproduce the same situation using build in Tomcat HTTP Connector\n(although we didn't do so until now).\n\nWe will retry using pooled, although I don't like the idea of having up to 25\nconnections (code constant) and threads for each pair of nodes in the cluster.\nAlso I had the impression, that in pooled mode TCP conections are only used a\nvery short time (I think I remember for only 100 messages? This application will\nbe under heavy load in production). \n\nWhy do I think asynchronous fits better?\n\nIn any synchronous situation if the replication is not fast enough I immediately\nget negative consequences for the application from the user point of view,\nbecause the request blocks ressources needed for accepting new requests as long\nas the replication hasn't finished. So if replication is slow for a few seconds\nI'm in danger of loosing all free Apache-Slots resp. Tomcat worker threads for\nincoming requests.\n\nWhen I do asynchronous replication I only loose timely replication of the sesion\nchanges. If I route my request to the primary container, then I still profit\nfrom the cluster with respect to availability and servicability (I can shutdown\none of the containers without users loosing sessions). For these features it\ndoesn't really matter, if all request are replicated within milliseconds all the\ntime.\n\nI'm sorry to bother you, but I think it's an important discussion.",
            "date": "20040402T16:45:39",
            "id": 1
        },
        {
            "author": null,
            "body": ">We will retry using pooled, although I don't like the idea of having up to 25\n\nthis is not really a big resource issue, since no threads are holding on to \nthese connections, they just grab one from the queue when it is available, \nthen return it.\n\nlets reopen this bug re:/ async, once I get all moved in and have my computers \nset up I can start testing this again\n\nFilip",
            "date": "20040402T17:01:28",
            "id": 2
        },
        {
            "author": null,
            "body": "also, the problem with Async, is that it is using only one channel, hence \nduring heavy load, you will not get milli seconds throughput, cause it queues \nall the messages\n\nthe solution would be to make an async pooled mode,",
            "date": "20040402T17:04:15",
            "id": 3
        },
        {
            "author": null,
            "body": "Maybe interesting: a simple \"Hello World\" JSP does NOT trigger the problem. But \nonce I add a scriptlet <%session.setAttribute(\"COUNT\",new Integer(10));%>\nthe problem arises.\n\nI can now reproduce without any apache or mod_jk involvement. I just start a 2 \nnode cluster and call the JSP via direct HTTP connections to node #1 200 times \nwith a delay between calls of 100ms.\n\nAfter calling 200 times, I can find 200 sessions on node #1, but only between \n170 and 195 sessions on node 2. I check session count via /manager/html, but I \nalso added debug output to see, that some sessions are indeed missing.\n\nI try to go deeper into cluster messages and the queue handling.",
            "date": "20040405T16:39:42",
            "id": 4
        },
        {
            "author": null,
            "body": "Next info: If I use this JSP, then synchronous and pooled are both EXTREMELY \nslow, response times between 1000ms and 5000ms. As soon as I reduce \ntcpSelectorTimeout from 1000 to 10, I get more reasonable response times (10-\n50ms). Any idea, why tcpSelectorTimeout show such a tremendous effect?\n\nThen when I use multiple parallel clients, synchronous again gets too slow, so \nonly pooled is an alternative. Synchronous once showed a freeze (getting no \nmore anserws) for 15 seconds.\n\nBoth, synchronous and pooled do not show the problem of missing sessions.\n\nNevertheless I like the idea of having one or few dedicated replication \nconnections fed by a queue of work load and not directly coupled to the \nfinishing of the original response (asynchronous) much more.\n",
            "date": "20040405T17:26:27",
            "id": 5
        },
        {
            "author": null,
            "body": "First: there are app. 6 System.out.print/ln in the cluster code. One of these \n(line 71 in the SmartQueue.java) prevented me from finding the solution earlier.\n\nHere is the SOLUTION: What happens is, that the \"smart\" feature of the smart \nqueue gets us into trouble. For my JSP two session messages are being send. One \nis of type 1 (EVT_SESSION_CREATED), and the second one is of type 13 \n(EVT_SESSION_DELTA). Both are being send very close to each other during the \nonly request in a session.\n\nMost of the time the system is fast enough to handle each message individually, \nbefore the next message is put into the queue. Every now and then the message \nof type 1 is not read from the queue before type 13 is generated. Then the \nqueue replaces the type 1 message in the queue by the type 13 message, and only \nthe type 13 message is send out. Then the receiving side seems to not create \nthe session, since the type 1 message is missing. I didn't check this last \npoint, because I think this is much clearer for you.\n\nIsn't there a general problem in using the Delta manager together with the \nsmart queue? Since you only send out delta messages, it doesn't look like a \ngood idea to replace pending messages with newer ones. In fact isn't it \nnecessary to send all deltas and to furthermore make sure, that they are send \nin the original order?\n\nAt least this makes clear, why the problem will only show up in asynchronous \nmode. In synchronous mode you will allways send all messages (and in the right \norder).\n\nMaybe it suffices to strip off the \"smart\" feature of the smart queue?",
            "date": "20040405T19:11:02",
            "id": 6
        },
        {
            "author": null,
            "body": "The Deltamanager now uses a different Id to assign to its messages, that will \nmake them unique. This all the messages will get through.\n\nHowever, the async could be pooled in the same way the the synchronous is in \npooled mode, would increase performance alot :)",
            "date": "20040407T20:41:42",
            "id": 7
        }
    ],
    "component": "Catalina:Cluster",
    "description": "Using AsyncSocketSender we see, that under heavy load some session get not \nreplicated.\n\nWe added a lot of debug output statements, so we can see, that in \nAsyncSocketSender Method sendMessage(data) is reached and we finally end at the \nline, where the data is written to the socket.\n\nUnfortunately on the remote side we miss the incoming messages corresponding to \nthe missing session, although most sessions are replicated and we can see \naccordingly debugging output.\n\nWe use a 2 node Cluster using apache/mod_jk with load balancing/routing to both \ncluster nodes. We create app. 1500 sessions in a few seconds. Nearly 5% of the \nsessions are missing. Some of the sessions of node 1 are missing on node 2, but \nalso some of the sessions from node 2 are missing on node 1. We couldn't \nobserve the problems under low load.\n\nCluster-Config: useDirtyFlag=true, mcastFrequency=500, mcastDropTime=3000, \ntcpSelectorTimeout=1000, tcpThreadCount=2, TC 5.0.21, OS is Solaris 9, JVM is \n1.4.2_03.\n\nAny help is highly appreciated, we are able to produce debugging output. Please \ngive hints.",
    "hasPatch": false,
    "hasScreenshot": false,
    "id": "28161",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "P3 major",
    "product": "Tomcat 5",
    "project": "TOMCAT",
    "summary": "Replication messages get lost with AsyncSocketSender",
    "systemSpecification": false,
    "version": "5.0.21"
}