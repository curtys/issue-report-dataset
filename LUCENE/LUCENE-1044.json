{
    "comments": [
        {
            "author": "Hoss Man",
            "body": "first off: there have been *numerous* changes to the way lucene writes to files (particularly relating to segment files, write locks, and fault tollerance) between 2.0 and 2.2 (not to mention differences between 1.4.3 and 2.0 that i may not be aware of) -- so you may see many differences in behavior if you upgrade.\n\nsecond: to quote myself from a recent thread regarding lucene and \"kill -9\" ...\n\nhttp://www.nabble.com/Help-with-Lucene-Indexer-crash-recovery-tf4572570.html#a13068939\n\n{quote}\n: That said, it should never in fact cause index corruption, as far as I\n: know.  Lucene is \"semi-transactional\": at any & all moments you should\n: be able to destroy the JVM and the index will be unharmed. I would\n: really like to get to the bottom of why this is not the case here.\n\nAt any point you can shutdown the JVM and the index will be unharmed, but\n\"destroying\" it with \"kill -9\" goes a little farther then that.  \n\nLucene can't make that claim because the JVM can't even garuntee that\nbytes are written to physical disk when we close() an OutputStream -- all\nit garuntees is that the bytes have been handed to the OS.  When you \"kill\n-9\" a process the OS is free to make *EVERYTHING* about that process\nvanish without cleaning up after it ... i'm pretty sure even pending IO\noperations are fair game for disappearing.\n{quote}\n\n...what's true for \"kill -9\" is true for hanking the power cord ... if the JVM isn't shut down cleanly, there is nothing Lucene or the JVM can do to guarantee that your index is in a consistent state.",
            "date": "2007-11-03T05:22:50.682+0000",
            "id": 0
        },
        {
            "author": "robert engels",
            "body": "The last comment is not correct, in that, there are many Java based applications (and non-java) that offer true transactional integrity.\n\nIt usually involves a log file, and using sync to ensure data is written to disk.\n\nThe Lucene structure allows for this VERY easily, as the 'segments' file controls everything.\n\nIf all previous files are \"synced\", and then the 'segments.new' file written, and synced (with a marker/checksum). Then the old 'segments' deleted, and 'segments.new' 'renamed to 'segments'. It is trivial to ensure transactional integrity.\n\nUpon index open, check for segments.new - if doesn't exist, or does not have a valid checksum, delete all segments not in 'segments', if it is valid, then reattempt the rename. Then open the index.\n",
            "date": "2007-11-03T07:13:30.130+0000",
            "id": 1
        },
        {
            "author": "venkat rangan",
            "body": "Robert,\nAre your comments applicable for version 1.4.3? The behavior of an all-zero 'segments' and 'deleted' files is very easily reproduced. Also, there is no left over 'segments.new' after a power-cord yank.\nThanks.\n",
            "date": "2007-11-03T21:52:51.649+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "See the healthy follow-on discussion here:\n\n    http://www.gossamer-threads.com/lists/lucene/java-dev/54300\n\nI plan to add optional argument when calling FSDirectory.getDirectory() to ask all created FSIndexOutputs to always call sync() on the file descriptor before closing it.",
            "date": "2007-11-04T10:34:31.301+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "This recent thread is also relevant here:\n\n    http://www.gossamer-threads.com/lists/lucene/java-dev/39898",
            "date": "2007-11-04T14:34:20.558+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "Attached patch that adds optional \"doSync\" boolean to\nFSDirectory.getDirectory(...).  It defaults to \"false\".  When true, I\ncall file.getFD().sync() just before file.close() in\nFSIndexOutput.close().\n\nHowever, I can't figure out how to also sync the directory.  Does\nanyone know how to do this in Java?\n\nAll tests pass if I default it to true or to false.\n",
            "date": "2007-11-04T15:28:33.307+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "\nAttached another rev of the patch, that adds \"fsdirectory.dosync\"\nboolean config option to contrib/benchmark.\n\nI ran a quick perf test of sync vs no sync.  I indexed all of\nWikipedia using this alg:\n\n  analyzer=org.apache.lucene.analysis.SimpleAnalyzer\n  \n  # Feed that knows how to process the line file format:\n  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n  \n  docs.file=/lucene/wikifull.txt\n  \n  doc.maker.forever=false\n  ram.flush.mb = 8\n  max.buffered = 0\n  directory = FSDirectory\n  max.field.length = 2147483647\n  doc.term.vector=false\n  doc.stored=false\n  fsdirectory.dosync = true\n  \n  ResetSystemErase\n  CreateIndex\n  {AddDoc >: *\n  CloseIndex\n  \n  RepSumByName\n\nThis is on a quad core Mac OS X (Mac Pro) with a 4-drive RAID 0 IO\nsystem.  The baseline (non-sync) test took 19:54 and the sync test\ntook 20:21, which I think is a fairly minor slowdown.\n\nI also tried opening the file descriptor with \"rws\", which I think is\noverkill for us (we don't need every IO operation to be sync'd) and it\ntook 31:11, which is a major slowdown :)\n\nMaybe we should actually make doSync=true the default?  It seems like\na small price to pay for the added safety.  The option would still be\nthere to turn off if people wanted to made the opposite tradeoff.\n",
            "date": "2007-11-04T18:29:22.449+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "Attached another rev of the patch.\n\nI changed the default to \"true\": I think the small performance hit is\nworth the added safety.\n\nAlso put a try/finally around the the call to sync to make sure we\nclose even if we hit exception during sync(), and improved the\njavadocs.  I plan to commit in a day or two.\n",
            "date": "2007-11-06T22:38:47.597+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "I just committed this.  Thanks Venkat!",
            "date": "2007-11-10T17:51:55.439+0000",
            "id": 8
        },
        {
            "author": "Yonik Seeley",
            "body": "{quote}This is on a quad core Mac OS X (Mac Pro) with a 4-drive RAID 0 IO\nsystem. The baseline (non-sync) test took 19:54 and the sync test\ntook 20:21, which I think is a fairly minor slowdown.\n{quote}\n\nWas that compound or non-compound index format?  I imagine non-compound will take a bigger hit since each file will be synchronized separately and in a serialized fashion.  I also imagine that the hit will be larger for a weaker disk subsystem, and for usage patterns that continually add a few docs and close?\n\nIs a sync before every file close really needed, or can some of them be avoided when autocommit==false?\n",
            "date": "2007-11-10T18:10:41.958+0000",
            "id": 9
        },
        {
            "author": "robert engels",
            "body": "I agree. Just for a baseline, I think the test needs to be done on a single drive system.\n\nAlso, the 'sync' should be optional. BerkleyDB offers similar functionality.\n\nThe reason being, if the index can be completely recreated from other sources, you might not want to pay the performance hit, instead recreate the index if corruption/hard failure occurs.\n\n",
            "date": "2007-11-10T18:17:25.559+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nWas that compound or non-compound index format?  I imagine\nnon-compound will take a bigger hit since each file will be\nsynchronized separately and in a serialized fashion.\n{quote}\n\nThe test was with compound file.\n\nBut, the close() on each component file that goes into the compound\nfile also does a sync, so compound file would be a slightly bigger hit\nbecause it has one additional sync()?\n\nWe can't safely remove the sync() on each component file before\nbuilding the compound file because we currently do a commit of the new\nsegments file before building the compound file.\n\nI guess we could revisit whether that commit (before building the\ncompound file) is really necessary?  I think it's there from when\nflushing & merging were the same thing, and you do want to do this\nwhen merging to save 1X extra peak on the disk usage, but now that\nflushing is separate from merging we could remove that intermediate\ncommit?\n\n{quote}\nI also imagine that the hit will be larger for a weaker disk\nsubsystem, and for usage patterns that continually add a few docs and\nclose?\n{quote}\n\nOK I'll run the same test, but once on a laptop and once over NFS to\nsee what the cost is for those cases.\n\nYes, continually adding docs & flushing/closing your writer will in\ntheory be most affected here.  I think for such apps performance is\nnot usually top priority (indexing latency is)?  Ie if you wanted\nperformance you would batch up the added docs more?  Anyway, for such\ncases users can turn off sync() if they want to risk it?\n\n{quote}\nIs a sync before every file close really needed, or can some of them\nbe avoided when autocommit==false?\n{quote}\n\nIt's somewhat tricky to safely remove sync() even when\nautoCommit=false, because you don't know at close() whether this file\nyou are closing will be referenced (and not merged away) when the\ncommit is finally done (when IndexWriter is closed).\n\nIf there were a way to sync a file after having closed it (is there?)\nthen we could go and sync() all new files we had created that are now\nreferenced by the segments file we are writing.\n\nAlso, I was thinking we could start simple (call sync() before every\nclose()) and then with time, and if necessary, work out smarter ways\nto safely remove some of those sync()'s.\n\n{quote}\nAlso, the 'sync' should be optional. BerkleyDB offers similar\nfunctionality.\n{quote}\n\nIt is optional: I added doSync boolean to\nFSDirectory.getDirectory(...).\n\nAnd, I agree: for cases where there is very low cost to regenerate the\nindex, and you want absolute best performance, you can turn off\nsyncing.",
            "date": "2007-11-10T19:04:20.635+0000",
            "id": 11
        },
        {
            "author": "Doug Cutting",
            "body": "> Is a sync before every file close really needed [...] ?\n\nIt might be nice if we could use the Linux sync() system call, instead of fsync().  Then we could call that only when the new segments file is moved into place rather than as each file is closed.  We could exec the sync shell command when running on Unix, but I don't know whether there's an equivalent command for Windows, and it wouldn't be Java...",
            "date": "2007-11-12T18:14:06.514+0000",
            "id": 12
        },
        {
            "author": "Michael McCandless",
            "body": "OK I ran sync/nosync tests across various platforms/IO system.  In\neach case I ran the test once with doSync=true and once with\ndoSync=false, using this alg:\n\n  analyzer=org.apache.lucene.analysis.SimpleAnalyzer\n  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n  docs.file=/lucene/wikifull.txt\n  \n  doc.maker.forever=false\n  ram.flush.mb = 8\n  max.buffered = 0\n  directory = FSDirectory\n  max.field.length = 2147483647\n  doc.term.vector=false\n  doc.stored=false\n  work.dir = /tmp/lucene\n  fsdirectory.dosync = false\n  \n  ResetSystemErase\n  CreateIndex\n  {AddDoc >: 150000\n  CloseIndex\n  \n  RepSumByName\n\nIe, time to index the first 150K docs from Wikipedia.\n\n\nResults for single hard drive:\n\n  Mac mini (10.5 Leopard) single 4200 RPM \"notebook\" (2.5\") drive -- 2.3% slower:\n  \n      sync - 296.80 sec\n    nosync - 290.06 sec\n  \n  Mac pro (10.4 Tiger), single external drive -- 35.5% slower:\n  \n      sync - 259.61 sec\n    nosync - 191.53 sec\n  \n  Win XP Pro laptop, single drive -- 38.2% slower\n  \n      sync - 536.00 sec\n    nosync - 387.90 sec\n  \n  Linux (2.6.22.1), ext3 single drive -- 23% slower\n  \n      sync - 185.42 sec\n    nosync - 150.56 sec\n  \nResults for multiple hard drives (RAID arrays):\n\n  Linux (2.6.22.1), reiserfs 6 drive RAID5 array -- 49% slower (!!)\n  \n      sync - 239.32 sec\n    nosync - 160.56 sec\n  \n  Mac Pro (10.4 Tiger), 4 drive RAID0 array -- 1% faster\n  \n      sync - 157.26 sec\n    nosync - 158.93 sec\n\n\nSo at this point I'm torn...\n\nThe performance cost of the simplest approach (sync() before close())\nis very costly in many cases (not just laptop IO subsystems).  The\nreiserfs test was rather shocking.  Then, it's oddly very lost cost in\nother cases: the Mac Mini test I find amazing.\n\nIt's frustrating to lose such performance \"out of the box\" for the\npresumably extremely rare event of OS/machine crash/power cut.\n\nMaybe we should leave the default as false for now?\n",
            "date": "2007-11-13T21:53:52.099+0000",
            "id": 13
        },
        {
            "author": "Doug Cutting",
            "body": "> Maybe we should leave the default as false for now?\n\nPerhaps for the short-term, but long-term it would be better to find a solution that's both reliable and doesn't have such a big performance impact.\n\nWe really don't need to sync until we commit.  It would be interesting to know how much it slows things to do that.  As a quick hack we could try running the 'sync' command line program at each commit.  If performance looks good, then we might look into implementing this in pure Java, changing FSDirectory.close() to queue FileDescriptors, add a background thread that syncs queued files, and add a Directory.sync() method that blocks until the queue is empty.\n",
            "date": "2007-11-13T22:47:05.197+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nPerhaps for the short-term, but long-term it would be better to find a solution that's both reliable and doesn't have such a big performance impact.\n{quote}\n\nAgreed.  I will default doSync back to false, for now.\n\n{quote}\nWe really don't need to sync until we commit. It would be interesting to know how much it slows things to do that. As a quick hack we could try running the 'sync' command line program at each commit.\n{quote}\n\nI will test this as a hack first just to see how performance compares\nto the current approach.\n\n{quote}\nIf performance looks good, then we might look into implementing this in pure Java, changing FSDirectory.close() to queue FileDescriptors, add a background thread that syncs queued files, and add a Directory.sync() method that blocks until the queue is empty.\n{quote}\n\nWill do!\n",
            "date": "2007-11-14T10:24:19.283+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "OK, I tested calling command-line \"sync\", after writing each segments\nfile.  It's in fact even slower than fsync on each file for these 3\ncases:\n\nLinux (2.6.22.1), reiserfs 6 drive RAID5 array 93% slower\n      sync - 330.74\n    nosync - 171.24\n\nLinux (2.6.22.1), ext3 single drive 60% slower\n      sync - 242.02\n    nosync - 150.91\n\nMac Pro (10.4 Tiger), 4 drive RAID0 array 28% slower\n      sync - 204.77\n    nosync - 159.90\n\nI'll look into the separate thread to sync/close files in the\nbackground next...\n",
            "date": "2007-11-20T23:19:56.745+0000",
            "id": 16
        },
        {
            "author": "Michael Busch",
            "body": "I think changing the only constructor in FSDirectory.FSIndexOutput is\nan API change. I have a class that extends FSIndexOutput and it \ndoesn't compile anymore after switching to the 2.3-dev jar.\n\nI think we should put this ctr back:\npublic FSIndexOutput(File path) throws IOException {\n  this(path, DEFAULT_DO_SYNC);\n}",
            "date": "2007-11-22T08:38:43.981+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "Woops, OK I will put it back ...",
            "date": "2007-11-22T10:32:48.329+0000",
            "id": 18
        },
        {
            "author": "Doron Cohen",
            "body": "{quote}\nI'll look into the separate thread to sync/close files in the\nbackground next...\n{quote}\n\nI was wondering if delaying sync to actual commit point would run faster\nthan a background thread. I thought it would, because the background\nthread, though not holding current thread from continue with indexing, \ndoes force the sync *now* rather than letting the IO subsystem actually \nwrite stuff on its time. I was also hoping that by doing them later, \nsome of the syncs would become no-ops, and hence faster. I found\nout however that delaying the syncs (but intending to sync) also \nmeans keeping the file handles open, and therefore  this is not \na practical approach. Still it was interesting to compare. \n\nSo... my small test sequentially writes M characters to N files \nand either do not sync (just close), or does sync in one of three \nways: (1) at the end, (2) immediately, (3) in a background thread. \nThe results (in millis) on my Windows XP were:\n\n|| num files || num chars per file || No Sync || Sync At End || Background Sync || Immediate Sync ||\n|   100 | 10000 |   631 |   5778 |   5729 |   5828 |\n|   100 | 10000 |   581 |   4486 |   4117 |   4687 |\n|  1000 |  1000 |  1612 |  38996 |  34900 |  35852 |\n|  1000 |  1000 |  1432 |  37153 |  35051 |  37263 |\n| 10000 |   100 | 10335 | 154262 | 162103 | 174251 |\n| 10000 |   100 | 11276 | 147752 | 159480 | 222450 |\n\nEach configuration ran twice and there are fluctuations, \nbut it is obvious (as Mike noticed) that no-sync is much faster\nthen sync. In fact in my test no-sync is at least 10 times faster\nthan any sync approach, while in Mike's test which is using \nLucene the penalty is smaller. Difference might be because \nin my test there is no CPU work involved, just IO. \n\nComparing \"immediate\" to \"background\" I it is not clearly worth it \nto add a background thread (unless Mike's test proves otherwise..)",
            "date": "2007-11-22T13:27:23.842+0000",
            "id": 19
        },
        {
            "author": "Doron Cohen",
            "body": "With some artificial CPU activity added to the test program:\n|| num files || num chars per file || No Sync || Sync At End || Background Sync || Immediate Sync ||\n|   100 | 10000 |  6690  |  11516 |  10706 |  11216 |\n|   100 | 10000 |  7200  |  11006 |  10575 |  10846 |\n|  1000 |  1000 |  8002  |  48570 |  48479 |  51825 |\n|  1000 |  1000 |  7801  |  43142 |  43693 |  43342 |\n| 10000 |   100 | 16303  | 152730 | 326810 | 207939 |\n| 10000 |   100 | 17805  | 156375 | 160040 | 165398 |\n\n",
            "date": "2007-11-22T14:11:59.328+0000",
            "id": 20
        },
        {
            "author": "Doron Cohen",
            "body": "Attached FSyncPerfTest.java is the standalone (non Lucene) perf test that I used.",
            "date": "2007-11-22T14:16:10.676+0000",
            "id": 21
        },
        {
            "author": "Doug Cutting",
            "body": "> I found out however that delaying the syncs (but intending to sync) also\nmeans keeping the file handles open [...]\n\nNot necessarily.  You could just queue the file names for sync, close them, and then have the background thread open, sync and close them.  The close could trigger the OS to sync things faster in the background.  Then the open/sync/close could mostly be a no-op.  Might be worth a try.",
            "date": "2007-11-26T18:13:03.127+0000",
            "id": 22
        },
        {
            "author": "Michael McCandless",
            "body": "OK I did a simplistic patch (attached) whereby FSDirectory has a\nbackground thread that re-opens, syncs, and closes those files that\nLucene has written.  (I'm using a modified version of the class from\nDoron's test).\n\nThis patch is nowhere near ready to commit; I just coded up enough so\nwe could get a rough measure of performance cost of syncing.  EG we\nmust prevent deletion of a commit point until a future commit point is\nfully sync'd to stable storage; we must also take care not to sync a\nfile that has been deleted before we sync'd it; don't sync until the\nend when running with autoCommit=false; merges if run by\nConcurrentMergeScheduler should [maybe] sync in the foreground; maybe\nforcefully throttle back updates if syncing is falling too far behind;\netc.\n\nI ran the same alg as the tests above (index first 150K docs of\nWikipedia).  I ran CFS and no CFS X sync and nosync (4 tests) for each\nIO system.  Time is the fastest of 2 runs:\n\n|| IO System || CFS sync || CFS nosync || CFS % slower || non-CFS sync || non-CFS nosync || non-CFS % slower ||\n| ReiserFS 6-drive RAID5 array Linux (2.6.22.1) | 188 | 157 | 19.7% | 143 | 147 | -2.7% |\n| EXT3 single internal drive Linux (2.6.22.1) | 173 | 157 | 10.2% | 136 | 132 | 3.0% |\n| 4 drive RAID0 array Mac Pro (10.4 Tiger) | 153 | 152 | 0.7% | 150 | 149 | 0.7% |\n| Win XP Pro laptop, single drive | 463 | 352 | 31.5% | 343 | 335 | 2.4% |\n| Mac Pro single external drive | 463 | 352 | 31.5% | 343 | 335 | 2.4% |\n\nThe good news is, the non-CFS case shows very little cost when we do\nBG sync'ing!\n\nThe bad news is, the CFS case still shows a high cost.  However, by\nnot sync'ing the files that go into the CFS (and also not committing a\nnew segments_N file until after the CFS is written) I expect that cost\nto go way down.\n\nOne caveat: I'm using a 8 MB RAM buffer for all of these tests.  As\nYonik pointed out, if you have a smaller buffer, or, you add just a\nfew docs and then close your writer, the sync cost as a pctg of net\nindexing time will be quite a bit higher.\n",
            "date": "2007-11-27T20:14:17.626+0000",
            "id": 23
        },
        {
            "author": "Michael McCandless",
            "body": "Woops, the last line in the table above is wrong (it's a copy of the line before it).  I'll re-run the test.",
            "date": "2007-11-27T22:56:33.029+0000",
            "id": 24
        },
        {
            "author": "Michael McCandless",
            "body": "\nHow about if we don't sync every single commit point?\n\nI think on a crash what's important when you come back up is 1) index\nis consistent and 2) you have not lost that many docs from your index.\nLosing the last N (up to mergeFactor) flushes might be acceptable?\n\nEG we could force a full sync only when we commit the merge, before we\nremove the merged segments.  This would mean on a crash that you're\n\"guaranteed\" to have the last successfully committed & sync'd merge to\nfall back to, and possibly a newer commit point if the OS had sync'd\nthose files on its own?\n\nThat would be a big simplification because I think we could just do\nthe sync() in the foreground since ConcurrentMergeScheduler is already\nusing BG threads to do merges.\n\nThis would also mean we cannot delete the commit points that were not\nsync'd.  So the first 10 flushes would result in 10 segments_N files.\nBut then when the merge of these segments completes, and the result is\nsync'd, those files could all be deleted.\n\nPlus we would have to fix retry logic on loading the segments file to\ntry more than just the 2 most recent commit points but that's a pretty\nminor change.\n\nI think it should mean better performance, because the longer you wait\nto call sync() presumably the more likely it is a no-op if the OS has\nalready sync'd the file.\n",
            "date": "2007-11-27T23:06:08.070+0000",
            "id": 25
        },
        {
            "author": "Doug Cutting",
            "body": "> How about if we don't sync every single commit point?\n\nI'm confused.  The semantics of commit should be that all changes prior are made permanent, and no subsequent changes are permanent until the next commit.  So syncs, if any, should map 1:1 to commits, no?  Folks can make indexing faster by committing/syncing less often.\n",
            "date": "2007-11-27T23:58:03.648+0000",
            "id": 26
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nI'm confused. The semantics of commit should be that all changes prior are made permanent, and no subsequent changes are permanent until the next commit. So syncs, if any, should map 1:1 to commits, no? Folks can make indexing faster by committing/syncing less often.\n{quote}\n\nBut must every \"automatic buffer flush\" by IndexWriter really be a\n\"permanent commit\"?  I do agree that when you close an IndexWriter, we\nshould should do a \"permanent commit\" (and block until it's done).\n\nEven if we use that policy, the BG sync thread can still fall behind\nsuch that the last few/many flushes are still in-process of being made\npermanent (eg I see this happening while a merge is running).  In fact\nI'll have to block further flushes if syncing falls \"too far\" behind,\nby some metric.  So, we already won't have any \"guarantee\" on when a\ngiven flush actually becomes permanent even if we adopt this policy.\n\nI think \"merge finished\" should be made a \"permanent commit\" because\notherwise we are tying up potentially alot of disk space,\ntemporarily.  But for a flush there's only a tiny amount of space (the\nold segments_N files) being tied up.\n\nMaybe we could make some flushes permanent but not all, depending on\nhow far behind the sync thread is.  EG if you do a flush, but, the\nsync thread is still trying to make the last flush permanent, don't\nforce the new flush to be permanent?\n\nIn general, I think the longer we can wait after flushing before\nforcing the OS to make those writes \"permanent\", the better the\nchances that the OS has in fact already sync'd those files anyway, and\nso the sync cost should be lower.  So maybe we could make every flush\npermanent, but wait a little while before doing so?\n\nRegardless of what policy we choose here (which commits must be made\n\"permanent\", and, when) I think the approach requires that\nIndexFileDeleter query the Directory so that it's only allowed to\ndelete older commit points once a newer commit point has successfully\nbecome permanent.\n\nI also worry about those applications that are accidentally flushing\ntoo often now.  Say your app now sets maxBufferedDocs=100.  Right now,\nthat gives you poor performance but not disastrous, but I fear if we\ndo the \"every commit is permanent\" policy then performance could\neasily become disastrous.  People who upgrade will suddenly get much\nworse performance.\n",
            "date": "2007-11-28T15:39:17.215+0000",
            "id": 27
        },
        {
            "author": "Doug Cutting",
            "body": "> But must every \"automatic buffer flush\" by IndexWriter really be a\n\"permanent commit\"?\n\nWhen autoCommit is true, then we should periodically commit automatically.  When autoCommit is false, then nothing should be committed until the IndexWriter is closed.  The ambiguous case is flush().  I think the reason for exposing flush() was to permit folks to commit without closing, so I think flush() should commit too, but we could add a separate commit() method that flushes and commits.\n\n> People who upgrade will suddenly get much worse performance.\n\nYes, that would be bad.  Perhaps the semantics of autoCommit=true should be altered so that it commits less than every flush.  Is that what you were proposing?  If so, then I think it's a good solution.  Prior to 2.2 the commit semantics were poorly defined.  Folks were encouraged to close() their IndexWriter to persist changes, and that's about all we said.  2.2's docs say that things are committed at every flush, but there was no sync, so I don't think changing this could break any applications.\n\nSo I'm +1 for changing autoCommit=true to sync less than every flush, e.g., only after merges.  I'd also argue that we should be vague in the documentation about precisely when autoCommit=true commits.  If someone needs to know exactly when things are committed then they should be encouraged to explicitly flush(), not to rely on autoCommit.",
            "date": "2007-11-28T17:20:20.848+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "\nI modified the CFS sync case to NOT bother syncing the files that go\ninto the CFS.  I also turned off syncing of segments.gen.  I also\ntested on a Windows Server 2003 box.\n\nNew patched attached (still a hack just to test performance!) and new\nresults.  All tests are with the \"sync every commit\" policy:\n\n||IO System||CFS sync||CFS nosync||CFS % slower||non-CFS sync||non-CFS nosync||non-CFS % slower||\n|2 drive RAID0 Windows 2003 Server R2 Enterprise x64|250|244|2.6%|241|241|0.1%|\n|ReiserFS 6-drive RAID5 array Linux (2.6.22.1)|186|166|11.9%|145|142|2.0%|\n|EXT3 single internal drive Linux (2.6.22.1)|160|158|0.9%|142|135|4.8%|\n|4 drive RAID0 array Mac Pro (10.4 Tiger)|152|155|-2.4%|149|147|1.3%|\n|Win XP Pro laptop, single drive|408|398|2.6%|343|346|-1.1%|\n|Mac Pro single external drive|211|209|1.0%|167|149|12.4%|\n",
            "date": "2007-11-28T17:33:44.212+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nWhen autoCommit is true, then we should periodically commit automatically. When autoCommit is false, then nothing should be committed until the IndexWriter is closed. The ambiguous case is flush(). I think the reason for exposing flush() was to permit folks to commit without closing, so I think flush() should commit too, but we could add a separate commit() method that flushes and commits.\n{quote}\n\nI think deprecating flush(), renaming it to commit(), and clarifying\nthe semantics to mean that commit() flushes pending docs/deletes,\ncommits a new segments_N, syncs all files referenced by this commit,\nand blocks until the sync is complete, would make sense?  And,\ncommit() would in fact commit even when autoCommit is false (flush()\ndoesn't commit now when autoCommit=false, which is indeed confusing).\n\n{quote}\nPerhaps the semantics of autoCommit=true should be altered so that it commits less than every flush. Is that what you were proposing? If so, then I think it's a good solution. Prior to 2.2 the commit semantics were poorly defined. Folks were encouraged to close() their IndexWriter to persist changes, and that's about all we said. 2.2's docs say that things are committed at every flush, but there was no sync, so I don't think changing this could break any applications.\n\nSo I'm +1 for changing autoCommit=true to sync less than every flush, e.g., only after merges. I'd also argue that we should be vague in the documentation about precisely when autoCommit=true commits. If someone needs to know exactly when things are committed then they should be encouraged to explicitly flush(), not to rely on autoCommit.\n{quote}\n\nOK, I will test the \"sync only when committing a merge\" approach for\nperformance.  Hopefully a foreground sync() is fine given that with\nConcurrentMergePolicy that's already in a background thread.  This\nwould be a nice simplification.\n\nAnd I agree we should be vague about, and users should never rely on,\nprecisely when Lucene has really committed (sync'd) the changes to\ndisk.  I'll fix the javadocs.\n",
            "date": "2007-11-28T17:46:08.065+0000",
            "id": 30
        },
        {
            "author": "Doug Cutting",
            "body": "> I think deprecating flush(), renaming it to commit()\n\n+1  That's clearer, since flushes are internal optimizations, while commits are important events to clients.",
            "date": "2007-11-28T18:55:17.951+0000",
            "id": 31
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nYou could just queue the file names for sync, close them, and then have the background thread open, sync and close them. The close could trigger the OS to sync things faster in the background. Then the open/sync/close could mostly be a no-op. Might be worth a try.\n{quote}\n\nI am taking this approach now, but one nagging question I have is: do\nwe know with some certainty that re-opening a file and then sync'ing\nit in fact syncs all writes that were ever done to this file in this\nJVM, even with previously opened and now closed descriptors?  VS, eg,\nonly sync'ing any new writes done with that particular descriptor?\n\nIn code:\n\n{code}\nfile = new RandomAccess(path, \"rw\");\n<do many writes to file>\nfile.close();\nnew RandomAccess(path, \"rw\").getFD().sync();\n{code}\t\n\nAre we pretty sure that all of the \"many writes\" will in fact be\nsync'd by that sync call, on all OSs?\n\nI haven't been able to find convincing evidence one way or another.  I\ndid run a timing test comparing overall time if you sync with the same\ndescriptor you used for writing vs closing it, opening a new one, and\nsyncing with that one, and on Linux at least it seems both approaches\nseem to be syncing because the total elapsed time is roughly the\nsame.\n\nRobert do you know?\n\nI sure hope the answer is yes ... because if not, the alternative is\nwe must sync() before closing the original descriptor, which makes\nthings less flexible because eg we cannot cleanly implement\nIndexWriter.commit().\n",
            "date": "2007-11-30T13:10:40.676+0000",
            "id": 32
        },
        {
            "author": "Michael McCandless",
            "body": "Another nuance here is ... say we do a \"soft commit\" (write a new\nsegment & segments_N but do not sync the files), and, the machine\ncrashes.  This is fine because there will always be an earlier commit\npoint (segments_M) that was a \"hard commit\" (sync was done).\n\nThen, machine comes back up and we open a reader.  The reader sees\nboth segments_M (the hard commit) and segments_N (the soft commit) and\nchooses segments_N because it's more recent.\n\nWe have retry logic in SegmentInfos to fallback to segments_M if we\nhit an IOException on opening the index described by segments_N.\n\nBut, the problem is: the extent of the \"corruption\" caused by the\ncrash could be somewhat subtle.  EG a given file might be the right\nlength, but, filled w/ zeroes.  This is a problem because we may not\nthen hit an IOException while opening the reader, but only later hit\nsome exception while searching.\n\nI think this means when we do a \"soft commit\" we should not in fact\nwrite a new segments_N file (as we do today).  When we do a \"hard\ncommit\" we should first sync all files except the new segments_N file,\nthen write the segments_N file, then sync it.\n\nThe thing is, while we have been (and want to continue to be) vague\nabout exactly when a \"commit\" takes place as you add docs to\nIndexWriter, users have presumably gotten used to every flush (when\nautoCommit=true) committing a new segments_N file that an IndexReader\ncan then see.  So, this change (do not write segments_N file except\nfor a hard commit) will break that behavior.  Maybe, with the addition\nof the explicit commit() method, this is OK?\n",
            "date": "2007-11-30T15:00:40.582+0000",
            "id": 33
        },
        {
            "author": "Michael McCandless",
            "body": "From java-dev, Robert Engels wrote:\n\n{quote}\nMy reading of the Unix specification shows it should work (the _commit under Windows is less clear, and since Windows is not inode based, there may be different issues).\n\nhttp://www.opengroup.org/onlinepubs/007908799/xsh/fsync.html\n{quote}\n\nOK thanks Robert.\n\nI think very likely this approach (let's call it \"sync after close\")\nwill work.  The _commit docs (for WIN32) also seems to indicate that\nthe file referenced by the descriptor is fully flushed (as we want):\n\n  http://msdn2.microsoft.com/en-us/library/17618685\n\nAlso at least PostgreSQL and Berkeley DB \"trust\" _commit as the\nequivalent of fsync (though I have no idea if they use it the same way\nwe want to).\n\nThough ... I am also a bit concerned about opening files for writing\nthat we had already previously closed.  It arguably makes Lucene \"not\nquite\" write-once.  And, we may need a retry loop on syncing because\non Windows, various tools might wake up and peek into a file right\nafter we close them, possibly interfering w/ our reopening/syncing.\n\nI think the alternative (\"sync before close\") is something like:\n\n  * Add a new method IndexOutput.close(boolean doSync)\n\n  * When a merge finishes, it must close all of its files with\n    doSync=true; and write the new segments_N with doSync=true.\n\n  * To implement commit() ... I think we'd have to force a merge of\n    all written segments that were not sync'd.  And on closing the\n    writer we'd call commit().  This is obviously non-ideal because\n    you can get very different sized level 1 segments out.  Although\n    the cost would be contained since it's only up to mergeFactor\n    level 0 segments that we will merge.\n\nOK ... I'm leaning towards sticking with \"sync after close\", so I'll\nkeep coding up this approach for now.\n",
            "date": "2007-11-30T17:34:52.374+0000",
            "id": 34
        },
        {
            "author": "Doug Cutting",
            "body": "> I think this means when we do a \"soft commit\" we should not in fact\n> write a new segments_N file (as we do today).\n\n+1  As long as we commit periodically when autoCommit=true I don't think we're breaking any previously advertised contract.\n",
            "date": "2007-12-01T00:28:00.763+0000",
            "id": 35
        },
        {
            "author": "Michael McCandless",
            "body": "I've moved this issue to 2.4.  I think it's too risky to rush it in\njust before 2.3 is released vs committing just after 2.3 and\ngiving it more time on the trunk.\n\nBut, I think for 2.3 we should revert the optional \"doSync\" argument\nto FSDirectory: I believe the performance impact of syncing is low enough\nwith the approach we're now taking, so I don't think we should make it\nso trivial to turn it off.  I've added a sync() method to Directory,\nso if someone really wants to prevent syncing they will be able to\nsubclass FSDirectory and make that method a noop.\n",
            "date": "2007-12-06T17:33:50.535+0000",
            "id": 36
        },
        {
            "author": "Doron Cohen",
            "body": "{quote}\nThough ... I am also a bit concerned about opening files for writing\nthat we had already previously closed. It arguably makes Lucene \"not\nquite\" write-once. \n{quote}\nI think this would work too?\n{code}\nFileInputStream fis = new FileInputStream(path);\nfis.getFD().sync();\nfis.close();\n{code}",
            "date": "2007-12-06T21:54:08.593+0000",
            "id": 37
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nI think this would work too?\n{code}\nFileInputStream fis = new FileInputStream(path);\nfis.getFD().sync();\nfis.close();\n{code}\n{quote}\n\nThis was suggested & debated on the java-dev list. But, the man page\nfor \"fsync\" on Linux lists this as one of the errors:\n\n{code}\nERRORS\n       EBADF  fd is not a valid file descriptor open for writing.\n{code}\n\nAnd Yonik found at least one JVM implementation (I think Harmony) that\nsimply skipped if the descriptor was not open for write.\n\nI think we're walking on thin ice if we do that...\n\n",
            "date": "2007-12-06T22:21:16.354+0000",
            "id": 38
        },
        {
            "author": "Doron Cohen",
            "body": "{quote}\nI think we're walking on thin ice if we do that...\n{quote}\nOh, I skimmed too fast that part of the discussion in the \ndev list.  I agree with \"thin ice\" now.",
            "date": "2007-12-06T22:42:24.306+0000",
            "id": 39
        },
        {
            "author": "Michael McCandless",
            "body": "\nInitial patch attached:\n\n  * Created new commit() method; deprecated public flush() method\n\n  * Changed IndexWriter to not write segments_N when flushing, only\n    when syncing (added new private sync() for this).  The current\n    \"policy\" is to sync only after merges are committed.  When\n    autoCommit=false we do not sync until close() or commit() is\n    called\n\n  * Added MockRAMDirectory.crash() to simulate a machine crash.  It\n    keeps track of un-synced files, and then in crash() it goes and\n    corrupts any unsynced files rather aggressively.\n\n  * Added a new unit test, TestCrash, to crash the MockRAMDirectory at\n    various interesting times & make sure we can still load the\n    resulting index.\n\n  * Added new Directory.sync() method.  In FSDirectory.sync, if I hit\n    an IOException when opening or sync'ing, I retry (currently after\n    waiting 5 msec, and retrying up to 5 times).  If it still fails\n    after that, the original exception is thrown and the new\n    segments_N will not be written (and, the previous commit will also\n    not be deleted).\n\nAll tests now pass, but there is still alot to do, eg at least:\n\n  * Javadocs\n\n  * Refactor syncing code so DirectoryIndexReader.doCommit can use it\n    as well.\n\n  * Change format of segments_N to include a hash of its contents, at\n    the end.  I think this is now necessary in case we crash after\n    writing segments_N but before we can sync it, to ensure that\n    whoever next opens the reader can detect corruption in this\n    segments_N file.\n\n",
            "date": "2007-12-11T20:20:57.531+0000",
            "id": 40
        },
        {
            "author": "Andrew Zhang",
            "body": "Hi, Any progress on this issue? \n\nI found sync call was removed from the source code. Is there an alternative to solve this problem? Thanks a lot!",
            "date": "2008-01-20T09:56:39.640+0000",
            "id": 41
        },
        {
            "author": "Michael McCandless",
            "body": "This is still in progress.  It's clearly a serious bug since it's something out of your control that can easily cause index corruption.\n\nsync was removed because the simple approach is far too costly on some IO systems.  The new approach (sync only on committing a merge) has more reasonable performance, but is not quite done yet.",
            "date": "2008-01-20T10:27:23.267+0000",
            "id": 42
        },
        {
            "author": "Michael McCandless",
            "body": "New rev of this patch.  All tests pass.  I think it's ready to\ncommit, but I'll wait a few days for comments.\n\nThis patch has a small change to the segments_N file: it adds a\nchecksum to the end.  I added ChecksumIndexInput/Output that wrap an\nexisting IndexInput/Output for this.  This is used to verify the file\nis \"intact\" before trusting its contents when opening the index.  We\nneed this to guard against the machine crashing after we've written\nsegments_N and before we've succeeded in syncing it.\n\nUnfortunately, in testing performance, I still see a sizable (~30-50%)\nperformance hit to indexing throughput, on windows computers (XP Pro\nlaptop & Win 2003 Server R64 computer).  It seems that calling sync\nwas causing IO in other threads (ie flushing a new segment) to\ndrasically slow down.  Note that this is only when autoCommit=true; if\nit's false then performance is only slightly worse (because only on\nclosing the writer do we sync)\n\nSo I tried sleeping, after writing and before syncing.  I sleep based\non number of bytes written, for up to 10 seconds, and amazingly, this\ngreadly reduces the performance loss on the windows computers, and\ndoesn't hurt performance on Linux/OS X computers.\n\nI think this must be because calling sync immediately forces the OS to\nwrite dirty buffers to disk \"in a rush\" (severely impacting IO writes\nfrom other threads), whereas if you wait first, you let the OS\nschedule those writes on its own, at good times (maybe when IO system\nis \"relatively\" idle).\n\nIt's disappointing to have to \"game\" the OS to gain back this\nperformance.  I wish Java had a \"waitUntilSync'd\" to do the same\nthings as fsync, but without \"rushing\" the OS.\n\nOn Linux 2.6.22 on a RAID5 array I still see a net performance cost of\n~12%, sleeping or no sleeping.  On Mac OS X it's ~3% loss.\n\nOther fixes:\n  * DirectoryIndexReader's doCommit now also syncs\n  * Improved logic on when we must sync-before-CFS: it's not necessary\n    if the just-merged segments are not referenced by the last commit\n    point (ie if they were all flushed during this writer session)\n  * Created SegmentInfos.commit() method, which writes and then syncs\n    the next segments_N file\n  * Simplified sync() logic now that merge threads are stopped before\n    writer is closed\n  * Changed CMS.newMergeThread to name its threads\n  * More test cases\n  * Various other small fixes\n\nHere are test details.  I index first 200K Wikipedia docs with this\nalg:\n\n  analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer\n  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n  docs.file=/Volumes/External/lucene/wiki.txt\n  doc.stored = true\n  doc.term.vector = true\n  doc.term.vector.offsets = true\n  doc.term.vector.positions = true\n\n  doc.maker.forever = false\n  directory=FSDirectory\n\n  { \"BuildIndex\"\n    CreateIndex\n    { \"AddDocs\" AddDoc > : 200000\n    CloseIndex\n  }\n\n  RepSumByPref BuildIndex\n\nWin2003 R64, JVM 1.6.0_03\n  trunk: 523 sec\n  patch: 547 sec (5% slower)\n\nWin XP Pro, laptop hard drive, JVM 1.4.2_15-b02\n  trunk: 1237 sec\n  patch: 1278 sec (3% slower)\n\nLinux ReiserFS on 6 drive RAID 5 array, JVM 1.5.0_08\n  trunk: 483 sec\n  patch: 539 sec (12% slower)\n\nMac OS X 10.4 4-drive RAID 0 array, JVM 1.5.0_13\n  trunk: 268 sec\n  patch: 276 sec (3% slower)\n",
            "date": "2008-02-05T10:52:57.115+0000",
            "id": 43
        },
        {
            "author": "Michael McCandless",
            "body": "On thinking through the above costs of committing, I now think we\nshould deprecate autoCommit=true entirely, making autocommit=false the\nonly choice in 3.0.\n\nWith that change, when you use an IndexWriter, its changes are never\nvisible to a reader until you call commit() or close().  I think this\nis how KinoSearch and Ferret work, for example.\n\nHere are some reasons:\n\n  * Commit has now become a costly event, because sync() is costly,\n    and is forcing us to use this \"syncPause\" logic (hack) to game the\n    OS, which really is ugly, dependent on OS/IO particulars, etc.\n\n  * Since we make no guarantee on when a commit specifically happens,\n    and this fix in particular will reduce its frequency from \"every\n    flush\" to \"every merge\", autoCommit=true really is not that useful\n    for applications (ie, they will have to call commit() on their\n    anyway if they need to rely on its frequency).\n\n  * It's always possible to build an autocommit layer above\n    IndexWriter by calling commit on your own schedule, to tradeoff\n    performance for commit frequency (but not vice/versa).\n\n  * Not autocommitting by default opens up some good future\n    optimizations on merging since we don't have to flush real\n    segments to disk until commit.  One simple example is we could\n    skip building CFS files as we flush, and only merge & build CFS on\n    commit/close.\n\nWhat do people think?\n\nIf we do this, I would right now deprecate all ctors that take\nautoCommit and add comment explaining that in 3.0 autoCommit is wired\nto \"false\".  I would leave the \"syncPause\" logic in there for now,\nbecause it's such a sizable performance gain on windows, but deprecate\nit, stating that with it will be removed when we switch to\nautoCommit=false in 3.0.\n",
            "date": "2008-02-05T10:59:38.411+0000",
            "id": 44
        },
        {
            "author": "Doug Cutting",
            "body": "> deprecate autoCommit=true entirely\n\n+1  This sounds like a good plan.\n\nAre your performance numbers above with autoCommit true or false?\n\nAlso, why not only sleep if Constants.WINDOWS?\n",
            "date": "2008-02-05T17:03:17.281+0000",
            "id": 45
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\n> deprecate autoCommit=true entirely\n\n+1 This sounds like a good plan.\n{quote}\nOK I'll work out a new patch with this approach.\n\n{quote}\nAre your performance numbers above with autoCommit true or false?\n{quote}\nThey were all with autoCommit=true.\n\n{quote}\nAlso, why not only sleep if Constants.WINDOWS?\n{quote}\nGood, I'll take that approach!",
            "date": "2008-02-05T17:38:02.248+0000",
            "id": 46
        },
        {
            "author": "Michael McCandless",
            "body": "OK I updated the patch:\n\n  * Deprecate all IW ctors that take autoCommit param, and updated javadocs stating that autoCommit will be hardwired to false starting in 3.0\n\n  * Default maxSyncPause to 10 seconds when Constant.WINDOWS; else, to 0\n\nI'll wait until end of week to commit!",
            "date": "2008-02-05T21:19:00.876+0000",
            "id": 47
        },
        {
            "author": "Michael McCandless",
            "body": "Attached new rev of the patch.  Only changes were to add caveats in javadcos about IO devices that ignore fsync, and, updated patch to apply cleanly on current trunk.\n\nI plan to commit in a day or two.",
            "date": "2008-02-08T20:31:59.300+0000",
            "id": 48
        },
        {
            "author": "Yeliz Eseryel",
            "body": "I had been following this thread. Just curious if the patch was committed.",
            "date": "2009-02-23T04:22:34.224+0000",
            "id": 49
        },
        {
            "author": "Michael McCandless",
            "body": "Yes, this is committed and available as of 2.4.0.",
            "date": "2009-02-23T10:22:24.725+0000",
            "id": 50
        },
        {
            "author": "Yeliz Eseryel",
            "body": "Thanks Michael!",
            "date": "2009-02-23T17:15:03.350+0000",
            "id": 51
        }
    ],
    "component": "core/index",
    "description": "When indexing a large number of documents, upon a hard power failure  (e.g. pull the power cord), the index seems to get corrupted. We start a Java application as an Windows Service, and feed it documents. In some cases (after an index size of 1.7GB, with 30-40 index segment .cfs files) , the following is observed.\n\nThe 'segments' file contains only zeros. Its size is 265 bytes - all bytes are zeros.\nThe 'deleted' file also contains only zeros. Its size is 85 bytes - all bytes are zeros.\n\nBefore corruption, the segments file and deleted file appear to be correct. After this corruption, the index is corrupted and lost.\n\nThis is a problem observed in Lucene 1.4.3. We are not able to upgrade our customer deployments to 1.9 or later version, but would be happy to back-port a patch, if the patch is small enough and if this problem is already solved.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1044",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Behavior on hard power shutdown",
    "systemSpecification": true,
    "version": ""
}