{
    "comments": [
        {
            "author": "Robert Muir",
            "body": "this fixes contrib too, as long as you apply the CJKTokenizer fix from LUCENE-2207.\n\nend() was incorrect for ChineseTokenizer, SmartChinese, and Wikipedia",
            "date": "2010-01-17T09:29:14.431+0000",
            "id": 0
        },
        {
            "author": "Uwe Schindler",
            "body": "Wikipedia does not call super.end().\n\nLooks good!",
            "date": "2010-01-17T10:00:26.875+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "bq. Wikipedia does not call super.end(). \n\nUwe, thanks for taking a look...\n\neven StandardTokenizer does not call super.end()!\n\nshould we really do this?",
            "date": "2010-01-17T10:02:55.134+0000",
            "id": 2
        },
        {
            "author": "Robert Muir",
            "body": "i merged Koji's fix and tests to CJK from LUCENE-2207 into this patch, and improved CJKTokenizer's tests to always use assertAnalyzesTo, for better checking.\n\ni plan to commit soon",
            "date": "2010-01-17T11:48:36.048+0000",
            "id": 3
        },
        {
            "author": "Uwe Schindler",
            "body": "I am fine with this patch!",
            "date": "2010-01-17T15:51:10.368+0000",
            "id": 4
        },
        {
            "author": "Robert Muir",
            "body": "Committed revision 900196 to trunk.",
            "date": "2010-01-17T19:28:03.685+0000",
            "id": 5
        }
    ],
    "component": "modules/analysis",
    "description": "If offsetAtt/end() is not implemented correctly, then there can be problems with highlighting: see LUCENE-2207 for an example with CJKTokenizer.\n\nIn my opinion you currently have to write too much code to test this.\n\nThis patch does the following:\n* adds optional Integer finalOffset (can be null for no checking) to assertTokenStreamContents\n* in assertAnalyzesTo, automatically fill this with the String length()\n\nIn my opinion this is correct, for assertTokenStreamContents the behavior should be optional, it may not even have a Tokenizer. If you are using assertTokenStreamContents with a Tokenizer then simply provide the extra expected value to check it.\n\nfor assertAnalyzesTo then it is implied there is a tokenizer so it should be checked.\n\nthe tests pass for core but there are failures in contrib even besides CJKTokenizer (apply Koji's patch from LUCENE-2207, it is correct). Specifically ChineseTokenizer has a similar problem.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2219",
    "issuetypeClassified": "OTHER",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "improve BaseTokenStreamTestCase to test end()",
    "systemSpecification": true,
    "version": "2.9, 3.0"
}