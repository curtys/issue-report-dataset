{
    "comments": [
        {
            "author": "Otis Gospodnetic",
            "body": "Included:\n  NGramTokenizer\n  NGramTokenizerTest\n  EdgeNGramTokenizer\n  EdgeNGramTokenizerTest\n",
            "date": "2006-12-22T23:43:04.000+0000",
            "id": 0
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Unit tests pass, committed.",
            "date": "2006-12-22T23:44:20.000+0000",
            "id": 1
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Reopening, because I'm bringing in Adam Hiatt's modifications that he uploaded in a patch for SOLR-81.  Adam's changes allow this tokenizer to create n-grams whose sizes are specified as a min-max range.\n\nThis patch fixes a bug in Adam's code, but has another bug that I don't know how to fix now.\nAdam's bug:\n  input: abcde\n  minGram: 1\n  maxGram: 3\n  output: a ab abc  -- and this is where tokenizing stopped, which was wrong, it should have continued: b bc bcd c cd cde d de e\n\nOtis' bug:\n  input: abcde\n  minGeam: 1\n  maxGram: 3\n  output: e de cde d cd bcd c bc abc b ab -- and this is where tokenizing stops, which is wrong, it should generate one more n-gram: a\n\nThis bug won't hurt SOLR-81, but it should be fixed.",
            "date": "2007-02-16T20:01:59.592+0000",
            "id": 2
        },
        {
            "author": "Otis Gospodnetic",
            "body": "The modified tokenizer and the extended unit test.",
            "date": "2007-02-16T20:04:20.440+0000",
            "id": 3
        },
        {
            "author": "Adam Hiatt",
            "body": "Otis: this really isn't a bug. The min/max gram code I added only applied to the EdgeNGramTokenizer.  I only want to generate _edge_ n-grams between the range of sizes provided.\n\nFor example, with the EdgeNGramTokenizer\n input: abcde\n  minGram: 1\n  maxGram: 3 \n\n'a ab abc' is in fact what I intended to produce.\n\nI think it makes more sense for the functionality to which you referred to be located in NGramTokenizer.\n",
            "date": "2007-02-16T23:10:04.744+0000",
            "id": 4
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Damn, I think you are right! :)  Once again, I'm making late night mistakes.  When will I learn!?\nBut I could take my code to NGramTokenizer then, at least.\nMy bug remains, though..... got an idea for a fix?\n\n",
            "date": "2007-02-17T08:10:45.906+0000",
            "id": 5
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Here is the proper version.  This one is essentially the Lucene-n-gram-analyzer-specific Adam's patch from SOLR-81 + some passing unit tests I wrote to exercise the new n-gram range functionality.\n\nI'll commit this by the end of the week unless Adam spots a bug.\n",
            "date": "2007-02-21T14:23:07.065+0000",
            "id": 6
        },
        {
            "author": "Otis Gospodnetic",
            "body": "In SVN.\n",
            "date": "2007-03-01T14:23:23.663+0000",
            "id": 7
        },
        {
            "author": "Doron Cohen",
            "body": "I have two comments/questions on the n-gram tokenizers:\n\n(1) Seems that only the first 1024 characters of the input are handled, and the rest is ignored (and I think as result the input stream would remain dangling open). \n\nIf you add this test case:\n\n    /**\n     * Test that no ngrams are lost, even for really long inputs\n     * @throws EXception\n     */\n    public void testLongerInput() throws Exception {\n      int expectedNumTokens = 1024;\n      int ngramLength = 2;\n      // prepare long string\n      StringBuffer sb = new StringBuffer();\n      while (sb.length()<expectedNumTokens+ngramLength-1) \n        sb.append('a');\n      \n      StringReader longStringReader = new StringReader (sb.toString());\n      NGramTokenizer tokenizer = new NGramTokenizer(longStringReader, ngramLength, ngramLength);\n      int numTokens = 0;\n      Token token;\n      while ((token = tokenizer.next())!=null) {\n        numTokens++;\n        assertEquals(\"aa\",token.termText());\n      }\n      assertEquals(\"wrong number of tokens\",expectedNumTokens,numTokens);\n    }\n\nWith expectedNumTokens = 1023 it would pass, but any larger number would fail. \n\n(2) It seems safer to read the characters like this\n            int n = input.read(chars);\n            inStr = new String(chars, 0, n);\n(This way not counting on String.trim(), which does work, but worries me).\n\n",
            "date": "2007-03-01T22:49:44.669+0000",
            "id": 8
        },
        {
            "author": "Otis Gospodnetic",
            "body": "More goodies coming.",
            "date": "2007-03-02T18:13:49.616+0000",
            "id": 9
        },
        {
            "author": "Otis Gospodnetic",
            "body": "N-gram-producting TokenFilters for Karl's mom.\n",
            "date": "2007-03-02T18:17:16.966+0000",
            "id": 10
        },
        {
            "author": "Doron Cohen",
            "body": "Hi Otis, \n\n>  (and I think as result the input stream would remain dangling open)\n\nI take this part back - closing tokenStream would close the reader, and at least for the case that I thought of, invertDocument, the tokenStream is properly closed. \n\nCan you comment on the input length: is it correct to handle only the first 1024 characters?\n\nThanks,\nDoron",
            "date": "2007-03-03T04:01:42.875+0000",
            "id": 11
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Ah, didn't see your comments here earlier, Doron.  Yes, I think you are correct about the 1024 limit  - when I wrote that Tokenizer I was thinking TokenFilter, and thus I was thinking that that input Reader represents a Token, which was wrong.  So, I thought, \"oh, 1024 chars/token, that will be enough\".  I ended up needing TokenFilters for SOLR-81, so that's what I checked in.  Those operate on tokens and don't have the 1024 limitation.\n\nAnyhow, feel free to slap your test + the fix in and thanks for checking!\n",
            "date": "2007-03-03T16:44:38.038+0000",
            "id": 12
        },
        {
            "author": "Patrick Turcotte",
            "body": "Is it just me or are the NGramTokenFilter and EdgeNGramTokenFilter class not committed to SVN and not in the patch either?\n\nNGramTokenFilterTest and EdgeNGramTokenFilterTest are referring to them, but I can not seem to find them.\n\nThanks and keep the good work.\n\nPatrick",
            "date": "2007-03-08T04:35:08.429+0000",
            "id": 13
        },
        {
            "author": "Hoss Man",
            "body": "Otis's most recent attachment contains only tests .. but previous attachemnts had implementations.\n\nall of which have been commited under contrib/analyzers\n\n(tip: if you click \"All\" at the top of the list of comments in Jira, you see every modification related to this issue, including subversion commits that Jira detects are related to the issue based on the commit message)",
            "date": "2007-03-08T18:14:11.206+0000",
            "id": 14
        },
        {
            "author": "Hoss Man",
            "body": "Ack! ... i'm sorry i completely missread Patrick's question.\n\nngram *Tokenizers* have been commited -- but there are no ngram TokenFilters ... there are tests for TokenFilters Otis commited on March2, but those tests do't currently compile/run without the TokenFilter's themselves.\n\nOtis: do you have these TokenFilter's in your working directory that you perhaps forgot to svn add before committing?",
            "date": "2007-03-08T21:43:37.525+0000",
            "id": 15
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Oh, look at that!\n\n[otis@localhost contrib]$ svn st\nA      analyzers/src/java/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.java\nA      analyzers/src/java/org/apache/lucene/analysis/ngram/NGramTokenFilter.java\n\n:)\nIt's in the repo now.  Sorry about that!\n",
            "date": "2007-03-13T00:31:10.685+0000",
            "id": 16
        },
        {
            "author": "Daniel Naber",
            "body": "Can this issue be closed or is there anything still open?\n",
            "date": "2007-06-01T18:53:51.683+0000",
            "id": 17
        },
        {
            "author": "Otis Gospodnetic",
            "body": "This should have been marked Fixed a while back.",
            "date": "2007-07-13T13:54:03.287+0000",
            "id": 18
        }
    ],
    "component": "modules/analysis",
    "description": "It would be nice to have some n-gram-capable tokenizers in contrib/analyzers.  Patch coming shortly.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-759",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Add n-gram tokenizers to contrib/analyzers",
    "systemSpecification": true,
    "version": ""
}