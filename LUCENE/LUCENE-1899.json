{
    "comments": [
        {
            "author": "Shai Erera",
            "body": "I don't know about doubling the array size every time. There's ArrayUtil.getNextSize (a Lucene class) which seems to grow arrays in a mild fashion. the method is well documented, and I think it should be used by ensureCapacityWords.",
            "date": "2009-09-08T13:41:43.432+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "bq. There's ArrayUtil.getNextSize (a Lucene class) which seems to grow arrays in a mild fashion. the method is well documented, and I think it should be used by ensureCapacityWords.\n\n+1",
            "date": "2009-09-08T14:13:54.633+0000",
            "id": 1
        },
        {
            "author": "Nadav Har'El",
            "body": "Hi Shai, I guess you're right that if there's such a utility function, we should probably use it.\nI just wonder what is the rationale behind the specific formula in this function - basically newsize = oldsize * 1.125 + 6. This formula ensures that at worst case, just 6% of the array space is wasted (instead of 50% in the doubling approach), but the number of reallocations and copies is 8 times higher, and performance is proportionally slower (although obviously, both are linear in amortized time - which the current code isn't). Was there any thought given to why the factor 0.125 is better than 0.25, 0.5, 0.01 or 1.0? I'm not saying that 1.0 (doubling) is best, just that I don't know why 0.125 is.",
            "date": "2009-09-09T06:28:45.632+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "bq. This formula ensures that at worst case, just 6% of the array space is wasted\n\nYou mean 12.5% right?\n\nbq. I just wonder what is the rationale behind the specific formula in this function\n\nIt's just a standard time/space tradeoff, that favors not wasting too much space.  This code was \"borrowed\" from Python's \"listobject.c\" sources, ie, it governs how Python over-allocates the storage for its list type.\n\nWe could explore different constants though I'd be nervous about making this value much higher.  Often the consumer of this API will see rapid growth initially, and then the collection stops growing and is re-used for a long time, in which case the long-term wasted RAM is (I think) more important than the one-time short-term CPU cost of finding the \"natural\" size.",
            "date": "2009-09-09T09:20:09.490+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "Attached trivial patch.  I plan to commit soon...",
            "date": "2009-09-09T09:35:49.271+0000",
            "id": 4
        },
        {
            "author": "Nadav Har'El",
            "body": "Yes, you're right, 12.5%. Or actually, 11%=0.125/(1+0.125) of the space after an elargment is wasted. I don't know where I got this 6% from ;-)",
            "date": "2009-09-09T10:36:47.858+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Nadav!",
            "date": "2009-09-09T11:52:05.255+0000",
            "id": 6
        },
        {
            "author": "Uwe Schindler",
            "body": "I wanted to add just one comment:\nFor nomal Lucene usage, the auto-grow of the array is not needed. All internal Lucene code (collectors, filters) use IndexReader.maxDoc() as initial size param to the ctor. If you write own HitCollectors/Collectors(2.9), use the maxDoc() of the current IndexReader. With the new 2.9 Collectors, you would initialize the OpenBitSet in Collector.setNextReader().",
            "date": "2009-09-09T11:58:51.286+0000",
            "id": 7
        }
    ],
    "component": "core/store",
    "description": "Hi, I found a potentially serious efficiency problem with OpenBitSet.\n\nOne typical (I think) way to build a bit set is to set() the bits one by one -\ne.g., have a HitCollector set() the bit for each matching document.\nThe underlying array of longs needs to grow as more as more bits are set, of\ncourse.\n\nBut looking at the code, it appears to me that the array grows very\nineefficiently - in the worst case (when doc ids are sorted, as they would\nnormally be in the HitCollector case for example), copying the array again\nand again for every added bit... The relevant code in OpenBitSet.java is:\n\n  public void set(long index) {\n    int wordNum = expandingWordNum(index);\n    ...\n  }\n\n  protected int expandingWordNum(long index) {\n    int wordNum = (int)(index >> 6);\n    if (wordNum>=wlen) {\n      ensureCapacity(index+1);\n    ...\n  }\n  public void ensureCapacityWords(int numWords) {\n    if (bits.length < numWords) {\n      long[] newBits = new long[numWords];\n      System.arraycopy(bits,0,newBits,0,wlen);\n      bits = newBits;\n    }\n  }\n\nAs you can see, if the bits array is not long enough, a new one is\nallocated at exactly the right size - and in the worst case it can grow\njust one word every time...\n\nShouldn't the growth be more exponential in nature, e.g., grow to the maximum\nof index+1 and twice the existing size?\n\nAlternatively, if the growth is so inefficient, this should be documented,\nand it should be recommended to use the variant of the constructor with the\ncorrect initial size (e.g., in the HitCollector case, the number of documents\nin the index). and the fastSet() method instead of set().\n\nThanks,\nNadav.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1899",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "BUG",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Inefficient growth of OpenBitSet",
    "systemSpecification": true,
    "version": "2.9"
}