{
    "comments": [
        {
            "author": "Steven Parkes",
            "body": "Here's a first cut at a factored merge policy.\n\nIt's not polished. Sparsely commented and there are probably a few changes that should be backed out.\n\nIt factors a merge policy interface out of IndexWriter and creates an implementation of the existing merge policy.\n\nActually, it's a tweak on the existing merge policy. Currently the merge policy is implemented in ways that assume certain things about the existing list of segments. The factored version doesn't make these assumptions. It simplifies the interface but I'm not yet sure if there are bad side effects. Among other things I want to run performance tests.\n\nThere is part of a pass at a concurrent version of the current merge policy. It's not complete. I've been pushing it to see if I understand the issues around concurrent merges. Interesting topics are 1) how to control the merges 2) how/when to cascade merges if they are happening in a parallel and 3) how to handle synchronization of IndexWriter#segmentInfos. That last one in particular is a bit touchy.\n\nI did a quick implementation of KS's fib merge policy but it's incomplete in that IndexWriter won't merge non-contiguous segment lists, but I think I can fix that fairly easily with no major side effects. The factored merge policy makes this plug in pretty clean ...",
            "date": "2007-03-23T19:24:21.972+0000",
            "id": 0
        },
        {
            "author": "Doug Cutting",
            "body": "How public should such an API be?  Should the interface be public?  Should the implementations?  The most conservative approach would be to make it all package private, to give more leeway for evolving the update API.  But that also decreases the utility.  Thoughts?",
            "date": "2007-03-23T20:25:51.458+0000",
            "id": 1
        },
        {
            "author": "Steven Parkes",
            "body": "Visibility is one of those things I haven't cleaned up yet.\n\nClient code is gonna want to create and set merge policies. And it'll want to set \"external\" merge policy parameters. That's all probably not controversial.\n\nAs for other stuff, I tend to leave things open, but I know that's debatable and don't have a strong opinion in this case.\n\nIn fact, there a few things here that are fairly subtle/important. The relationship/protocol between the writer and policy is pretty strong. This can be seen in the strawman concurrent merge code where the merge policy holds state and relies on being called from a synchronized writer method.   If that goes forward anything like it is, it would argue for tightening that api up some. Chris suggested a way to make the writer<>polcy relationship \"atomic\". I didn't add the code (yet) but I'm not against it.\n\n\n\n\n\n",
            "date": "2007-03-23T20:45:01.600+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "Steven, I looked through the patch quickly.  It looks great!  First\nsome general comments and then I'll add more specifics as\nseparate comments.\n\nCan you open separate issues for the other new and interesting merge\npolicies here?  I think the refactoring of merge policy plus creation\nof the default policy that is identical to today's merge policy, which\nshould be a fairly quick and low-risk operation, would then remain\nunder this issue?\n\nThen, iterating / vetting / debugging the new interesting merge\npolicies can take longer under their own separate issues and time\nframe.\n\nOn staging I think we could first do this issue (decouple MergePolicy\nfrom writer), then LUCENE-845 because it blocks LUCENE-843 (which\nwould then be fixing LogarithmicMergePolicy to use segment sizes\ninstead of docs counts as basis for determing levels) then LUCENE-843\n(performance improvements for how writer uses RAM)?\n\n\n",
            "date": "2007-03-25T12:46:09.520+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "My first comment, which I fear will be the most controversial feedback\nhere :), is a whitespace style question: I'm not really a fan of\n\"cancerous whitespace\" where every ( [ etc has its own whitespace\naround it.\n\nI generally prefer minimal whitespace within reason (ie as long as it\ndoes not heavily hurt readability).  The thing is I don't know that\nLucene has settled on this / if anyone else shares my opinion?  It\ndoes seem that \"two space indentation\" is the standard, which I agree\nwith, but I don't know if whitespace style has otherwise been agreed\non?  Many people will say it's unimportant to agree on whitespace but\nI feel it's actually quite important.\n",
            "date": "2007-03-25T12:46:33.934+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "OK some specific comments, only on the refactoring (ie I haven't\nreally looked at the new merge policies yet):\n\n  * I think maxBufferedDocs should not be exposed in any *MergePolicy\n    classes or interfaces?  I'm planning on deprecating this param\n    with LUCENE-843 when we switch by default to \"buffering by RAM\n    usage\" and it really relates to \"how/when should writer flush its\n    RAM buffer\".\n\n  * I also think \"minMergeDocs\" (which today is one and the same as\n    maxBufferedDocs in IndexWriter but conceptually could be a\n    different configuration) also should not appear in the MergePolicy\n    interface.  I think it should only appear in\n    LogarithmicMergePolicy?\n\n    If we remove these from the MergePolicy interface then maybe we\n    don't need MergePolicyBase?  (Just to makes things simpler).\n\n  * I think we should not create a LegacyMergePolicy interface.  But I\n    realize you need this so the deprecated methods in IndexWriter\n    (setMergeFactor, setMaxBufferedDocs, setMaxMergeDocs, etc.) can be\n    implemented.  How about instead these methods will only work if\n    the current merge policy is the LogarithmicMergePolicy?  You can\n    check if the current mergePolicy is an instanceof\n    LogarithmicMergePolicy and then throw eg an IllegalStateException\n    if it's not?\n\n    Ie, going forward, with new and interesting merge policies,\n    developers should interact with their merge policy to configure\n    it.\n\n  * I was a little spooked by this change to TestAddIndexesNoOptimize:\n\n      -    assertEquals(2, writer.getSegmentCount());\n      +    assertEquals(3, writer.getSegmentCount());\n\n    I think with just the refactoring, there should not need to be any\n    changes to unit tests right?\n\n  * It's interesting that you've pulled \"useCompoundFile\" into the\n    LegacyMergePolicy.  I'm torn on whether it belongs in MergePolicy\n    at all, since this is really a file format issue?\n\n    For example, newly written segments (no longer a \"merge\" with\n    LUCENE-843) must also know whether to write in compound file\n    format.  If we make interesting file format changes in the future\n    that are configurable by the developer we wouldn't want to change\n    all MergePolicy classes to propogate that.  It feels like using\n    compound file or not should remain only in IndexWriter?\n",
            "date": "2007-03-25T12:47:27.334+0000",
            "id": 5
        },
        {
            "author": "Ning Li",
            "body": "Here is a patch for concurrent merge as discussed in:\nhttp://www.gossamer-threads.com/lists/lucene/java-dev/45651?search_string=concurrent%20merge;#45651\n\nI put it under this issue because it helps design and verify a factored merge policy which would provide good support for concurrent merge.\n\nAs described before, a merge thread is started when a writer is created and stopped when the writer is closed. The merge process consists of three steps: first, create a merge task/spec; then, carry out the actual merge; finally, \"commit\" the merged segment (replace segments it merged in segmentInfos), but only after appropriate deletes are applied. The first and last steps are fast and synchronous. The second step is where concurrency is achieved. Does it make sense to capture them as separate steps in the factored merge policy?\n\nAs discussed in http://www.gossamer-threads.com/lists/lucene/java-dev/45651?search_string=concurrent%20merge;#45651: documents can be buffered while segments are merged, but no more than maxBufferedDocs can be buffered at any time. So this version provides limited concurrency. The main goal is to achieve short ingestion hiccups, especially when the ingestion rate is low. After the factored merge policy, we could provide different versions of concurrent merge policies which provide different levels of concurrency. :-)\n\nAll unit tests pass. If IndexWriter is replaced with IndexWriterConcurrentMerge, all unit tests pass except the following:\n  - TestAddIndexesNoOptimize and TestIndexWriter*\n    This is because they check segment sizes expecting all merges are done. These tests pass if these checks are performed after the concurrent merges finish. The modified tests (with waits for concurrent merges to finish) are in TestIndexWriterConcurrentMerge*.\n  - testExactFieldNames in TestBackwardCompatibility and testDeleteLeftoverFiles in TestIndexFileDeleter\n    In both cases, file name segments_a is expected, but the actual is segments_7. This is because with concurrent merge, if compound file is used, only the compound version is \"committed\" (added to segmentInfos), not the non-compound version, thus the lower segments generation number.\n\nCheers,\nNing\n",
            "date": "2007-03-29T03:21:07.530+0000",
            "id": 6
        },
        {
            "author": "Steven Parkes",
            "body": "Here are some numbers comparing the load performance for the factored vs. non-factored merge policies.\n\nThe setup uses enwiki, loads 200K documents, and uses 4 different combinations of maxBufferedDocs and mergeFactor (just the default from the standard benchmark, not because I necessarily thought it was a good idea.)\n\nThe factored merge policy seems to be on the order of 1% slower loading than the non-factored version ... and I'm not sure why, so I'm going to check into this. The factored version does more examination of segment list than the non-factored version, so there's compute overhead, but I would expect that to be swamped by I/O Maybe that's not a good assumption? Or it might be doing different merges for reasons I haven't considered, which I'm going to check.\n\nRelating this to some of the merge discussions, I'm going to look at monitoring both the number of merges taking place and the size of those merges. I think that's helpful in understand different candidate merge policies, in addition to absolute difference in runtime.\n\nI also think histogramming  the per-doc cost would also be interesting, since mitigating the long delay at cascading merges is at least one goal of a concurrent merge policy.\n\nAnd all this doesn't even consider testing the recent stuff on merging multiple indexes. That's an area where the factored merge policy differs (because of the simpler interface.)\n\nI'm curious if anyone is surprised by these numbers, the 60 docs/sec, in particular. This machine is a dual dual-core xeon, writing to a single local disk.  My dual opty achieved ~85-100 docs/sec on a three disk SATA3 RAID5 array.\n\nNon-factored (current) merge policy\n\n     [java] ------------> Report sum by Prefix (MAddDocs) and Round (8 about 8 out of 33)\n     [java] Operation       round mrg buf   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem\n     [java] MAddDocs_200000     0  10  10        1       200000         41.6    4,804.11    11,758,928     12,591,104\n     [java] MAddDocs_200000 -   1 100  10 -  -   1 -  -  200000 -  -  - 50.0 -  4,000.25 -  34,831,992 -   52,563,968\n     [java] MAddDocs_200000     2  10 100        1       200000         49.9    4,004.95    42,158,232     60,444,672\n     [java] MAddDocs_200000 -   3 100 100 -  -   1 -  -  200000 -  -  - 57.9 -  3,455.97 -  45,646,680 -   61,083,648\n     [java] MAddDocs_200000     4  10  10        1       200000         44.9    4,458.66    36,928,616     61,083,648\n     [java] MAddDocs_200000 -   5 100  10 -  -   1 -  -  200000 -  -  - 50.4 -  3,965.98 -  47,855,064 -   61,083,648\n     [java] MAddDocs_200000     6  10 100        1       200000         49.7    4,023.51    52,506,448     64,217,088\n     [java] MAddDocs_200000 -   7 100 100 -  -   1 -  -  200000 -  -  - 57.9 -  3,451.39 -  64,466,128 -   73,220,096\n\nFactored (new) merge policy\n\n     [java] ------------> Report sum by Prefix (MAddDocs) and Round (8 about 8 out of 33)\n     [java] Operation       round mrg buf   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem\n     [java] MAddDocs_200000     0  10  10        1       200000         41.4    4,828.25    10,477,976     12,386,304\n     [java] MAddDocs_200000 -   1 100  10 -  -   1 -  -  200000 -  -  - 50.4 -  3,968.27 -  38,333,544 -   46,170,112\n     [java] MAddDocs_200000     2  10 100        1       200000         50.3    3,973.52    33,539,824     63,860,736\n     [java] MAddDocs_200000 -   3 100 100 -  -   1 -  -  200000 -  -  - 58.6 -  3,413.87 -  44,580,528 -   87,781,376\n     [java] MAddDocs_200000     4  10  10        1       200000         45.3    4,411.50    57,850,104     87,781,376\n     [java] MAddDocs_200000 -   5 100  10 -  -   1 -  -  200000 -  -  - 51.0 -  3,921.48 -  62,793,432 -   87,781,376\n     [java] MAddDocs_200000     6  10 100        1       200000         50.4    3,969.87    49,625,496     93,966,336\n     [java] MAddDocs_200000 -   7 100 100 -  -   1 -  -  200000 -  -  - 58.7 -  3,409.51 -  68,100,288 -  129,572,864\n",
            "date": "2007-04-19T22:41:58.601+0000",
            "id": 7
        },
        {
            "author": "Steven Parkes",
            "body": "Here's an update to the patch. I wouldn't say it's ready to be committed, but I think it's significantly closer than it was.\n\nThe concurrent and other misc. stuff have been pulled out (that part still needs work, figuring out how to get the concurrency right.)\n\nThe new patch works against trunk, which means it handles docswriter and is more compatible with merging by # of docs or merging by ram (or size, to be more accurate?)\n\nMy take on the migration path here was that we could well be going towards merging by size but need to keep merging by # docs for parallel index cases. The current patch still only does merging by # docs.\n\nI think I commented on a couple of other things dev, but to reiterate:\n\nThere's a small change in the test results because the new merge policy simplifies the treatatement of addIndexes operations. The change is understood and shouldn't be a problem.\n\nuseCompoundFile is delegated to the merge policy so a smart merge policy could make decisions looking at the state of all segments rather than all-or-nothing. There are a couple of fixme's in IndexWriter related to this and the segments being created by the docswriter.\n\nI'm going to look at that, plus the concurrent stuff: Ning's stuff plus by old approach (which has to change, given the new docswriter stuff).",
            "date": "2007-08-06T20:11:28.002+0000",
            "id": 8
        },
        {
            "author": "Steven Parkes",
            "body": "For the time being, the patch also contains some of the code from LUCENE-971 since that's how I test it.",
            "date": "2007-08-06T20:25:56.792+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "This looks great Steve!\n\nMore specific feeedback soon, but ... in thinking about concurrency\n(and from reading your comments about it in LogDocMergePolicy), I\nthink we ideally would like concurrency to be fully independent of the\nmerge policy.\n\nIe, just like you can take any shell command and choose to run it in\nthe background by sticking an \"&\" on the end, I should be able to take\nmy favorite MergePolicy instance X and \"wrap\" it inside a \"concurrent\nmerge policy wrapper\".  Eg, instantiate ConcurrentMergePolicy(X), and\nthen ConcurrentMergePolicy would take the merges requested by X and do\nthem in the background.\n\nI think with one change to your MergePolicy API & control flow, we\ncould make this work very well: instead of requiring the MergePolicy\nto call IndexWriter.merge, and do the cascading, it should just return\nthe one MergeSpecification that should be done right now.  This would\nmean the \"MergePolicy.merge\" method would return null if no merge is\nnecessary right now, and would return a MergeSpecification if a merge\nis required.\n\nThis way, it is IndexWriter that would execute the merge.  When the\nmerge is done, IndexWriter would then call the MergePolicy again to\ngive it a chance to \"cascade\".  This simplifies the locking because\nIndexWriter can synchronize on SegmentInfos when it calls\n\"MergePolicy.merge\" and so MergePolicy no longer has to deal with this\ncomplexity (that SegmentInfos change during merge).\n\nThen, with this change, we could make a ConcurrentMergePolicy that\ncould (I think) easily \"wrap\" itself around another MergePolicy X,\nintercepting the calls to \"merge\".  When the inner MergePolicy wants\nto do a merge, the ConcurrentMergePolicy would in turn kick off that\nmerge in the BG but then return null to the IndexWriter allowing\nIndexWriter to return to its caller, etc.\n\nThen, this also simplifies MergePolicy implementations because you no\nlonger have to deal w/ thread safety issues around driving your own\nmerges, cascading merges, dealing with sneaky SegmentInfos changing\nwhile doing the merge, etc....\n",
            "date": "2007-08-07T14:40:06.369+0000",
            "id": 10
        },
        {
            "author": "Steven Parkes",
            "body": "\tI\n\tthink we ideally would like concurrency to be fully independent of the\n\tmerge policy.\n\nI thought of that, too, while taking a fresh look at things again. It's my current approach, though I'm not yet sure there won't be stumbling blocks. More soon, hopefully.\n\n\tI think with one change to your MergePolicy API & control flow, we\n\tcould make this work very well: instead of requiring the MergePolicy\n\tto call IndexWriter.merge, and do the cascading, it should just return\n\tthe one MergeSpecification that should be done right now.\n\nHmm ... interesting idea. I thought about it briefly, though I didn't pursue it (see below). It would end up changing the possible space of merge policies subtly. You \twouldn't be able to have any state in the algorithm. Arguably this is a good thing. There is also a bit more overhead, since starting the computation of potential merges from scratch each time could imply a little more computation, but I suspect this is not significant.\n\t\n\tWhen the inner MergePolicy wants\n\tto do a merge, the ConcurrentMergePolicy would in turn kick off that\n\tmerge in the BG but then return null to the IndexWriter allowing\n\tIndexWriter to return to its caller, etc.\n\nI'm a little unsure here. Are you saying the ConcurrentMergePolicy does the merges itself, rather than using the writer? That's going to mean a synchronization dance between the CMP and the writer. There's no question but that there has to be some synch dance, but my current thinking was to try to keep as cleanly within one class, IW, as I could.\n",
            "date": "2007-08-07T14:58:35.577+0000",
            "id": 11
        },
        {
            "author": "Michael McCandless",
            "body": "\nSome more feedback:\n\n  - Is the separate IndexMerger interface really necessary?  Can't we\n    just use IndexWriter directly?  It's somewhat awkward/forced now,\n    because the interface has getters for ramBufferSizeMB and\n    maxBufferedDocs that are really a \"writer\" (flushing) thing not a\n    \"merging\" thing.\n\n    While LogDocMergePolicy does need \"maxBufferedDocs\", I think,\n    instead, in IndexWriter's \"setMaxBufferedDocs()\" it should \"write\n    through\" to the LogDocMergePolicy if that is the merge policy in\n    use (and LogDocMergePolicy should store its own private\n    \"minMergeDocs\").\n\n    I think the three getters may not even be needed (based on\n    comments below), in which case it seems like we shouldn't be\n    creating a new interface.\n\n  - In LogDocMergePolicy, it seems like the checking that's done for\n    whether a SegmentInfo is in a different directory, as well as the\n    subsequent copy to move it over to the IndexWriter's directory,\n    should not live in the MergePolicy?\n\n    Otherwise, every MergePolicy is going to have to duplicate this\n    code.  Not to mention, we may someday create a more efficient way\n    to copy than running a single-segment merge (which is a very\n    inefficient, but, we only do it in addIndexes* I think).  I'd like\n    to capture this in one place (IndexWriter).\n\n    EG, the writer could have its own \"copyExternalSegments\" method\n    which is called in addIndexes* and also optimize(), after the\n    merge policy has done its merge, which does the check for wrong\n    directory and subsequent copy.\n\n    I think this would mean IndexMerger.getDirectory() is not really\n    needed.\n\n  - The \"checkOptimize\" method in LogDocMergePolicy seems like it\n    belongs back in IndexWriter: I think we don't want every\n    MergePolicy having to replicate that tricky while condition.\n\n    Maybe we could change MergePolicy.merge to take a boolean \"forced\"\n    which, if true, means that the MergePolicy must now pick a merge\n    even if it didn't think it was time to.  This would let us move\n    that tricky while condition logic back into IndexWriter.\n\n    Also, I think at some point we may want to have an optimize()\n    method that takes an int parameter stating the max # segments in\n    the resulting index.  This would allow you to optimize down to <=\n    N segments w/o paying full cost of a complete \"only one segment\"\n    optimize.  If we had a \"forced\" boolean then we could build such\n    an optimize method in the future, whereas as it stands now it\n    would not be so easy to retrofit previously created MergePolicy\n    classes to do this.\n\nAnd some more minor feedback:\n\n  - I love the simplification of addIndexesNoOptimize :)  Though (same\n    comment as above) I think we should move that final \"copy if\n    different directory\" step back in IndexWriter.\n\n  - There are some minor things that should not be committed eg the\n    added \"infoStream = null\" in IndexFileDeleter.  I typically try to\n    put a comment \"// nocommit\" above such changes as I make them to\n    remind myself and keep them from slipping in.\n\n  - In the deprecated IndexWriter methods you're doing a hard cast to\n    LogDocMergePolicy which gives a bad result if you're using a\n    different merge policy; maybe catch the class cast exception (or,\n    better, check upfront if it's an instanceof) and raise a more\n    reasonable exception if not?\n\n  - IndexWriter around line 1908 has for loop that has commented out\n    \"System.err.println\"; we should just comment out/remove for loop\n\n  - These commented out synchronized spook me a bit:\n\n      /* synchronized(segmentInfos) */ {\n\n    Are they needed in these contexts?  Is this only once we have\n    concurrent merging?  (This ties back to the first feedback to\n    simplify MergePolicy API so that this kind of locking is only\n    needed inside IndexWriter).\n\n  - Can we support non-contiguous merges?  If I understand it\n    correctly, the MergeSpecification can express such a merge, it's\n    just that the current IndexMerger (IndexWriter) cannot execute it,\n    right?  So we are at least not precluding fixing this in the\n    future.\n\n  - It confuses me that MergePolicy has a method \"merge(...)\" -- can\n    we rename it to \"maybeMerge(..)\" or \"checkForMerge(...)\"?  Ie,\n    this method determines whether a merge is necessary and, if so, it\n    then asks IndexMerger to in fact execute the merge (or, returns\n    the spec)?\n\n  - Instead of IndexWriter.releaseMergePolicy() can we have\n    IndexWriter only close the merge policy if it was the one that had\n    created it?  (Similar to how IndexWriter closes the dir if it has\n    opened it from a String or File, but does not close it if it was\n    passed in).\n\n",
            "date": "2007-08-07T16:34:44.630+0000",
            "id": 12
        },
        {
            "author": "Michael McCandless",
            "body": "> > I think we ideally would like concurrency to be fully independent of\n> > the merge policy.\n>\n> I thought of that, too, while taking a fresh look at things\n> again. It's my current approach, though I'm not yet sure there won't\n> be stumbling blocks. More soon, hopefully.\n\nWell I think the current MergePolicy API (where the \"merge\" method\ncalls IndexWriter.merge itself, must cascade itself, etc.) makes it\nhard to build a generic ConcurrentMergePolicy \"wrapper\" that you could\nuse to make any MergePolicy concurrent (?).  How would you do it?\n\nEG I'm working on a new MergePolicy for LUCENE-845, which would be\nnice to run concurrently, but I'd really rather not have to figure out\nhow to build my own concurrency/locking/etc in it.  Ideally\n\"concurrency\" is captured as a single wrapper class that we all can\nre-use on top of any MergePolicy.  I think we can do that with the\nproposed simplification.\n\n> > I think with one change to your MergePolicy API & control flow, we\n> > could make this work very well: instead of requiring the MergePolicy\n> > to call IndexWriter.merge, and do the cascading, it should just\n> > return the one MergeSpecification that should be done right now.\n\n> Hmm ... interesting idea. I thought about it briefly, though I\n> didn't pursue it (see below). It would end up changing the possible\n> space of merge policies subtly. You wouldn't be able to have any\n> state in the algorithm. Arguably this is a good thing. There is also\n> a bit more overhead, since starting the computation of potential\n> merges from scratch each time could imply a little more computation,\n> but I suspect this is not significant.\n\nI think you can still have state (as instance variables in your\nclass)?  How would this simplification restrict the space of merge\npolicies?\n\n> > When the inner MergePolicy wants to do a merge, the\n> > ConcurrentMergePolicy would in turn kick off that merge in the BG but\n> > then return null to the IndexWriter allowing IndexWriter to return to\n> > its caller, etc.\n>\n> I'm a little unsure here. Are you saying the ConcurrentMergePolicy\n> does the merges itself, rather than using the writer? That's going\n> to mean a synchronization dance between the CMP and the\n> writer. There's no question but that there has to be some synch\n> dance, but my current thinking was to try to keep as cleanly within\n> one class, IW, as I could.\n\nOh, no: ConcurrentMergePolicy would still call IndexWriter.merge(spec),\njust with a separate thread.  And so all synchronization required is\nstill inside IndexWriter (I think?).\n\nIn fact, if we stick with the current MergePolicy API, aren't you\ngoing to have to put some locking into eg the LogDocMergePolicy when\nconcurrent merges might be happening?  With the new approach,\nIndexWriter could invoke MergePolicy.merge under a\n\"synchronized(segmentInfos)\", and then each MergePolicy doesn't have\nto deal with locking at all.\n",
            "date": "2007-08-07T16:47:06.358+0000",
            "id": 13
        },
        {
            "author": "Steven Parkes",
            "body": "    Is the separate IndexMerger interface really necessary?\n\nI wrestled with this, so in the past, it wouldn't have taken much to convince me otherwise. The reason for the extra interface is I was hoping to discourage or create a little extra friction for merge policies in terms of looking too much into the internals of IndexWriter.\n\nAs an example, it's not a good idea for merge policies to be able to access IndexWriter#segmentInfos directly. (That's a case I would like to solve by making segmentInfos private, but I haven't looked at that completely yet and even beyond that case, I'd still like merge policies to have a very clean interface with IWs.)\n\nIt feels kinda weird for me to be arguing for constraints since I work mostly in dynamic languages that have none of this stuff. But it reflects my goal that merge policies simply be algorithms, not real workers.\n\nMoreover, I think it may be useful for implementing concurrency (see below).\n\n    While LogDocMergePolicy does need \"maxBufferedDocs\", I think,\n    instead, in IndexWriter's \"setMaxBufferedDocs()\" it should \"write\n    through\" to the LogDocMergePolicy if that is the merge policy in\n    use (and LogDocMergePolicy should store its own private\n    \"minMergeDocs\").\n\nThe thing here is that the merge policy has nothing to do with max buffered docs, right? The code for buffering docs for the first segment is wholly in the IndexWriter. LogDocMergePolicy happens to need it (in its current incarnation) in order to handle the way the log levels are computed. This could, of course, be changed. There's nothing that says a merge policy has to look at these values, just that they're available should the merge policy want to look.\n\nI guess my idea was that the index writer was responsible for handling the initial segment (with DocsWriter, if it wants) and also to provide an indication to the merge policies how it was handling them.\n\nIt's possible to have the merge policy influence the first segment size but that opens up a lot of issues. Does every merge policy then have to know how to trade between buffering by doc count and buffering by ram? I was hoping to avoid that.\n\nI'm not all that happy with this the way it is, but supporting both by-doc and by-ram is messy but seems necessary. This was my take on making it least messy?\n\n    In LogDocMergePolicy, it seems like the checking that's done for\n    whether a SegmentInfo is in a different directory, as well as the\n    subsequent copy to move it over to the IndexWriter's directory,\n    should not live in the MergePolicy?\n\nI see two parts to this.\n\nFirst, I hesitate to make it not possible for merge policy to see the directory information, i.e., remove IndexMerger#getDirectory(). Copying a segment is one way to get it into the current directory, but merging with other segments does to. A merge policy could decide to go ahead and merge a bunch of segments that are in other directories rather than just copy them individually. Taking away getDirectory() removes that option.\n\nAs to how to handle the case where single segments are copied, I guess my main reason for leaving that in the merge policy would be for simplicity. Seems nicer to have all segment amalgamation management in one place, rather than some in one class and some in another. Could be factored into an optional base merge policy for derived classes to use as they might like.\n\n    The \"checkOptimize\" method in LogDocMergePolicy seems like it\n    belongs back in IndexWriter: I think we don't want every\n    MergePolicy having to replicate that tricky while condition.\n\nThe reason for not doing this was I can imagine different merge policies having a different model of what optimize means. I can imagine some policies that would not optimize everything down to a single segment but instead obeyed a max segment size. But we could factor the default conditional into an optional base class, as above.\n\nIt is an ugly conditional that there might be better way to formulate, so that policies don't have to look at the grody details like hasSeparateNorms. But I'd still like the merge policies to be able to decide what optimize means at a high level.\n\n    Maybe we could change MergePolicy.merge to take a boolean \"forced\"\n    which, if true, means that the MergePolicy must now pick a merge\n    even if it didn't think it was time to.  This would let us move\n    that tricky while condition logic back into IndexWriter.\n\nI didn't follow this. forced == optimize? If not, what does pick a merge mean? Not sure what LogDoc should do for merge(force=true) if it thinks everything is fine?\n\n    Also, I think at some point we may want to have an optimize()\n    method that takes an int parameter stating the max # segments in\n    the resulting index.\n\nI think this is great functionality for a merge policy, but what about just making that part of the individual merge policy interface, rather than linked to IndexWriter? That was one goal of making a factored merge policy: that you could do these tweaks without changing IndexWriter.\n\n    This would allow you to optimize down to <=\n    N segments w/o paying full cost of a complete \"only one segment\"\n    optimize.  If we had a \"forced\" boolean then we could build such\n    an optimize method in the future, whereas as it stands now it\n    would not be so easy to retrofit previously created MergePolicy\n    classes to do this.\n\nI haven't looked at how difficult it would be to make LogDoc sufficiently derivable to allow a derived class to add this tweak. If I could, would it be enough?\n\n    There are some minor things that should not be committed eg the\n    added \"infoStream = null\" in IndexFileDeleter.  I typically try to\n    put a comment \"// nocommit\" above such changes as I make them to\n    remind myself and keep them from slipping in.\n\nYou're right and thanks for the idea. Obvious now.\n\n    In the deprecated IndexWriter methods you're doing a hard cast to\n    LogDocMergePolicy which gives a bad result if you're using a\n    different merge policy; maybe catch the class cast exception (or,\n    better, check upfront if it's an instanceof) and raise a more\n    reasonable exception if not?\n\nAgreed.\n\n    IndexWriter around line 1908 has for loop that has commented out\n    \"System.err.println\"; we should just comment out/remove for loop\n\nThe whole loop will be gone before commit but I didn't want to delete it yet since I might need to turn it back on for more debugging.  It should, of course, have a \"// nocommit\" comment.\n\n    These commented out synchronized spook me a bit:\n\n      /* synchronized(segmentInfos) */ {\n\nThis is from my old code. I left it in there as a hint as I work on the concurrent stuff again. It's only a weak hint, though, because things have changed a lot since that code was even partially functional. Ignore it. It won't go into the serial patch and anything for LUCENE-870 will have to have its own justification.\n\n    Can we support non-contiguous merges?  If I understand it\n    correctly, the MergeSpecification can express such a merge, it's\n    just that the current IndexMerger (IndexWriter) cannot execute it,\n    right?  So we are at least not precluding fixing this in the\n    future.\n\nAs far as I've seen so far, there are no barriers to non-contiguous merges. Maybe something will crop up that is, but in what I've done, I haven't seen any.\n\n    It confuses me that MergePolicy has a method \"merge(...)\" -- can\n    we rename it to \"maybeMerge(..)\" or \"checkForMerge(...)\"?\n\nI suppose. I'm not a big fan of the \"maybeFoo\" style of naming. I think of \"merge\" like \"optimize\": make it so / idempotent. But I'm certainly willing to write whatever people find clearest. \n\n    Instead of IndexWriter.releaseMergePolicy() can we have\n    IndexWriter only close the merge policy if it was the one that had\n    created it?  (Similar to how IndexWriter closes the dir if it has\n    opened it from a String or File, but does not close it if it was\n    passed in).\n\nThis precludes\n\n\tiw.setMergePolicy(new MyMergePolicy(...));\n      ...\n\tiw.close();\n\nYou're always going to have to\n\n\tMergePolicy mp = new MyMergePolicy(...);\n\tiw.setMergePolicy(mp);\n      ...\n      iw.close();\n      mp.close();\n\nThe implementation's much cleaner using the semantics you describe, but I was thinking it'd be better to optimize for the usability of the common client code case?\n\t\n\tWell I think the current MergePolicy API (where the \"merge\" method\n\tcalls IndexWriter.merge itself, must cascade itself, etc.) makes it\n\thard to build a generic ConcurrentMergePolicy \"wrapper\" that you could\n\tuse to make any MergePolicy concurrent (?).  How would you do it?\n\nI really haven't had time to go heads down on this (the old concurrent merge policy was a derived class rather than a wrapper class). But I was thinking that perhaps ConurrentMergePolicy would actually wrap IndexWriter as well as the serial merge policy, i.e., implement IndexMerger (my biggest argument for IM at this point). But I haven't looked deeply at whether this will work but I think it has at least a chance.\n\nI should know more about this is a day or two.\n\n\tI think you can still have state (as instance variables in your\n\tclass)?  How would this simplification restrict the space of merge\n\tpolicies?\n\ns/state/stack state/. Yeah, you can always unwind your loops and lift your recursions, put all that stack state into instance variables. But, well, yuck. I'd like to make it easy to write simple merge policies and take up the heavy lifting elsewhere. Hopefully there will be more merge policies than index writers.\n\n\tOh, no: ConcurrentMergePolicy would still call IndexWriter.merge(spec),\n\tjust with a separate thread.  And so all synchronization required is\n\tstill inside IndexWriter (I think?).\n\nThat's my idea.\n\nThe synchronization is still tricky, since parts of segmentInfos are getting changed at various times and there are references and/or copies of it other places. And as Ning pointed out to me, we also have to deal with buffered delete terms. I'd say I got about 80% of the way there on the last go around. I'm hoping to get all the way this time.\n\n\tIn fact, if we stick with the current MergePolicy API, aren't you\n\tgoing to have to put some locking into eg the LogDocMergePolicy when\n\tconcurrent merges might be happening?\n\nYes and no. If CMP implements IndexMerger, I think we might be okay? In the previous iteration, I used derivation so that ConcurrentLogDocMergePolicy derived from the serial version but had the necessary threading. I agree that a wrapper is better solution if it can be made to work.",
            "date": "2007-08-07T17:54:53.037+0000",
            "id": 14
        },
        {
            "author": "Steven Parkes",
            "body": "On a related note, Mike, there a few FIXME's in IW related to useCompoundFile: it doesn't exist in IW anymore (other than as a deprecated feature). The goal was to allow merge policies to decide when to use compound files, perhaps on a segment-by-segment basis. That all works fine for merge operations.\n\nBut there's also the case of new segments, what format they should be in. These are segments that are going to be created by IW (via DocsWriter?) My stab at this was to have IW ask the merge policy. Since this isn't a merge, per say, the IW passes to the merge policy the current set of segment infos and the new segment info, asking what format the new segment info should use. So MergePolicy has\n\n\tboolean useCompoundFile(SegmentInfos segments, SegmentInfo newSegment);\n\nLooking at IW, with the new DocsWriter stuff, it looks like there isn't a segmentInfo object for the new segment at the time the predicate is being called. Would it be possible to make one? Something like DocumentsWriter#getDocStoreSegmentInfo() analogous to DocumentsWriter#getDocStoreSegment()? It could be an object just thrown away.\n\nIs this a bad idea?",
            "date": "2007-08-07T18:35:49.915+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "> Looking at IW, with the new DocsWriter stuff, it looks like there\n> isn't a segmentInfo object for the new segment at the time the\n> predicate is being called. Would it be possible to make one?\n> Something like DocumentsWriter#getDocStoreSegmentInfo() analogous to\n> DocumentsWriter#getDocStoreSegment()? It could be an object just\n> thrown away.\n\nHmmm: it looks like you're calling\n\"mergePolicy.useCompoundFile(segmentInfos,null)\" twice: once inside\nflushDocStores and immediately thereafter when flushDocStores returns\ninto the flush() code.  Maybe you should change flushDocStores to\nreturn whether or not the flushed files are in compound format\ninstead?\n\nSince flushDocStores() is flushing only the \"doc store\" index files\n(stored fields & term vectors) it's not a real \"segment\" so it's a\nsomewhat forced fit to make a SegmentInfo in this case.  I guess we\ncould make a \"largely empty\" SegmentInfo and fill in what we can, but\nthat's somewhat dangerous (eg methods like files() would have to be\nfixed to deal with this).\n\nMaybe, instead, we could use one of the SegmentInfo instances from a\nsegment that refers to this doc store segment?  This would just mean\nstepping through all SegmentInfo's and finding the first one (say)\nwhose docStoreSegment equals the one we are now flushing?  Still it\nwould be good to differentiate this case (creating compound file for\ndoc store files vs for a real segment) to MergePolicy somehow (maybe\nadd a boolean arg \"isDocStore\" or some such?).",
            "date": "2007-08-07T19:47:51.774+0000",
            "id": 16
        },
        {
            "author": "Steven Parkes",
            "body": "Ah. I understand better now. I have to admit, I haven't kept up to date on some of the deeper file stuff in LUCENE-843.\n\nIt seems to me there's quite a bit of difference between segment files and doc store files. Since doc store files can be referred to by multiple segments, they can get quite large. I don't have any trouble imaging that a merge policy might want to CFS 10MB segments but not 10GB doc stores?\n\nI'm thinking maybe a MergePolicy#useCompoundDocStore( SegmentInfos ) makes sense? The naive case would still just use the static setting we have now, but we could think about a better implementation:\n\n- Maybe never cfs doc store files? Is that an unreasonable default? It just strikes me that there should be far fewer of these so that we don't need to and on the other end, if they are big, CFS'ing them is going to be expensive.\n- Do something smart but easy depending on number and size",
            "date": "2007-08-07T20:50:37.731+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "> I'm thinking maybe a MergePolicy#useCompoundDocStore( SegmentInfos )\n> makes sense? The naive case would still just use the static setting\n> we have now, but we could think about a better implementation:\n\nI think adding that new method to MergePolicy is great!  And, just\nusing the same \"useCompoundFile\" as normal segmetns is good for\nstarters (and, this matches what's done today).\n",
            "date": "2007-08-07T21:37:47.756+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "New feedback:\n\n  * Are you going to fix all unit tests that call the now-deprecated\n    APIs?  (You should still leave a few tests using the deprecated\n    APIs to make sure they in fact continue to work, but most tests\n    should not use the deprecated APIs).\n\nResponses to previous feedback:\n\n> As an example, it's not a good idea for merge policies to be able to\n> access IndexWriter#segmentInfos directly. (That's a case I would\n> like to solve by making segmentInfos private, but I haven't looked\n> at that completely yet and even beyond that case, I'd still like\n> merge policies to have a very clean interface with IWs.)\n\nAgreed, but the solution to that is to make segmentInfos private, not\nto make a whole new interface.  Ie, this is an IndexWriter problem, so\nwe should fix it in IndexWriter.\n\n> > While LogDocMergePolicy does need \"maxBufferedDocs\", I think,\n> > instead, in IndexWriter's \"setMaxBufferedDocs()\" it should \"write\n> > through\" to the LogDocMergePolicy if that is the merge policy in\n> > use (and LogDocMergePolicy should store its own private\n> > \"minMergeDocs\").\n>\n> The thing here is that the merge policy has nothing to do with max\n> buffered docs, right? The code for buffering docs for the first\n> segment is wholly in the IndexWriter. LogDocMergePolicy happens to\n> need it (in its current incarnation) in order to handle the way the\n> log levels are computed. This could, of course, be changed. There's\n> nothing that says a merge policy has to look at these values, just\n> that they're available should the merge policy want to look.\n\nExactly: these settings decide when a segment is flushed, so, why put\nthem into IndexMerger interface?  They shouldn't have anything to with\nmerging; I think they should be removed.\n\nFor LUCENE-845 I'm working on a replacement for LogDocMergePolicy that\ndoes not use maxBufferedDocs.\n\n> I guess my idea was that the index writer was responsible for\n> handling the initial segment (with DocsWriter, if it wants) and also\n> to provide an indication to the merge policies how it was handling\n> them.\n>\n> It's possible to have the merge policy influence the first segment\n> size but that opens up a lot of issues. Does every merge policy then\n> have to know how to trade between buffering by doc count and\n> buffering by ram? I was hoping to avoid that.\n\nYeah, I don't think merge policy should dictate flushing either;\nespecially because app logic above IndexWriter can already easily call\nflush() if necessary.\n\n> > In LogDocMergePolicy, it seems like the checking that's done for\n> > whether a SegmentInfo is in a different directory, as well as the\n> > subsequent copy to move it over to the IndexWriter's directory,\n> > should not live in the MergePolicy?\n>\n> I see two parts to this.\n> \n> First, I hesitate to make it not possible for merge policy to see\n> the directory information, i.e., remove\n> IndexMerger#getDirectory(). Copying a segment is one way to get it\n> into the current directory, but merging with other segments does\n> to. A merge policy could decide to go ahead and merge a bunch of\n> segments that are in other directories rather than just copy them\n> individually. Taking away getDirectory() removes that option.\n\nAgreed, a \"sophisticated\" merge policy would go and merge segments in\nother directories.  But, it should not have to.\n\nI'm not proposing making it \"not possible\"; I'm proposing the merge\npolicy is given IndexWriter at which point it can getDirectory() from\nit.  (Ie the extra interface solely for this purpose is overkill).\n\n> As to how to handle the case where single segments are copied, I\n> guess my main reason for leaving that in the merge policy would be\n> for simplicity. Seems nicer to have all segment amalgamation\n> management in one place, rather than some in one class and some in\n> another. Could be factored into an optional base merge policy for\n> derived classes to use as they might like.\n\nI don't see this as simpler: I see it as making the MergePolicy\nwriter's job more complex.  I also see it as substantial duplicated\ncode (I just had to copy/paste a bunch of code in working on my\nMergePolicy in LUCENE-845).\n\nI think factoring into a base class is an OK solution, but, it\nshouldn't be MergePolicy's job to remember to call this final \"move\nany segments in the wrong directory over\" code.  As long as its in one\nplace and people don't have to copy/paste code between MergePolicy\nsources.\n\n> > The \"checkOptimize\" method in LogDocMergePolicy seems like it\n> > belongs back in IndexWriter: I think we don't want every\n> > MergePolicy having to replicate that tricky while condition.\n>\n> The reason for not doing this was I can imagine different merge\n> policies having a different model of what optimize means. I can\n> imagine some policies that would not optimize everything down to a\n> single segment but instead obeyed a max segment size. But we could\n> factor the default conditional into an optional base class, as\n> above.\n>\n> It is an ugly conditional that there might be better way to\n> formulate, so that policies don't have to look at the grody details\n> like hasSeparateNorms. But I'd still like the merge policies to be\n> able to decide what optimize means at a high level.\n\nAgreed: I too don't want to preclude variants to optimize like\n\"optimize to at most N segments\".  (Maybe we should add that method,\nnow, to IndexWriter, and fix MergePolicy to work with this?).\n\nSo, yes, please at least factor this out into a base class.  In\nLUCENE-845 this was another copy/paste for me (ick).  I think there\nshould in fact be a default optimize() in the base class that does\nwhat current IndexWriter now does so that a MergePolicy need not\nimplement optimize at all.\n\n> > Maybe we could change MergePolicy.merge to take a boolean \"forced\"\n> > which, if true, means that the MergePolicy must now pick a merge\n> > even if it didn't think it was time to. This would let us move\n> > that tricky while condition logic back into IndexWriter.\n>\n> I didn't follow this. forced == optimize? If not, what does pick a\n> merge mean? Not sure what LogDoc should do for merge(force=true) if\n> it thinks everything is fine?\n\nNo, forced would mean the merge policy must do a merge; whereas,\nnormally, it's free not to do a merge until it wants to.  Instead of\nboolean argument we could have two methods, one called \"merge\" (you\nmust merge) and one called \"maybeMerge\" or \"checkForMerges\".\n\nIe, optimize is really a series of forced merges: we are merging\nsegments from different levels, N times, until we are down to 1\nsegment w/ no deletes, norms, etc.\n\n> > Also, I think at some point we may want to have an optimize()\n> > method that takes an int parameter stating the max # segments in\n> > the resulting index.\n>\n> I think this is great functionality for a merge policy, but what\n> about just making that part of the individual merge policy\n> interface, rather than linked to IndexWriter? That was one goal of\n> making a factored merge policy: that you could do these tweaks\n> without changing IndexWriter.\n\nWell, it's sort of awkward if you want to vary that max # segments.\nSay during the day you optimize down to 15 segments every time you\nupdate the index, but then at night you want to optimize down to 5.\nIf we don't add method to IndexWriter you then must have instance var\non your MergePolicy that you set, then you call optimize.  It's not\nclean since really it should be a parameter.\n\nAnd, with the merge/maybeMerge separation above, every merge policy\ncould have a default implementation for optimize(int maxNumSegments)\n(in MergePolicy base class or in IndexWriter).\n\n> > Can we support non-contiguous merges? If I understand it\n> > correctly, the MergeSpecification can express such a merge, it's\n> > just that the current IndexMerger (IndexWriter) cannot execute it,\n> > right? So we are at least not precluding fixing this in the\n> > future.\n>\n> As far as I've seen so far, there are no barriers to non-contiguous\n> merges. Maybe something will crop up that is, but in what I've done,\n> I haven't seen any.\n\nWait: there is a barrier, right?  In IndexWriter.replace we don't do\nthe right thing with non-contiguous merges?  What I'm asking is: is\nthat the only barrier?  Ie MergePolicy API will not need to change in\nthe future once we fix IndexWriter.replace to handle non-contiguous\nmerges?\n\n> > It confuses me that MergePolicy has a method \"merge(...)\" -- can\n> > we rename it to \"maybeMerge(..)\" or \"checkForMerge(...)\"?\n>\n> I suppose. I'm not a big fan of the \"maybeFoo\" style of naming. I\n> think of \"merge\" like \"optimize\": make it so / idempotent. But I'm\n> certainly willing to write whatever people find clearest.\n\nI'm not wed to \"maybeMerge()\" but I really don't like \"merge\" :)  It's\noverloaded now.\n\nEG IndexMerger interface has a method called \"merge\" that is named\ncorrectly because it will specifically go a do the requested merge.\nSo does IndexWriter.\n\nThen, you have other [overloaded] methods in LogDocMergePolicy called\n\"merge\" that are named appropriately (they will do a specific merge).\n\nHow about \"checkForMerges()\"?\n\n> > Instead of IndexWriter.releaseMergePolicy() can we have\n> > IndexWriter only close the merge policy if it was the one that had\n> > created it? (Similar to how IndexWriter closes the dir if it has\n> > opened it from a String or File, but does not close it if it was\n> > passed in).\n> \n> This precludes\n> \n> iw.setMergePolicy(new MyMergePolicy(...));\n>      ...\n> iw.close();\n>\n> The implementation's much cleaner using the semantics you describe,\n> but I was thinking it'd be better to optimize for the usability of the\n> common client code case? \n\nThe thing is, that method leaves IndexWriter in a broken state (null\nmergePolicy).  What if you keep adding docs after that then suddenly\nhit an NPE?\n\nAlso, I'm OK if people need to separately close their MergePolicy\ninstances: this is an advanced use of Lucene so it's OK to expect that\n(\"simple things should be simple; complex things should be possible\").\n\nMaybe we could add a \"setMergePolicy(MergePolicy policy, boolean\ndoClose)\" and default doClose to true?\n\nFinally: does MergePolicy really need a close()?  Is this overkill (at\nthis point)?\n\n> > Well I think the current MergePolicy API (where the \"merge\" method\n> calls IndexWriter.merge itself, must cascade itself, etc.) makes it\n> hard to build a generic ConcurrentMergePolicy \"wrapper\" that you\n> could use to make any MergePolicy concurrent (?). How would you do\n> it?\n>\n> I really haven't had time to go heads down on this (the old\n> concurrent merge policy was a derived class rather than a wrapper\n> class). But I was thinking that perhaps ConurrentMergePolicy would\n> actually wrap IndexWriter as well as the serial merge policy, i.e.,\n> implement IndexMerger (my biggest argument for IM at this\n> point). But I haven't looked deeply at whether this will work but I\n> think it has at least a chance.\n>\n> I should know more about this is a day or two. \n\nI don't see how it can work (building the generic concurrency wrapper\n\"under\" IndexMerger) because the MergePolicy is in \"serial control\",\neg, when it wants to cascade merges.  How will you return that thread\nback to IndexWriter?\n\nAlso it feels like the wrong place for concurrency -- I think\ngenerally for \"macro\" concurrency you want it higher up, not lower\ndown, in the call stack.\n\nWith concurrency wrapper \"on the top\" it's able to easily take a merge\nrequest as returned by the policy, kick it off in the backrground, and\nimmediately return control of original thread back to IndexWriter.\n\nBut if you see a way to make it work \"on the bottom\", let's definitely\nexplore it & understand the tradeoffs.\n\n> > I think you can still have state (as instance variables in your\n> > class)? How would this simplification restrict the space of merge\n> > policies?\n>\n> s/state/stack state/. Yeah, you can always unwind your loops and\n> lift your recursions, put all that stack state into instance\n> variables. But, well, yuck. I'd like to make it easy to write simple\n> merge policies and take up the heavy lifting elsewhere. Hopefully\n> there will be more merge policies than index writers.\n\nCan you give an example of the kind of \"state\" we're talking about?\nIs this just academic?\n\nSince the segments change in an unpredictable way (as seen by\nMergePolicy) eg from addIndexes*, flushing, concurrent merge swapping\nthings at random times (thus requiring careful locking), etc, it seems\nlike you can't be keeping much state (stack or instance) that depends\non what segments are in the index?\n\n> > Oh, no: ConcurrentMergePolicy would still call\n> > IndexWriter.merge(spec), just with a separate thread. And so all\n> > synchronization required is still inside IndexWriter (I think?).\n>\n> That's my idea.\n\nActually I was talking about my idea (to \"simplify MergePolicy.merge\nAPI\").  With the simplification (whereby MergePolicy.merge just\nreturns the MergeSpecification instead of driving the merge itself) I\nbelieve it's simple to make a concurrency wrapper around any merge\npolicy, and, have all necessary locking for SegmentInfos inside\nIndexWriter.\n\n> > In fact, if we stick with the current MergePolicy API, aren't you\n> > going to have to put some locking into eg the LogDocMergePolicy\n> > when concurrent merges might be happening?\n>\n> Yes and no. If CMP implements IndexMerger, I think we might be okay?\n\nIf CMP implements IndexMerger you must have locking inside any\nMergePolicy that's calling into CMP?  Whereas with the simplified\nMergePolicy.merge API, no locking is necessary because IndexWriter\nwould lock segmentInfos whenever it calls MergePolicy.merge.\n\n> In the previous iteration, I used derivation so that\n> ConcurrentLogDocMergePolicy derived from the serial version but had\n> the necessary threading. I agree that a wrapper is better solution\n> if it can be made to work.\n\nI think it (concurrency wrapper around any merge policy) can be made\nto work, if we do simplify the MergePolicy.merge API.  I'm not sure it\ncan be made to work if we don't, but if you have an approach we should\nwork through it!\n",
            "date": "2007-08-08T12:48:18.526+0000",
            "id": 19
        },
        {
            "author": "Ning Li",
            "body": "On 8/8/07, Michael McCandless (JIRA) <jira@apache.org> wrote:\n> Actually I was talking about my idea (to \"simplify MergePolicy.merge\n> API\").  With the simplification (whereby MergePolicy.merge just\n> returns the MergeSpecification instead of driving the merge itself) I\n> believe it's simple to make a concurrency wrapper around any merge\n> policy, and, have all necessary locking for SegmentInfos inside\n> IndexWriter.\n\nI agree with Mike. In fact, MergeSelector.select, which is the counterpart\nof MergePolicy.merge in the patch I submitted for concurrent merge,\nsimply returns a MergeSpecification. It's simple and sufficient to have\nall necessary lockings for SegmentInfos in one class, say IndexWriter.\nFor example, IndexWriter locks SegmentInfos when MergePolicy(MergeSelector)\npicks a merge spec. Another example, when a merge is finished, say\nIndexWriter.checkin is called which locks SegmentInfos and replaces\nthe source segment infos with the target segment info.\n\n\nOn 8/7/07, Steven Parkes (JIRA) <jira@apache.org> wrote:\n> The synchronization is still tricky, since parts of segmentInfos are\n> getting changed at various times and there are references and/or\n> copies of it other places. And as Ning pointed out to me, we also\n> have to deal with buffered delete terms. I'd say I got about 80% of\n>the way there on the last go around. I'm hoping to get all the way\n> this time.\n\nIt just occurred to me that there is a neat way to handle deletes that\nare flushed during a concurrent merge. For example, MergePolicy\ndecides to merge segments B and C, with B's delete file 0001 and\nC's 100. When the concurrent merge finishes, B's delete file becomes\n0011 and C's 110. We do a simple computation on the delete bit\nvectors and check in the merged segment with delete file 00110.\n",
            "date": "2007-08-08T14:23:44.613+0000",
            "id": 20
        },
        {
            "author": "Ning Li",
            "body": "The following comments are about the impact on merge if we add\n\"deleteDocument(int doc)\" (and deprecate IndexModifier). Since it\nconcerns the topic in this issue, I also post it here to get your opinions.\n\nI'm thinking about the impact of adding \"deleteDocument(int doc)\" on\nLUCENE-847, especially on concurrent merge. The semantics of\n\"deleteDocument(int doc)\" is that the document to delete is specified\nby the document id on the index at the time of the call. When a merge\nis finished and the result is being checked into IndexWriter's\nSegmentInfos, document ids may change. Therefore, it may be necessary\nto flush buffered delete doc ids (thus buffered docs and delete terms\nas well) before a merge result is checked in.\n\nThe flush is not necessary if there is no buffered delete doc ids. I\ndon't think it should be the reason not to support \"deleteDocument(int\ndoc)\" in IndexWriter. But its impact on concurrent merge is a concern.",
            "date": "2007-08-08T16:02:47.338+0000",
            "id": 21
        },
        {
            "author": "Michael McCandless",
            "body": "> It just occurred to me that there is a neat way to handle deletes\n> that are flushed during a concurrent merge. For example, MergePolicy\n> decides to merge segments B and C, with B's delete file 0001 and C's\n> 100. When the concurrent merge finishes, B's delete file becomes\n> 0011 and C's 110. We do a simple computation on the delete bit\n> vectors and check in the merged segment with delete file 00110\n\nExcellent!  This lets you efficiently merge in the additional deletes\n(if any) that were flushed against each of the merged segments after\nthe merge had begun.  Furthermore, I think this is all contained\nwithin IndexWriter, right?\n\nIe when we go to \"replace/checkin\" the newly merged segment, this\n\"merge newly flushed deletes\" would execute at that time.  And, I\nthink, we would block flushes while this is happening, but\naddDocument/deleteDocument/updateDocument would still be allowed?\n\nIt should in fact be quite fast to run since delete BitVectors is all\nin RAM.\n\n> I'm thinking about the impact of adding \"deleteDocument(int doc)\" on\n> LUCENE-847, especially on concurrent merge. The semantics of\n> \"deleteDocument(int doc)\" is that the document to delete is\n> specified by the document id on the index at the time of the\n> call. When a merge is finished and the result is being checked into\n> IndexWriter's SegmentInfos, document ids may change. Therefore, it\n> may be necessary to flush buffered delete doc ids (thus buffered\n> docs and delete terms as well) before a merge result is checked in.\n>\n> The flush is not necessary if there is no buffered delete doc ids. I\n> don't think it should be the reason not to support\n> \"deleteDocument(int doc)\" in IndexWriter. But its impact on\n> concurrent merge is a concern.\n\nCouldn't we also just update the docIDs of pending deletes, and not\nflush?  Ie we know the mapping of old -> new docID caused by the\nmerge, so we can run through all deleted docIDs and remap?\n",
            "date": "2007-08-08T17:09:29.894+0000",
            "id": 22
        },
        {
            "author": "Ning Li",
            "body": "> Furthermore, I think this is all contained within IndexWriter, right?\n> Ie when we go to \"replace/checkin\" the newly merged segment, this\n> \"merge newly flushed deletes\" would execute at that time. And, I\n> think, we would block flushes while this is happening, but\n> addDocument/deleteDocument/updateDocument would still be allowed?\n\nYes and yes. :-)\n\n> Couldn't we also just update the docIDs of pending deletes, and not\n> flush? Ie we know the mapping of old -> new docID caused by the\n> merge, so we can run through all deleted docIDs and remap? \n\nHmm, I was worried quite a number of delete docIDs could be buffered,\nbut I guess it's still better than having to do a flush. So yes, this is better!",
            "date": "2007-08-08T17:48:34.342+0000",
            "id": 23
        },
        {
            "author": "Steven Parkes",
            "body": "\tAre you going to fix all unit tests that call the now-deprecated APIs?\n\nYeah. Thanks for the reminder.\n\nAs to the IndexWriter vs. IndexMerger issue, I still think having the interface is useful if not only that it makes my testing much easier. I have a MockIndexMerger that implements only the functions in the interface and therefore I can test merge policies without creating a writer. For me this has been a big win ...\n\n\tExactly: these settings decide when a segment is flushed, so, why put\n\tthem into IndexMerger interface?  They shouldn't have anything to with\n\tmerging; I think they should be removed.\n\n\tFor LUCENE-845 I'm working on a replacement for LogDocMergePolicy that\n\tdoes not use maxBufferedDocs.\n\nI can see that one could write a merge policy that didn't have any idea of how the initial buffering was done, but I worry about precluding it. Maybe the LUCENE-845 patch will show a strong enough pattern to believe no merge policies will need it?\n\n\tI think factoring into a base class is an OK solution, but, it\n\tshouldn't be MergePolicy's job to remember to call this final \"move\n\tany segments in the wrong directory over\" code.  As long as its in one\n\tplace and people don't have to copy/paste code between MergePolicy\n\tsources.\n\nIn the case of concurrent merges, I think this gets more complicated. When do you do those directory copies? I think you can't do them at the return from the merge policy because the merge policy may want to do them, but later.\n\nI don't think IndexWriter has enough information to know when the copies need to done. Doubly so if we have concurrent merges?\n\nI still stand by it should be the merge policy making the choice. You could have the code in IndexWriter too, but then there'd be duplicate code. To put the code only in IndexWriter removes the choice from the merge policy.\n\n\tI think there\n\tshould in fact be a default optimize() in the base class that does\n\twhat current IndexWriter now does so that a MergePolicy need not\n\timplement optimize at all.\n\nIt'd be nice, but I don't know how to do it: the merge factor is not generic, so I don't know how to implement the loop generically.\n\nAh ... I see: with your forced merge ... hmmm.\n\n\tNo, forced would mean the merge policy must do a merge; whereas,\n\tnormally, it's free not to do a merge until it wants to.\n\nHmmmm ...\n\nI think adding a forced merge concept here is new ... If it's simply to support optimize, I'm not sure I find it too compelling. LogDoc as it stands uses different algorithms for incremental merges and optimize, so there's not too much of a concept of forced merges vs. optional merges to be factored out. So I guess I'm not seeing a strong compelling case for creating it?\n\n\tWell, it's sort of awkward if you want to vary that max # segments.\n\tSay during the day you optimize down to 15 segments every time you\n\tupdate the index, but then at night you want to optimize down to 5.\n\tIf we don't add method to IndexWriter you then must have instance var\n\ton your MergePolicy that you set, then you call optimize.  It's not\n\tclean since really it should be a parameter.\n\nWell, I don't know if I buy the argument that it should be a parameter. The merge policy has lots of state like docs/seg. I don't really see why segs/optimize is different.\n\nMy main reason for not wanting put this into IndexWriter is then every merge policy must support it.\n\n\tWait: there is a barrier, right?  In IndexWriter.replace we don't do\n\tthe right thing with non-contiguous merges?\n\nYeah, I meant that I'm not aware of any barriers except fixing IndexWriter#replace, in other words, I'm not aware of any other places where non-contiguity would cause a failure.\n\n\tI'm not wed to \"maybeMerge()\" but I really don't like \"merge\" :)  It's\n\toverloaded now.\n\n\tEG IndexMerger interface has a method called \"merge\" that is named\n\tcorrectly because it will specifically go a do the requested merge.\n\tSo does IndexWriter.\n\n\tThen, you have other [overloaded] methods in LogDocMergePolicy called\n\t\"merge\" that are named appropriately (they will do a specific merge).\n\n\tHow about \"checkForMerges()\"?\n\nI don't find it ambiguous based on class and argument type. It's all personal, of course.\n\nI'd definitely prefer maybe over checkFor because that sounds like a predicate.\n\n\tMaybe we could add a \"setMergePolicy(MergePolicy policy, boolean\n\tdoClose)\" and default doClose to true?\n\nThat sounds good.\n\n\tFinally: does MergePolicy really need a close()?\n\nI think so. The concurrent merge policy maintains all sorts of state.\n\n\tI don't see how it can work (building the generic concurrency wrapper\n\t\"under\" IndexMerger) because the MergePolicy is in \"serial control\",\n\teg, when it wants to cascade merges.  How will you return that thread\n\tback to IndexWriter?\n\nSo this is how it looks now: the concurrent merge policy is both a merge policy and an index merger. The serial merge policy knows nothing about it other than it does not get IndexWriter as its merge.\n\nThe index writer wants its merge, so it does it merge/maybeMerge call on the concurrent merge policy. The CMP calls merge on the serial policy, but substitutes itself for the merger rather than IndexWriter.\n\nThe serial merge policy goes on its merry way, looking for merges to do (in the current model, this is a loop; more on that in a minute). Each time it has a subset of segments to merge, it calls merger.merge(...).\n\nAt this point, the concurrent merge policy takes over again. It looks at the segments to be merged and other segments being processed by all existing merge threads and determines if there's a conflict (a request to merge a segment that's currently in a merge). If there's no conflict, it starts a merge thread and calls IndexWriter#merge on the thread. The original calling thread returns immediately. (I have a few ideas how to handle conflicts, the simplest of which is to wait for the conflicting merge and the restart the serial merge, e.g., revert to serial).\n\nThis seems to work pretty well, so far. The only difference in API for the serial merges is that the merge operation can't return the number of documents in the result (since it isn't known how many docs will be deleted).   \n\n\tWith concurrency wrapper \"on the top\" it's able to easily take a merge\n\trequest as returned by the policy, kick it off in the backrground, and\n\timmediately return control of original thread back to IndexWriter.\n\nWhat I don't know how to do with this is figure out how to do a bunch of merges. Lets say I have two levels in LogDoc that are merge worthy. If I call LogDoc, it'll return the lower level. That's all good. But what about doing the higher level in parallel? If I call LogDoc again, it's going to return the lower level again because it knows nothing about the current merge going on.\n\nLogDoc already does things in a loop: it's pretty much set up to call all possible merges at one time (if they return immediately).\n\nIt would be possible to have it return a vector of segmentInfo subsets, but I don't see the gain (and it doesn't work out as well for my putative conflict resolution).\n\n\thave all necessary locking for SegmentInfos inside\n\tIndexWriter\n\nThis was a red-herring on my part. All the \"segmentInfos locking\" has always been in IndexWriter. That's note exactly sufficient. The fundamental issue is that IndexWriter#merge has to operate without a lock on IndexWriter. At some point, I was thinking that meant it would have to lock SegmentInfos but that's ludicrous, actually. It's sufficient for IndexWriter#replace to be synchronized.\n\n\tIf CMP implements IndexMerger you must have locking inside any\n\tMergePolicy that's calling into CMP?\n\nNo. CMP does it's own locking (for purposes of thread management) but the serial merge policies no nothing of this (and they can expect to be called synchronously).",
            "date": "2007-08-14T21:49:25.695+0000",
            "id": 24
        },
        {
            "author": "Steven Parkes",
            "body": "\tIt just occurred to me that there is a neat way to handle deletes that\n\tare flushed during a concurrent merge. For example, MergePolicy\n\tdecides to merge segments B and C, with B's delete file 0001 and\n\tC's 100. When the concurrent merge finishes, B's delete file becomes\n\t0011 and C's 110. We do a simple computation on the delete bit\n\tvectors and check in the merged segment with delete file 00110.\n\nWell, that makes my life much easier. Now I don't have to figure out what to do, just have to make it so ...\n\nThanks!",
            "date": "2007-08-14T21:52:07.051+0000",
            "id": 25
        },
        {
            "author": "Steven Parkes",
            "body": "Updated patch:\n\n* Don't call deprecated methods\n  - note: currently renamed with \"_\" prepended to make easy to find; don't commit\n    those\n* Factor MergePolicyBase\n* comments to remind to delete before commit (though might still have missed some)\n* Make LDMP casts not throw bad cast\n* Get rid of releaseMergePolicy and add doClose parameter on set\n\n* Didn't factor copy from other dirs: requires compound file choices\n* Didn't (yet) rename merge -> maybeMerge\n   - Does this mean optimize -> maybeOptimize, too?",
            "date": "2007-08-15T19:32:29.339+0000",
            "id": 26
        },
        {
            "author": "Michael McCandless",
            "body": "One new small item: you've added a \"public void merge()\" to\nIndexWriter so that people can externally kick off a merge request,\nwhich is good I think.\n\nBut, is it really necessary to flush here?  It would be better to not\nflush so that users then have two separate methods (flush() and\nmerge()) to do each function independently.\n\n> > Are you going to fix all unit tests that call the now-deprecated\n> > APIs?\n> \n> Yeah. Thanks for the reminder.\n\nOn thinking about this more ... and on seeing all the diffs ... I no\nlonger feel we should be deprecating \"get/setUseCompoundFile()\" nor\n\"get/setMergeFactor()\" nor \"get/setMaxMergeDocs()\" in IndexWriter.\n\nThe vast majoriy of Lucene users will not make their own merge policy\n(just use the default merge policy) and so I don't think we should be\ncomplicating their lives with having to now write lines like this when\nthey want to change settings:\n\n   ((LogDocMergePolicy)writer.getMergePolicy()).setUseCompoundFile(useCompoundFile);\n\nAlso, this ties our hands if ever we want to change the default merge\npolicy (which, under LUCENE-845, I'd like to do).\n\nI think instead we should leave the methods, not deprecated, as\nconvenience (sugar) methods.  Simple things should be simple; complex\nthings should be possible.  Sorry I didn't think of this before you\nmade the new patch Steve :(\n\n> > I'm not wed to \"maybeMerge()\" but I really don't like \"merge\" :)\n> > It's overloaded now.\n> > \n> > EG IndexMerger interface has a method called \"merge\" that is named\n> > correctly because it will specifically go a do the requested\n> > merge.  So does IndexWriter.\n> >\n> > Then, you have other [overloaded] methods in LogDocMergePolicy\n> > called \"merge\" that are named appropriately (they will do a\n> > specific merge).\n> > \n> > How about \"checkForMerges()\"?\n>\n> I don't find it ambiguous based on class and argument type. It's all\n> personal, of course.\n> \n> I'd definitely prefer maybe over checkFor because that sounds like a\n> predicate.\n\nOK let's settle on \"maybeMerge\"?\n\n>    - Does this mean optimize -> maybeOptimize, too?\n\nUh, no: when someone calls optimize that means it really must be done,\nright?  So \"optimize\" is the right name I think.\n\n> * Make LDMP casts not throw bad cast \n\nCan you factor this out, eg add a private method\n\"getLogDocMergePolicy(String reason)\" that would be the one place that\ndoes the class casting & throwing an error message from one single\nsource line?  Right now the message is copied in multiple places and,\nit's wrong for mergeFactor (was copied from useCompoundFile).\n\n> * Get rid of releaseMergePolicy and add doClose parameter on set\n\nLooks good, thanks.  Can you add javadocs (w/ params) for both of\nthese new methods?\n\n> As to the IndexWriter vs. IndexMerger issue, I still think having\n> the interface is useful if not only that it makes my testing much\n> easier. I have a MockIndexMerger that implements only the functions\n> in the interface and therefore I can test merge policies without\n> creating a writer. For me this has been a big win ...\n\nSubclassing IndexWriter to make MockIndexMerger would also work for\ntesting?  This is what MockRAMDirectory does for example...\n\n> > Exactly: these settings decide when a segment is flushed, so, why\n> > put them into IndexMerger interface? They shouldn't have anything\n> > to with merging; I think they should be removed.\n> > \n> > For LUCENE-845 I'm working on a replacement for LogDocMergePolicy\n> > that does not use maxBufferedDocs.\n\n> I can see that one could write a merge policy that didn't have any\n> idea of how the initial buffering was done, but I worry about\n> precluding it. Maybe the LUCENE-845 patch will show a strong enough\n> pattern to believe no merge policies will need it?\n\nWe wouldn't be precluding it (people can still get it from their\nIndexWriter).  This is one of the big reasons that I don't like making\nan interface out of IndexMerger: here we are having to pick & choose\nwhich settings from IndexWriter a merge policy is \"allowed\" to use.  I\ndon't think that's necessary (we are just making extra work for\nourselves) and inevitably we won't get it right...\n\n> > I think factoring into a base class is an OK solution, but, it\n> > shouldn't be MergePolicy's job to remember to call this final\n> > \"move any segments in the wrong directory over\" code. As long as\n> > its in one place and people don't have to copy/paste code\n> > between MergePolicy sources.\n> \n> In the case of concurrent merges, I think this gets more\n> complicated. When do you do those directory copies? I think you\n> can't do them at the return from the merge policy because the merge\n> policy may want to do them, but later.\n>\n> I don't think IndexWriter has enough information to know when the\n> copies need to done. Doubly so if we have concurrent merges? \n\nAhh, good point.  Though, this raises the tricky question of index\nconsistency ... IndexWriter commits the new segments file right after\nmergePolicy.merge returns ... so for CMP we suddenly have an unusable\nindex (as seen by an IndexReader).  EG if things crash any time after\nthis point and before the background merging finishes & commits,\nyou're hosed.\n\nMaybe it's too ambitious to allow merges of segments from other\ndirectories to run concurrently?\n\nI would consider it a hard error in IndexWriter if after calling\nmergePolicy.merge from any of the addIndexes*, there remain segments\nin other directories.  I think we should catch this & throw an\nexception?\n\n> I still stand by it should be the merge policy making the\n> choice. You could have the code in IndexWriter too, but then there'd\n> be duplicate code. To put the code only in IndexWriter removes the\n> choice from the merge policy.\n\nI agree that merge policy should be the one making the choice, but the\nexecution of it should be a centralized place (IndexWriter).  EG with\nthe simplified API, the merge policy would just return, one by one,\neach of the segments that is in a different directory...\n\nWe can't all be copy/pasting code (like I had to do for LUCENE-845)\nfor checking & then moving segments across directories.  I think we\nneed single source for this, somehow.\n\n> > I think there should in fact be a default optimize() in the base class\n> > that does what current IndexWriter now does so that a MergePolicy need\n> > not implement optimize at all.\n>\n> It'd be nice, but I don't know how to do it: the merge factor is not\n> generic, so I don't know how to implement the loop generically.\n\nHmmm, OK.  I think what you did (factoring out that massive\nconditional) is good here.\n\n> Ah ... I see: with your forced merge ... hmmm.\n> \n> No, forced would mean the merge policy must do a merge; whereas,\n> normally, it's free not to do a merge until it wants to.\n>\n> I think adding a forced merge concept here is new ... If it's simply\n> to support optimize, I'm not sure I find it too compelling. LogDoc\n> as it stands uses different algorithms for incremental merges and\n> optimize, so there's not too much of a concept of forced merges\n> vs. optional merges to be factored out. So I guess I'm not seeing a\n> strong compelling case for creating it?\n\nOK, I agree, let's not add \"forced\".  How about, instead we only\nrequire mergePolicy to implement optimize(int maxNumSegments)?  (And\ncurrent IndexWriter.optimize() calls this with parameter \"1\").\n\n> > Well, it's sort of awkward if you want to vary that max #\n> > segments.  Say during the day you optimize down to 15 segments\n> > every time you update the index, but then at night you want to\n> > optimize down to 5.  If we don't add method to IndexWriter you\n> > then must have instance var on your MergePolicy that you set,\n> > then you call optimize. It's not clean since really it should be\n> > a parameter.\n>\n> Well, I don't know if I buy the argument that it should be a\n> parameter. The merge policy has lots of state like docs/seg. I don't\n> really see why segs/optimize is different.\n\nI think this would be a useful enough method that it should be \"made\nsimple\" (ie, this is different from the \"other state\" that a merge\npolicy would store).  I opened a separate issue LUCENE-982 to track\nthis.\n\n> My main reason for not wanting put this into IndexWriter is then\n> every merge policy must support it.\n\nThis is why I want to address it now, while we are cementing the\nMergePolicy API: I don't want to preclude it.\n\n> > Wait: there is a barrier, right? In IndexWriter.replace we don't do\n> > the right thing with non-contiguous merges?\n> \n> Yeah, I meant that I'm not aware of any barriers except fixing\n> IndexWriter#replace, in other words, I'm not aware of any other\n> places where non-contiguity would cause a failure.\n\nOK, good, that's my impression too.\n\nAlthough ... do you think we need need some way for merge policy to\nstate where the new segment should be inserted into SegmentInfos?  For\nthe contiguous case it seems clear that we should default to what is\ndone now (new segment goes into same spot where old segments were).\nBut for the non-contiguous case, how would IndexWriter know where to\nput the newly created segment?\n\n> > Finally: does MergePolicy really need a close()?\n> \n> I think so. The concurrent merge policy maintains all sorts of\n> state.\n\nOK.  Hmmm, does CMP block on close while it joins to any running merge\nthreads?  How can the user close IndexWriter and abort the running\nmerges?  I guess CMP would provide a method to abort any running\nmerges, and user would first call that before calling\nIndexWriter.close?\n\n> > I don't see how it can work (building the generic concurrency\n> > wrapper \"under\" IndexMerger) because the MergePolicy is in \"serial\n> > control\", eg, when it wants to cascade merges. How will you return\n> > that thread back to IndexWriter?\n>\n> So this is how it looks now: the concurrent merge policy is both a\n> merge policy and an index merger. The serial merge policy knows\n> nothing about it other than it does not get IndexWriter as its\n> merge.\n>\n> The index writer wants its merge, so it does it merge/maybeMerge\n> call on the concurrent merge policy. The CMP calls merge on the\n> serial policy, but substitutes itself for the merger rather than\n> IndexWriter.\n>\n> The serial merge policy goes on its merry way, looking for merges to\n> do (in the current model, this is a loop; more on that in a\n> minute). Each time it has a subset of segments to merge, it calls\n> merger.merge(...).\n>\n> At this point, the concurrent merge policy takes over again. It\n> looks at the segments to be merged and other segments being\n> processed by all existing merge threads and determines if there's a\n> conflict (a request to merge a segment that's currently in a\n> merge). If there's no conflict, it starts a merge thread and calls\n> IndexWriter#merge on the thread. The original calling thread returns\n> immediately. (I have a few ideas how to handle conflicts, the\n> simplest of which is to wait for the conflicting merge and the\n> restart the serial merge, e.g., revert to serial).\n>\n> This seems to work pretty well, so far. The only difference in API\n> for the serial merges is that the merge operation can't return the\n> number of documents in the result (since it isn't known how many\n> docs will be deleted).\n\nHmmm.  This looks more complex than the proposed API simplification,\nbecause you now have CMP on the top and on the bottom.  Also, this\nrequires the IndexMerger interface, but with the simplification we\nwould not need a separate interface.  Finally, I'm pretty sure you\nhave locking issues (more below...), which are required of all merge\npolicies, that the simplified API wouldn't have.\n\nHow we handle conflicts is important but I think independent of this\nAPI discussion (ie both your CMP and my CMP have this same challenge,\nand I agree we should start simple by just blocking when the selected\nmerge conflicts with a previous one that's still in progress).\n\n> > With concurrency wrapper \"on the top\" it's able to easily take a\n> > merge request as returned by the policy, kick it off in the\n> > backrground, and immediately return control of original thread\n> > back to IndexWriter.\n>\n> What I don't know how to do with this is figure out how to do a\n> bunch of merges. Lets say I have two levels in LogDoc that are merge\n> worthy. If I call LogDoc, it'll return the lower level. That's all\n> good. But what about doing the higher level in parallel? If I call\n> LogDoc again, it's going to return the lower level again because it\n> knows nothing about the current merge going on.\n\nTrue, LogDoc as it now stands would never exploit concurrency (it will\nalways return the highest level that needs merging).  But, we could\nrelax that such that if ever the lowest level has > 2*mergeFactor\npending segments to merge then we select the 2nd set.  This would\nexpose concurrency that would only be used when CMP is in use.  But I\nthink we should do this, later, as an enhancement.  Let's focus on\nsimplifying the API now...\n\n> It would be possible to have it return a vector of segmentInfo\n> subsets, but I don't see the gain (and it doesn't work out as well\n> for my putative conflict resolution).\n\nYeah that would make the API even more complex, which is the wrong\ndirection here :)\n\n> > have all necessary locking for SegmentInfos inside IndexWriter\n>\n> This was a red-herring on my part. All the \"segmentInfos locking\"\n> has always been in IndexWriter. That's note exactly sufficient. The\n> fundamental issue is that IndexWriter#merge has to operate without a\n> lock on IndexWriter. At some point, I was thinking that meant it\n> would have to lock SegmentInfos but that's ludicrous, actually. It's\n> sufficient for IndexWriter#replace to be synchronized.\n\nRight: merging certainly shouldn't hold lock on IndexWriter nor\nsegmentInfos.\n\n> > If CMP implements IndexMerger you must have locking inside any\n> > MergePolicy that's calling into CMP?\n>\n> No. CMP does it's own locking (for purposes of thread management)\n> but the serial merge policies no nothing of this (and they can\n> expect to be called synchronously).\n\nThis I don't get: it seems to me that the serial merge policies must\ndo their own locking when they access the SegmentInfos that's passed\nin?  And that lock must be released, somehow, when they call merge?\nWould merge (inside IndexWriter) somehow release the lock on being\ncalled?  I don't see how you're going to make the locking work, but I\nthink it's required with the current API.\n\nThis is another benefit of the simplified API: MergePolicy.maybeMerge\nwould only be called with a lock already acquired (by IndexWriter) on\nthe segmentInfos.  Then maybeMerge looks @ the segmentInfos, makes its\nchoice, and returns it, and the lock is released.  The lock is not\nheld for an extended period of time...\n",
            "date": "2007-08-16T14:20:50.482+0000",
            "id": 27
        },
        {
            "author": "Steven Parkes",
            "body": "\tOne new small item: you've added a \"public void merge()\" to\n\tIndexWriter so that people can externally kick off a merge request,\n\twhich is good I think.\n\n\tBut, is it really necessary to flush here?  It would be better to not\n\tflush so that users then have two separate methods (flush() and\n\tmerge()) to do each function independently.\n\nThat makes sense.\n\nNote that merge() was added not for users (which I have no strong opinion about) but so that, potentially, CMP can check again for merges when a set of merge threads completes, i.e., cascade.\n\n\tI think instead we should leave the methods, not deprecated, as\n\tconvenience (sugar) methods.  Simple things should be simple; complex\n\tthings should be possible.\n\nI think this argues for a LegacyMergePolicy interface again, then? If we change the default merge policy and someone changes their code to use LogDoc for their own purposes, in both cases the getters/setters should work? So cast to the interface and as long as the merge policy supports this, the getters/setters work (unless the merge policy decides to throw within), otherwise the getters/setters throw? \n\n\tUh, no: when someone calls optimize that means it really must be done,\n\tright?  So \"optimize\" is the right name I think.\n\nYeah, but it might do nothing. Just as merge might do nothing.\n\n\tCan you factor this out, eg add a private method\n\t\"getLogDocMergePolicy(String reason)\"\n\nSure.\n\n\tLooks good, thanks.  Can you add javadocs (w/ params) for both of\n\tthese new methods?\n\nSure.\n\n\tThough, this raises the tricky question of index\n\tconsistency ...\n\nDefinitely. I'm still trying to understand all the subtleties here.\n\n\tIndexWriter commits the new segments file right after\n\tmergePolicy.merge returns ... so for CMP we suddenly have an unusable\n\tindex (as seen by an IndexReader).\n\nHow so? I figured that after mergePolicy.merge returns, in the case of CMP with an ongoing merge, segmentInfos won't have changed at all. Is that a problem?\n\nI thought the issue would be on the other end, where the concurrent merge finishes and needs to update segmentInfos.\n\n\tMaybe it's too ambitious to allow merges of segments from other\n\tdirectories to run concurrently?\n\nYeah, that might be the case. At least as a default?\n\n\tI would consider it a hard error in IndexWriter if after calling\n\tmergePolicy.merge from any of the addIndexes*, there remain segments\n\tin other directories.  I think we should catch this & throw an\n\texception?\n\nIt would be easy enough for CMP to block in this case, rather than returning immediately. Wouldn't that be better? And I suppose it's possible to imagine an API on CMP for specifying this behavior?\n\n\tI opened a separate issue LUCENE-982 to track this.\n\nI think this is good. I think it's an interesting issue but not directly related to the refactor?\n\n\tAlthough ... do you think we need need some way for merge policy to\n\tstate where the new segment should be inserted into SegmentInfos?\n\nRight now I assumed it would replace the left most-segment.\n\nSince I don't really know the details of what such a merge policy would like, I don't really know what it needs.\n\nIf you've thought about this more, do you have a suggestion? I suppose we could just add an int. But, then again, I'd do that as a separate function, leaving the original available, so we can do this later, completely compatibly?\n\n\tHmmm, does CMP block on close while it joins to any running merge\n\tthreads?\n\nYeah, at least in my sandbox.\n\n\tHow can the user close IndexWriter and abort the running\n\tmerges?  I guess CMP would provide a method to abort any running\n\tmerges, and user would first call that before calling\n\tIndexWriter.close?\n\nI hadn't really thought about this but I can see that should be made possible. It's always safe to abandon a merge so it should be available, for fast, safe, and clean shutdown.\n\n\tTrue, LogDoc as it now stands would never exploit concurrency (it will\n\talways return the highest level that needs merging).  But, we could\n\trelax that such that if ever the lowest level has > 2*mergeFactor\n\tpending segments to merge then we select the 2nd set.\n\nOkay. But it will always return that? Still doesn't sound concurrent?\n\nThe thing is, the serial merge policy has no concept of concurrent merges, so if the API is always to select the best merge, until a pervious merge finishes, it will always return that as the best merge.\n\nConcurrent is going to require, by hook or by crook, that a merge policy be able to generate a set of non-conflicting merges, is it not?\n\nI think the LUCENE-845 merge policy does this now, given that CMP gathers up the merge calls. I'm not sure the current LUCENE-847 merge policy does (I'd have to double check) because it sometimes will try to use the result of the current merge in the next merge. The LUCENE-845 merge doesn't try to do this which is a(n) (inconsequential) change?\n\n\tThis is another benefit of the simplified API: MergePolicy.maybeMerge\n\twould only be called with a lock already acquired (by IndexWriter) on\n\tthe segmentInfos.\n\nDo you really mean a lock on segmentInfos or just the lock on IndexWriter? I'm assuming the latter and I think this is the case for both API models.\n\nI don't think it's feasible to have a lock on segmentInfos separately. Only IndexWriter should change segmentInfos and no code should try to look at segmentInfos w/o being called via an IW synch method.\n\nThis does imply that CMP has to copy any segmentInfos data it plans to use during concurrent merging, since the IW lock is not held during these periods. Then, when the merge is done, segmentInfos is updated in IndexWriter via a synch call to IW#replace.\n\nThis means IW#segmentInfos can change while a merge is in progress and this has to be accounted for. That's what I'm walking through now.\n",
            "date": "2007-08-16T16:26:34.020+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "> Note that merge() was added not for users (which I have no strong\n> opinion about) but so that, potentially, CMP can check again for\n> merges when a set of merge threads completes, i.e., cascade.\n\nOK, got it.  In fact, then it seems more important that we NOT flush\nat this point?\n\n> > I think instead we should leave the methods, not deprecated, as\n> > convenience (sugar) methods. Simple things should be simple;\n> > complex things should be possible.\n>\n> I think this argues for a LegacyMergePolicy interface again, then?\n> If we change the default merge policy and someone changes their code\n> to use LogDoc for their own purposes, in both cases the\n> getters/setters should work? So cast to the interface and as long as\n> the merge policy supports this, the getters/setters work (unless the\n> merge policy decides to throw within), otherwise the getters/setters\n> throw?\n\nI don't think so: I think if someone changes the merge policy to\nsomething else, it's fine to require that they then do settings\ndirectly through that merge policy.  I don't think we should bring\nback the LegacyMergePolicy interface.\n\n> > Uh, no: when someone calls optimize that means it really must be\n> > done, right? So \"optimize\" is the right name I think.\n> \n> Yeah, but it might do nothing. Just as merge might do nothing.\n\nWell... that's the exception not the rule.  My vote would be for\n\"maybeMerge(...)\"  and \"optimize(..)\".\n\n> > Though, this raises the tricky question of index consistency ...\n> \n> Definitely. I'm still trying to understand all the subtleties here.\n>\n> > IndexWriter commits the new segments file right after\n> > mergePolicy.merge returns ... so for CMP we suddenly have an\n> > unusable index (as seen by an IndexReader).\n>\n> How so? I figured that after mergePolicy.merge returns, in the case\n> of CMP with an ongoing merge, segmentInfos won't have changed at\n> all. Is that a problem?\n\nThis is inside addIndexes that we're talking about.  It will have\nchanged because the added indexes were stuck into the segmentInfos.\nIf you commit that segmentInfos, which now references segments in\nother directories, the index is inconsistent, until the merge policy\nfinishes its work (including copying over segments from other dirs).\nIn fact this used to be an issue but was fixed in LUCENE-702.\n\n> > Maybe it's too ambitious to allow merges of segments from other\n> > directories to run concurrently?\n> \n> Yeah, that might be the case. At least as a default?\n\nI think it's worse: I think we shouldn't allow any mergePolicy to\nleave the index inconsistent (failing to copy over segments from other\ndirectories).  I think it's a bug if the mergePolicy does that and we\nshould check & raise an exception, and not commit the new segments\nfile.   IndexWriter should in general protect itself from a mergePolicy\nthat makes the index inconsistent (and, refuse to commit the resulting\nsegments file).\n\nWith the proposed \"stateless API\" we would keep calling the\nmergePolicy, after each merge, until it returned null, and then do the\ncheck that index is consistent.\n\n> > I would consider it a hard error in IndexWriter if after calling\n> > mergePolicy.merge from any of the addIndexes*, there remain\n> > segments in other directories. I think we should catch this &\n> > throw an exception?\n>\n> It would be easy enough for CMP to block in this case, rather than\n> returning immediately. Wouldn't that be better? And I suppose it's\n> possible to imagine an API on CMP for specifying this behavior?\n\nI think CMP should indeed block in this case.  I wouldn't add an API\nto change it.  It's too dangerous to allow an index to become\ninconsistent.\n\n> > I opened a separate issue LUCENE-982 to track this.\n>\n> I think this is good. I think it's an interesting issue but not\n> directly related to the refactor?\n\nI think it is related: we should not preclude it in this refactoring.\nI think we should fix MergePolicy.optimize to take \"int\nmaxNumSegments\"?\n\n> > Although ... do you think we need need some way for merge policy\n> > to state where the new segment should be inserted into\n> > SegmentInfos?\n>\n> Right now I assumed it would replace the left most-segment.\n>\n> Since I don't really know the details of what such a merge policy\n> would like, I don't really know what it needs.\n>\n> If you've thought about this more, do you have a suggestion? I\n> suppose we could just add an int. But, then again, I'd do that as a\n> separate function, leaving the original available, so we can do this\n> later, completely compatibly?\n\nI don't have a suggestion :)  And I agree, this is safely postponed while\nkeeping future backwards compatibility, so, punt!\n\n> > How can the user close IndexWriter and abort the running merges?  I\n> > guess CMP would provide a method to abort any running merges, and\n> > user would first call that before calling IndexWriter.close?\n>\n> I hadn't really thought about this but I can see that should be made\n> possible. It's always safe to abandon a merge so it should be\n> available, for fast, safe, and clean shutdown.\n\nOK.  Seems like a CMP specific issue (doesn't impact this discussion).\n\n> > True, LogDoc as it now stands would never exploit concurrency (it\n> > will always return the highest level that needs merging). But, we\n> > could relax that such that if ever the lowest level has >\n> > 2*mergeFactor pending segments to merge then we select the 2nd\n> > set.\n> \n> Okay. But it will always return that? Still doesn't sound\n> concurrent?\n\nNo, after another N (= mergeFactor) flushes, it would return a new\nsuggested merge.  I think this gives CMP concurrency to work with.\nAlso I think other merge policies (eg the rough suggestion in\nLUCENE-854) could provide substantial concurrency.\n\n> The thing is, the serial merge policy has no concept of concurrent\n> merges, so if the API is always to select the best merge, until a\n> pervious merge finishes, it will always return that as the best\n> merge.\n>\n> Concurrent is going to require, by hook or by crook, that a merge\n> policy be able to generate a set of non-conflicting merges, is it\n> not?\n\nCorrect, if we want more than 1 merge running at once then\nmergePolicy must provide non-conflicting merges.\n\nBut, providing just a single concurrent merge already gains us\nconcurrency of merging with adding of docs.  Just that is a great step\nforward, and, it's not clear we can expect performance gains by doing\n2 merges simultaneously with adding docs.  Have you tested this to\nsee?\n\nIf we think there are still gains there, we can use the idea above, or\napps can use other merge policies (like LUCENE-854) that don't always\nchoose non-concurrent (conflicting) merges.\n\n> I think the LUCENE-845 merge policy does this now, given that CMP\n> gathers up the merge calls. I'm not sure the current LUCENE-847\n> merge policy does (I'd have to double check) because it sometimes\n> will try to use the result of the current merge in the next\n> merge. The LUCENE-845 merge doesn't try to do this which is a(n)\n> (inconsequential) change?\n\nRight, the LUCENE-845 merge policy doesn't look @ the return result of\n\"merge\".  It just looks at the newly created SegmentInfos.\n\nHmmmm, in fact, I think your CMP wrapper would not work with the merge\npolicy in LUCENE-845, right?  Ie, won't it will just recurse forever?\nSo actually I don't see how your CMP (using the current API) can in\ngeneral safely \"wrap\" around a merge policy w/o breaking things?\n\nWhereas w/ stateless API, where merge policy just returns what should\nbe merged rather than executing it itself and cascading, would work\nfine.\n\n> > This is another benefit of the simplified API:\n> > MergePolicy.maybeMerge would only be called with a lock already\n> > acquired (by IndexWriter) on the segmentInfos.\n>\n> Do you really mean a lock on segmentInfos or just the lock on\n> IndexWriter? I'm assuming the latter and I think this is the case\n> for both API models.\n\nBut, if you lock on IndexWriter, what about apps that use multiple\nthreads to add documents and but don't use CMP?  When one thread gets\ntied up merging, you'll then block on the other synchronized methods?\nAnd you also can't flush from other threads either?  I think flushing\na new segment should be allowed to run concurrently with the merge?\n\nWhereas if you lock only segmentInfos, and use the proposed stateless\nAPI, I think the other threads would not be blocked?  I guess I don't\nsee the reason to synchronize on IndexWriter instead of segmentInfos.\n\nNet/net I'm still thinking we should simplify this API to be\nstateless.  I think there are a number of benefits:\n\n  * We would no longer need to add a new IndexMerger interface that\n    adds unecessary complexity to Lucnee (and, make the awkward\n    decisions up front on which IndexWriter fields are allowed to be\n    visible through the interface).\n\n  * Keep CMP simpler (only top of stack (where I think \"macro\"\n    concurrency should live), not top and bottom).\n\n  * Work correctly as wrapper around other merge policies (ie not hit\n    infinite recursion because mergePolicy had naturally assumed that\n    \"merge\" would have changed the segmentInfos)\n\n  * Allows locking on segmentInfos (not IndexWriter), and allows\n    concurrency on multiple threads adding docs even without using\n    CMP.",
            "date": "2007-08-16T21:11:13.108+0000",
            "id": 29
        },
        {
            "author": "Steven Parkes",
            "body": "\tI don't think so: I think if someone changes the merge policy to\n\tsomething else, it's fine to require that they then do settings\n\tdirectly through that merge policy.\n\nYou're going to want to change the default merge policy, right?  So you're going to change the hard cast in IW to that policy? So it'll fail for anyone that wants to just getMergePolicy back to the old policy?\n\nIf that's the case, I'm going to keep those tests the way they are because when you do change the policy, I'm assuming you'll keep many of them, just add the manual setMergePolicy(), and they'll need to have those casts put back in?\n\nMaybe we just put it in MergePolicy interface and let them throw (e.g., via MergePolicyBase) if called on an unsupported merge policy? That's moving from compile time checking to run time checking, but ... \n\n\tThis is inside addIndexes that we're talking about.\n\nAh. Right.\n\n\tI think we shouldn't allow any mergePolicy to\n\tleave the index inconsistent (failing to copy over segments from other\n\tdirectories).\n\nThat makes sense to me. CMP could enforce this, even in the case of concurrent merges.\n\n\tNo, after another N (= mergeFactor) flushes, it would return a new\n\tsuggested merge.\n\nOkay. I think I'm following you here.\n\nHere's what I understand: in your model, (1) each call to merge will only ever generate one merge thread (regardless of how many levels might be full) and (2) you can get concurrency out of this as long as you consider a level \"merge worthy\" as different from \"full\", i.e., blocking).\n\nYou did say  \n\n> > But, we\n> > could relax that such that if ever the lowest level has >\n> > 2*mergeFactor pending segments to merge then we select the 2nd\n> > set.\n\nAnd I think you'd want to modify that to select the lowest sufficiently over subscribed level, not just the lowest level if it's oversubscribed?\n\nPerhaps this is sufficient, but not necessary? I see it as simpler just to have the merge policy (abstractly) generate a set of non-conflicting merges and let someone else worry about scheduling them.\n\n\tBut, providing just a single concurrent merge already gains us\n\tconcurrency of merging with adding of docs.\n\nI'm worried about when you start the leftmost merge, that, say, is going to take a day. With a steady influx of docs, it's not going to be long before you need another merge and if you have only one thread, you're going to block for the rest of the day. You've bought a little concurrency, but it's the almost day-long block I really want to avoid.\n\nWith a log-like policy, I think it's feasible to have logN threads. You might not want them all doing disk i/o at the same time: you'd want to prioritize threads on the small merges and/or suspend large merge threads.  The speed with which the larger merge threads can vary when other merges are taking place, you just have to not stop them and start over. \n\n\tRight, the LUCENE-845 merge policy doesn't look @ the return result of\n\t\"merge\".  It just looks at the newly created SegmentInfos.\n\nYeah. My thinking was this would be tweaked. If merger.merge returns a valid number of docs, it could recurse as it does. If merger.merge returned -1 (which CMP does), it would not recurse but simply continue the loop.\n\n\tHmmmm, in fact, I think your CMP wrapper would not work with the merge\n\tpolicy in LUCENE-845, right?  Ie, won't it will just recurse forever?\n\tSo actually I don't see how your CMP (using the current API) can in\n\tgeneral safely \"wrap\" around a merge policy w/o breaking things?\n\nI think it's safe, just not concurrent. The recursion would generate the same set of segments to merge and CMP would make the second call block (abstractly, anyway: it actually throws an exception that unwinds the stack and causes the call to start again from the top when the conflicting merge finishes).\n\n\tBut, if you lock on IndexWriter, what about apps that use multiple\n\tthreads to add documents and but don't use CMP?  When one thread gets\n\ttied up merging, you'll then block on the other synchronized methods?\n\tAnd you also can't flush from other threads either?  I think flushing\n\ta new segment should be allowed to run concurrently with the merge?\n\nI'm not sure I'm following this. That's what happens now, right? Are you trying to get more concurrency then there is now w/o using CMP? I certainly haven't been trying to do that.\n\n\tI guess I don't\n\tsee the reason to synchronize on IndexWriter instead of segmentInfos.\n\nI looked at trying to make IW work when a synchronization of IW didn't imply a synchronization of segmentInfos. It's a very, very heavily used little data structure. I found it very hard to convince myself I could catch all the places locks would be required. And at the same time, I seemed to be able to do everything I needed with IW locking.\n\nThat said, the code's not done, so ....\n\n\tNet/net I'm still thinking we should simplify this API to be\n\tstateless.  I think there are a number of benefits:\n\n \t * We would no longer need to add a new IndexMerger interface that\n\t    adds unecessary complexity to Lucnee (and, make the awkward\n\t    decisions up front on which IndexWriter fields are allowed to be\n\t    visible through the interface).\n\n\t  * Keep CMP simpler (only top of stack (where I think \"macro\"\n\t    concurrency should live), not top and bottom).\n\n\t  * Work correctly as wrapper around other merge policies (ie not hit\n\t    infinite recursion because mergePolicy had naturally assumed that\n\t    \"merge\" would have changed the segmentInfos)\n\n\t  * Allows locking on segmentInfos (not IndexWriter), and allows\n \t   concurrency on multiple threads adding docs even without using\n \t   CMP.\n\nHmmm ... I guess our approaches are pretty different. If you want to take a stab at this ...",
            "date": "2007-08-16T23:20:38.564+0000",
            "id": 30
        },
        {
            "author": "Michael McCandless",
            "body": "> > I don't think so: I think if someone changes the merge policy to\n> > something else, it's fine to require that they then do settings\n> > directly through that merge policy.\n>\n> You're going to want to change the default merge policy, right?  So\n> you're going to change the hard cast in IW to that policy? So it'll\n> fail for anyone that wants to just getMergePolicy back to the old\n> policy?\n\nI don't really follow... my feeling is we should not deprecate\nsetUseCompoundFile, setMergeFactor, setMaxMergeDocs.\n\n> > I think we shouldn't allow any mergePolicy to leave the index\n> > inconsistent (failing to copy over segments from other\n> > directories).\n>\n> That makes sense to me. CMP could enforce this, even in the case of\n> concurrent merges.\n\nI think IndexWriter should enforce it?  Ie no merge policy should be\nallowed to leave segments in other dirs (= at inconsistent index) at\npoint of commit.\n\n> Perhaps this is sufficient, but not necessary? I see it as simpler\n> just to have the merge policy (abstractly) generate a set of\n> non-conflicting merges and let someone else worry about scheduling\n> them.\n\nI like that idea :)  It fits well w/ the stateless API.  Ie, merge\npolicy returns all possible merges and \"someone above\" takes care of\nscheduling them.\n\n> > But, providing just a single concurrent merge already gains us\n> > concurrency of merging with adding of docs.\n>\n> I'm worried about when you start the leftmost merge, that, say, is\n> going to take a day. With a steady influx of docs, it's not going to\n> be long before you need another merge and if you have only one\n> thread, you're going to block for the rest of the day. You've bought\n> a little concurrency, but it's the almost day-long block I really\n> want to avoid.\n\nAhh ... very good point.  I agree.\n\n> With a log-like policy, I think it's feasible to have logN\n> threads. You might not want them all doing disk i/o at the same\n> time: you'd want to prioritize threads on the small merges and/or\n> suspend large merge threads.  The speed with which the larger merge\n> threads can vary when other merges are taking place, you just have\n> to not stop them and start over.\n\nAgreed: CMP should do this.\n\n> > Right, the LUCENE-845 merge policy doesn't look @ the return\n> > result of \"merge\".  It just looks at the newly created\n> > SegmentInfos.\n>\n> Yeah. My thinking was this would be tweaked. If merger.merge returns\n> a valid number of docs, it could recurse as it does. If merger.merge\n> returned -1 (which CMP does), it would not recurse but simply\n> continue the loop.\n\nHmm.  This means each merge policy must know whether it's talking to\nCMP or IndexWriter underneith?  With the stateless approach this\nwouldn't happen.\n\n> > Hmmmm, in fact, I think your CMP wrapper would not work with the\n> > merge policy in LUCENE-845, right?  Ie, won't it will just recurse\n> > forever?  So actually I don't see how your CMP (using the current\n> > API) can in general safely \"wrap\" around a merge policy w/o\n> > breaking things?\n>\n> I think it's safe, just not concurrent. The recursion would generate\n> the same set of segments to merge and CMP would make the second call\n> block (abstractly, anyway: it actually throws an exception that\n> unwinds the stack and causes the call to start again from the top\n> when the conflicting merge finishes).\n\nOh I see...  that's kind of sneaky (planning on using exceptions to\nabort a merge requested by the policy).  I think the stateless\napproach would be cleaner here.\n\n> > But, if you lock on IndexWriter, what about apps that use multiple\n> > threads to add documents and but don't use CMP?  When one thread\n> > gets tied up merging, you'll then block on the other synchronized\n> > methods?  And you also can't flush from other threads either?  I\n> > think flushing a new segment should be allowed to run concurrently\n> > with the merge?\n>\n> I'm not sure I'm following this. That's what happens now, right? Are\n> you trying to get more concurrency then there is now w/o using CMP?\n> I certainly haven't been trying to do that.\n\nTrue, this is something new.  But since you're already doing the work\nto allow a merge to run in the BG without blocking adding of docs,\nflushing, etc, wouldn't this come nearly for free?  Actually I think\nall that's necessary, regardless of sync'ing on IndexWriter or\nSegmentInfos is to move the \"if (triggerMerge)\" out of the\nsynchronized method/block.\n\n> > I guess I don't see the reason to synchronize on IndexWriter\n> > instead of segmentInfos.\n>\n> I looked at trying to make IW work when a synchronization of IW\n> didn't imply a synchronization of segmentInfos. It's a very, very\n> heavily used little data structure. I found it very hard to convince\n> myself I could catch all the places locks would be required. And at\n> the same time, I seemed to be able to do everything I needed with IW\n> locking.\n\nWell, eg flush() now synchronizes on IndexWriter: we don't want 2\nthreads doing this at once.  But, the touching of segmentInfos inside\nflush (to add the new SegmentInfo) is a tiny fleeting event (like\nreplace) and so you would want segmentInfos to be free to change while\nthe flushing was running (eg by a BG merge that has finished).\n\n> Hmmm ... I guess our approaches are pretty different. If you want to\n> take a stab at this ...\n\nOK I will try to take a rough stab a the stateless approach....\n",
            "date": "2007-08-18T18:01:10.603+0000",
            "id": 31
        },
        {
            "author": "Steven Parkes",
            "body": "\tmy feeling is we should not deprecate\n\tsetUseCompoundFile, setMergeFactor, setMaxMergeDocs\n\nI understood that you didn't want to deprecate them in IndexWriter. I wasn't sure that you meant that they should be added to the MergePolicy interface? If you do, everything makes sense. Otherwise, it sounds like there's still a cast in there and I'm not sure about that.\n\n\tI think IndexWriter should enforce it?  Ie no merge policy should be\n\tallowed to leave segments in other dirs (= at inconsistent index) at\n\tpoint of commit.\n\nI think it's just about code location: since a merge policy might want to factor into it's algorithm the directories used, it needs the info and it will presumably sometimes do it. Presumably you could provide code in MergePolicyBase so the merges could decide when but wouldn't have to write the copy loop. If you put the code in IndexWriter too, it sounds duplicated, again presuming sometimes a policy might want to do it itself. \n\n\tI like that idea :)  It fits well w/ the stateless API.  Ie, merge\n\tpolicy returns all possible merges and \"someone above\" takes care of\n\tscheduling them.\n\nSo it returns a vector of specs?\n\nThat's essentially what the CMP as an above/below wrapper does. I can see that above/below is strange enough to be less clever (I wasn't trying to be so much clever as backwards compatible) and more insane.\n\nSane is good.\n\n\tHmm.  This means each merge policy must know whether it's talking to\n\tCMP or IndexWriter underneith?  With the stateless approach this\n\twouldn't happen.\n\nWell, I wouldn't so much say it has to know. All it cares is what merge returns. Doesn't have to know who returned it or why.\n\nThe only real difference between this and the \"generate a vector of merges\" is that in the merge policy can take advantage immediately of merge results in the serial case where if you're generating a vector of merges, it can't know.\n\nOf course, I guess in that case, if IndexWriter gets a vector of merges, it can always take the lowest and ignore the rest, calling the merge policy again incase it wants to request a different set. Then you only have the excess computation for merges you never really considered.\n\n\tOh I see...  that's kind of sneaky (planning on using exceptions to\n\tabort a merge requested by the policy).\n\nThere's always going to be the chance of an exception to a merge. I'm pretty sure of that. But you're right, if the merge policy isn't in the control path, it would never see them. They'll be there, but it's out of the path.\n\n\tBut since you're already doing the work\n\tto allow a merge to run in the BG without blocking adding of docs,\n\tflushing, etc, wouldn't this come nearly for free?\n\nI haven't looked at this.\n\n\tWell, eg flush() now synchronizes on IndexWriter\n\nYeah, and making it not is less than straightforward. I've looked at his code a fair amount, experimented with different ideas, but hadn't gotten all the way to a working model.\n\nYou can look at locking segmentInfos but there are many places that segmentInfos is iterated over that would require locks if the lock on IW wasn't sufficient to guarantee that the iteration was safe.\n\nI did look at that early on, so maybe my understanding was still too lacking and it's more feasible than I was thinking ...",
            "date": "2007-08-18T18:29:29.307+0000",
            "id": 32
        },
        {
            "author": "Michael McCandless",
            "body": "OK I started from the original patch and made the changes described\nbelow.\n\nThis is still a work in progress, but I think I think the new\nstateless approach works very well.\n\nAll unit tests pass (one assert had to be changed in\nTestAddIndexesNoOptimize).\n\nI created a ConcurrentMergePolicyWrapper along with this (I'll post\npatch to LUCENE-870).\n\nI've also included the two merge policies from LUCENE-845 (still\ndefaulting to LogDocMergePolicy).\n\nHere are the changes:\n\n  - Renamed merge -> maybeMerge\n\n  - Changed the API to be \"stateless\" meaning the merge policy is no\n    longer responsible for running the merges itself.  Instead, it\n    quickly returns the specification, which describes which merges\n    are needed, back to the writer and the writer then runs them.  I\n    also changed MergeSpecification to contain a list of OneMerge\n    instances.\n\n  - Removed IndexMerger interface (just use IndexWriter instead)\n\n  - Put isOptimized() logic into LogMergePolicy: on thinking about\n    this more (and seeing response to a thread on java-dev), I now\n    agree with Steve that this logically belongs in LogMergePolicy\n    because each MergePolicy is free to define just what it considers\n    \"optimized\" to mean.  Then I removed the MergePolicyBase.\n\n  - Un-deprecated {get/set}{UseCompoundFile,MergeFactor,MaxMergeDocs}.\n    But I did leave the static constants deprecated.\n\n  - IndexWriter keeps track of which segments are involved in running\n    merges and throws a MergeException if it's asked to initiate a\n    merge that involves a segment that's already being merged.\n\n  - Fixed LogMergePolicy to return all possible merges (exposes\n    concurrency).\n\n  - Implemented the \"merge deletes when commiting the merge\" algorithm\n    that Ning suggested (this is in commitMerge).\n\n  - Assert that the merge request is in fact contiguous (at start &\n    finish of merge) & throw MergeException if not.\n\n  - Fixed a number of sneaky concurrency issues so that CMPW would\n    work.  Broke \"merge\" into mergeInit, mergeMiddle and mergeFinish.\n    The first & last are carefully sychronized.\n\n  - I put copyDirFiles in IW and call this in addIndexesNoOptimize\n    before committing new segments file: we can't let mergePolicy\n    leave the index inconsistent.\n\n  - I reverted the changes to addIndexes(IndexReader[]): I think the\n    change here wasn't valid: you can't assume that you can re-create\n    any IndexReader instance by loading from its directory; I put the\n    original back for this method.\n\n  - the changes to addIndexes I'm not sure are good.\n\n  - Fixed LogMergePolicy to return more than 1 merge\n\n  - Made CMPW\n\n  - Renamed replace -> commitMerge; made it private.\n\n",
            "date": "2007-08-24T16:38:21.112+0000",
            "id": 33
        },
        {
            "author": "Michael McCandless",
            "body": "OK new patch:\n\n  - Added the missing MergePolicy.java from last time that Ning caught\n    (thanks!)\n\n  - Fixed some javadocs\n\n  - Relaxed synchronization of merging so that merges can run\n    concurrently with flushing if you are using multiple thread to do\n    indexing.  This gains concurrency of merging even if you are not\n    using CMPW.  But I left flushing as synchronized; I think we can\n    relax this at some point in the future.\n\n  - Fixed some concurrency issues\n\n  - Added \"minMergeDocs\" to LogDocMergePolicy and \"minMergeMB\" to\n    LogByteSizeMergePolicy; set their defaults as described in\n    LUCENE-845.\n\nStill a few small things to do.  I think it's getting close.\n",
            "date": "2007-08-27T21:05:35.687+0000",
            "id": 34
        },
        {
            "author": "Ning Li",
            "body": "I include comments for both LUCENE-847 and LUCENE-870 here since they are closely related.\n\nI like the stateless approach used for refactoring merge policy. But modeling concurrent merge (ConcurrentMergePolicyWrapper) as a MergePolicy seems to be inconsistent with the MergePolicy interface:\n  1 As you pointed out, \"the merge policy is no longer responsible for running the merges itself\". MergePolicy.maybeMerge simply returns a merge specification. But ConcurrentMergePolicyWrapper.maybeMerge actually starts concurrent merge threads thus doing the merges.\n  2 Related to 1, cascading is done in IndexWriter in non-concurrent case. But in concurrent case, cascading is also done in merge threads which are started by ConcurrentMergePolicyWrapper.maybeMerge.\n\nMergePolicy.maybeMerge should continue to simply return a merge specification. (BTW, should we rename this maybeMerge to, say, findCandidateMerges?) Can we carve the merge process out of IndexWriter into a Merger? IndexWriter still provides the building blocks - merge(OneMerge), mergeInit(OneMerge), etc. Merger uses these building blocks. A ConcurrentMerger extends Merger but starts concurrent merge threads as ConcurrentMergePolicyWrapper does.\n\n\nOther comments:\n1 UpdateDocument's and deleteDocument's bufferDeleteTerm are synchronized on different variables in this patch. However, the semantics of updateDocument changed since LUCENE-843. Before LUCENE-843, updateDocument, which is a delete and an insert, guaranteed the delete and the insert are committed together (thus an update). Now it's possible that they are committed in different transactions. If we consider DocumentsWriter as the RAM staging area for IndexWriter, then deletes are also buffered in RAM staging area and we can restore our previous semantics, right?\n\n2 OneMerge.segments seems to rely on its segment infos' reference to segment infos of IndexWriter.segmentInfos. The use in commitMerge, which calls ensureContiguousMerge, is an example. However, segmentInfos can be a cloned copy because of exceptions, thus the reference broken.\n\n3 Calling optimize of an IndexWriter with the current ConcurrentMergePolicyWrapper may cause deadlock: the one merge spec returned by MergePolicy.optimize may be in conflict with a concurrent merge (the same merge spec will be returned without changes to segmentInfos), but a concurrent merge cannot finish because optimize is holding the lock.\n\n4 Finally, a couple of minor things:\n  1 LogMergePolicy.useCompoundFile(SegmentInfos infos, SegmentInfo info) and useCompoundDocStore(SegmentInfos infos): why the parameters?\n  2 Do we need doMergeClose in IndexWriter? Can we simply close a MergePolicy if not null?",
            "date": "2007-08-29T16:22:04.466+0000",
            "id": 35
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks for the detailed review Ning!\n\n> 1 As you pointed out, \"the merge policy is no longer responsible for\n> running the merges itself\". MergePolicy.maybeMerge simply returns a\n> merge specification. But ConcurrentMergePolicyWrapper.maybeMerge\n> actually starts concurrent merge threads thus doing the merges.\n\nTrue, but I was thinking CMPW could be an exception to this rule.  I\nguess I would change the rule to \"simple merge policies don't have to\nrun their own merges\"?\n\nHowever I do agree, CMPW is clearly a different beast from a typical\nmerge policy because it entails scheduling, not selection, of merges.\n\n> 2 Related to 1, cascading is done in IndexWriter in non-concurrent\n> case. But in concurrent case, cascading is also done in merge\n> threads which are started by\n> ConcurrentMergePolicyWrapper.maybeMerge.\n\nGood point...  I think I could refactor this so that cascading logic\nlives entirely in one place IndexWriter.\n\n> MergePolicy.maybeMerge should continue to simply return a merge\n> specification. (BTW, should we rename this maybeMerge to, say,\n> findCandidateMerges?)\n\nGood!  I like findCandidateMerges better; I'll change it.\n\n> Can we carve the merge process out of IndexWriter into a Merger?\n> IndexWriter still provides the building blocks - merge(OneMerge),\n> mergeInit(OneMerge), etc. Merger uses these building blocks. A\n> ConcurrentMerger extends Merger but starts concurrent merge threads\n> as ConcurrentMergePolicyWrapper does.\n\nHow would this be used?  Ie, how would one make an IndexWriter that\nuses the ConcurrentMerger?  Would we add expert methods\nIndexWriter.set/getIndexMerger(...)?  (And presumably the mergePolicy\nis now owned by IndexMerger so it would have the\nset/getMergePolicy(...)?)\n\nAlso, how would you separate what remains in IW vs what would be in\nIndexMerger?\n\nI like this approach in principle; I'm just trying to hash out the\ndetails...\n\n> 1 UpdateDocument's and deleteDocument's bufferDeleteTerm are\n> synchronized on different variables in this patch.\n\nWoops, good catch!  I'll fix.\n\n> However, the semantics of updateDocument changed since\n> LUCENE-843. Before LUCENE-843, updateDocument, which is a delete and\n> an insert, guaranteed the delete and the insert are committed\n> together (thus an update). Now it's possible that they are committed\n> in different transactions. If we consider DocumentsWriter as the RAM\n> staging area for IndexWriter, then deletes are also buffered in RAM\n> staging area and we can restore our previous semantics, right?\n\nHmm ... you're right.  This is a separate issue from merge policy,\nright?  Are you proposing buffering deletes in DocumentsWriter\ninstead?\n\n> 2 OneMerge.segments seems to rely on its segment infos' reference to\n> segment infos of IndexWriter.segmentInfos. The use in commitMerge,\n> which calls ensureContiguousMerge, is an example. However,\n> segmentInfos can be a cloned copy because of exceptions, thus the\n> reference broken.\n\nGood catch!  How to fix?  One thing we could do is always use\nSegmentInfo.reset(...) and never swap in clones at the SegmentInfo\nlevel.  This way using the default 'equals' (same instance) would\nwork.  Or we could establish identity (equals) of a SegmentInfo as\nchecking if the directory plus segment name are equal?  I think I'd\nlean to the 2nd option....\n\n> 3 Calling optimize of an IndexWriter with the current\n> ConcurrentMergePolicyWrapper may cause deadlock: the one merge spec\n> returned by MergePolicy.optimize may be in conflict with a\n> concurrent merge (the same merge spec will be returned without\n> changes to segmentInfos), but a concurrent merge cannot finish\n> because optimize is holding the lock.\n\nHmmm yes.  In fact I think we can remove synchronized from optimize\naltogether since within it we are synchronizing(this) at the right\nplaces?  If more than one thread calls optimize at once, externally,\nit is actually OK: they will each pick a merge that's viable\n(concurrently) and will run the merge, else return once there is no\nmore concurrency left.  I'll add a unit test that confirms this.\n\n> 4 Finally, a couple of minor things:\n>\n>   1 LogMergePolicy.useCompoundFile(SegmentInfos infos, SegmentInfo\n>     info) and useCompoundDocStore(SegmentInfos infos): why the\n>     parameters?\n\nWell, useCompoundFile(...) is given a single newly flushed segment and\nshould decide whether it should be CFS.  Whereas\nuseCompoundDocStore(...) is called when doc stores are flushed.  When\nautoCommit=false, segments can share a single set of doc stores, so\nthere's no single SegmentInfo to pass down.\n\n> 2 Do we need doMergeClose in IndexWriter? Can we simply close a\n>   MergePolicy if not null?\n\nGood point.  I think this is reasonable -- I'll fix.\n",
            "date": "2007-08-30T09:09:44.766+0000",
            "id": 36
        },
        {
            "author": "Ning Li",
            "body": "> True, but I was thinking CMPW could be an exception to this rule.  I\n> guess I would change the rule to \"simple merge policies don't have to\n> run their own merges\"?\n\n:) Let's see if we have to make that exception.\n\n> Good point...  I think I could refactor this so that cascading logic\n> lives entirely in one place IndexWriter.\n\nAnother problem of the current cascading in CMPW.MergeThread is, if multiple candidate merges are found, all of them are added to IndexWriter.mergingSegments. But all but the first should be removed because only the first merge is carried out (thus removed from mergeSegments after the merge is done).\n\nHow do you make cascading live entirely in IndexWriter? Just removing cascading from CMPW.MergeThread has one drawback. For example, segment sizes of an index are: 40, 20, 10, buffer size is 10 and merge factor is 2. A buffer full flush of 10 will trigger merge of 10 & 10, then cascade to 20 & 20, then cascade to 40 & 40. CMPW without cascading will stop after 10 & 10 since IndexWriter.maybeMerge has already returned. Then we have to wait for the next flush to merge 20 & 20.\n\n> How would this be used?  Ie, how would one make an IndexWriter that\n> uses the ConcurrentMerger?  Would we add expert methods\n> IndexWriter.set/getIndexMerger(...)?  (And presumably the mergePolicy\n> is now owned by IndexMerger so it would have the\n> set/getMergePolicy(...)?)\n> \n> Also, how would you separate what remains in IW vs what would be in\n> IndexMerger?\n\nMaybe Merger does and only does merge (so IndexWriter still owns MergePolicy)? Say, base class Merger.merge simply calls IndexWriter.merge. ConcurrentMerger.merge creates a merge thread if possible. Otherwise it calls super.merge, which does non-concurrent merge. IndexWriter simply calls its merger's merge instead of its own merge. Everything else remains in IndexWriter.\n\n\n1\n> Hmm ... you're right.  This is a separate issue from merge policy,\n> right?  Are you proposing buffering deletes in DocumentsWriter\n> instead?\n\nYes, this is a separate issue. And yes if we consider DocumentsWriter as staging area.\n\n2\n> Good catch!  How to fix?  One thing we could do is always use\n> SegmentInfo.reset(...) and never swap in clones at the SegmentInfo\n> level.  This way using the default 'equals' (same instance) would\n> work.  Or we could establish identity (equals) of a SegmentInfo as\n> checking if the directory plus segment name are equal?  I think I'd\n> lean to the 2nd option....\n\nI think the 2nd option is better.\n\n3\n> Hmmm yes.  In fact I think we can remove synchronized from optimize\n> altogether since within it we are synchronizing(this) at the right\n> places?  If more than one thread calls optimize at once, externally,\n> it is actually OK: they will each pick a merge that's viable\n> (concurrently) and will run the merge, else return once there is no\n> more concurrency left.  I'll add a unit test that confirms this.\n\nThat seems to be the case. The fact that \"the same merge spec will be returned without changes to segmentInfos\" reminds me: MergePolicy.findCandidateMerges finds merges which may not be eligible; but CMPW checks for eligibility when looking for candidate merges. Maybe we should unify the behaviour? BTW, MergePolicy.optimize (a rename?) doesn't check for eligibility either.\n\n4\n> Well, useCompoundFile(...) is given a single newly flushed segment and\n> should decide whether it should be CFS.  Whereas\n> useCompoundDocStore(...) is called when doc stores are flushed.  When\n> autoCommit=false, segments can share a single set of doc stores, so\n> there's no single SegmentInfo to pass down.\n\nThe reason I asked is because none of them are used right now. So they might be used in the future?",
            "date": "2007-08-30T22:33:30.725+0000",
            "id": 37
        },
        {
            "author": "Michael McCandless",
            "body": "\n> > Good point...  I think I could refactor this so that cascading logic\n> > lives entirely in one place IndexWriter.\n>\n> Another problem of the current cascading in CMPW.MergeThread is, if\n> multiple candidate merges are found, all of them are added to\n> IndexWriter.mergingSegments. But all but the first should be removed\n> because only the first merge is carried out (thus removed from\n> mergeSegments after the merge is done).\n\nYou're right -- I'm only doing the first non-conflicting merge in\nCMPW (but then not releasing the rest of them).  I think this would be\nfixed by having cascading logic only in IndexWriter.\n\n> How do you make cascading live entirely in IndexWriter? Just\n> removing cascading from CMPW.MergeThread has one drawback.  For\n> example, segment sizes of an index are: 40, 20, 10, buffer size is\n> 10 and merge factor is 2. A buffer full flush of 10 will trigger\n> merge of 10 & 10, then cascade to 20 & 20, then cascade to 40 &\n> 40. CMPW without cascading will stop after 10 & 10 since\n> IndexWriter.maybeMerge has already returned. Then we have to wait\n> for the next flush to merge 20 & 20.\n\nOh, I would remove from CMPW and add then add it into IndexWriter (so\nthe scenario above would cascade normally).  Meaning, IndexWriter,\nupon completing a merge, would always consult the policy for whether\nthe completed merge has now enabled any new merges.\n\nThis is somewhat messy though (with CMPW as a MergePolicy) because\nthen findCandidateMerges would need to know if it was being called\n(due to cascading) under one of its own threads and if so act\ndifferently.  Another good reason to make it a separate Merger\nsubclass.\n\n> > How would this be used?  Ie, how would one make an IndexWriter\n> > that uses the ConcurrentMerger?  Would we add expert methods\n> > IndexWriter.set/getIndexMerger(...)?  (And presumably the\n> > mergePolicy is now owned by IndexMerger so it would have the\n> > set/getMergePolicy(...)?)\n> > \n> > Also, how would you separate what remains in IW vs what would be\n> > in IndexMerger?\n>\n> Maybe Merger does and only does merge (so IndexWriter still owns\n> MergePolicy)? Say, base class Merger.merge simply calls\n> IndexWriter.merge. ConcurrentMerger.merge creates a merge thread if\n> possible. Otherwise it calls super.merge, which does non-concurrent\n> merge. IndexWriter simply calls its merger's merge instead of its\n> own merge. Everything else remains in IndexWriter.\n\nOK I will test out this approach.\n\n> > Hmm ... you're right.  This is a separate issue from merge policy,\n> > right?  Are you proposing buffering deletes in DocumentsWriter\n> > instead?\n>\n> Yes, this is a separate issue. And yes if we consider\n>  DocumentsWriter as staging area.\n\nI will open new issue.\n\n> > Good catch!  How to fix?  One thing we could do is always use\n> > SegmentInfo.reset(...) and never swap in clones at the SegmentInfo\n> > level.  This way using the default 'equals' (same instance) would\n> > work.  Or we could establish identity (equals) of a SegmentInfo as\n> > checking if the directory plus segment name are equal?  I think\n> > I'd lean to the 2nd option....\n>\n> I think the 2nd option is better.\n\nI'll take this approach.\n\n> > Hmmm yes.  In fact I think we can remove synchronized from\n> > optimize altogether since within it we are synchronizing(this) at\n> > the right places?  If more than one thread calls optimize at once,\n> > externally, it is actually OK: they will each pick a merge that's\n> > viable (concurrently) and will run the merge, else return once\n> > there is no more concurrency left.  I'll add a unit test that\n> > confirms this.\n>\n> That seems to be the case.\n\nI'll add unit test to confirm.\n\n> The fact that \"the same merge spec will be returned without changes\n> to segmentInfos\" reminds me: MergePolicy.findCandidateMerges finds\n> merges which may not be eligible; but CMPW checks for eligibility\n> when looking for candidate merges. Maybe we should unify the\n> behaviour?\n\nNot quite following you here... not being eligible because the merge\nis in-progress in a thread is something I think any given MergePolicy\nshould not have to track?  Once I factor out CMPW as its own Merger\nsubclass I think the eligibility check happens only in IndexWriter?\n\n> BTW, MergePolicy.optimize (a rename?) doesn't check for eligibility\n> either.\n\nRename to/from what?  (It is currently called MergePolicy.optimize).\nIndexWriter steps through the merges and only runs the ones that do\nnot conflict (are eligible)?\n\n> > Well, useCompoundFile(...) is given a single newly flushed segment\n> > and should decide whether it should be CFS.  Whereas\n> > useCompoundDocStore(...) is called when doc stores are flushed.\n> > When autoCommit=false, segments can share a single set of doc\n> > stores, so there's no single SegmentInfo to pass down.\n> \n> The reason I asked is because none of them are used right now. So\n> they might be used in the future?\n\nBoth of these methods are now called by IndexWriter (in the patch),\nupon flushing a new segment.\n",
            "date": "2007-08-31T08:31:22.979+0000",
            "id": 38
        },
        {
            "author": "Ning Li",
            "body": "> Not quite following you here... not being eligible because the merge\n> is in-progress in a thread is something I think any given MergePolicy\n> should not have to track?  Once I factor out CMPW as its own Merger\n> subclass I think the eligibility check happens only in IndexWriter?\n\nI was referring to the current patch: LogMergePolicy does not check for eligibility, but CMPW, a subclass of MergePolicy, checks for eligibility. Yes, the eligibility check only happens in IndexWriter after we do Merger class.\n\n> Rename to/from what?  (It is currently called MergePolicy.optimize).\n> IndexWriter steps through the merges and only runs the ones that do\n> not conflict (are eligible)?\n\nMaybe rename to MergePolicy.findMergesToOptimize?\n\n> > The reason I asked is because none of them are used right now. So\n> > they might be used in the future?\n> \n> Both of these methods are now called by IndexWriter (in the patch),\n> upon flushing a new segment.\n\nI was referring to the parameters. The parameters are not used.",
            "date": "2007-08-31T13:39:44.379+0000",
            "id": 39
        },
        {
            "author": "Michael McCandless",
            "body": "\n> > Not quite following you here... not being eligible because the\n> > merge is in-progress in a thread is something I think any given\n> > MergePolicy should not have to track?  Once I factor out CMPW as\n> > its own Merger subclass I think the eligibility check happens only\n> > in IndexWriter?\n>\n> I was referring to the current patch: LogMergePolicy does not check\n> for eligibility, but CMPW, a subclass of MergePolicy, checks for\n> eligibility. Yes, the eligibility check only happens in IndexWriter\n> after we do Merger class.\n\nOK, let's leave eligibility check in IW.\n\n> > Rename to/from what?  (It is currently called\n> > MergePolicy.optimize).  IndexWriter steps through the merges and\n> > only runs the ones that do not conflict (are eligible)?\n> \n> Maybe rename to MergePolicy.findMergesToOptimize?\n\nOK, that's good.\n\n> > > The reason I asked is because none of them are used right\n> > > now. So they might be used in the future?\n> > \n> > Both of these methods are now called by IndexWriter (in the\n> > patch), upon flushing a new segment.\n> \n> I was referring to the parameters. The parameters are not used.\n\nAhh, got it.  Yes the thinking is merge policies in the future may\nwant to look @ segmentinfos to decide.\n",
            "date": "2007-08-31T17:01:51.611+0000",
            "id": 40
        },
        {
            "author": "Michael McCandless",
            "body": "\nAttached new patch (take5) incorporating Ning's feedback.\n\nThis patch includes LUCENE-845 (a new merge default merge policy plus\na \"merge by size in bytes of segment\" merge policy), LUCENE-847\n(factor merge policy/scheduling out of IndexWriter) and LUCENE-870\n(ConcurrentMergeScheduler).\n\nThe one thing remaining after these are done, that I'll open a\nseparate issue for and commit separately, is to switch IndexWriter to\nflush by RAM usage by default (instead of by docCount == 10) as well\nas merge by size-in-bytes by default.\n\nI broke out a separate MergeScheduler interface.  SerialMergeScheduler\nis the default (matches how merges are executed today: sequentially,\nusing the calling thread).  ConcurrentMergeScheduler runs the merges\nas separate threads (up to a max number at which point the extras are\ndone sequentially).\n\nOther changes:\n\n  - Allow multiple threads to call optimize().  I added a unit test\n    for this.\n\n  - Tightnened calls to deleter.refresh(), which remove partially\n    created files on an exception, to remove only those files that the\n    given piece of code would create.  This is very important because\n    otherwise refresh() could remove the files being created by a\n    background merge.\n\n  - Added some unit tests\n",
            "date": "2007-09-07T17:17:01.810+0000",
            "id": 41
        },
        {
            "author": "Doug Cutting",
            "body": "Is there any reason not to make ConcurrentMergeScheduler the default too after this is committed?",
            "date": "2007-09-07T18:19:36.475+0000",
            "id": 42
        },
        {
            "author": "Michael McCandless",
            "body": "> Is there any reason not to make ConcurrentMergeScheduler the default too after this is committed?\n\nGood question.  The only downsides I can think of are:\n\n  * It's all fresh code so until we let it \"age\" some, it's a higher\n    risk that something broke.  That said there is decent unit test\n    coverage for it and these unit tests did find some sneaky issues\n    (which I fixed!).\n\n  * It only actually helps on machines that have some concurrency.\n    But in this case we are largely talking about IO concurrent w/ CPU\n    which nearly all machines have I think.\n\nI think the benefits are sizable:\n\n  * Good performance gains (25% speedup of net indexing time for all\n    of Wikipedia content -- details in LUCENE-870)\n\n  * Trivial way to leverage concurrency (ie you don't need to manage\n    your own threads).\n\n  * No more unexpected long pauses on certain addDocument calls.\n\nSo I think it would make sense to make it the default.  I'll include\nthat in the new issue for changing defaults in IndexWriter.\n",
            "date": "2007-09-07T18:47:07.269+0000",
            "id": 43
        },
        {
            "author": "Michael McCandless",
            "body": "OK, as a better test of ConcurrentMergeScheduler, and towards making it\nthe default merge scheduler, I tried making it the default in\nIndexWriter and then ran all unit tests, and uncovered problems with\nthe current patch (notably how optimize works!).  So I'm working on an\nnew patch now....\n\n",
            "date": "2007-09-08T10:11:24.132+0000",
            "id": 44
        },
        {
            "author": "Ning Li",
            "body": "Comments on optimize():\n\n  - In the while loop of optimize(), LogMergePolicy.findMergesForOptimize returns a merge spec with one merge. If ConcurrentMergeScheduler is used, the one merge will be started in MergeScheduler.merge() and findMergesForOptimize will be called again. Before the one merge finishes, findMergesForOptimize will return the same spec but the one merge is already started. So only one concurrent merge is possible and the main thread will spin on calling findMergesForOptimize and attempting to merge.\n\n  - One possible solution is to make LogMergePolicy.findMergesForOptimize return multiple merge candidates. It allows higher level of concurrency. It also alleviates a bit the problem of main thread spinning. To solve this problem, maybe we can check if a merge is actually started, then sleep briefly if not (which means all merges candidates are in conflict)?\n\n\nA comment on concurrent merge threads:\n\n  - One difference between the current approach on concurrent merge and the patch I posted a while back is that, in the current approach, a MergeThread object is created and started for every concurrent merge. In my old patch, maxThreadCount of threads are created and started at the beginning and are used throughout. Both have pros and cons.",
            "date": "2007-09-09T21:28:23.916+0000",
            "id": 45
        },
        {
            "author": "Michael McCandless",
            "body": "OK, another rev of the patch (take6).  I think it's close!\n\nThis patch passes all unit tests with SerialMergeScheduler (left as\nthe default for now) and also passes all unit tests once you switch\nthe default to ConcurrentMergeScheduler instead.\n\nI made one simplification to the approach: IndexWriter now keeps track\nof \"pendingMerges\" (merges that mergePolicy has declared are necessary\nbut have not yet been started), and \"runningMerges\" (merges currently\nin flight).  Then MergeScheduler just asks IndexWriter for the next\npending merge when it's ready to run it.  This also cleaned up how\ncascading works.\n\nOther changes:\n\n  * Optimize: optimize is now fully concurrent (it can run multiple\n    merges at once, new segments can be flushed during an optimize,\n    etc).  Optimize will optimize only those segments present when it\n    started (newly flushed segments may remain separate).\n\n  * New API: optimize(boolean doWait) allows you to not wait for\n    optimize to complete (it runs in background).  This only works\n    when MergeScheduler uses threads.\n\n  * New API: close(boolean doWait) allows you to not wait for running\n    merges if you want to \"close in a hurry\".  Also only works when\n    MergeScheduler uses threads.\n\n  * I fixed LogMergePolicy to expose merge concurrency during optimize\n    by first calling the \"normal\" merge policy to see if it requires\n    merges and returning those merges if so, and then falling back to\n    the normal \"merge the tail <= mergeFactor segments until there is\n    only 1 left\".\n\n  * Because IndexModifier synchronizes on directory, it can't use\n    ConcurrentMergeScheduler since this quickly leads to deadlock at\n    least during IndexWriter.close.  So I set it back to\n    SerialMergeScheduler (it is deprecated anyway).\n\n  * Added private IndexWriter.message(...) that prints message to the\n    infoStream prefixed by the thread name and changed all\n    infoStream.print*'s to message(...).  Also added more messages in\n    the exceptional cases to aid future diagnostics.\n\n  * Added more unit tests\n",
            "date": "2007-09-10T23:20:03.831+0000",
            "id": 46
        },
        {
            "author": "Michael McCandless",
            "body": "\n> In the while loop of optimize(), LogMergePolicy.findMergesForOptimize\n> returns a merge spec with one merge. If ConcurrentMergeScheduler is\n> used, the one merge will be started in MergeScheduler.merge() and\n> findMergesForOptimize will be called again. Before the one merge\n> finishes, findMergesForOptimize will return the same spec but the\n> one merge is already started. So only one concurrent merge is\n> possible and the main thread will spin on calling\n> findMergesForOptimize and attempting to merge.\n\nYes.  The new patch has cleaned this up nicely, I think.\n\n> One possible solution is to make LogMergePolicy.findMergesForOptimize\n> return multiple merge candidates. It allows higher level of\n> concurrency.\n\nGood idea!  I took exactly this approach in patch I just attached.  I\nmade a simple change: LogMergePolicy.findMergesForOptimize first\nchecks if \"normal merging\" would want to do merges and returns them if\nso.  Since \"normal merging\" exposes concurrent merges, this gains us\nconcurrency for optimize in cases where the index has too many\nsegments.  I wasn't sure how otherwise to expose concurrency...\n\n> It also alleviates a bit the problem of main thread spinning. To\n> solve this problem, maybe we can check if a merge is actually\n> started, then sleep briefly if not (which means all merges\n> candidates are in conflict)?\n\nThis is much cleaner in new patch: there is no more spinning.  In new\npatch if multiple threads are merging (either spawned by\nConcurrentMergeaScheduler or provided by the application or both) then\nthey all pull from a shared queue of \"merges needing to run\" and then\nreturn when that queue is empty.  So no more spinning.\n\n> One difference between the current approach on concurrent merge and\n> the patch I posted a while back is that, in the current approach, a\n> MergeThread object is created and started for every concurrent\n> merge. In my old patch, maxThreadCount of threads are created and\n> started at the beginning and are used throughout. Both have pros and\n> cons.\n\nYeah I thought I would keep it simple (launch thread when needed then\nlet it finish when it's done) rather than use a pool.  This way\nthreads are only created (and are only alive) while concurrency is\nactually needed (ie > N merges necessary at once).  But yes there are\npros/cons either way.\n",
            "date": "2007-09-10T23:28:26.865+0000",
            "id": 47
        },
        {
            "author": "Ning Li",
            "body": "> OK, another rev of the patch (take6).  I think it's close!\n\nYes, it's close! :)\n\n> I made one simplification to the approach: IndexWriter now keeps track\n> of \"pendingMerges\" (merges that mergePolicy has declared are necessary\n> but have not yet been started), and \"runningMerges\" (merges currently\n> in flight).  Then MergeScheduler just asks IndexWriter for the next\n> pending merge when it's ready to run it.  This also cleaned up how\n> cascading works.\n\nI like this simplification.\n\n>   * Optimize: optimize is now fully concurrent (it can run multiple\n>     merges at once, new segments can be flushed during an optimize,\n>     etc).  Optimize will optimize only those segments present when it\n>     started (newly flushed segments may remain separate).\n\nThis semantics does add a bit complexity - segmentsToOptimize, OneMerge.optimize.\n\n> Good idea!  I took exactly this approach in patch I just attached.  I\n> made a simple change: LogMergePolicy.findMergesForOptimize first\n> checks if \"normal merging\" would want to do merges and returns them if\n> so.  Since \"normal merging\" exposes concurrent merges, this gains us\n> concurrency for optimize in cases where the index has too many\n> segments.  I wasn't sure how otherwise to expose concurrency...\n\nAnother option is to schedule merges for the newest N segments and the next newest N segments and the next next... N is the merge factor.\n\n\nA couple of other things:\n\n  - It seems you intended sync() to be part of the MergeScheduler interface?\n\n  - IndexWriter.close([true]), abort(): The behaviour should be the same whether the calling thread is the one that actually gets to do the closing. Right now, only the thread that actually does the closing waits for the closing. The others do not wait for the closing.\n",
            "date": "2007-09-11T22:59:52.572+0000",
            "id": 48
        },
        {
            "author": "Michael McCandless",
            "body": "> > Good idea! I took exactly this approach in patch I just attached. I\n> > made a simple change: LogMergePolicy.findMergesForOptimize first\n> > checks if \"normal merging\" would want to do merges and returns them if\n> > so. Since \"normal merging\" exposes concurrent merges, this gains us\n> > concurrency for optimize in cases where the index has too many\n> > segments. I wasn't sure how otherwise to expose concurrency...\n>\n> Another option is to schedule merges for the newest N segments and\n> the next newest N segments and the next next... N is the merge\n> factor.\n\nOK, that is simpler.  I'll take that approach (and not call the\n\"normal\" merge policy first).\n\n> A couple of other things:\n> \n>   - It seems you intended sync() to be part of the MergeScheduler\n>     interface?\n\nI had started down this route but then backed away from it: I think\nIndexWriter should handle this rather than making every MergeScheduler\nhave duplicated code for doing so.  Oh I see, I had left empty sync()\nin SerialMergeScheduler; I'll remove that.\n\n>  - IndexWriter.close([true]), abort(): The behaviour should be the\n>    same whether the calling thread is the one that actually gets to do\n>    the closing. Right now, only the thread that actually does the\n>    closing waits for the closing. The others do not wait for the\n>    closing.\n\nAhh good point.  OK, I'll have other threads wait() until the\nclose/abort is complete.\n",
            "date": "2007-09-12T08:56:16.238+0000",
            "id": 49
        },
        {
            "author": "Michael McCandless",
            "body": "New patch (take 7).\n\nI folded in Ning's comments (above) and Yonik's comments from\nLUCENE-845, added javadocs & fixed Javadoc warnings and fixed two\nother small issues.  All tests pass on Linux, OS X, win32, with either\nSerialMergeScheduler or ConcurrentMergeScheduler as the default.\n\nI plan to commit in a few days time...\n",
            "date": "2007-09-12T17:42:33.475+0000",
            "id": 50
        },
        {
            "author": "Ning Li",
            "body": "Access of mergeThreads in ConcurrentMergeScheduler.merge() should be synchronized.",
            "date": "2007-09-13T19:11:06.612+0000",
            "id": 51
        },
        {
            "author": "Michael McCandless",
            "body": "Ahh, good catch.  Will fix!",
            "date": "2007-09-13T19:14:50.647+0000",
            "id": 52
        },
        {
            "author": "Ning Li",
            "body": "Hmm, it's actually possible to have concurrent merges with SerialMergeScheduler.\n\nMaking SerialMergeScheduler.merge synchronize on SerialMergeScheduler will serialize all merges. A merge can still be concurrent with a ram flush.\n\nMaking SerialMergeScheduler.merge synchronize on IndexWriter will serialize all merges and ram flushes.",
            "date": "2007-09-13T20:22:06.010+0000",
            "id": 53
        },
        {
            "author": "Michael McCandless",
            "body": "> Hmm, it's actually possible to have concurrent merges with\n> SerialMergeScheduler.\n\nThis was actually intentional: I thought it fine if the application is\nsending multiple threads into IndexWriter to allow merges to run\nconcurrently.  Because, the application can always back down to a\nsingle thread to get everything serialized if that's really required?\n",
            "date": "2007-09-13T21:02:30.995+0000",
            "id": 54
        },
        {
            "author": "Ning Li",
            "body": "> This was actually intentional: I thought it fine if the application is\n> sending multiple threads into IndexWriter to allow merges to run\n> concurrently.  Because, the application can always back down to a\n> single thread to get everything serialized if that's really required?\n\nToday, applications use multiple threads on IndexWriter to get some concurrency on document parsing. With this patch, applications that want concurrent merges would simply use ConcurrentMergeScheduler, no?\n\nOr a rename since it doesn't really serialize merges?",
            "date": "2007-09-13T21:49:45.207+0000",
            "id": 55
        },
        {
            "author": "Mark Miller",
            "body": "I have to triple check, but on first glance, my apps performance halfed using the ConcurrentMergeScheduler on a recent core duo with 2 GB RAM (As compared to the SerialMergeSceduler). Seems unexpected?",
            "date": "2007-09-13T21:59:55.552+0000",
            "id": 56
        },
        {
            "author": "Michael McCandless",
            "body": "\n> Today, applications use multiple threads on IndexWriter to get some\n> concurrency on document parsing. With this patch, applications that\n> want concurrent merges would simply use ConcurrentMergeScheduler,\n> no?\n\nTrue.  OK I will make SerialMergeScheduler.merge serialized.  This way\nonly one merge can happen at a time even when the application is using\nmultiple threads.\n",
            "date": "2007-09-13T22:06:38.309+0000",
            "id": 57
        },
        {
            "author": "Michael McCandless",
            "body": "> I have to triple check, but on first glance, my apps performance\n> halfed using the ConcurrentMergeScheduler on a recent core duo with\n> 2 GB RAM (As compared to the SerialMergeSceduler). Seems unexpected?\n\nWhoa, that's certainly unexpected!  I'll go re-run my perf test.",
            "date": "2007-09-13T22:07:31.752+0000",
            "id": 58
        },
        {
            "author": "Mark Miller",
            "body": "Looks like some anomalous tests. Last night I checked twice, but today results are: 58 to 48 in favor of Concurrent. I am going to assume my first results where invalid. Sorry for the noise and thanks for the great patch. Has passed quite a few stress tests I run on my app without any problems so far. Do both merge policies allow for a closer to constant add time or is it just the Concurrent policy?",
            "date": "2007-09-13T22:42:15.125+0000",
            "id": 59
        },
        {
            "author": "Michael McCandless",
            "body": "> Looks like some anomalous tests. Last night I checked twice, but\n> today results are: 58 to 48 in favor of Concurrent. I am going to\n> assume my first results where invalid. Sorry for the noise and\n> thanks for the great patch.\n\nOK, phew!\n\n> Has passed quite a few stress tests I run on my app without any\n> problems so far.\n\nI'm glad to hear that :)  Thanks for being such an early adopter!\n\n> Do both merge policies allow for a closer to constant add time or is\n> it just the Concurrent policy?\n\nNot sure I understand the question -- you mean addDocument?  Yes it's\nonly ConcurrentMergeScheduler that should keep addDocument calls\nconstant time, because SerialMergeScheduler will hijack the addDocument\nthread to do its merges.",
            "date": "2007-09-13T22:50:44.579+0000",
            "id": 60
        },
        {
            "author": "Michael McCandless",
            "body": "Attached take8, incorporating Ning's feedback plus some small\nrefactoring and fixing one case where optimize() would do an\nunecessary merge.",
            "date": "2007-09-14T16:19:53.907+0000",
            "id": 61
        }
    ],
    "component": "core/index",
    "description": "If we factor the merge policy out of IndexWriter, we can make it pluggable, making it possible for apps to choose a custom merge policy and for easier experimenting with merge policy variants.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-847",
    "issuetypeClassified": "REFACTORING",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Factor merge policy out of IndexWriter",
    "systemSpecification": true,
    "version": ""
}