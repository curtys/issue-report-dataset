{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "I'm hitting this, when trying to convert the 20090306 Wikipedia export to a line file:\n\n{code}\nException in thread \"Thread-0\" java.lang.ArrayIndexOutOfBoundsException: 2048\n\tat org.apache.xerces.impl.io.UTF8Reader.read(Unknown Source)\n\tat org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)\n\tat org.apache.xerces.impl.XMLEntityScanner.scanContent(Unknown Source)\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanContent(Unknown Source)\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)\n\tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n\tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n\tat org.apache.xerces.parsers.XMLParser.parse(Unknown Source)\n\tat org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)\n\tat org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:77)\n\tat java.lang.Thread.run(Thread.java:619)\n{code}\n\nFrom this:\n\n  http://marc.info/?l=xerces-j-user&m=120452263925040&w=2\n\nIt sounds likely an upgrade to xerces 2.9.1 will fix it.  I'm testing it now... if it fixes the issue, I'll commit the upgrade to contrib/benchmark.",
            "date": "2009-04-08T18:38:24.588+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "So, after upgrading to xerces 2.9.1, I then hit this error:\n\n{code} \nException in thread \"Thread-0\" java.lang.RuntimeException: org.apache.xerces.impl.io.MalformedByteSequenceException: Invalid byte 2 of 4-byte UTF-8 sequence.\n\tat org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:101)\n\tat java.lang.Thread.run(Thread.java:619)\nCaused by: org.apache.xerces.impl.io.MalformedByteSequenceException: Invalid byte 2 of 4-byte UTF-8 sequence.\n\tat org.apache.xerces.util.ErrorHandlerWrapper.createSAXParseException(Unknown Source)\n\tat org.apache.xerces.util.ErrorHandlerWrapper.fatalError(Unknown Source)\n\tat org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)\n\tat org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)\n\tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n\tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n\tat org.apache.xerces.parsers.XMLParser.parse(Unknown Source)\n\tat org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)\n\tat org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:77)\n\t... 1 more\n{code} \n\nIt appears I'm hitting XERCESJ-1257 which, hideously, is still\nopen. Worse, we are already doing the suggested workaround at the\nbottom of the issue. Hmm.\n",
            "date": "2009-04-08T19:46:20.836+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "After some iterations on XERCESJ-1257, I managed to apply the original patch on that issue (thank you Robert!), which indeed allows me to process all of Wikipedia's XML export.  I'll commit a recompiled xerces 2.9.1 jar with that patch shortly.",
            "date": "2009-04-08T21:49:17.944+0000",
            "id": 2
        },
        {
            "author": "Shai Erera",
            "body": "I wonder why does EnwikiDocMaker extend LineDocMaker? The latter assumes the input is given in lines, while the former assumes an XML format ... so why the inheritance?\n\nThis affects EnwikiDocMaker today when LDM.openFile() instantiates a BufferedReader, which is never used by EDM. Is it because of DocState? Perhaps some of the logic in LDM can be pulled up to BasicDocMaker, or a new abstract DocStateDocMaker?\nIf there is a good reason, then maybe introduce a protected member useReader and set it to false in EDM? Or override openFile() in EDM and not instantiate the reader?\n\nAlso, somewhat unrelated to this issue, but I found two issues in LDM:\n# In makeDocument(), if the read line is null, then we first call openFile() and then check 'forever' (and possibly throw a NoMoreDataException). Should we first check forever, and only if it's true call openFile()?\n# resetInputs() reads the docs.file property and throws an exception if it's not set. Shouldn't this code belong to setConfig?\nI can include those two in the patch as well.",
            "date": "2009-04-11T10:47:38.819+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I wonder why does EnwikiDocMaker extend LineDocMaker? \n\nI'm not sure... I agree it'd be cleaner to not subclass LineDocMaker, and factor out DocState into BasicDocMaker.\n\nbq. Should we first check forever, and only if it's true call openFile()?\n\nYes, let's fix that!\n\nbq. resetInputs() reads the docs.file property and throws an exception if it's not set. Shouldn't this code belong to setConfig?\n\nI think it should, but I vaguely remember some odd reason why I put it in resetInputs... try moving it and see?",
            "date": "2009-04-11T11:45:08.017+0000",
            "id": 4
        },
        {
            "author": "Shai Erera",
            "body": "resetInputs() is called from PerfRunData's ctor (as is setConfig), but also from ResetInputsTask. Unless it is possible to change the file name in the middle of execution, I see no reason why not move it to setConfig.\n\nI'll move it to setConfig and also switch to throw IllegalArgEx, insteas of RuntimeEx.\n\nAnother change I'd like to do is remove the while(true) in makeDoc. All it does is read 1 line and breaks, unless that line is null in which case it reopens the file and reads a line again. I think that in that case, which will happen only after all docs were consumed, and if forever is set to true, we can just call makeDoc again, and avoid the 1-instruction loop in every makeDoc call.",
            "date": "2009-04-11T11:59:56.763+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "OK sounds good!",
            "date": "2009-04-11T12:19:09.174+0000",
            "id": 6
        },
        {
            "author": "Shai Erera",
            "body": "Before I post a patch I wanted to test reading the 20090306 enwiki dump and write it as a one line document, all using the bz2 in/out streams. After 9 hours and 2881000 documents (!!!), I've hit the following exception:\n\n{code}\nException in thread \"Thread-1\" java.lang.ArrayIndexOutOfBoundsException\n\tat org.apache.xerces.impl.io.UTF8Reader.read(Unknown Source)\n\tat org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)\n\tat org.apache.xerces.impl.XMLEntityScanner.scanContent(Unknown Source)\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanContent(Unknown Source)\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)\n\tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)\n\tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n\tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n\tat org.apache.xerces.parsers.XMLParser.parse(Unknown Source)\n\tat org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)\n\tat org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:76)\n\tat java.lang.Thread.run(Thread.java:810)\n{code}\n\nSame exception like Mike hit, only from a different method. I'm using the latest xerces jar Mike put. I'm beginning to think this enwiki dump is jinxed :)\n\nAnyway, I'll post the patch shortly and run on the 20070527 version to verify.",
            "date": "2009-04-12T03:20:00.149+0000",
            "id": 7
        },
        {
            "author": "Shai Erera",
            "body": "The patch touches LineDocMaker, EnwikiDocMaker and WriteLineDocTask.\nAlso, put ant-1.7.1 in benchmark/lib",
            "date": "2009-04-12T03:26:28.001+0000",
            "id": 8
        },
        {
            "author": "Uwe Schindler",
            "body": "Do you know http://commons.apache.org/compress/ ?\n\nIt is a commons project that replicates the internals from ANT and othe projects for general usage. It is not yet released, but available as snapshot jars. TIKA uses it, too. It also contains BZIPInputStream. I would prefer this instead of polluting the classpath with a full ant distribution.",
            "date": "2009-04-12T08:33:29.072+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "Odd -- with that patched xerces JAR I was able to parse the full XML.  Is it possible your bunzipping code is messing up the XML?\n\nShai, why did it take 9 hours to get to that exception?  Is bunzip that slow?  That seems crazy.  (Or are you running tests on a snail-of-a-machine? ;) )\n\nCan you run only your bunzip code and confirm it produces an XML file that's identical to what bunzip2 from the command line produces?  (And measure how long it takes vs the command line).",
            "date": "2009-04-12T09:09:33.662+0000",
            "id": 10
        },
        {
            "author": "Shai Erera",
            "body": "That's the way I wrap FIS with BZIP:\n\n{code}\n      if (doBzipCompression) {\n        // According to CBZip2InputStream's documentation, we should first\n        // consume the first two file header chars ('B' and 'Z'), as well as \n        // wrap the underlying stream with a BufferedInputStream, since CBZip2IS\n        // uses the read() method exclusively.\n        fileIS = new BufferedInputStream(fileIS, READER_BUFFER_BYTES);\n        fileIS.read(); fileIS.read();\n        fileIS = new CBZip2InputStream(fileIS);\n      }\n{code}\n\nbq. Is it possible your bunzipping code is messing up the XML?\n\nI successfully read the file and compressed it with Java's GZIP classes, however I did not attempt to parse the XML itself. Did you run EnwikiDocMaker on the actual XML or the bz2 archive?\nThe 20070527 run should end soon (I hope - it reached 2.2M documents, so if it doesn't fail, I guess that bzip wrapping is very unlikely to affect the XML parsing.\n\nbq. Shai, why did it take 9 hours to get to that exception? Is bunzip that slow? That seems crazy. \n\nI run the test on my TP 60, which is not a snail-of-a-machine, but definitely not a strong server. You can download the patch and the jar and try it out on your machine.\nBut yes, I did notice bzip is very slow compared to gzip, however it has better compression ration. I do want to measure the times though, to give more accurate numbers, but in order to do that I need to finish a successful run first.\n\nbq. Can you run only your bunzip code and confirm it ...\n\nI would have done that, but the output XML is 17GB, and doing it twice is not an option on my TP. That's why I wanted this bzip thing in the first place :)\nI'll try to do that with the 20070527 version, which hopefully will be ~half the size ... \n",
            "date": "2009-04-12T11:30:08.084+0000",
            "id": 11
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Did you run EnwikiDocMaker on the actual XML or the bz2 archive?\n\nI downloaded the bz2 2008036 Wikipedia export, ran bunzip2 on the command line, then had to patch Xerces JAR to get it to parse the XML successfully.\n\nbq. I run the test on my TP 60, which is not a snail-of-a-machine, but definitely not a strong server. \n\nHmm -- I wonder how long bunzip2 would take on the TP 60.  Time to upgrade ;)  Get yourself an X25 SSD!\n\nbq. I would have done that, but the output XML is 17GB, and doing it twice is not an option on my TP. That's why I wanted this bzip thing in the first place \n\nAhh OK :)",
            "date": "2009-04-12T11:55:06.538+0000",
            "id": 12
        },
        {
            "author": "Shai Erera",
            "body": "bq. I downloaded the bz2 2008036\n\nI'm almost sure its a typo, but just to verify - did download the 20090306 (enwiki-20090306-pages-articles.xml.bz2), or 2008036?\n\nAnyway, I think I've found a problem. In the javadocs, they document that the IS version uses the readByte() exclusively, but don't say anything regarding their OS version. I read the code and noticed it always calls write() and never uses the array version.\nSo I wrapped the FOS with a BOS (bufSize=64k) and then with BZOS. I did a short test, reading 2000 records from the 20070527 file, before and after the change:\n\n|| Num Docs || Before || After || %tg\n| 2000 | 106s | 30s | {color:green}72{color}\n\nI think that if that improvement is stable, than the 9 hours run should drop to ~3 hours, which seems right. I didn't measure the time to unzip the file using WinRAR (the first time I tried it), but it was a couple of hours run.\n\nOnce the current run will complete, I'll kick off a new one with that code change and note the time difference. I'm eager to see it speeds up, but I want to complete a successful run before :)",
            "date": "2009-04-12T12:11:22.997+0000",
            "id": 13
        },
        {
            "author": "Shai Erera",
            "body": "Another thing I noticed is that WriteLineDocTask calls flush() after every document it writes. Any reason to do it? We use BufferedWriter, and calling flush() after every document is a bit expensive, I think.\nI quickly measured the same 2000 documents run and it finished in 28 seconds, 7% improvement compared to the 'after' run and 74% improvement compared to the 'before'.\nSo if there's a good reason, we can keep it - the performance gain is not that high, but otherwise I think we should remove it, and count on PerfTask.close() being called at the end of the run (perhaps the absence of close() was the reason to call flush() in the first place?).",
            "date": "2009-04-12T12:19:47.402+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I'm almost sure its a typo, but just to verify - did download the 20090306 (enwiki-20090306-pages-articles.xml.bz2), or 2008036?\n\nSorry I meant 20090306.\n\nbq. I did a short test, reading 2000 records from the 20070527 file, before and after the change:\n\nExcellent!\n\nbq. Another thing I noticed is that WriteLineDocTask calls flush() after every document it writes. Any reason to do it?\n\nHmm that should not be needed; I'd say remove it?  But, implement close() to actually close the stream?",
            "date": "2009-04-12T14:45:40.924+0000",
            "id": 15
        },
        {
            "author": "Shai Erera",
            "body": "bq. But, implement close() to actually close the stream?\n\nAlready did, I had to because otherwise the bzip file wasn't sealed properly (that's why I started the other thread about tracking task resources). It already exists in the attached patch.\n\nI'm finishing a run with the updated code (wrapping w/ BOS), so once that finishes, I'll post an updated patch and some numbers.",
            "date": "2009-04-12T15:11:16.196+0000",
            "id": 16
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Already did, I had to because otherwise the bzip file wasn't sealed properly (that's why I started the other thread about tracking task resources). It already exists in the attached patch.\n\nOh yeah right, I already forgot.  Feels so long ago ;)",
            "date": "2009-04-12T19:26:18.477+0000",
            "id": 17
        },
        {
            "author": "Shai Erera",
            "body": "Here some numbers:\n\n* Reading the enwiki bz2 file with CBZip2InputStream, wrapped as a BufferedReader and reading one line at a time took *28m*. Unzipping with WinRAR took about *~30m* (this includes also writing the uncompressed data to disk). So in that respect, the code does not fall short of other bunzip tools (at least not WinRAR).\n* Before the change, the time to read the compressed data, parse and write to a one-line file, compressed took 7h (3.1M documents were read). After the change (wrapping with BOS and removing flush()) it took 2h, so significant improvement here.\n\nOverall, I think the performance of the BZIP classes is reasonable. Most of the time spent in the algorithm is in compressing the data, which is usually a process done only once. The result is a 2.5GB enwiki file compressed to a 2.31GB one-line file (8.5GB uncompressed content).\n\nI compared the time it takes to read 100k lines from the compressed and un-compressed one-line file: compressed-2.26m, un-compressed-1.36m ({color:red}-66%{color}). The difference is significant, however I'm not sure how much is it from the overall process (i.e., reading the documents and indexing them). On my machine it would take 1.1 hours to read the data, but I'm sure it will take more to index it, and the indexing time is the same whether we read the data from a bzip archive or not.\n\nI'll attach the patch shortly, and I think overall this is a good addition. It is off by default, and configurable, so if someone doesn't care about disk space, he can always run the indexing algorithm on an un-compressed one-line file.",
            "date": "2009-04-12T19:55:30.692+0000",
            "id": 18
        },
        {
            "author": "Shai Erera",
            "body": "Patch includes:\n* Wrapping the FileOutputStream with a BufferedOutputStream.\n* Removing the calls to flush().\n* Enhancement to EnwikiDocMaker's startElement and endElement - instead of calling String.equals on the qualified name and compare on 5 different strings, I added a static map from String to Integer and a static method getElementType which returns an int. I then changed those methods to do a 'switch' on the type. I haven't measured the perf. gain, but it's clear it should improve things ...\n\nThere is an open question regarding the ant-1.7.1.jar dependency. Uwe mentioned the commons Compress project, which handles the bzip format (as well as others). I took a look and found no place to download a jar, as well as this looks like a 'young' project, with very little documentation. This is not to say the code is of low quality or not be trusted, it's just that I prefer the ant dependency, at least until this project matures enough. And anyway I guess everyone who uses Lucene has Ant in his system, so this doesn't look like a major dependency.\n\nHowever, if you think otherwise, then we should get a jar from there (checking out the code and building it manually is the only way I see, but please correct me if I'm wrong) and adapt the code to use it, do perf. measurements again etc.",
            "date": "2009-04-12T20:06:57.361+0000",
            "id": 19
        },
        {
            "author": "Shai Erera",
            "body": "BTW, the enhancements to EnwikiDocMaker yielded another 2% improvement to the process of converting the enwiki file to a one-line file. Just a FYI.\nI basically wait with 1595 (refactoring to benchmark) until this one is committed, so the sooner the better ;)",
            "date": "2009-04-13T03:35:02.532+0000",
            "id": 20
        },
        {
            "author": "Michael McCandless",
            "body": "Should we consider using compress form Apache commons (from Uwe's comment [above|#action_12698191]) instead of full ant jar?\n\nbq. I basically wait with 1595 (refactoring to benchmark) until this one is committed, so the sooner the better\n\nDoes this issue depend on LUCENE-1595?",
            "date": "2009-04-13T10:07:54.078+0000",
            "id": 21
        },
        {
            "author": "Uwe Schindler",
            "body": "The problem is, that the project is currently moving to commons top-level. The SVN pathes changed, but website was not updated and so on. The snapshot jars are not accessible at the moment.\nI could quickly build a JAR and attach it here. To get the code running, you only have to change the package imports. Ideally one would use the Factory to create the decompressor (and then he do not need to skip the 2 bytes with \"BZ\").\nUwe",
            "date": "2009-04-13T10:11:28.563+0000",
            "id": 22
        },
        {
            "author": "Shai Erera",
            "body": "bq. Does this issue depend on LUCENE-1595?\n\nNo, the other way around. Well ... it's not an actual dependency, just that 1595 will touch a lot of files, and I want to minimize the noise of working on two issues that touch the same files (1595 will touch all the files this one touches) simultaneously. It's just a matter of convenience ...\n\nBesides, I don't see what else can be done as part of this issue. The performance is reasonable, the code is quite simple. The patch includes some more enhancements to those files that is unrelated to bzip per sei, but are still required.\n\nBTW, I successfully executed indexLineFile.alg on the 20070527 one-line bz2 file and the overall indexing process ended in 1h, which seems reasonable to me.\n\nRegarding Apache Compress, I asked the same question, so it's not fair to return it with a question ;). I don't think we should decide that now. It can be changed in 1595 if we think Compress is the better approach. Personally I prefer the ant jar, even though I realize it's adding a large dependency for just 3-4 classes ...",
            "date": "2009-04-13T10:19:38.740+0000",
            "id": 23
        },
        {
            "author": "Shai Erera",
            "body": "Uwe, if you can attach the jar here, I can make the necessary code changes and run some tests again. We can the decide based on whether it's working with the Compress classes or not.",
            "date": "2009-04-13T10:21:09.176+0000",
            "id": 24
        },
        {
            "author": "Uwe Schindler",
            "body": "Here the latest snapshot build of commons compress. All test passed through \"mvn install\" run.\nAbout the initial \"BZh\" bytes. In the javadocs still stands, that they should be read before opening the strea, But the examples on the website and the BZip2Decompressor code is:\n{code}\nprivate void init() throws IOException {\n        if (null == in) {\n            throw new IOException(\"No InputStream\");\n        }\n        if (in.available() == 0) {\n            throw new IOException(\"Empty InputStream\");\n        }\n        checkMagicChar('B', \"first\");\n        checkMagicChar('Z', \"second\");\n        checkMagicChar('h', \"third\");\n{code}\n\nSo I think, the reading of the initial two bytes can be left out. If something is wrong, this class should throw an IOException.\n\nHere some usage: http://wiki.apache.org/commons/Compress (this shows, that decompressing a bzip2 file does not need to skip the header),\nhere the javadocs: http://commons.apache.org/compress/apidocs/index.html",
            "date": "2009-04-13T10:29:12.088+0000",
            "id": 25
        },
        {
            "author": "Shai Erera",
            "body": "Ok I'm convinced. I moved to commons-compress and it works great. The jar is smaller and it does add the logical dependency. Since this project is still young we should expect changes, which is good since it means we can actually improve the In(Out) compressing streams to use more efficient methods, such as read(byte[]) and write(byte[]).",
            "date": "2009-04-13T11:52:28.017+0000",
            "id": 26
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good!\n\nCould you add a test case that eg writes a bzip'd line file, then reads it back & indexes it, or something along those lines?\n\nAlso: should we make \"use bzip\" pay attention to suffix when defaulting itself?  Ie if I explicitly specify \"bzip.compression\" then listen to me, but if I didn't specify it and my line file source ends with .bz2, default it to true?  (And likewise for WriteLineDoc)?",
            "date": "2009-04-13T12:19:33.457+0000",
            "id": 27
        },
        {
            "author": "Uwe Schindler",
            "body": "I created my first bug report for Compress handling the inconsistency in javadocs and the compressor part with the Bzip2 header (compression does not add header, decompression needs header): COMPRESS-69",
            "date": "2009-04-13T14:55:42.654+0000",
            "id": 28
        },
        {
            "author": "Shai Erera",
            "body": "argh, you bit me here - I planned to do so myself :)\nfor some reason their OutputStream has the file headers commented out with a comment saying \"this is added by the caller\", however their InputStream reads them ... strange. Anyway, once that's fixed and we upgrade to a proper jar, the unit test I am working on now will fail, and it will remind us to remove writing the headers in WriteLineDocTask.\n\nMike - I am working on the unit test as well as defaulting by extension. I hope a patch will be available sometime later today.",
            "date": "2009-04-13T16:08:18.921+0000",
            "id": 29
        },
        {
            "author": "Uwe Schindler",
            "body": "It is fixed *now*, including the JavaDocs: COMPRESS-69",
            "date": "2009-04-13T16:11:18.303+0000",
            "id": 30
        },
        {
            "author": "Shai Erera",
            "body": "I updated the code from SVN, but I still see wrong javadocs. In the class javadocs, for both classes, first line still says \"(without file headers)\". Also, Bzip2TestCase has a xtestBzipCreation() - the 'x' prevents this test from running as JUnit - is that intentional? I removed the 'x' and the test passes.",
            "date": "2009-04-13T16:35:38.329+0000",
            "id": 31
        },
        {
            "author": "Uwe Schindler",
            "body": "I added as comment to COMPRESS-69:\nbq. you forgot to enable the test again... \n\nHe disabled the test (he added the de/encode test directly after opening the issue because of my comment of a missing test) because it failed until he had a solution.",
            "date": "2009-04-13T16:44:30.094+0000",
            "id": 32
        },
        {
            "author": "Uwe Schindler",
            "body": "Now it's really fixed: compression and decompression are working similar, test case enabled, and javadocs fixed. That was really fast issue fixing, congratulations to COMPRESS :-)",
            "date": "2009-04-13T17:53:46.611+0000",
            "id": 33
        },
        {
            "author": "Shai Erera",
            "body": "Great !\nUwe, can you please update the jar in this issue? I will make sure the test passes with it.",
            "date": "2009-04-13T18:35:45.356+0000",
            "id": 34
        },
        {
            "author": "Uwe Schindler",
            "body": "Here is it. I thought you had checked it out, too, and created a JAR yourself. I have not done anything other. It's the (renamed) JAR file from the \"target\" dir after \"mvn install\".",
            "date": "2009-04-13T18:49:04.201+0000",
            "id": 35
        },
        {
            "author": "Shai Erera",
            "body": "Sorry about that. I didn't know what to do with the pom.xml. Given your comment above, I'll install maven and use it next time :)",
            "date": "2009-04-13T19:57:16.093+0000",
            "id": 36
        },
        {
            "author": "Shai Erera",
            "body": "Patch includes:\n* BenchmarkTestCase (currently just sets the working directory, but can be added functionality in the future).\n* LineDocMakerTest\n* WriteLineDocTaskTest\n* Update code according to the latest commons-compress.jar (i.e., not read/write file header chars).",
            "date": "2009-04-13T21:16:49.562+0000",
            "id": 37
        },
        {
            "author": "Michael McCandless",
            "body": "I had some trouble w/ the patch...\n\nFirst, I had to edit contrib/benchmark's build.xml to add the compress JAR onto the classpath (things wouldn't compile otherwise).\n\nThen I see failures in TestPerfTasksParse, eg:\n\n{code}\n    [junit] java.lang.Exception: Error: cannot understand algorithm!\n    [junit] \tat org.apache.lucene.benchmark.byTask.Benchmark.<init>(Benchmark.java:63)\n    [junit] \tat org.apache.lucene.benchmark.byTask.TestPerfTasksParse.doTestAllTasksSimpleParse(TestPerfTasksParse.java:171)\n    [junit] \tat org.apache.lucene.benchmark.byTask.TestPerfTasksParse.testAllTasksSimpleParse(TestPerfTasksParse.java:140)\n    [junit] \tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    [junit] \tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    [junit] \tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    [junit] \tat java.lang.reflect.Method.invoke(Method.java:597)\n    [junit] \tat junit.framework.TestCase.runTest(TestCase.java:164)\n    [junit] \tat junit.framework.TestCase.runBare(TestCase.java:130)\n    [junit] \tat junit.framework.TestResult$1.protect(TestResult.java:106)\n    [junit] \tat junit.framework.TestResult.runProtected(TestResult.java:124)\n    [junit] \tat junit.framework.TestResult.run(TestResult.java:109)\n    [junit] \tat junit.framework.TestCase.run(TestCase.java:120)\n    [junit] \tat junit.framework.TestSuite.runTest(TestSuite.java:230)\n    [junit] \tat junit.framework.TestSuite.run(TestSuite.java:225)\n    [junit] \tat org.junit.internal.runners.OldTestClassRunner.run(OldTestClassRunner.java:35)\n    [junit] \tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:32)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:421)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:912)\n    [junit] \tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:766)\n    [junit] Caused by: java.lang.reflect.InvocationTargetException\n    [junit] \tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    [junit] \tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\n    [junit] \tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n    [junit] \tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n    [junit] \tat org.apache.lucene.benchmark.byTask.utils.Algorithm.<init>(Algorithm.java:69)\n    [junit] \tat org.apache.lucene.benchmark.byTask.Benchmark.<init>(Benchmark.java:61)\n    [junit] \t... 19 more\n    [junit] Caused by: java.lang.IllegalArgumentException: line.file.out must be set\n    [junit] \tat org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTask.<init>(WriteLineDocTask.java:73)\n    [junit] \t... 25 more\n{code}\n\nAnd the new LineDocMakerTest fails with this:\n{code}\n    [junit] Testcase: testBZip2WithBzipCompressionDisabled(org.apache.lucene.benchmark.byTask.feeds.LineDocMakerTest):\tFAILED\n    [junit] expected:<1> but was:<0>\n    [junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>\n    [junit] \tat org.apache.lucene.benchmark.byTask.feeds.LineDocMakerTest.doIndexAndSearchTest(LineDocMakerTest.java:96)\n    [junit] \tat org.apache.lucene.benchmark.byTask.feeds.LineDocMakerTest.testBZip2WithBzipCompressionDisabled(LineDocMakerTest.java:119)\n{code}\n\nWriteLineDocTest shows a similar failure.  Not sure what's up...",
            "date": "2009-04-14T09:41:05.441+0000",
            "id": 38
        },
        {
            "author": "Shai Erera",
            "body": "That's strange ...\nAbout the build.xml, I think the problem lies in line 110, where the classpath defines explicit jars. I changed it to:\n{code}\n    <path id=\"classpath\">\n        <pathelement path=\"${common.dir}/build/classes/java\"/>\n        <pathelement path=\"${common.dir}/build/classes/demo\"/>\n        <pathelement path=\"${common.dir}/build/contrib/highlighter/classes/java\"/>\n    \t<fileset dir=\"lib\">\n    \t\t<include name=\"**/*.jar\"/>\n    \t</fileset>\n    </path>\n{code}\nand it compiled successfully. I think this change is good since it will prevent such problems in the future (in case more dependencies will be added).\n\nAbout the test failures - they pass for me in eclipse however fail in Ant. I believe I know the reason - previously, WriteLineDocTask's ctor logic was in its setUp method. I moved it to ctor since setUp is called for every document, and the initialization there did not seem right to me. The \"line.file.out' property is indeed mandatory, and hence the exception.\nThe reason it doesn't fail in eclipse is because this task is not explicitly defined in findTasks(), and I don't have the \"tasks.dir\" env variable defined. As soon as I add this line:\n{code}\ntsks.add(  \" WriteLineDoc             \"  );\n{code}\nto findTasks(), the test fails.\n\nI see several ways to solve it:\n* Make line.file.doc optional, and if not set create ByteArrayOutputStream, instead of FileOutputStream. This can also help the tests not create unnecessary files.\n* Move the logic back to setup while checking a boolean if we've been initialized yet. I don't like it very much - I think setup and teardown should be reseved for per-doLogic call.\n* Add INDENT+\"line.file.out=test/line.file\" + NEW_LINE to TestPerfTasksParse.propPart. I don't like it too since propPart is reserved for properties that are common for all tasks.\nI like (1) most.\n",
            "date": "2009-04-14T10:16:40.467+0000",
            "id": 39
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I think this change is good since it will prevent such problems in the future (in case more dependencies will be added).\n\nThat sounds good.\n\nI agree WriteLineDocTask should pull its config in ctor, not setUp.\n\nBut: I don't think WriteLineDocTask should be created when it's not going to be used, ie the TestPerfTasksParse.doTestAllTasksSimpleParse seems wrong?",
            "date": "2009-04-14T10:43:43.691+0000",
            "id": 40
        },
        {
            "author": "Shai Erera",
            "body": "Not sure what you mean. The test does use any Task, just attempts to parse algorithm texts with those tasks defined. Do you suggest we exclude WriteLineDocTask from the test?\nPerhaps we can wrap the new Benchmark() call with a try-catch on IAE and log such tests but don't fail? That way, if a certain task has mandatory properties, it shouldn't fail the test ...\nAnother option is to define for each tested task mandatory properties, in addition to the common ones used for all tasks ...\n\nUnless I misunderstand you, I don't see why this test is wrong.",
            "date": "2009-04-14T11:30:25.690+0000",
            "id": 41
        },
        {
            "author": "Michael McCandless",
            "body": "The test seems to assume you can take any Task in the source tree, and make an alg that simply creates that task.\n\nI think that assumption is in fact wrong, because tasks like WriteLineDocTask indeed require certain configuration (line.file.out) be set, and the test can't know that.  Other tasks in the future will presumably hit the same issue.\n\nAlso, thinking about the test, I think it doesn't add much value?  Elsewhere we heavily test that the .alg parser works properly.  And all this test does is take every task, and stick it in either \"XXX\",  \"[ XXX ] : 2\"  or \"{ XXX } : 3\", parse it, and verify it parsed properly.\n\nI think we should simply turn those three tests off?  Or, if that seems to drastic, simply skipping WriteLineDocTask seems OK too?",
            "date": "2009-04-14T11:44:22.692+0000",
            "id": 42
        },
        {
            "author": "Shai Erera",
            "body": "We can turn them off, or wrap new Benchmark with try-catch Exception, logging a failed task. Alternatively, we can add an 'exclude' list which will define tasks that should be discarded by the test, and add WriteLineDocTask to it.\n\nHowever, if you think those are useless, i.e. we test .alg parsing elsewhere (and I agree these tests don't add much value), then I agree we should remove them, rather than working hard to mask the test's limitations.",
            "date": "2009-04-14T12:25:58.988+0000",
            "id": 43
        },
        {
            "author": "Michael McCandless",
            "body": "OK, let's just remove them.  Can you post new patch?  Thanks.",
            "date": "2009-04-14T14:52:38.556+0000",
            "id": 44
        },
        {
            "author": "Shai Erera",
            "body": "All benchmark tests pass. Note: when you apply the patch, make sure you include the latest commons-compress jar Uwe uploaded.",
            "date": "2009-04-14T16:57:23.422+0000",
            "id": 45
        },
        {
            "author": "Michael McCandless",
            "body": "Hmm I'm still hitting some errors, eg:\n{code}\n[junit] Testcase: testRegularFileWithBZipCompressionEnabled(org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTaskTest):\tFAILED\n[junit] expected:<3> but was:<1>\n[junit] junit.framework.AssertionFailedError: expected:<3> but was:<1>\n[junit] \tat org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTaskTest.doReadTest(WriteLineDocTaskTest.java:87)\n[junit] \tat org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTaskTest.testRegularFileWithBZipCompressionEnabled(WriteLineDocTaskTest.java:144)\n[junit] \n{code}\n\nand\n\n{code}\n[junit] Testcase: testBZip2WithBzipCompressionDisabled(org.apache.lucene.benchmark.byTask.feeds.LineDocMakerTest):\tFAILED\n[junit] expected:<1> but was:<0>\n[junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>\n[junit] \tat org.apache.lucene.benchmark.byTask.feeds.LineDocMakerTest.doIndexAndSearchTest(LineDocMakerTest.java:96)\n[junit] \tat org.apache.lucene.benchmark.byTask.feeds.LineDocMakerTest.testBZip2WithBzipCompressionDisabled(LineDocMakerTest.java:119)\n{code}",
            "date": "2009-04-14T17:27:36.221+0000",
            "id": 46
        },
        {
            "author": "Shai Erera",
            "body": "That's strange ... I did the following:\n* Checkout trunk to a new project.\n* Download latest commons-compress jar Uwe added.\n* Applied the patch.\n* Ran \"ant test\".\nThe result is: BUILD SUCCESSFUL and I see those two test cases pass ... I also ran all tests from eclipse, they pass too.\n\ntestRegularFileWithBZipCompressionEnabled simulates an attempt to read a bz2 file as a regular file. The very first readLine() should throw a MalformedException or something ... that's what the test is counting on. It seems that in your case this line succeeds, reading something, and then fails on String.split(), since probably it didn't read something meaningful. I don't understand why this would happen though ....\nCan you run this test alone, w/o the rest? Perhaps debug-trace it? The test does not delete the in/output file before and after the test, but relies on FileInputStream(String/File) ctor which is supposed to re-create the file, even if it exists. Could it be that in your case it doesn't happen?\n\nI assume the second exception is thrown for the same reason. Following the steps I've done above to apply the patch, I don't understand why the test fails on your machine ...",
            "date": "2009-04-14T19:38:48.088+0000",
            "id": 47
        },
        {
            "author": "Michael McCandless",
            "body": "So, for LineDocMakerTest.testBZip2WithBzipCompressionDisabled, indeed LineDocMaker opens the binary file, but then no exception is hit: it looks for a tab delimeter, and when it can't find one, sets body/title/date to \"\" and adds the doc anyway.\n\nIn your case you hit some exception -- can you e.printStackTrace(System.out) and post back what exception that is?  Maybe somehow your bzip2 is putting a tab in the binary but mine's not?",
            "date": "2009-04-14T20:45:00.306+0000",
            "id": 48
        },
        {
            "author": "Shai Erera",
            "body": "{code}\nsun.io.MalformedInputException\n\tat sun.io.ByteToCharUTF8.convert(ByteToCharUTF8.java:262)\n\tat sun.nio.cs.StreamDecoder$ConverterSD.convertInto(StreamDecoder.java:314)\n\tat sun.nio.cs.StreamDecoder$ConverterSD.implRead(StreamDecoder.java:364)\n\tat sun.nio.cs.StreamDecoder.read(StreamDecoder.java:250)\n\tat java.io.InputStreamReader.read(InputStreamReader.java:212)\n\tat java.io.BufferedReader.fill(BufferedReader.java:157)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:320)\n\tat java.io.BufferedReader.readLine(BufferedReader.java:383)\n\tat org.apache.lucene.benchmark.byTask.feeds.LineDocMaker.makeDocument(LineDocMaker.java:187)\n\tat org.apache.lucene.benchmark.byTask.tasks.AddDocTask.setup(AddDocTask.java:61)\n\tat org.apache.lucene.benchmark.byTask.tasks.PerfTask.runAndMaybeStats(PerfTask.java:92)\n\tat org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doSerialTasks(TaskSequence.java:148)\n\tat org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doLogic(TaskSequence.java:129)\n\tat org.apache.lucene.benchmark.byTask.feeds.LineDocMakerTest.doIndexAndSearchTest(LineDocMakerTest.java:92)\n\tat org.apache.lucene.benchmark.byTask.feeds.LineDocMakerTest.testBZip2WithBzipCompressionDisabled(LineDocMakerTest.java:119)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:79)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:618)\n\tat junit.framework.TestCase.runTest(TestCase.java:164)\n\tat junit.framework.TestCase.runBare(TestCase.java:130)\n\tat junit.framework.TestResult$1.protect(TestResult.java:106)\n\tat junit.framework.TestResult.runProtected(TestResult.java:124)\n\tat junit.framework.TestResult.run(TestResult.java:109)\n\tat junit.framework.TestCase.run(TestCase.java:120)\n\tat org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)\n\tat org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)\n{code}",
            "date": "2009-04-14T20:55:39.835+0000",
            "id": 49
        },
        {
            "author": "Shai Erera",
            "body": "A long shot - can you please print the line read in makeDocument? Could it be that the line is not null, but 0 length (or contains just whitespaces)? I just thought that we're running on two different OSs (I run on Windows and you on Linux/Mac?) and perhaps on your OS the first readLine() succeeds, reading a blank line or something, and the second will fail, attempting to read the actual information?\nWeird though ... ",
            "date": "2009-04-14T21:05:58.378+0000",
            "id": 50
        },
        {
            "author": "Michael McCandless",
            "body": "Here's the line I see, nice and binary (copy/past lost the exact chars I'm sure...): BZh91AY&SY@9J",
            "date": "2009-04-14T21:22:15.351+0000",
            "id": 51
        },
        {
            "author": "Michael McCandless",
            "body": "Yeah I'm on OS X Leopard.  I just tested on a Debian linux derivative and also see the test failing.  Weird.  Not quite \"write once run anywhere\" ;)",
            "date": "2009-04-14T21:25:39.907+0000",
            "id": 52
        },
        {
            "author": "Shai Erera",
            "body": "Well ... that worries me ... when I open the bz2 file (with notepad++), I see the same line, but on my machine, readLine() fails with that MIE. It's as if on my machine the readLine() call attempts to fill the buffer of BR, and then hits the exception, while on your machine it just stops in the middle.\n\nSo I wonder how to fix it - LineDocMaker's logic is ok - makeDocument() just reads lines.. There's no point adding code which tries to compensate on any OS specific weridness. Perhaps we can change the 'else' part (which assigns title, body, date to \"\") to throw a RuntimeException (or MIE) in that case, since obviously this shouldn't happen and if it does - it's really a bug in the file format?\n\nOr, I can just remove the test ... but I think the above suggestion makes sense, and will solve it. Mike, if you agree, can you quickly apply that to your env. and note if the test fails? (it must fail, but I just want to be sure).",
            "date": "2009-04-15T02:06:49.767+0000",
            "id": 53
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Mike, if you agree, can you quickly apply that to your env. and note if the test fails?\n\nYou mean confirm the test passes on adding the RuntimeException on the else clause, right?\n\nYes, indeed the test passes with this change.  And I like the change (making LineDocMaker more brittle on receiving a malformed line).  So let's go forward with that?",
            "date": "2009-04-15T12:55:51.034+0000",
            "id": 54
        },
        {
            "author": "Shai Erera",
            "body": "Let's try with this one. Changes:\n* Added testInvalidFormat to LineDocMakerTest\n* Changed LineDocMaker to throw RuntimeException in case a line does not have two TABs.",
            "date": "2009-04-15T15:05:27.910+0000",
            "id": 55
        },
        {
            "author": "Michael McCandless",
            "body": "I'm still seeing the one WriteLineDocTest failure:\n{code}\n    [junit] Testcase: testRegularFileWithBZipCompressionEnabled(org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTaskTest):\tFAILED\n    [junit] expected:<3> but was:<1>\n    [junit] junit.framework.AssertionFailedError: expected:<3> but was:<1>\n    [junit] \tat org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTaskTest.doReadTest(WriteLineDocTaskTest.java:87)\n    [junit] \tat org.apache.lucene.benchmark.byTask.tasks.WriteLineDocTaskTest.testRegularFileWithBZipCompressionEnabled(WriteLineDocTaskTest.java:144)\n{code}\n\nI think it's a similar issue -- the doReadTest must hit an exception in readline() on your OS, but not mine.",
            "date": "2009-04-15T21:45:06.795+0000",
            "id": 56
        },
        {
            "author": "Shai Erera",
            "body": "I removed this test from WriteLineDocTaskTest, since it doesn't really belong there. It tested that if WLDT created a bz2 file, an attempt to read it as regular would fail. But reading is not part of WLDT's logic, and that test case belongs (and already exists) in LDM test.\n\nI'm tempted to say \"this patch should be fine\", but given the history of this issue and the OS weird-ness I'm being careful :)",
            "date": "2009-04-16T03:16:10.718+0000",
            "id": 57
        },
        {
            "author": "Michael McCandless",
            "body": "All tests pass!  And patch looks good.  I'll commit shortly.  Thanks Shai!",
            "date": "2009-04-16T09:46:01.872+0000",
            "id": 58
        },
        {
            "author": "Shai Erera",
            "body": "Mike, did you commit the commons-compress jar too?",
            "date": "2009-04-16T10:12:53.473+0000",
            "id": 59
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Mike, did you commit the commons-compress jar too?\n\nWoops, forgot, and now fixed -- thanks for catching that!",
            "date": "2009-04-16T11:29:04.100+0000",
            "id": 60
        },
        {
            "author": "Jason Rutherglen",
            "body": "Related to the new xerces-2.9.1-patched-XERCESJ-1257.jar in\ncontrib/benchmark I get a\n\"java.lang.UnsupportedClassVersionError: Bad version number in\n.class file\" message when building. \n\nCan you please verify?\n\nEnvironment: Java(TM) 2 Runtime Environment, Standard Edition\n(build 1.5.0_16-b06-284) Java HotSpot(TM) Client VM (build\n1.5.0_16-133, mixed mode, sharing)",
            "date": "2009-04-17T22:12:43.969+0000",
            "id": 61
        },
        {
            "author": "Shai Erera",
            "body": "Hmmm ... Mike built that file from the xerces project, after patching it with XERCESJ-1257. I don't know though which JRE he used to build it. Can you please post the full stack trace (mostly interested in the .class file with the problem and the major/minor version it reports).\n\nI use 1.5 as well and don't experience this error. I \"cd benchmark\" then \"ant jar\" and it finished successfully.\n",
            "date": "2009-04-18T02:39:46.764+0000",
            "id": 62
        },
        {
            "author": "Michael McCandless",
            "body": "For the record, here's the patch I had applied to XercesJ 2.9.1 sources:\n{code}\n--- UTF8Reader.java\t2006-11-23 00:36:53.000000000 +0100\n+++ /home/rainman/lucene/xerces-2_9_0/src/org/apache/xerces/impl/io/UTF8Reader.java\t2008-04-04 00:40:58.000000000 +0200\n@@ -534,6 +534,16 @@\n                     invalidByte(4, 4, b2);\n                 }\n \n+                // check if output buffer is large enough to hold 2 surrogate chars\n+                if( out + 1 >= offset + length ){\n+                    fBuffer[0] = (byte)b0;\n+                    fBuffer[1] = (byte)b1;\n+                    fBuffer[2] = (byte)b2;\n+                    fBuffer[3] = (byte)b3;\n+                    fOffset = 4;\n+                    return out - offset;\n+\t\t}\n+\n                 // decode bytes into surrogate characters\n                 int uuuuu = ((b0 << 2) & 0x001C) | ((b1 >> 4) & 0x0003);\n                 if (uuuuu > 0x10) {\n{code}",
            "date": "2009-04-18T11:13:27.258+0000",
            "id": 63
        },
        {
            "author": "Michael McCandless",
            "body": "I just committed a JDK 1.4 build of the patched XercesJ jar (I think I had used 1.5 previously, though I don't understand why Jason was having trouble using it).\n\nJason can you try with this new JAR?",
            "date": "2009-04-18T11:17:14.644+0000",
            "id": 64
        },
        {
            "author": "Uwe Schindler",
            "body": "Commons-Compress 1.0 is now released, we should use the official JAR file:\nhttp://commons.apache.org/compress/download_compress.cgi\n\nShould I update and test compilation?",
            "date": "2009-05-22T08:02:11.439+0000",
            "id": 65
        },
        {
            "author": "Michael McCandless",
            "body": "Excellent!  Yes I think so?",
            "date": "2009-05-22T09:27:43.016+0000",
            "id": 66
        },
        {
            "author": "Uwe Schindler",
            "body": "I replaced the dev version by 1.0 and it compiled fine. All tests fine. But I did not test the enwiki (takes too long), but according to the changelog of compress, there were no changes in Bzip code.\nI commit shortly.",
            "date": "2009-05-22T10:33:18.868+0000",
            "id": 67
        },
        {
            "author": "Uwe Schindler",
            "body": "Committed revision 777458.",
            "date": "2009-05-22T10:39:39.592+0000",
            "id": 68
        },
        {
            "author": "Mark Miller",
            "body": "some java 1.5 code got in with this patch",
            "date": "2009-07-06T18:10:10.646+0000",
            "id": 69
        },
        {
            "author": "Mark Miller",
            "body": "Looks like this spread a little in the docmaker/contentsource breakup issue as well. This patch takes care of both (a few Integer.valueOfs).",
            "date": "2009-07-06T18:14:48.526+0000",
            "id": 70
        },
        {
            "author": "Michael McCandless",
            "body": "Thank Mark!",
            "date": "2009-07-06T18:19:17.258+0000",
            "id": 71
        },
        {
            "author": "Mark Miller",
            "body": "committed",
            "date": "2009-07-06T19:20:50.395+0000",
            "id": 72
        },
        {
            "author": "Michael McCandless",
            "body": "Alas, horribly, I'm hitting this bug again, with the 2.10.0 Xerces JAR currently checked in.\n\nI downloaded the latest XML dump from Wikipedia (en), enwiki-20110115-pages-articles.xml, and after ~2.8M docs I hit this:\n{noformat}\n     [java] 592.4 sec --> main Wrote 2807000 line docs\n     [java] 592.51 sec --> main Wrote 2808000 line docs\n     [java] 592.59 sec --> main Wrote 2809000 line docs\n     [java] 592.78 sec --> main Wrote 2810000 line docs\n     [java] Exception in thread \"Thread-0\" java.lang.RuntimeException: org.apache.xerces.impl.io.MalformedByteSequenceException: Invalid byte 2 of 4-byte UTF-8 sequence.\n     [java] \tat org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource$Parser.run(EnwikiContentSource.java:197)\n     [java] \tat java.lang.Thread.run(Thread.java:619)\n     [java] Caused by: org.apache.xerces.impl.io.MalformedByteSequenceException: Invalid byte 2 of 4-byte UTF-8 sequence.\n     [java] \tat org.apache.xerces.util.ErrorHandlerWrapper.createSAXParseException(Unknown Source)\n     [java] \tat org.apache.xerces.util.ErrorHandlerWrapper.fatalError(Unknown Source)\n     [java] \tat org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)\n     [java] \tat org.apache.xerces.impl.XMLErrorReporter.reportError(Unknown Source)\n     [java] \tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)\n     [java] \tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)\n     [java] \tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n     [java] \tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n     [java] \tat org.apache.xerces.parsers.XMLParser.parse(Unknown Source)\n     [java] \tat org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)\n     [java] \tat org.apache.lucene.benchmark.byTask.feeds.EnwikiContentSource$Parser.run(EnwikiContentSource.java:174)\n     [java] \t... 1 more\n     [java] Caused by: org.apache.xerces.impl.io.MalformedByteSequenceException: Invalid byte 2 of 4-byte UTF-8 sequence.\n     [java] \tat org.apache.xerces.impl.io.UTF8Reader.invalidByte(Unknown Source)\n     [java] \tat org.apache.xerces.impl.io.UTF8Reader.read(Unknown Source)\n     [java] \tat org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)\n     [java] \tat org.apache.xerces.impl.XMLEntityScanner.scanContent(Unknown Source)\n     [java] \tat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanContent(Unknown Source)\n     [java] \t... 8 more\n     [java] ####################\n     [java] ###  D O N E !!! ###\n     [java] ####################\n{noformat}\n\nI went back to the old patched Xerces JAR, and it got past that point just fine...",
            "date": "2011-01-31T18:18:07.240+0000",
            "id": 73
        },
        {
            "author": "Michael McCandless",
            "body": "I think we should just rollback to the old (patched) JAR for 3.1/4.0?",
            "date": "2011-01-31T18:22:17.058+0000",
            "id": 74
        },
        {
            "author": "Michael McCandless",
            "body": "I also tested the latest Xerces release (2.11) and it hits the same exception as above.\n\nFeel free to go vote for XERCESJ-1257!\n\nI'll just revert to our patched JAR (based on Xerces 2.9.1).",
            "date": "2011-01-31T19:12:15.942+0000",
            "id": 75
        },
        {
            "author": "Grant Ingersoll",
            "body": "Bulk close for 3.1",
            "date": "2011-03-30T15:49:51.137+0000",
            "id": 76
        },
        {
            "author": "Michael McCandless",
            "body": "Note that enwiki-20110115-pages-articles.xml.bz2 also hits XERCESJ-1257 ...",
            "date": "2012-03-29T17:51:27.986+0000",
            "id": 77
        }
    ],
    "component": "modules/benchmark",
    "description": "bzip compression can aid the benchmark package by not requiring extracting bzip files (such as enwiki) in order to index them. The plan is to add a config parameter bzip.compression=true/false and in the relevant tasks either decompress the input file or compress the output file using the bzip streams.\nIt will add a dependency on ant.jar which contains two classes similar to GZIPOutputStream and GZIPInputStream which compress/decompress files using the bzip algorithm.\n\nbzip is known to be superior in its compression performance to the gzip algorithm (~20% better compression), although it does the compression/decompression a bit slower.\n\nI wil post a patch which adds this parameter and implement it in LineDocMaker, EnwikiDocMaker and WriteLineDoc task. Maybe even add the capability to DocMaker or some of the super classes, so it can be inherited by all sub-classes.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1591",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Enable bzip compression in benchmark",
    "systemSpecification": true,
    "version": ""
}