{
    "comments": [
        {
            "author": "Shon Vella",
            "body": "This is similar to LUCENE-505, but applies to all readers, not just MultiReader and proposes a different, less complex solution. Attaching preliminary patch.",
            "date": "2009-04-14T17:25:28.841+0000",
            "id": 0
        },
        {
            "author": "Shon Vella",
            "body": "Preliminary patch",
            "date": "2009-04-14T17:26:41.708+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "I completely agree it's silly to make huge arrays instead of accepting null.  One turns off norms to avoid huge arrays getting allocated, in the first place.\n\nUnfortunately, this is a break in back compatibility, though I think in 3.0 this change would be OK.  Perhaps, we could commit it today, w/ a deprecated method exposed in IndexReader to \"allow null to be returned by getNorms()\", which defaults to off (ie the current trunk behavior).  Then, in 3.0, we remove that method and hardwire to true.\n\nThere is also a [presumably smallish] performance hit by adding the \"norms != null\" check inside TermScorer, for every hit, but I think that's an OK tradeoff.",
            "date": "2009-04-14T17:40:02.565+0000",
            "id": 2
        },
        {
            "author": "Shon Vella",
            "body": "Will look at adding the option to turn on as time permits. Shouldn't be too hard except as it affects the unit tests - ferreting out all the tests that need to be duplicated with it on and with it off could get very tedious.",
            "date": "2009-04-14T17:53:55.547+0000",
            "id": 3
        },
        {
            "author": "Earwin Burrfoot",
            "body": "bq. There is also a [presumably smallish] performance hit by adding the \"norms != null\" check inside TermScorer, for every hit, but I think that's an OK tradeoff.\nIf guys at Java HotSpot Compiler are doing their homework, checking something != null before accessing something should come almost for free, because each access checks for null anyway.",
            "date": "2009-04-14T18:26:41.793+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "bq. If guys at Java HotSpot Compiler are doing their homework, checking something != null before accessing something should come almost for free, because each access checks for null anyway.\n\nThe problem is, depending on the result of the null check, we do very different things; I think we are relying on the CPU to correctly predict the branch, which I even wonder about since multiple threads could be running that code with the if going different ways.  And even a correct prediction there's still [small] added cost...",
            "date": "2009-04-14T18:47:27.016+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "bq. ferreting out all the tests that need to be duplicated with it on and with it off could get very tedious.\n\nI don't think you need to dup all tests w/ on & off.  If you can get Lucene's tests to pass when [temporarily] hardwiring it to on (ie, allowed to return null), I think that's fine?  Then, for the duration up until 2.9, we're testing the \"off\" case, and when we flip it in 3.0, we're testing the \"on\" case.",
            "date": "2009-04-14T18:49:37.542+0000",
            "id": 6
        },
        {
            "author": "Shon Vella",
            "body": "The check isn't for free because the HotSpot compiler doesn't check every reference for null, it just traps the resulting segmentation fault that dereferencing a null pointer causes. We could leverage this by not checking for null, but catch a null pointer exception, though I believe the extra overhead of a try/catch block would be more expensive than just checking.\n\nThe performance hit is likely only in the case where it isn't equal null - in the equal null case you end up saving a multiply plus a memory reference that is likely going to cause many CPU cache faults over the coarse of a search long enough to matter, which together probably add up to more than the cost of the check.\n\nAn alternate approach that would eliminate this overhead is to subclass the scorers that use norms and create an appropriate scorer that doesn't require the check. The drawback of this approach would be that it is harder to maintain.\n\n",
            "date": "2009-04-14T19:04:58.608+0000",
            "id": 7
        },
        {
            "author": "Earwin Burrfoot",
            "body": "Yep, that was my blunder. :)\n\nbq. An alternate approach that would eliminate this overhead is to subclass the scorers that use norms and create an appropriate scorer that doesn't require the check. The drawback of this approach would be that it is harder to maintain.\nSomebody recently raised a topic on specialized Lucene, with classes generated from templates, hardwiring various choices. Guys at MG4J generate a custom indexReader for each possible combination of index settings. So the idea of separate scorers might be valid, if we're going optimization-freak way. (and we don't have to support them separately) \n\nBy the way, why everything surrounding norms map is heavily synchronized? I haven't found a single write to the map outside of initialize().",
            "date": "2009-04-14T19:24:25.036+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "bq. By the way, why everything surrounding norms map is heavily synchronized? I haven't found a single write to the map outside of initialize().\n\nYou mean the SegmentReader.Norm class?  It's so only one thread attempts to load norms at once.  And because cloned SegmentReaders share norms (incRef/decRef).  Also, the reader can change norms (setNorm) which requires copy-on-write if more than one clone is sharing the norms.  We'll need similar care for CSFs once they accept live changes, too.\n\nbq. Somebody recently raised a topic on specialized Lucene\n\nRight, that's LUCENE-1594.  I'll fold the norms/nonorms code gen into there....\n\nSource code specialization gives sizable search performance gains w/ Lucene, but it'll be quite a while before it's committable.  Hopefully as we pull some of the optimizations there back into core (eg using random-access API to filter: LUCENE-1536), that gap shrinks.\n\nLet's proceed for now with the null check in single-source scorer.  I'll run some perf tests on it vs trunk, with the \"norms present\" case, to see where we stand.",
            "date": "2009-04-14T20:09:37.554+0000",
            "id": 9
        },
        {
            "author": "Shon Vella",
            "body": "Patch updated to add method to enable/disable the disabling of fake norms",
            "date": "2009-04-15T19:15:50.079+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "OK, patch looks good.  All tests pass, even if I temporarily default \"disableFakeNorms\" to true (but back-compat tests fail, which is expected and is why we won't flip the default until 3.0).  Thanks Shon!\n\nI still need to test perf cost of this change...",
            "date": "2009-04-16T09:48:27.329+0000",
            "id": 11
        },
        {
            "author": "Shon Vella",
            "body": "Working on an update to the patch - MultiSegmentReader needs to set disableFakeNorms transitively to it's subReaders as well as to new subReaders on reopen.",
            "date": "2009-04-16T12:30:58.814+0000",
            "id": 12
        },
        {
            "author": "Shon Vella",
            "body": "Setting disableFakeNorms transitively isn't really needed because MultiSegmentReader doesn't make any calls to the subreaders that would cause it to create it's own fake norms. We probably ought to preserve the flag on clone() and reopen() though, which is going to be a little messy because IndexReader doesn't really implement either so it would have to be handled at the root of each concrete class hierarchy that does implement those. Any thoughts on whether we need this or not?",
            "date": "2009-04-16T14:15:33.649+0000",
            "id": 13
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Setting disableFakeNorms transitively isn't really needed because MultiSegmentReader doesn't make any calls to the subreaders that would cause it to create it's own fake norms\n\nBut since we score per-segment, TermScorer would ask each SegmentReader (in the MultiSegmentReader) for its norms?  So I think the sub readers need to know the setting.\n\nbq. Any thoughts on whether we need this or not?\n\nI think we do need each class implementing clone() and reopen() to properly carryover this setting.\n",
            "date": "2009-04-16T14:37:39.948+0000",
            "id": 14
        },
        {
            "author": "Shon Vella",
            "body": "What should the transitive behavior of MultiReader, FilterReader, and ParallelReader be? I'm inclined to say they shouldn't pass through to their subordinate readers because they don't really \"own\" them. ",
            "date": "2009-04-16T21:21:18.654+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I'm inclined to say they shouldn't pass through to their subordinate readers because they don't really \"own\" them.\n\nI agree.",
            "date": "2009-04-17T00:21:37.228+0000",
            "id": 16
        },
        {
            "author": "Shon Vella",
            "body": "Updated patch that preserves disableNorms flag across clone and reopen() and applies flag transitively to MultiSegmentReader.",
            "date": "2009-04-24T20:07:09.784+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "\nI tested this change on a Wikipedia index, with query \"1\", on a field\nthat has norms.\n\nOn Linux, JDK 1.6.0_13, I can see no performance difference (both get\n7.2 qps, best of 10 runs).\n\nOn Mac OS X 10.5.6, I see some difference (13.0 vs 12.3, best of 10\nruns), but given quirkiness I've seen on OS X's results not matching\nother platforms, I think we can disgregard this.\n\nAlso, given the performance gain one sees when norms are disabled, I\nthink this is net/net a good change.\n\nWe'll leave the default as false (for back compat), but this setting\nis deprecated with a comment that in 3.0 it hardwires to true.\n",
            "date": "2009-04-27T12:32:49.562+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "New patch attached:\n\n  * Fixed contrib/instantiated & contrib/misc to pass if I change\n    default for disableFakeNorms to true (which we will hardwire in\n    3.0)\n\n  * Tweaked javadocs\n\n  * Removed unused imports\n\n  * Added CHANGES.txt entry\n\nI still need to review the rest of the patch...\n\nWith this patch, all tests pass with the default set to false\n(back-compat).  If I temporarily set it to true, all tests now pass,\nexcept back-compat (which is expected & fine).\n\nI had started down the path of having contrib/instantiated \"respect\"\nthe disableFakeNorms setting, but rapidly came to realize how little I\nunderstand contrib/instantiated's code ;) So I fell back to fixing the\nunit tests to accept null returns from the normal\nIndexReader.norms(...).\n",
            "date": "2009-04-27T12:38:10.251+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "Attached patch.\n\nI also added \"assert !getDisableFakeNorms();\" inside SegmentReader.fakeNorms().",
            "date": "2009-04-27T12:49:02.512+0000",
            "id": 20
        },
        {
            "author": "Michael McCandless",
            "body": "OK patch looks good!  I plan to commit in a day or two.  Thanks Shon!",
            "date": "2009-04-27T14:29:56.487+0000",
            "id": 21
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Shon!",
            "date": "2009-04-28T20:39:11.349+0000",
            "id": 22
        }
    ],
    "component": "core/index",
    "description": "Creating and keeping around huge arrays that hold a constant value is very inefficient both from a heap usage standpoint and from a localility of reference standpoint. It would be much more efficient to use null to represent a missing norms table.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1604",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Stop creating huge arrays to represent the absense of field norms",
    "systemSpecification": true,
    "version": "2.9"
}