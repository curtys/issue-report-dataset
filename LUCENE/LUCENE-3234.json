{
    "comments": [
        {
            "author": "Robert Muir",
            "body": "I like this tradeoff Mike, thanks!\n\nshould we consider setting some kind of absurd default like 10,000 to really prevent some pathological cases with huge documents?\nWe could document in CHANGES.txt that if you want the old behavior, set it to -1 or Integer.MAX_VALUE (I think we can use this here? offsets are ints?)",
            "date": "2011-06-23T18:01:02.142+0000",
            "id": 0
        },
        {
            "author": "Mike Sokolov",
            "body": "Yes, although a smaller number might be fine.  Maybe Koji will comment: I don't completely understand the scaling here, but it seemed to me that I had a case with around 2000 occurrences of a term that lead to a 15-20 sec evaluation time on my desktop.  The max value will be an int, sire, although I think the number is going to scale like positions, not offsets FWIW.\n",
            "date": "2011-06-23T18:12:15.766+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "yeah, you are right.. but seeing as how positions are ints too, I think it might be easier to do Integer.MAX_VALUE versus the -1 parameter.\n",
            "date": "2011-06-23T18:21:38.693+0000",
            "id": 2
        },
        {
            "author": "Mike Sokolov",
            "body": "Yes, that makes sense to me - default to 5000, say, and set explicitly to either MAX_VALUE or -1 to get the unlimited behavior (I prefer to allow -1 since otherwise you should probably treat it as an error).  Do you want me to change the patch, or should I just leave that to the committer?",
            "date": "2011-06-23T21:00:16.284+0000",
            "id": 3
        },
        {
            "author": "Robert Muir",
            "body": "You can change it if you don't mind. However, I think I agree it would be good to figure out if there is an n^2 here. This might have some affect on what the default value should be... ideally there is some way we could fix the n^2.\n\nIs there a way to turn your test case into a benchmark, or do you have a separate benchmark (the example you mentioned where it blows up really bad). This could help in looking at what's going on.\n",
            "date": "2011-06-23T21:31:32.877+0000",
            "id": 4
        },
        {
            "author": "Mike Sokolov",
            "body": "I don't think I can share the test documents I have - they belong to someone else.  I can look at trying to make something bad happen with the wikipedia data, but I'm curious why a benchmark is preferable to a test case? ",
            "date": "2011-06-23T21:53:20.847+0000",
            "id": 5
        },
        {
            "author": "Robert Muir",
            "body": "oh thats ok, i just meant a little tiny benchmark, hitting the nasty case that we might think might be n^2.\nIf the little test case does that... then that will work, just wasn't sure if it did.\n\neither way just something to look at in the profiler, etc.",
            "date": "2011-06-23T22:06:30.625+0000",
            "id": 6
        },
        {
            "author": "Mike Sokolov",
            "body": "I did go back and look at the original case that made me worried; in that case the \"bad\" document is 650K, and the matched term occurs 23000 times in it.  The search still finishes in 24 sec or so on my desktop, which isn't too bad I guess, considering.\n\nAfter looking at that and measuring the change in the test case in the patch as the number of terms increase, I don't think there actually is an n^2 - just linear, but the growth is still enough that the patch has value. The test case in the patch is closely targeted at the method which takes all the time when you have large numbers of matching terms in a single document.",
            "date": "2011-06-23T23:02:20.649+0000",
            "id": 7
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Mike, thank you for your continuous interest to FVH! Can you add the parameter for Solr, with an appropriate default value if you would like. I don't know assertTrue test in testManyRepeatedTerms() is ok, for JENKINS?",
            "date": "2011-06-24T00:29:57.203+0000",
            "id": 8
        },
        {
            "author": "Mike Sokolov",
            "body": "Added solr parameter hl.phraseLimit (default=5000)\n\nKoji - I'm not sure what the issue w/assertTrue is?  It looked to me as if the test case ultimately inherits from org.junit.Assert, which defines the method?   Is there a different version of junit on Jenkins without that method?",
            "date": "2011-06-24T02:03:09.052+0000",
            "id": 9
        },
        {
            "author": "Robert Muir",
            "body": "Oh I see, I think i'm nervous about testRepeatedTerms too.\nMaybe we can comment it out and just mention its more of a benchmark?\n\nThe problem could be that the test is timing-based... in general a machine could suddenly get busy at any time,\nespecially since we run many tests in parallel, so I'm worried it could intermittently fail.",
            "date": "2011-06-24T09:37:09.387+0000",
            "id": 10
        },
        {
            "author": "Mike Sokolov",
            "body": "Sure - the test is fragile.  It was just meant to illustrate the use case; not really a good unit test for regression.  The last patch has it commented.",
            "date": "2011-06-25T15:07:58.136+0000",
            "id": 11
        },
        {
            "author": "Digy",
            "body": "I am not sure how much it is related to this issue but there was\na similar issue in Lucene.Net.\nhttps://issues.apache.org/jira/browse/LUCENENET-350\n\n",
            "date": "2011-06-25T17:22:25.044+0000",
            "id": 12
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Updated patch attached. I added CHANGES.txt entries for Lucene and Solr, used Integer.MAX_VALUE for the default and added @param for phraseLimit in the new constructor javadoc. Will commit soon.",
            "date": "2011-06-27T01:08:55.987+0000",
            "id": 13
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Oops, wrong patch. This one is correct.",
            "date": "2011-06-27T01:12:16.151+0000",
            "id": 14
        },
        {
            "author": "Koji Sekiguchi",
            "body": "trunk: Committed revision 1139995.\n3x: Committed revision 1139997.\n\nThanks, Mike!",
            "date": "2011-06-27T02:08:31.679+0000",
            "id": 15
        },
        {
            "author": "Mike Sokolov",
            "body": "Thank you, Koji - it's nice to have my first patch committed!\n\num - one little comment; since you made the default be MAX_VALUE, there is a javadoc comment that should be updated which says it is 5000.",
            "date": "2011-06-27T02:51:05.038+0000",
            "id": 16
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Thank you again for checking the commit, Mike! The javadoc has been fixed.",
            "date": "2011-06-27T03:34:31.647+0000",
            "id": 17
        },
        {
            "author": "David Smiley",
            "body": "Why is the default unlimited; shouldn't it be the suggested 5000?  I doubt it could be for backwards compatibility since I can't see how an app might depend on the unlimited behavior. I think Solr should have good defaults that protect against pathological cases. Other defaults in Lucene/Solr have such defaults, in general (e.g. hl.maxAnalyzedChars).",
            "date": "2011-09-25T06:01:28.790+0000",
            "id": 18
        },
        {
            "author": "Koji Sekiguchi",
            "body": "It could be the suggested 5000. I don't have a persistence on it. The current default is just for back-compat.",
            "date": "2011-09-25T12:51:07.552+0000",
            "id": 19
        },
        {
            "author": "David Smiley",
            "body": "Koji; you didn't react to my comment that an app would be incredibly unlikely to actually depend on the unlimited behavior; thus there is no back-compat.  Again, I think it should be 5000.",
            "date": "2011-09-26T05:40:33.274+0000",
            "id": 20
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Ok, thank you, David. I opened SOLR-2794.",
            "date": "2011-09-26T06:40:32.005+0000",
            "id": 21
        }
    ],
    "component": "",
    "description": "With larger documents, FVH can spend a lot of time trying to find the best-scoring snippet as it examines every possible phrase formed from matching terms in the document.  If one is willing to accept\nless-than-perfect scoring by limiting the number of phrases that are examined, substantial speedups are possible.  This is analogous to the Highlighter limit on the number of characters to analyze.\n\nThe patch includes an artifical test case that shows > 1000x speedup.  In a more normal test environment, with English documents and random queries, I am seeing speedups of around 3-10x when setting phraseLimit=1, which has the effect of selecting the first possible snippet in the document.  Most of our sites operate in this way (just show the first snippet), so this would be a big win for us.\n\nWith phraseLimit = -1, you get the existing FVH behavior. At larger values of phraseLimit, you may not get substantial speedup in the normal case, but you do get the benefit of protection against blow-up in pathological cases.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-3234",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Provide limit on phrase analysis in FastVectorHighlighter",
    "systemSpecification": true,
    "version": "2.9.4, 3.0.3, 3.1, 3.2, 3.3"
}