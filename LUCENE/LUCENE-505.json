{
    "comments": [
        {
            "author": "Steven Tamm",
            "body": "This patch doesn't include my previous change to TermScorer.   It passes all of the lucene unit tests in addition to our set of tests.",
            "date": "2006-03-02T06:29:44.000+0000",
            "id": 0
        },
        {
            "author": "Steven Tamm",
            "body": "Sorry, I didn't remove whitespace in the previous patch.  This one's easier to read.\n\nsvn diff --diff-cmd diff -x \"-b -u\" works better than svn diff --diff-cmd diff -x -b -x -u",
            "date": "2006-03-02T06:57:22.000+0000",
            "id": 1
        },
        {
            "author": "Yonik Seeley",
            "body": "> MultiReader.norms() is very inefficient: it has to construct a byte array that's as long as all the documents in every\n> segment. This doubles the memory requirement for scoring MultiReaders vs. Segment Readers.\n\nAre you positive?  It shouldn't.  MultiReader.norms(field) does not call subReader.norms(field), it calls\nnorms(String field, byte[] result, int offset) that puts the results directly in the norm array without causing it to be cached in the subReader.\n\nOf course if you call norms() on both the MultiReader and subReaders yourself, then things will be doubly cached.\n\nWhat was the performance impact of your patches?",
            "date": "2006-03-02T07:15:11.000+0000",
            "id": 2
        },
        {
            "author": "Steven Tamm",
            "body": "I made the change less for MultiReader, but to prevent the instantiation of the fakeNorms array (which is an extra 1MB of useless memory for us).  In addition, we don't have long lived indexes, so keeping the index loading memory consumption down is critical.  And being able to avoid all byte[] in the future is a necessity for us.\n\nYou are correct that it won't help MultiReader.norms() that much, unless you are also calling doSetNorm (where upon you get the double instantiation, since the subreader will cache its norms as well). ",
            "date": "2006-03-02T07:24:18.000+0000",
            "id": 3
        },
        {
            "author": "Doug Cutting",
            "body": "I don't see how the memory requirements of MultiReader are twice that of SegmentReader.  MultiReader does not call norms(String) on each sub-reader, but rather norms(String, byte[], int), storing them in a previously allocated array, so the sub-reader normally never constructs an array for its norms.\n\nI also worry about performance with this change.  Have you benchmarked this while searching large indexes?  For example, in TermScorer.score(HitCollector, int), Lucene's innermost loop, you change two array accesses into a call to an interface.  That could make a substantial difference.  Small changes to that method can cause significant performance changes.\n\nThe biggest advantage of this to my eye is the removal of fakeNorms, but I think those are only rarely used, and even those uses can be eliminated.  One can now omit norms when indexing, and, if such a field is searched with a normal query then fakeNorms will be used.  But a ConstantScoringQuery of the field should return the same results, and faster too!  So the bug to fix is that, when a query is run against a field with omitted norms it should automatically be rewritten as a ConstantScoringQuery, both for speed and to avoid allocating fakeNorms.\n\nFinally, a note for other committers: we should try not to deprecate anything in Lucene until we finish removing all of the methods that were deprecated in 1.9, to minimize confusion.  Ideally we can avoid having anything deprecated until after 2.0 is out the door.",
            "date": "2006-03-02T07:24:37.000+0000",
            "id": 4
        },
        {
            "author": "Steven Tamm",
            "body": "> I also worry about performance with this change. Have you benchmarked this while searching large indexes?\nyes.  see below.  \n\n> For example, in TermScorer.score(HitCollector, int), Lucene's innermost loop, you change two array accesses into a call to an interface. That could make a substantial difference. Small changes to that method can cause significant performance changes. \n\nSpecifically \"you change two array accesses into a call to an interface.\"  I have changed two byte array references (one of which is static), to a method call on an abstract class.  I'm using JDK 1.5.0_06.  Hotspot inlines both calls and performance was about the same with a 1M docs index (we have a low term/doc ratio, so we have about 8.5M terms).  HPROF doesn't even see the call to Similarity.decodeNorm.  If I was using JDK 1.3, I'd probably agree with you, but HotSpot is very good at figuring this stuff out and autoinlining the calls.\n\nAs for the numbers: an average request returning 5000 hits from our 0.5G index was at ~485ms average on my box before.  It's now at ~480ms.  (50 runs each).  Most of that is overhead, granted.  \n\nThe increase in performance may be obscured by my other change in TermScorer (LUCENE-502).  I'm not sure of the history of TermScorer, but it seems heavily optimized for a Large # Terms/Document.  We have a low # Terms/Document, so performance suffers greatly..  Performance was dramatically improved by not unnecessarily caching things.   TermScorer seems to be heavily optimized for a non-modern VM (like inlining next() into score(), caching result of Math.sqrt for each term being queried, having a doc/freq cache that provides no benefit unless iterating backwards, etc).  The total of the term scorer changes brought the average down from ~580ms. \n\nSince we use a lot of large indexes and don't keep them in memory all that often, our performance increases dramatically due to the reduction in GC overhead.  As we move to not actually storing the Norms array in memory but instead using the disk, this change will have an even higher benefit.  I'm in the process of preparing a set of patches that will help people that don't have long-lived indexes, and this is just one part.",
            "date": "2006-03-02T08:00:43.000+0000",
            "id": 5
        },
        {
            "author": "Steven Tamm",
            "body": "Here's a patch where if you turn LOAD_NORMS_INTO_MEM to false, it will instead load the norms from the disk all the time.  When combined with LUCENE-508 (the prefetching patch), you can dramatically reduce the amount of memory generated when you have 1 query/index.",
            "date": "2006-03-02T12:23:52.000+0000",
            "id": 6
        },
        {
            "author": "Yonik Seeley",
            "body": "> I made the change less for MultiReader, but to prevent the instantiation of the fakeNorms array (which is an extra 1MB of useless memory for us).\nAre you using the omitNorms feature?  If not, what is causing the fakeNorms to be allocated?",
            "date": "2006-03-02T12:38:09.000+0000",
            "id": 7
        },
        {
            "author": "Yonik Seeley",
            "body": "> One can now omit norms when indexing, and, if such a field is searched with a normal query then fakeNorms will be used.\n>  But a ConstantScoringQuery of the field should return the same results, and faster too!\n\nIf you mean ConstantScoreQuery, that doesn't currently include tf or idf, so the scores won't match.\n\nThere is a memory related optimization that can be made in SegmentReader.norms(field,array,offset) though.\nSystem.arraycopy(fakeNorms(), 0, bytes, offset, maxDoc());\ncould be replaced with Arrays.fill",
            "date": "2006-03-02T12:49:46.000+0000",
            "id": 8
        },
        {
            "author": "Steven Tamm",
            "body": "We're still using TermScorer, which generates the fakeNorms() regardless of omitNorms on or off.  ConstantTermScorer is a step in the right direction, but you already said what I was going to say about it .\n\nSpecifically, we have one field where we want norms, and one field where we don't.  As you can see by the \"LazyNorms\" patch, the big bang for the buck for us (besides optimizing TermScorer.score()), was to lazily load the norms from the disk since we don't keep the indexes in memory.  If you only are scoring 3 docs (ever), why should it load a 1MB array (or in the case of TermScorer, a 1MB array from disk and then an empty 1MB array).  A better ConstantScoreQuery would also work, but only if the caller knew which fields had omitNorms on or off.",
            "date": "2006-03-02T13:03:04.000+0000",
            "id": 9
        },
        {
            "author": "Yonik Seeley",
            "body": ">We're still using TermScorer, which generates the fakeNorms() regardless of omitNorms on or off.\n\nLet me focus on that point for the moment so we can see if there is a bug in fakeNorms or not.\nIf omitNorms is off (the normal behavior of all indexed fields having norms), then fakeNorms won't ever be allocated,\n*except* in the case of a MultiReader calling norms() on a subreader that doesn't know about that field (because no docs had that field indexed).  That allocation of fakeNorms() would also be eliminated by the Arrays.fill() fix I mentioned above.  \nI'll look into that after Lucene 2.0 comes out.\n",
            "date": "2006-03-02T13:47:37.000+0000",
            "id": 10
        },
        {
            "author": "Doug Cutting",
            "body": "It is not clear to me that your uses are typical uses.  These optimizations were added because they made big improvements.  They were not premature.  In some cases JVM's may have evolved so that some of them are no longer required.  But some of them may still make significant improvements for lots of users.\n\nI'd like to see some benchmarks from other applications before we commit big changes to such inner loops.",
            "date": "2006-03-04T03:15:18.000+0000",
            "id": 11
        },
        {
            "author": "Steven Tamm",
            "body": "There was a bug in MultiReader.java where I wasn't handling the caches correctly, specifically in getNormFactors and doSetNorm.\n\nThis is also smaller and updated for 2.0",
            "date": "2006-04-13T04:24:11.000+0000",
            "id": 12
        },
        {
            "author": "Mark Miller",
            "body": "From my experience with LUCENE-831, changing array access to method invocation does come with a good size penalty (5-15% was what I generally saw I believe). These are not identical things, but I think are close enough to take away that there will be a good measurable size hit to such a change. On the other hand, you could do cool norms reopen stuff with the method calls (eg have each IndexReader maintain its own norms and MultiReaders sub delegate norms calls)...",
            "date": "2008-11-17T00:28:08.218+0000",
            "id": 13
        },
        {
            "author": "Shon Vella",
            "body": "Without this sort of change, searching a large index (think 100 million or more) uses an inordinate amount of heap.",
            "date": "2009-01-15T20:37:37.118+0000",
            "id": 14
        },
        {
            "author": "Uwe Schindler",
            "body": "In my opinion the problem with large indexes is more, that each SegmentReader has a cache of the last used norms. If you have many fields with norms enabled the cache grows and is never freed. In my opinion, the cache should be a LRU cache or a WeakHashMap or something like that.\nYou can see this problem, if you create an index with many fields with norms (I tested with about 4,000 fields) and many documents (half a million). If you then call CheckIndex, that calls norms() for each (!) field in the Segment and each of this calls creates a new cache entry, you get OutOfMemoryExceptions after short time (I tested with the above index: I was not able to do a CheckIndex even with \"-Xmx 16GB\" on 64bit Java).",
            "date": "2009-01-15T21:28:33.672+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "bq. In my opinion the problem with large indexes is more, that each SegmentReader has a cache of the last used norms.\n\nI believe when MultiReader.norms is called (as Doug & Yonik said above), the underlying SegmentReaders do not in fact cache the norms (this is not readily obvious until you scrutinize the code).  Ie, it's only MultiReader that caches the full array.\n\nBut I agree there would be good benefits (not creating fakeNorms) to moving away from byte[] for norms.  I think an iterator only API might be fine (giving us more freedom on the impl.), though I would worry about performance impact.\n\nOr we could make a new method to replace norms() that returns null when the field has no norms, and then Scorers that use this API would handle the null correctly.  We could fix all core/contribs to use the new API...\n\nAlso note that with LUCENE-1483, we are moving to searching each segment at a time, so MultiReader.norms should not normally be called, unless it doesn't expose its underlying readers.",
            "date": "2009-01-15T21:49:42.782+0000",
            "id": 16
        },
        {
            "author": "Uwe Schindler",
            "body": "{quote}\nbq. In my opinion the problem with large indexes is more, that each SegmentReader has a cache of the last used norms.\n\nI believe when MultiReader.norms is called (as Doug & Yonik said above), the underlying SegmentReaders do not in fact cache the norms (this is not readily obvious until you scrutinize the code). Ie, it's only MultiReader that caches the full array.\n{quote}\nIn my opinion, this is not correct. I did not use a MultiReader. CheckIndex opens and then tests each segment with a separate SegmentReader. The big index with the OutOfMemory problem was optimized, so consisting of one segment with about half a million docs and about 4,000 fields. Each byte[] array takes about a half MiB for this index. The CheckIndex funtion created the norm for 4000 fields and the SegmentReader cached them, which is about 2 GiB RAM. So OOMs are not unusal.\n\nThe code taken from SegmentReader is here:\n{code}\n  protected synchronized byte[] getNorms(String field) throws IOException {\n    Norm norm = (Norm) norms.get(field);\n    if (norm == null) return null;  // not indexed, or norms not stored\n    synchronized(norm) {\n      if (norm.bytes == null) {                     // value not yet read\n        byte[] bytes = new byte[maxDoc()];\n        norms(field, bytes, 0);\n        norm.bytes = bytes;                         // cache it\n        // it's OK to close the underlying IndexInput as we have cached the\n        // norms and will never read them again.\n        norm.close();\n      }\n      return norm.bytes;\n    }\n  }\n{code}\nEach reader contains a Map of Norm entries for each field. When for the first time the norm for a specific field are read, norm.bytes==null and then it is cached inside this Norm object. And it is never freed.\n\nIn my opinion, the best would be to use a Weak- or better a SoftReference so norms.bytes gets java.lang.ref.SoftReference<byte[]> and used for caching.\n\nI will prepare a patch, should I open a new issue for that? I found this problem yesterday when testing with very large indexes (you may have noticed my mail about removing norms from Trie fields).\n\nUwe",
            "date": "2009-01-15T22:15:20.907+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. CheckIndex opens and then tests each segment with a separate SegmentReader.\n\nYou're right: CheckIndex loads the norms of every field (even those\nw/o norms), and then that memory is not released until that reader is\nclosed & unreferenced.\n\nBut that's a different issue than this one (this one is about MultiReader).\n\nCan you open a new issue?  I don't think Soft/WeakReference is the right\nsolution (they give us little control on when the cache is evicted); we\ncould do something first specifically for CheckIndex (eg it could\nsimply use the 3-arg non-caching bytes method instead) to prevent OOM\nerrors when using it.\n",
            "date": "2009-01-15T22:30:48.962+0000",
            "id": 18
        },
        {
            "author": "Uwe Schindler",
            "body": "Mike: I created new issue LUCENE-1520 and added some remarks to your last message, too.",
            "date": "2009-01-15T22:44:58.555+0000",
            "id": 19
        },
        {
            "author": "Uwe Schindler",
            "body": "Since Lucene 2.9 we search on each segment separately, so MultiReader's norms cache would never be used, exept in custom code that calls norms() on the MultiReader/DirectoryReader. Since Lucene 4.0 this is also not allowed anymore, non-atomic readers don't support norms. If you still need to get global norms, you can use MultiNorms but that is discouraged.\n\nSee also: LUCENE-2771",
            "date": "2011-01-25T15:18:11.424+0000",
            "id": 20
        }
    ],
    "component": "core/index",
    "description": "MultiReader.norms() is very inefficient: it has to construct a byte array that's as long as all the documents in every segment.  This doubles the memory requirement for scoring MultiReaders vs. Segment Readers.  Although this is cached, it's still a baseline of memory that is unnecessary.\n\nThe problem is that the Normalization Factors are passed around as a byte[].  If it were instead replaced with an Object, you could perform a whole host of optimizations\na.  When reading, you wouldn't have to construct a \"fakeNorms\" array of all 1.0fs.  You could instead return a singleton object that would just return 1.0f.\nb.  MultiReader could use an object that could delegate to NormFactors of the subreaders\nc.  You could write an implementation that could use mmap to access the norm factors.  Or if the index isn't long lived, you could use an implementation that reads directly from the disk.\n\nThe patch provided here replaces the use of byte[] with a new abstract class called NormFactors.  \nNormFactors has two methods on it\n    public abstract byte getByte(int doc) throws IOException;  // Returns the byte[doc]\n    public float getFactor(int doc) throws IOException;            // Calls Similarity.decodeNorm(getByte(doc))\n\nThere are four implementations of this abstract class\n1.  NormFactors.EmptyNormFactors - This replaces the fakeNorms with a singleton that only returns 1.0\n2.  NormFactors.ByteNormFactors - Converts a byte[] to a NormFactors for backwards compatibility in constructors.\n3.  MultiNormFactors - Multiplexes the NormFactors in MultiReader to prevent the need to construct the gigantic norms array.\n4.  SegmentReader.Norm - Same class, but now extends NormFactors to provide the same access.\n\nIn addition, Many of the Query and Scorer classes were changes to pass around NormFactors instead of byte[], and to call getFactor() instead of using the byte[].  I have kept around IndexReader.norms(String) for backwards compatibiltiy, but marked it as deprecated.  I believe that the use of ByteNormFactors in IndexReader.getNormFactors() will keep backward compatibility with other IndexReader implementations, but I don't know how to test that.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-505",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "MultiReader.norm() takes up too much memory: norms byte[] should be made into an Object",
    "systemSpecification": true,
    "version": "2.0.0"
}