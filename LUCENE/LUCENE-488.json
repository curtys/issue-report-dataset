{
    "comments": [
        {
            "author": "Hoss Man",
            "body": "two things i forgot to mention before...\n\n1) It seems i can as many 4mb documents as my heart desires, but once i go up to 5 all hell breaks loose.\n\n2) I didn't try playing with the various IndexWriter options to see what affect they had on the breaking point.\n",
            "date": "2006-01-21T11:37:33.000+0000",
            "id": 0
        },
        {
            "author": "Daniel Naber",
            "body": "writer.setMaxBufferedDocs(5); solves to OOM error, at least for binary stuff that's 5MB. So with writer.setMaxBufferedDocs(1) you can probably add documents that are almost as big as your JVM maximum memory I guess.\n",
            "date": "2006-01-22T05:15:47.000+0000",
            "id": 1
        },
        {
            "author": "Daniel Naber",
            "body": "writer.setMaxBufferedDocs(1) was a bad idea, it doesn't work because of an off-by-one bug. writer.setMaxBufferedDocs(2) should work, but I had to stop the unit test because it's too slow because of the many disk accesses. Other things to try:\n\n-get stack trace of OOM (requires java 1.5)\n-use writer.setUseCompoundFile(false) and look at the index directory after the crash\n-use writer.setInfoStream(System.out) to get some (not much) more output from Lucene\n\nBTW, this seems to affect all big stored fields, not just binary fields.\n\n(Please reply here in the issue tracker, not on the mailing list. This way things can be properly tracked).\n",
            "date": "2006-01-22T22:35:23.000+0000",
            "id": 2
        },
        {
            "author": "george washington",
            "body": "Daniel, a combination of :\n\n      iwriter.setMaxBufferedDocs(2);\n      iwriter.setMergeFactor(2);\n      iwriter.setUseCompoundFile(false);\n\nseems to help. I still get OOM errors but only with several larger docs (>10MB),  in succession, a significant improvement from the 5MB docs limit.\nPerhaps this issue should be kept open so that a more satisfactory solution is found.\nThank you for your help.\n",
            "date": "2006-01-23T17:58:56.000+0000",
            "id": 3
        },
        {
            "author": "Doron Cohen",
            "body": "This problem was resolved by LUCENE-843, after which stored fields are written directly into the directory (therefore not consuming aggregated RAM). \n\nIt is interesting that the test provided here was allocating a new byte buffer of 2 - 10 MB for each added doc. This by itself couldeventually lead to OOMs because as the program ran longer it was becoming harder to alocate consecutive chunks of those sizes.  Enhancing binary fields with offset and length (?)  would allow applications to reuse the input byte array and allocate less of those.",
            "date": "2007-12-31T10:56:32.498+0000",
            "id": 4
        }
    ],
    "component": "",
    "description": "as reported by George Washington in a message to java-user@lucene.apache.org with subect \"Storing large text or binary source documents in the index and memory usage\" arround 2006-01-21 there seems to be a problem with adding docs containing really large fields.\n\nI'll attach a test case in a moment, note that (for me) regardless of how big i make my heap size, and regardless of what value I set  MIN_MB to, once it starts trying to make documents of containing 5mb of data, it can only add 9 before it rolls over and dies.\n\nhere's the output from the code as i will attach in a moment...\n\n    [junit] Testsuite: org.apache.lucene.document.TestBigBinary\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 78.656 sec\n\n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: directory will not be cleaned up automatically...\n    [junit] Dir: /tmp/org.apache.lucene.document.TestBigBinary.97856146.100iters.4mb\n    [junit] iters completed: 100\n    [junit] totalBytes Allocated: 419430400\n    [junit] NOTE: directory will not be cleaned up automatically...\n    [junit] Dir: /tmp/org.apache.lucene.document.TestBigBinary.97856146.100iters.5mb\n    [junit] iters completed: 9\n    [junit] totalBytes Allocated: 52428800\n    [junit] ------------- ---------------- ---------------\n    [junit] Testcase: testBigBinaryFields(org.apache.lucene.document.TestBigBinary):    Caused an ERROR\n    [junit] Java heap space\n    [junit] java.lang.OutOfMemoryError: Java heap space\n\n\n    [junit] Test org.apache.lucene.document.TestBigBinary FAILED\n",
    "hasPatch": false,
    "hasScreenshot": false,
    "id": "LUCENE-488",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "adding docs with large (binary) fields of 5mb causes OOM regardless of heap size",
    "systemSpecification": true,
    "version": "1.9"
}