{
    "comments": [
        {
            "author": "Njal Karevoll",
            "body": "Adds two unit tests, one showing each behavior, and a fix for both issues.",
            "date": "2011-09-06T20:06:50.191+0000",
            "id": 0
        },
        {
            "author": "Njal Karevoll",
            "body": "The above patch is trivial to backport for 3.3/3.4.\n\nIt is similar to LUCENE-3038, but is not duplicated by LUCENE-3022, which deals with issues surrounding the interpretation of onlyLongestMatch.",
            "date": "2011-09-06T20:12:39.006+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "thanks for the patch: all tests pass here with it, and I think the added tests are clear.",
            "date": "2011-09-07T14:52:00.953+0000",
            "id": 2
        },
        {
            "author": "Robert Muir",
            "body": "Thanks Njal!",
            "date": "2011-09-08T15:18:12.509+0000",
            "id": 3
        },
        {
            "author": "Uwe Schindler",
            "body": "Bulk close after release of 3.5",
            "date": "2011-11-27T12:29:23.547+0000",
            "id": 4
        }
    ],
    "component": "modules/analysis",
    "description": "Due to an off-by-one error, a subword placed at the end of a compound word will not get a token added to the token stream.\n\n\nFor example (from the unit test in the attached patch):\nDictionary: {\"ab\", \"cd\", \"ef\"}\nInput: \"abcdef\"\nCreated tokens: {\"abcdef\", \"ab\", \"cd\"}\nExpected tokens: {\"abcdef\", \"ab\", \"cd\", \"ef\"}\n\n\nAdditionally, it could produce tokens that were shorter than the minSubwordSize due to another off-by-one error. For example (again, from the attached patch):\n\n\nDictionary: {\"abc\", \"d\", \"efg\"}\nMinimum subword length: 2\nInput: \"abcdefg\"\nCreated tokens: {\"abcdef\", \"abc\", \"d\", \"efg\"}\nExpected tokens: {\"abcdef\", \"abc\", \"efg\"}\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-3417",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "DictionaryCompoundWordTokenFilter does not properly add tokens from the end compound word.",
    "systemSpecification": true,
    "version": "3.3, 4.0-ALPHA"
}