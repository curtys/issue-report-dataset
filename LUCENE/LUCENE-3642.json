{
    "comments": [
        {
            "author": "Robert Muir",
            "body": "screenshot from the user",
            "date": "2011-12-12T12:25:02.078+0000",
            "id": 0
        },
        {
            "author": "Robert Muir",
            "body": "I thought up a hackish way we can test for these invalid offsets for all filters... I'll see if it works.",
            "date": "2011-12-12T12:37:05.358+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "here's a test.\n\nthe problem is a previous filter 'lengthens' this term by folding \u00e6 -> ae, but EdgeNGramFilter computes the offsets \"additively\": offsetAtt.setOffset(tokStart + start, tokStart + end);\n\nBecause of this if a word has been 'lengthened' by a previous filter, edgengram will produce offsets that are longer than the original text. (and probably bogus ones if its been shortened).\n\nI think we should what WDF does here, if the original offsets have already been changed (startOffset + termLength != endOffset), then we should simply preserve them for the new subwords.\n\nI added a check for this to basetokenstreamtestcase... now to see if anything else fails... ",
            "date": "2011-12-12T13:19:09.094+0000",
            "id": 2
        },
        {
            "author": "Robert Muir",
            "body": "so my assert trips for shit like whitespacespacetokenizer + lowercase... how horrible is that?\n\nThere must be offset bugs in CharTokenizer... i'll dig into it.",
            "date": "2011-12-12T13:37:27.422+0000",
            "id": 3
        },
        {
            "author": "Robert Muir",
            "body": "Here's a patch fixing the (edge)ngrams filters, using the same logic as wdf (its well-defined, i think its the only thing we can do here).\n\nStill need to fix the chartokenizer bug, and also add some tests for any other \"filters that are actually tokenizers\" we might have.",
            "date": "2011-12-12T14:16:06.402+0000",
            "id": 4
        },
        {
            "author": "Max Beutel",
            "body": "Robert, that patch for the EdgeNGramTokenFilter worked. If there occur any problems I let you know. Thanks!",
            "date": "2011-12-12T14:58:40.933+0000",
            "id": 5
        },
        {
            "author": "Robert Muir",
            "body": "Thanks Max, I am currently adding more tests/fixes for other broken tokenizers/filters with offset bugs.\n\nI'll update the patch when these are passing, but i think the ngrams stuff is ok.",
            "date": "2011-12-12T15:25:56.954+0000",
            "id": 6
        },
        {
            "author": "Robert Muir",
            "body": "updated patch with a test+fix for smartchinese, and with a test for CharTokenizer... it currently fails with an off by one (incorrect startOffset) which is in turn jacking up the endOffsets too. ",
            "date": "2011-12-12T16:27:50.604+0000",
            "id": 7
        },
        {
            "author": "Robert Muir",
            "body": "here's the fix for CharTokenizer.\n\nTests are passing, I will commit soon.",
            "date": "2011-12-12T16:35:12.901+0000",
            "id": 8
        },
        {
            "author": "Robert Muir",
            "body": "Just looking i see another bug in CharTOkenizer... i'll add another test.",
            "date": "2011-12-12T16:41:40.780+0000",
            "id": 9
        },
        {
            "author": "Robert Muir",
            "body": "patch with tests and fix for the additional bug in CharTokenizer.",
            "date": "2011-12-12T17:22:36.615+0000",
            "id": 10
        }
    ],
    "component": "",
    "description": "A user reported this because it was causing his highlighting to throw an error.",
    "hasPatch": true,
    "hasScreenshot": true,
    "id": "LUCENE-3642",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "EdgeNgrams creates invalid offsets",
    "systemSpecification": true,
    "version": "3.5"
}