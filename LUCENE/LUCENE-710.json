{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "There has been some great design discussions / iterations recently\non how to approach this:\n\n    http://www.gossamer-threads.com/lists/lucene/java-dev/44162\n\n    http://www.gossamer-threads.com/lists/lucene/java-dev/44236\n\n\nI think we've iterated to a good approach now.  Here's the summary:\n\n  * First, add an option to IndexWriter to \"commit (write segments_N)\n    only on close\" vs writing a segments_N every time there is a\n    flush, merge, etc., during a single IndexWriter session.\n\n    This means a reader won't see anything a writer has been doing\n    until it's closed.\n\n    We would still have an \"autoCommit\" true/false (default true) to\n    keep backwards compatibility.  If true, the IndexWriter writes a\n    new segments_N every time it flushes, merges segments, etc.; else\n    it only writes one on close.\n\n    We would add an \"abort()\" to IndexWriter to not commit, clean up\n    any temp files created, and rollback.\n\n    \"Commit on close\" will also address / enable fixes for other\n    issues like prevent readers from refreshing half way through\n    something like \"bulk delete then bulk add\", preventing readers\n    from refreshing during optimize() thus tying up lots of disk\n    space, enabling a write session to be transactional (all or\n    none), etc.\n\n\n  * Second, change how IndexFileDeleter works: have it keep track of\n    which commits are still live and which one is pending (as the\n    SegmentInfos in IndexWriter, not yet written to disk).\n\n    Allow IndexFileDeleter to be subclassed to implement different\n    \"deletion policies\".\n\n    The base IndexFileDeleter class will use ref counts to figure out\n    which individual index files are still referenced by one or more\n    \"segments_N\" commits or by the uncommitted \"in-memory\"\n    SegmentInfos.  Then the policy is invoked on commit (and also on\n    init) and can choose which commits (if any) to now remove.\n\n    Add constructors to IndexWriter allowing you to pass in your own\n    deleter. The default policy would still be \"delete all past\n    commits as soon as a new commit is written\" (this is how deleting\n    happens today).\n\n    For NFS we can then try different policies as discussed on those\n    threads above (there were at least 4 proposals).  They all have\n    different tradeoffs.  I would open separate issues for these\n    policies after this issue is resolved.\n",
            "date": "2007-01-19T15:09:01.855+0000",
            "id": 0
        },
        {
            "author": "Marvin Humphrey",
            "body": "(This is a continuation of the discussion from one of the threads quoted in the previous comment, with some summations to provide context.)\n\nReferring to a proposal to implement advisory locking where possible, and prevent index creation on volumes/systems where the delete mechanism fails, Michael McCandless wrote:\n\nBut what I don't like about it is it doesn't \"gracefully degrade\" to\nthe common NFS case where locking does not work.  And, this is often\noutside our user's control. \n\nThe stratagem of publicly exposing IndexFileDeleter does not \"degrade gracefully\", either.  \n\nIf today's default deletions policy remains the default, then when an index is put on an NFS system, at some point search-time exceptions will occur after an update to an index.  The user has to 1) detect this scenario without Lucene's help, probably from logs or user complaints 2) diagnose it without the aid of a meaningful error message, and 3) either implement their own IndexFileDeleter or choose an appropriate provided subclass, a task which will require that they grok both the innards of Lucene and the subtleties of NFS.\n\nThe first step to graceful degradation is a meaningful error message.  That means detecting a problem which normally requires both a indexing process and a search process to trigger, so we have to simulate it artificially with a test.\n\n   1) Open a file for read/write and write a few bytes to it.\n   2) Delete the test file (without closing the handle).\n   3) Try to read from the handle and if a \"stale NFS filehandle\" \n      exception is caught, throw something more informative.\n\nOur first opportunity to perform this test occurs at index-creation time.  This is essentially cost free.\n\nA second opportunity arises whenever deletions are performed.  Here, there's a small cost involved, and it may not be worth it, as this would only catch cases where an index was copied onto an NFS volume rather than created there, then subsequently modified.\n\nThere's also another stratagem available to us: we can have IndexReaders establish advisory read locks against their relevant segments_N files on systems which support such locks.  This won't help us to \"degrade gracefully\".  However, it will help us degrade less often, as modern versions of NFS support file locking - but still do NOT support the \"delete-on-last-close\" mechanism Lucene depends on.\n\nImplementing advisory read locks is orthogonal to the addition of IndexFileDeleter, but diminishes the justification for adding it as a public API.\n\nBut the good news is since we will allow subclassing to make your own\ndeletion policy, we can eventually do both of these approaches and our\nusers can pick one or do their own.\n\nThe number of users this class will serve is diminishingly small.    Other mitigation strategies are available.    \n\n1) If we implement advisory read locks, many people who see this error will no longer see it.  For those who do, the best option is to upgrade the OS to a version which supports advisory locks over NFS.  Then an index on an NFS volume will behave as any other.\n2) If you don't actually need to put the index on an NFS volume, put it somewhere else.\n3) Catch stale NFS filehandle exceptions in your search application and refresh the reader when they occur.\n4) Maintain two copies of an index and do an rsync/switch.\n5) Hack Lucene.\n\nFlexibility is not free.  There have been recent lamentations on java-dev about how difficult it will be to merge the write interfaces of IndexReader and IndexWriter to provide a single, unified class through which all index modifications can be performed.  The exposure of the IndexFileDeleter mechanism contributes to this problem -- it's one more small step in the wrong direction.  \n\nProviding a subclassing/callback API is often an elegant strategy, and it is surely better in this case than it would be to provide a list of deletion policies for the user to select from.  However, whenever possible, _no_ API is always a better solution -- especially in a case like this one, where the functionality provided has nothing to do with Lucene's core mission and is there solely to work around an implmentation-specific bug.  \n",
            "date": "2007-01-19T18:25:43.962+0000",
            "id": 1
        },
        {
            "author": "Marvin Humphrey",
            "body": "There are two sections in the previous comment that are supposed to be quoted, but which are not because JIRA ate the email-style \"greater than\" quoting.  They are:\n\n  \"But what I don't like about it is it doesn't \"gracefully degrade\" to\n   the common NFS case where locking does not work. And, this is often\n   outside our user's control.\"\n\n \"But the good news is since we will allow subclassing to make your own\n  deletion policy, we can eventually do both of these approaches and our\n  users can pick one or do their own.\"\n\nI wish  it were possible to edit that note, as it's really confusing as things stand.  :(  A preview mechanism would have been handy.\n ",
            "date": "2007-01-19T18:33:27.589+0000",
            "id": 2
        },
        {
            "author": "Doron Cohen",
            "body": ">   * Second, change how IndexFileDeleter works: have it keep track of\n>     which commits are still live and which one is pending (as the\n>     SegmentInfos in IndexWriter, not yet written to disk).\n> \n>     Allow IndexFileDeleter to be subclassed to implement different\n>     \"deletion policies\".\n> \n>     The base IndexFileDeleter class will use ref counts to figure out\n>     which individual index files are still referenced by one or more\n>     \"segments_N\" commits or by the uncommitted \"in-memory\"\n>     SegmentInfos.  Then the policy is invoked on commit (and also on\n>     init) and can choose which commits (if any) to now remove.\n> \n>     Add constructors to IndexWriter allowing you to pass in your own\n>     deleter. The default policy would still be \"delete all past\n>     commits as soon as a new commit is written\" (this is how deleting\n>     happens today).\n> \n>     For NFS we can then try different policies as discussed on those\n>     threads above (there were at least 4 proposals).  They all have\n>     different tradeoffs.  I would open separate issues for these\n>     policies after this issue is resolved.\n> \n\nThis ties solving the NFS issue with an extendable-file-deletion policy.\nI am wondering is this the right way, or, perhaps, should the reference \ncounting be considered alone, apart from the deletion policy.\n(Would modifying IndexFileDeleter to base on ref-count make it simpler\nor harder to maintain?)\n\nAlso, IndexFileDeleter is doing delicate work - not sure you want \napplications to mess with it. Better let applications control some\nsimple well defined behavior, maybe the same way that a sorter \nallows applications to provide a comparator, but keeps the sorting \nalgorithm for itself.\n\nBack to reference counting,- how about the following approach:\n- Add to Directory a FileReferenceCounter data member, get()/set() etc.\n- Add a class FileReferenceCounter with simple general methods:\n  void increment (String name)\n  void decrement (String name)\n  int getRefCount (String name)\n- Default implementation would do nothing, i.e. would not record \n  references, and always return 0.\n- IndexReader, upon opening a segment, would call increment(segName)\n- IndexReader, upon closing a segment, would call decrement(segName)\n- IndexFileDeleter, before removing a file belonging to a certain segment, \n  would verify getRefCount(segName)==0.\n- Notice that the FilereferenceCounter is available from the Directory, \n  so no constructors should be added to IndexWriter/Reader.\n\nSo, this is adding to Directory a general file utility, no knowledge of \nindex structure required in Directory. Also, IndexFileDeleter can remain \nas today, and at some later point can be made more powerful with various \ndeletion policies - but those policies remain unrelated to the NFS \nissue - they can focus on point-in-time issues, where I think it \nstemmed from. \n\nAn NFS geared FileReferenceCounter would then be able to keep alive \n\"counter files\", name those files based on counted fileName plus\nprocessID plus machID, base getRefCount on safety window since file \nwas last touched, etc. All this is left out from point-in-time \npolicies (how many/time points-in-time should be retained).",
            "date": "2007-01-19T18:55:21.411+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "OK, a few top level summary comments and then some specifics below:\n\n   * I don't want to modify the Lucene core (adding advisory read\n     locks, etc) just to handle the NFS case.  That risks hurting the\n     non-NFS cases.  \"First do no harm.\"  \"Keep it simple.\"  We've been\n     working hard to remove locking lately (lockless commits) and I\n     rather not add more locking back.\n\n     By implementing each approach (I think there are now 5 different\n     ideas now) instead as its own deletion policy (subclass of\n     IndexFileDeleter) we contain the added complexity of locking,\n     file-based ref counts, etc, to just that one subclass of\n     IndexFileDeleter.\n\n   * I think NFS support is part of Lucene's core mission.\n\n     Lucene should try hard to be as portable as possible.\n\n     NFS is used *alot*.\n\n     It's tempting to tell users to \"upgrade OS\", \"upgrade NFS server\n     and/or client\", etc, but taking that approach will only hurt our\n     users because typically this is not something they can control.\n\n     Now, if we had to bend over backwards for NFS, then I would agree\n     it's not worth it.  But, we don't have to: by allowing custom\n     deletion policies (which is a minor change) we can try out all of\n     the approaches suggested so far on this thread.\n\n     Rather than baking any one of these approaches into the Lucene\n     core, I'd rather just enable \"custom deletion policies\" then\n     people can build out these polices outside of the core (eg in\n     \"contrib\" first).\n\n   * I agree that \"giving a good error message when index is on NFS\" is\n     really important and that custom deletion policy alone doesn't\n     address this.\n\n     Marvin I don't think your test will work either (see below).\n\n     But I really like this direction: does anyone know how (Java\n     friendly way) to determine that a given directory is on an NFS\n     mount?  That would be wonderful.  I will spin off a new thread\n     here.\n\n\nSome specifics below:\n\nMarvin Humphrey wrote:\n\n > The first step to graceful degradation is a meaningful error message. \n  That means detecting a problem which normally requires both a indexing \nprocess and a search process to trigger, so we have to simulate it \nartificially with a test.\n >\n >    1) Open a file for read/write and write a few bytes to it.\n >    2) Delete the test file (without closing the handle).\n >    3) Try to read from the handle and if a \"stale NFS filehandle\"\n >       exception is caught, throw something more informative.\n >\n > Our first opportunity to perform this test occurs at index-creation \ntime.  This is essentially cost free.\n >\n > A second opportunity arises whenever deletions are performed.  Here, \nthere's a small cost involved, and it may not be worth it, as this would \nonly catch cases where an index was copied onto an NFS volume rather \nthan created there, then subsequently modified.\n\nI think this test won't work (though I haven't tested...).  Typically\nan NFS client will catch this case and locally emulate \"delete on last\nclose\".  Worse, even if it doesn't, those bytes would likely be cached\nand then would fail to hit \"stale NFS filehandle\".\n\n > But the good news is since we will allow subclassing to make your own\n > deletion policy, we can eventually do both of these approaches and our\n > users can pick one or do their own.\n >\n > The number of users this class will serve is diminishingly small. \nOther mitigation strategies are available.\n >\n > 1) If we implement advisory read locks, many people who see this \nerror will no longer see it.  For those who do, the best option is to \nupgrade the OS to a version which supports advisory locks over NFS. \nThen an index on an NFS volume will behave as any other.\n > 2) If you don't actually need to put the index on an NFS volume, put \nit somewhere else.\n > 3) Catch stale NFS filehandle exceptions in your search application \nand refresh the reader when they occur.\n > 4) Maintain two copies of an index and do an rsync/switch.\n > 5) Hack Lucene.\n\n5) isn't really a good option since we all can't even agree how to\n\"hack Lucene\" to make this work!  1) I think is too dangerous as part\nof the core.  2) typically this is not an option.  People choose NFS\nbecause they want to share the index.  4) is a fair amount of added\ncomplexity.  3) is the most viable option I see here, but it's not\ngreat because you're forced to refresh \"right now\".  What if warming\ntakes 8 minutes?  What if \"now\" is a bad time because deletes were\ndone by the writer but not yet adds?\n\n > Flexibility is not free.  There have been recent lamentations on \njava-dev about how difficult it will be to merge the write interfaces of \nIndexReader and IndexWriter to provide a single, unified class through \nwhich all index modifications can be performed.  The exposure of the \nIndexFileDeleter mechanism contributes to this problem -- it's one more \nsmall step in the wrong direction.\n\nYes there is an open question now on what to do about the confusion on\nusing IndexReader vs IndexWriter.  I think moving towards \"use\nIndexWriter for changes, use IndexReader for reading\" is the best\nsolution here.  But I don't see how this relates to allowing\nsubclassing of IndexFileDeleter to make your own deletion policy.\n\n > Providing a subclassing/callback API is often an elegant strategy, \nand it is surely better in this case than it would be to provide a list \nof deletion policies for the user to select from.  However, whenever \npossible, _no_ API is always a better solution -- especially in a case \nlike this one, where the functionality provided has nothing to do with \nLucene's core mission and is there solely to work around an \nimplmentation-specific bug.\n\nI disagree on this point (\"no\" API is better than subclassing).  As\nyou've said, this issue won't affect that many people (though I think\nit's a fairly large subset of our users).  Given that, I would not\nwant to add file locking & additional complexity into the Lucene core,\njust to handle NFS.\n\nBy allowing a different delete policy as a subclass of\nIndexFileDeleter we keep the changes required for supporting NFS way\noutside the Lucene core.  Since there's so much debate about which\ndeletion policy is best we should create all of these in contrib to\nbegin with and if something proves reliable we can eventually promote\nit into core Lucene.\n\nI think subclassing is perfect for this sort of situation.  It's like\nthe various LockFactory implementations we have: there is no \"one size\nfits all\".\n\nMike\n",
            "date": "2007-01-19T22:03:30.418+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "\nDoron Cohen wrote:\n\n> This ties solving the NFS issue with an extendable-file-deletion policy.\n> I am wondering is this the right way, or, perhaps, should the reference \n> counting be considered alone, apart from the deletion policy.\n> (Would modifying IndexFileDeleter to base on ref-count make it simpler\n> or harder to maintain?)\n>\n> Also, IndexFileDeleter is doing delicate work - not sure you want \n> applications to mess with it. Better let applications control some\n> simple well defined behavior, maybe the same way that a sorter \n> allows applications to provide a comparator, but keeps the sorting \n> algorithm for itself.\n\nThe solution I have in mind abstracts away all tricky details of\ndeleting files.  EG something like:\n\n  public class OnlyLastCommitDeleter extends IndexFileDeleter {\n\n    void onInit(List commits) {\n      onCommit(commits);\n    }\n\n    void onCommit(List commits) {\n      if (commits.size() > 1) {\n        for(int i=0;i<commits.size()-1;i++) {\n          deleteCommit(commits.get(i));\n        }\n      }\n    }\n\nIe, the sole responsibility of the IndexFileDeleter subclass (policy)\nis to decide when to delete a commit.  The rest of the details\n(figuring out what actual files can be deleted now that a given commit\nsegments_N is deleted) are handled by the base class (with in-memory\nref counting).\n\n> Back to reference counting,- how about the following approach:\n> - Add to Directory a FileReferenceCounter data member, get()/set() etc.\n> - Add a class FileReferenceCounter with simple general methods:\n>   void increment (String name)\n>   void decrement (String name)\n>   int getRefCount (String name)\n> - Default implementation would do nothing, i.e. would not record \n>   references, and always return 0.\n> - IndexReader, upon opening a segment, would call increment(segName)\n> - IndexReader, upon closing a segment, would call decrement(segName)\n> - IndexFileDeleter, before removing a file belonging to a certain segment, \n>   would verify getRefCount(segName)==0.\n> - Notice that the FilereferenceCounter is available from the Directory, \n>   so no constructors should be added to IndexWriter/Reader.\n> \n> So, this is adding to Directory a general file utility, no knowledge of \n> index structure required in Directory. Also, IndexFileDeleter can remain \n> as today, and at some later point can be made more powerful with various \n> deletion policies - but those policies remain unrelated to the NFS \n> issue - they can focus on point-in-time issues, where I think it \n> stemmed from. \n> \n> An NFS geared FileReferenceCounter would then be able to keep alive \n> \"counter files\", name those files based on counted fileName plus\n> processID plus machID, base getRefCount on safety window since file \n> was last touched, etc. All this is left out from point-in-time \n> policies (how many/time points-in-time should be retained).\n\nI think this approach could work, but, rather than implementing in the\nLucene core (adding methods to Directory) I'd like to see it tested as\na custom deletion policy + wrappers around IndexReader\ncreation/destruction.\n\nWe have so much debate about the best \"deletion policy\" for NFS that\nI'd like to make the minimal extension to the core (ability to make\nyour own \"deletion policy\") and then people can build their own and\ntry them out.\n\nMike",
            "date": "2007-01-19T22:09:17.521+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "One clarification on \"different deletion policies\": to support \"commit\non close\" in IndexWriter, I already have to improve IndexFileDeleter\nto give it a different deletion policy than the current one.\n\nSpecifically, the deleter must not delete anything referenced by the\nlast commit nor anything referenced by the in-memory SegmentInfos.\n\nFor example, if a writer is opened with autoCommit=false (\"commit on\nclose\") on an existing index, and lots of docs are added/deleted/etc,\nthere will have been flushes & merges of segments.  The deletion\npolicy should not delete anything that existed \"at the start\" because\nit's referenced by the segments_N commit, nor anything that is now\nreferenced by the in-memory SegmentInfos.  But it should delete\nanything \"in between\" (any newly written segments that have now been\nmerged away).\n\nTo the deleter this would just be a different policy, one that keeps\ntwo SegmentInfos alive (one on disk and one not yet committed, in\nmemory).  And the default deletion policy would be selected depending\non whether the writer is in autoCommit mode or not.\n",
            "date": "2007-01-20T10:12:15.443+0000",
            "id": 6
        },
        {
            "author": "Marvin Humphrey",
            "body": "\nOn Jan 19, 2007, at 2:04 PM, Michael McCandless (JIRA) wrote:\n>   * I think NFS support is part of Lucene's core mission.\n\nWhen I asserted that IndexFileDeleter had nothing to do with Lucene's core\nmission, I meant: you don't use Lucene to build yourself an app which helps\nyou delete files.  \n\n> Yes there is an open question now on what to do about the confusion on using\n> IndexReader vs IndexWriter.  I think moving towards \"use IndexWriter for\n> changes, use IndexReader for reading\" is the best solution here.  But I\n> don't see how this relates to allowing subclassing of IndexFileDeleter to\n> make your own deletion policy.\n\nThey're hard to refactor because they're both big, hence adding either code or\nAPI commitments to them should be avoided when possible.  We're in agreement\nabout the desirability of simplicity.  We part ways in how we measure\nsimplicity: I give greater emphasis to simplicity of API design. \n \n> I disagree on this point (\"no\" API is better than subclassing). \n\nWe're talking past each other.  I was generalizing: a 100% successful, purely\ninternal \"black box\" solution is always better than a solution that involves\nthe user.\n\n> I would not want to add file locking & additional complexity into the Lucene\n> core, just to handle NFS.\n\nThis is where our differing priorities manifest.  I would rather add some\ncode complexity to the Lucene \"core\" than accept the increased \nsupport burden of an expanded API.\n\nIronically, though you consider supporting NFS \"part of Lucene's core\nmission\", for the average user your proposal as it stands is not very\nuser-friendly.  People like Chuck, Doron, and Robert will have no trouble with\nit, but if you're a newcomer to Lucene and you \"just want to put an index on\nNFS\", subclassing IndexFileDeleter will pose a challenge.\n\nI also think you may be over-estimating the amount of effort it will take to\nexploit advisory read locks.  (The vexing problem of how to issue a warning \nwhen index creation is attempted on an NFS volume is orthogonal to the \nread-locks approach as well.) They should be easy in KS; I'll know soon enough.  \nHowever, there are some OO discipline issues which complicate applying what I\nhave in mind to Java Lucene.  In KS, the public API is defined solely via\ndocumentation, so I can have code in Index call methods from Store without\nhaving to expose it.  With Lucene divided into multiple packages, that's a\nproblem.  \n\n> I think subclassing is perfect for this sort of situation.  \n\nI'm not so enamored of subclassing. It's less constraining than some other\napproaches, but it's still constraining.\n\nCase in point: it's not possible to provide a subclass of IndexFileDeleter\nwhich exploits advisory read locking under your current proposal.\n\nIn theory, your proposal even prevents the addition of such read locks to\nLucene later, because doing so could conflict with a deletions policy you've\nallowed the user to set.  (; Given that locks over NFS make you \"nervous\",\nperhaps you consider foreclosing that option a feature. ;)\n\n> It's like the various LockFactory implementations we have: there is no \"one\n> size fits all\".\n\nI don't think LockFactory ought to be exposed either.  :)\n\nReading from an index -- any index, on any volume -- should Just Work.\nWriting to an index from a single IndexWriter should Just Work.  In a perfect\nworld, writing from multiple writers simultaneously would Just Work, too, but\nas that's inherently impractical given Lucene's current design, opening a\nsecond writer must fail.  That failure should be the only visible evidence\nthat a locking mechanism even exists.  \n\nIn my view, any deviance from that ideal API due to implementation defects\nshould be handled with exceptions rather than API additions.\n\nIn keeping with this philosophy, Lock is not publicly exposed in KinoSearch.\nIn fact, nothing about the locking mechanism is publicly exposed.  So far,\nthere seem to be three bugs with the current implementation: \n\n  * Stale NFS Filehandle exceptions.\n  * Stale lock files interfering with unattended indexing sessions.  I plan \n    to mitigate this by moving to advisory write locks when possible.\n  * Multiple machines can cause index corruption when attempting to write \n    simultaneously to a shared volume.  Moving the write.lock file to the\n    index directory, as enabled by lockless commits, solves this problem.\n\nOnce advisory file locks are in place, and if they work as expected under\nrecent versions of NFS, I expect no further problems under any common, recent\nUnix.\n\nWith the switch to lockless commits in KinoSearch, I've able to refactor Lock\nand eliminate all of Lock's subclasses, simplifying the KinoSearch \"core\".\n\"No more subclassing of Lock\" was originally a line-item in my list of\n\"great stuff\" about lockless commits, but I had to take it out because it\nwasn't true for Lucene!\n\nWith Otis signing on to your solution, it looks like momentum is gathering for\nit.  For the record, I don't think it's a catastrophic change, just suboptimal\nand IMO not ready for addition until improved.  \n\nI think you can do better.\n",
            "date": "2007-01-22T01:22:49.403+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "Quick summary first:\n\nOK, as you said (and I agree) I think we just have a difference of\nopinion on what's the \"lesser evil\" tradeoff here.  You would prefer to\nchange the core of KinoSearch to always use advisory read locks for\nall indices.\n\nWhereas I prefer to leave the Lucene core untouched since things work\nfine in most cases today (\"if it ain't broke don't fix it\"), and then\nopen up an API so for those cases (NFS) where it doesn't work, users\nat least have possible solutions to try.\n\nI think you also have a high confidence that the locking approach will\nwork fine (be perfect) on the first go and will not alienate too many\nusers, but I don't: I have had problems with locking in the past and I\nthink most users don't have the freedom to \"upgrade OS/fileserver\".\n\nSo I would prefer instead to open a minimal API in the core of Lucene\n(so users can use different deletion policies), and then try the 5\ndifferent ideas proposed so far (and more later I'm sure) as their own\ndeletion policy, external to Lucene's core (eg in contrib).  If one of\nthem proves to work well, universally, then sometime down the road we\ncan promote it as the default deletion policy.\n\n\n\nOK details below:\n\n> > * I think NFS support is part of Lucene's core mission.\n> \n> When I asserted that IndexFileDeleter had nothing to do with Lucene's core\n> mission, I meant: you don't use Lucene to build yourself an app which helps\n> you delete files.\n\nWell, \"custom deletion policies\" is in support of the core mission of\n\"working over NFS\".\n\n> > Yes there is an open question now on what to do about the confusion on using\n> > IndexReader vs IndexWriter. I think moving towards \"use IndexWriter for\n> > changes, use IndexReader for reading\" is the best solution here. But I\n> > don't see how this relates to allowing subclassing of IndexFileDeleter to\n> > make your own deletion policy.\n> \n> They're hard to refactor because they're both big, hence adding either code or\n> API commitments to them should be avoided when possible. We're in agreement\n> about the desirability of simplicity. We part ways in how we measure\n> simplicity: I give greater emphasis to simplicity of API design.\n>  \n> > I disagree on this point (\"no\" API is better than subclassing).\n> \n> We're talking past each other. I was generalizing: a 100% successful, purely\n> internal \"black box\" solution is always better than a solution that involves\n> the user.\n\nOK, yes in the ideal case, no API is better than API if your situation\nallows for no API.  I just don't think this is one of those\nsituations: I don't think we have a clear cut \"one size fits all\"\nsolution.\n\n> > I would not want to add file locking & additional complexity into the Lucene\n> > core, just to handle NFS.\n> \n> This is where our differing priorities manifest. I would rather add some\n> code complexity to the Lucene \"core\" than accept the increased\n> support burden of an expanded API.\n> \n> Ironically, though you consider supporting NFS \"part of Lucene's core\n> mission\", for the average user your proposal as it stands is not very\n> user-friendly. People like Chuck, Doron, and Robert will have no trouble with\n> it, but if you're a newcomer to Lucene and you \"just want to put an index on\n> NFS\", subclassing IndexFileDeleter will pose a challenge.\n\nYes, but at least having the option (picking one of the deletion\npolicies in \"contrib\" once we've built them out) is quite a bit better\nthan what we have today (no option besides \"you must refresh now\").  I\nwould love to have the \"perfect\" solution (which you are aiming for in\none step), but I'll settle today for just good progress: \"progress not\nperfection\".\n\n> I also think you may be over-estimating the amount of effort it will take to\n> exploit advisory read locks. (The vexing problem of how to issue a warning\n> when index creation is attempted on an NFS volume is orthogonal to the\n> read-locks approach as well.) They should be easy in KS; I'll know soon enough.\n> However, there are some OO discipline issues which complicate applying what I\n> have in mind to Java Lucene. In KS, the public API is defined solely via\n> documentation, so I can have code in Index call methods from Store without\n> having to expose it. With Lucene divided into multiple packages, that's a\n> problem.\n\nYes detection of NFS is orthogonal and I would love to find a solution\nhere.  And yes Java's method/field protection is quite different from\nwhat KS can do.\n\n> > I think subclassing is perfect for this sort of situation.\n> \n> I'm not so enamored of subclassing. It's less constraining than some other\n> approaches, but it's still constraining.\n> \n> Case in point: it's not possible to provide a subclass of IndexFileDeleter\n> which exploits advisory read locking under your current proposal.\n> \n> In theory, your proposal even prevents the addition of such read locks to\n> Lucene later, because doing so could conflict with a deletions policy you've\n> allowed the user to set. (; Given that locks over NFS make you \"nervous\",\n> perhaps you consider foreclosing that option a feature. ;)\n\nNo, I would not consider foreclosing that option a feature!\n\nYes, I am nervous about relying on advisory read locks 100% today in\nthe Lucene core.  But, I would love to be proven wrong in the future:\nif your lock based solution actually works out \"perfectly\" in the\nfuture then users can indeed fire up Lucene/KS regardless of what\nfilesystem the index is on.  I would equally love to see file-based\nreference counting work out, etc: if any option proves reliable enough\nin the future then we can make it the default deletion policy.\n\nYes users who set their own deletion policies would not get this\ndefault but that's OK: such users understand what they've done.\n\nAnd, I don't want to change the default policy now (\"first do no\nharm\").\n\n\n> > It's like the various LockFactory implementations we have: there is no \"one\n> > size fits all\".\n> \n> I don't think LockFactory ought to be exposed either. :)\n>\n> Reading from an index -- any index, on any volume -- should Just Work.\n> Writing to an index from a single IndexWriter should Just Work. In a perfect\n> world, writing from multiple writers simultaneously would Just Work, too, but\n> as that's inherently impractical given Lucene's current design, opening a\n> second writer must fail. That failure should be the only visible evidence\n> that a locking mechanism even exists.\n>\n> In my view, any deviance from that ideal API due to implementation defects\n> should be handled with exceptions rather than API additions.\n>\n> In keeping with this philosophy, Lock is not publicly exposed in KinoSearch.\n> In fact, nothing about the locking mechanism is publicly exposed. So far,\n> there seem to be three bugs with the current implementation:\n> \n>   * Stale NFS Filehandle exceptions.\n>   * Stale lock files interfering with unattended indexing sessions. I plan\n>     to mitigate this by moving to advisory write locks when possible.\n>   * Multiple machines can cause index corruption when attempting to write\n>     simultaneously to a shared volume. Moving the write.lock file to the\n>     index directory, as enabled by lockless commits, solves this problem.\n> \n> Once advisory file locks are in place, and if they work as expected under\n> recent versions of NFS, I expect no further problems under any common, recent\n> Unix.\n> \n> With the switch to lockless commits in KinoSearch, I've able to refactor Lock\n> and eliminate all of Lock's subclasses, simplifying the KinoSearch \"core\".\n> \"No more subclassing of Lock\" was originally a line-item in my list of\n> \"great stuff\" about lockless commits, but I had to take it out because it\n> wasn't true for Lucene!\n>\n> With Otis signing on to your solution, it looks like momentum is gathering for\n> it. For the record, I don't think it's a catastrophic change, just suboptimal\n> and IMO not ready for addition until improved.\n\nI agree it would be great to reach this perfect world.  It would be\neven better to get there in just one jump from where we are today.\nIt's just not nearly clear to me that a locking solution (or reference\ncounting, time based expiration, etc.) for NFS is or will evolve to\nthat pefect solution.  And I think \"not alienating users\" who are\nstuck on past UNIX versions is more important than \"not adding any\nAPI\".  I think we are just picking a different \"lesser evil\".\n \n> I think you can do better.\n\nWith time, I hope so too.  Progress not perfection!\n",
            "date": "2007-01-22T23:30:23.784+0000",
            "id": 8
        },
        {
            "author": "Doron Cohen",
            "body": "Michael McCandless wrote: \n\n> The solution I have in mind abstracts away all tricky details of\n> deleting files.  EG something like:\n> \n>   public class OnlyLastCommitDeleter extends IndexFileDeleter {\n> \n>     void onInit(List commits) {\n>       onCommit(commits);\n>     }\n> \n>     void onCommit(List commits) {\n>       if (commits.size() > 1) {\n>         for(int i=0;i<commits.size()-1;i++) {\n>           deleteCommit(commits.get(i));\n>         }\n>       }\n>     }\n> \n> Ie, the sole responsibility of the IndexFileDeleter subclass (policy)\n> is to decide when to delete a commit.  The rest of the details\n> (figuring out what actual files can be deleted now that a given commit\n> segments_N is deleted) are handled by the base class (with in-memory\n> ref counting).\n> \n\nI don't really understand this interface and so I cannot see how \nyou intend to rewrite the IndexFileDeleter as you describe, but I \nagree that if this can be done it is a better solution. So I am \nokay with waiting for this approach to mature into code. \n\n(I would prefer the DeletionPolicy to be a \npluggable *interface* and the IndexFileDeleter to be \nan internal *class*, so that at least we do not expose now something \nthat would stand in our way in the future. But again, since I do not \nfully understand your solution maybe please bear with me if this is \nnot making sense.)\n",
            "date": "2007-01-23T04:25:07.863+0000",
            "id": 9
        },
        {
            "author": "Marvin Humphrey",
            "body": "I found a flaw in my plan. If the read locks are always applied, they will\nslow deletion of obsolete segments for everybody where delete-on-last-close\ncurrently works as intended.  Right now, the files are unlinked and disappear\nas soon as the last reader holding them open goes away.  With read locks, the\nunlink op wouldn't take place if a reader was open.\n\nI also spent a good bit of time yesterday further researching the subtleties\nof locks over NFS.  Summing up: flock can work, but dot-lock files are more\nreliable.\n\nSo, new proposal: \n\nAdd a new public method IndexReader.aquireReadLock(String hostId), which would\nwrite a dot-lock file to the index directory with hostId, a pid, and an\nincrementing integer spliced into the file name.  The relevant segments_N file\nname would be written to the lockfile.  Calling it would only be necessary on NFS,\nand an exception would occur if the attempt to create the lockfile\nfailed.\n\nThe deletions policy would work as in my earlier proposal, and the user\nwouldn't have to grok its innards.  Troubleshooting stale lockfiles\nby snooping the index directory contents would be straightforward and\nintuitive.\n\nWe might want aquireReadLock() to automatically zap any locks associated with\nhostId for which the pid couldn't be found, or we might want to break that out\ninto another method.\n\n",
            "date": "2007-01-23T15:09:15.594+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "> I don't really understand this interface and so I cannot see how you\n> intend to rewrite the IndexFileDeleter as you describe, but I agree\n> that if this can be done it is a better solution. So I am okay with\n> waiting for this approach to mature into code.\n\nThe deletion policy is called on creation of a writer (onInit) and\nonce per commit (onCommit) and is given a List of existing commits (=\nSegmentInfos instances) in the index.  The policy then decides which\ncommits should be removed and IndexFileDeleter translates that request\n(using reference counting, because a given index file may still be\nreference by commits that are not yet deleted) into which specific\nfiles to remove.\n\nFor example, onCommit you would typically see a List of length 2: the\nprior commit and the new one.  And the default policy\n(KeepOnlyLastCommit) would at this point remove the prior one.\n\nRealize that the \"commit on close\" mode (autoCommit=false) for\nIndexWriter (that I'm doing as part of this issue) actually keeps 2\nSegmentInfos alive at any given time: first is the segments_N file in\nthe index, and second is the \"in memory\" SegmentInfos that haven't yet\nbeen committed to a segments_N file.  It's only on close when the\ncommit takes place that the deleter then deletes the previous\nsegments_N commit.\n\n> (I would prefer the DeletionPolicy to be a pluggable *interface* and\n> the IndexFileDeleter to be an internal *class*, so that at least we\n> do not expose now something that would stand in our way in the\n> future. But again, since I do not fully understand your solution\n> maybe please bear with me if this is not making sense.)\n\nGood point: I agree an interface here is cleaner.  I will use an\ninterface (not subclass) and make IndexFileDeleter entirely internal.\nThe deletion policy doesn't need to see any details of the\nIndexFileDeleter class.\n",
            "date": "2007-01-23T22:00:52.682+0000",
            "id": 11
        },
        {
            "author": "Michael McCandless",
            "body": "\n> I found a flaw in my plan. If the read locks are always applied,\n> they will slow deletion of obsolete segments for everybody where\n> delete-on-last-close currently works as intended. Right now, the\n> files are unlinked and disappear as soon as the last reader holding\n> them open goes away. With read locks, the unlink op wouldn't take\n> place if a reader was open.\n\nAhh good point.  This is why I don't want to risk changes to Lucene\ncore: most of the time Lucene's \"point in time\" searching works\nperfectly now.  It's just NFS (so far) that's problematic which is why\nI want to keep the solution \"external\" to Lucene by allowing custom\ndeletion policies.  Plus we obviously have alot of deletion policies\nto try on NFS.  First do no harm.\n\n> I also spent a good bit of time yesterday further researching the\n> subtleties of locks over NFS. Summing up: flock can work, but\n> dot-lock files are more reliable.\n\nWell, dot-locks (= \"create file exclusively\") have problems too.\nThere have been issues with bugs in at least certain Linux NFS\nclients.  And Sun's Javadocs on the equivalent Java method,\nFile.createNewFile, has a warning about not relying on this for\nlocking:\n\n  http://java.sun.com/j2se/1.4.2/docs/api/java/io/File.html#createNewFile()\n\nThis warning is why we created the NativeFSLockFactory for Directory\nlocking in the first place.\n\n> So, new proposal:\n>\n> Add a new public method IndexReader.aquireReadLock(String hostId),\n> which would write a dot-lock file to the index directory with\n> hostId, a pid, and an incrementing integer spliced into the file\n> name. The relevant segments_N file name would be written to the\n> lockfile. Calling it would only be necessary on NFS, and an\n> exception would occur if the attempt to create the lockfile failed.\n>\n> The deletions policy would work as in my earlier proposal, and the\n> user wouldn't have to grok its innards. Troubleshooting stale\n> lockfiles by snooping the index directory contents would be\n> straightforward and intuitive.\n>\n> We might want aquireReadLock() to automatically zap any locks\n> associated with hostId for which the pid couldn't be found, or we\n> might want to break that out into another method.\n\nOK.  You could implement this in Lucene as a custom deletion policy\nonce we get this commmitted (I think this is 6 proposals now for\n\"deletion policy\" for NFS), plus a wrapper around IndexReader.\n\n",
            "date": "2007-01-23T22:18:18.844+0000",
            "id": 12
        },
        {
            "author": "Marvin Humphrey",
            "body": "On Jan 23, 2007, at 2:19 PM, Michael McCandless (JIRA) wrote:\n\n> First do no harm.\n\nIf that was really your guiding philosophy, you would never change anything.\n\n> And Sun's Javadocs on the equivalent Java method, File.createNewFile, has a\n> warning about not relying on this for locking:\n> \n>   http://java.sun.com/j2se/1.4.2/docs/api/java/io/File.html#createNewFile()\n\nThat page recommends that you use FileLock instead, which maps to Fcntl on\nsome systems.  The FreeBSD manpage on Fcntl uses less delicate language than\nSun in pointing out the drawbacks:\n\n     This interface follows the completely stupid semantics of System V and\n     IEEE Std 1003.1-1988 (``POSIX.1'') that require that all locks associated\n     with a file for a given process are removed when any file descriptor for\n     that file is closed by that process.  This semantic means that applica-\n     tions must be aware of any files that a subroutine library may access.\n\nTrying to guarantee that kind of discipline from library code severely limits\nyour options.\n\n> This warning is why we created the NativeFSLockFactory for Directory locking\n> in the first place.\n\nTake a look at this bug, which explains how that warning got added.\n\nhttp://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4676183\n\nRead the comment below -- the problem with the \"protocol\" they warn you\nagainst using is with deleteOnExit(), not createNewFile().  I think you're\nbetter off with dot-locks.\n\n> OK.  You could implement this in Lucene as a custom deletion policy once we\n> get this commmitted (I think this is 6 proposals now for \"deletion policy\"\n> for NFS), plus a wrapper around IndexReader.\n\nThis was the response I got on the KinoSearch list:\n\n    We do not enable NFS writes, only reads (which is why Slashdot is able to\n    reliably use NFS for its heavy load :-).  So I don't think that will work,\n    if I understand you correctly.\n\nLack of bulletproof support for NFS ain't gonna hold up my next release any\nlonger.  What a freakin' nightmare...",
            "date": "2007-01-24T05:55:36.249+0000",
            "id": 13
        },
        {
            "author": "Doron Cohen",
            "body": "> The deletion policy is called on creation of a writer (onInit) and\n> once per commit (onCommit) and is given a List of existing commits (=\n> SegmentInfos instances) in the index.  The policy then decides which\n> commits should be removed and IndexFileDeleter translates that request\n> (using reference counting, because a given index file may still be\n> reference by commits that are not yet deleted) into which specific\n> files to remove.\n> ...\n\nMichael thanks for explaining this further - yes, now it makes sense to me. ",
            "date": "2007-01-24T07:00:39.946+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "OK, I've attached a patch to implement \"commit on close\" and \"custom\ndeletion policies\".  The design is exactly what's described above.\n\nThere are no changes to the file format.\n\nAll tests pass and I've added additional tests for this new\nfunctionality.\n\nSummary of the external changes:\n\n  * For \"commit on close\":\n\n    - Added new IndexWriter constructors that take \"autoCommit\"\n      boolean: if it's false, then readers will not see any actions\n      done by this writer (no new segments_N is written) until\n      writer.close() is called.\n\n    - Added IndexWriter.abort() which closes the writer without\n      committing, cleaning up any temp files it had added to the\n      index.\n\n  * For \"custom deletion policies\":\n\n    - Created IndexDeletionPolicy interface and added constructors to\n      IndexReader/IndexWriter allowing you to specify a deletion\n      policy.\n\n    - Created IndexCommitPoint interface: this is passed to the\n      deletion policy to represent each commit.  The policy calls the\n      delete method on this interface to remove a commit.\n\n    - Created one deletion policy (KeepOnlyLastCommitDeletionPolicy)\n      and made that the default policy.  (The unit test for this has\n      other \"interesting\" policies like \"delete by age since this\n      commit was obsoleted\" initially discussed on java-dev.)\n\nSummary of internal changes:\n\n  * Created \"files()\" method in SegmentInfo (and changed\n    SegmentReader.files() to use it).\n\n  * Changed IndexFileDeleter to use reference counting to keep track\n    of which files are deletable because no commit(s) (nor the\n    in-memory SegmentInfos) reference them.\n\n    This is a nice simplification of IndexFileDeleter: previously it\n    had detailed knowledge about which files, extensions, etc., to\n    look for and delete.  Now it has far less of that because it\n    relies entirely on SegmentInfo.files() to compute that.\n\n  * Changed IndexReader/IndexWriter to not directly delete files and\n    instead notify IndexFileDeleter when there has been a change to\n    the in-memory SegmentInfos.  The deleter then incref/decref's to\n    determine what files can safely be deleted.\n\n    This is also a nice simplification for the same reason as above:\n    now the writers just make changes to SegmentInfo(s) without having\n    to compute/track the consequences to specific index files.\n\n  * Simplified the fix for LUCENE-702 (addIndexes corrupts index on\n    disk full) to just temporarily set autoCommit=false if it's not\n    already.\n\n  * Added get/setDefaultInfoStream to IndexWriter so you could see\n    things that happen during IndexWriter constructor.\n\n  * No longer store/propogate persistent IndexFileDelter inside\n    IndexReader (removed protected method get/setDeleter).  This is a\n    nice simplification because the deleter is now only needed briefly\n    during \"commit()\".\n\n  * Reworked the toplevel javadoc for IndexWriter.\n\n  * Added try/finally to remove a partially written segments_N if we\n    hit IOException when trying to write it.\n\n  * Other small change (small refactoring, fixes to javadocs, fixed\n    spelling, etc).\n",
            "date": "2007-03-02T15:52:01.337+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "Rebased the patch to the current trunk.  I plan to commit this probably end of this week.",
            "date": "2007-03-07T09:56:35.657+0000",
            "id": 16
        },
        {
            "author": "Doron Cohen",
            "body": "Mike, patching take2 on current trunk fails for IndexFileDeleter.java.\n  patching file src/java/org/apache/lucene/index/IndexFileDeleter.java\n    Hunk #1 FAILED at 18.\nAlso some noise in SegmentInfo.java\n  patching file src/java/org/apache/lucene/index/SegmentInfo.java\n    Hunk #7 succeeded at 291 (offset 3 lines).\n  ",
            "date": "2007-03-09T06:26:45.105+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "Woops, looks like the commit for LUCENE-825 messed up the patch.  OK I updated and re-diff'd and attached take3.\n\nIt's too bad we don't have a patch that's better integrated with svn such that if even you have a more recent svn revision checked out, applying the patch would do so back against the revision it was based on, and then svn would merge the changes committed to the trunk since then.  In this case an svn update on the checkout with the diffs produced no conflicts, so if we had such a combined patch tool, it would have worked find here.  I suppose the person applying the patch could first \"svn update\" to its base revision, apply the patch, then svn up, but that's kind of a hassle",
            "date": "2007-03-09T09:44:04.933+0000",
            "id": 18
        },
        {
            "author": "Doron Cohen",
            "body": "I was too slow in reviewing this, so while I was studying the new code it was committed... \n\nAnyhow I have a few comments and a question - I think JIRA LUCENE-710 is still the place for this discussion even though the issue is already resolved. \n\nThe attached 710.comments.diff  implements a few suggested changes.\n\nI like the definition and use of IndexDeletePolicy and CommitPoint - this is very flexible and clear, and would indeed allow to implement NFS suited logic. These two concepts are central to implementing such logic, and I thought their Javadocs should be enhanced (included in the attached).\n\nIndexFileDeleter - it is nice that this became non public and somewhat simpler. I added some internal documentation (not javadocs) in that file as I learned how it works. I think these would be useful for others diving into this code. I also modified some variable names for clarity (in the attached). \n\nI don't understand yet why we allow a deletion policy to delete *all* commits (including the most recent) - TestDeletionPolicy explains this as: \"This is useful for adding to a big index w/ autoCommit =false when you know readers are not using it.\" - so, would I risk losing the big index should uncommited additions fail? what does one earn by this? I first thought we should prevent (exception) deleting the most recent commit, but I must be missing something - could you elaborate on this?\n\ncheckpoints() is another - more internal - new concept in this code. At writing this I don't fully understand it. IndexWriter has its own checkpoint() method, but it also calls IndexFileDeleter.checkpoint(). IndexReader only calls IndexFileDeletion.checkpoint() - it does not have a checkpoint() itself.   ...mmm... For IndexReader it makes sense since it always commits only at close(), or at explicit calls to commit(). Perhaps I understand it better now... Ok, I added some documentation for this in IndexWriter, I think it would also help others. (in the attached.)\n\nThis issue also introduced constants for file names - hasSingleNorms (i.e. nrm)  and SINGLE_NORMS_EXTENSION (.fN) were confusing/collating - so I modified .fN to PLAIN_NORMS_EXTENSION.\n\nThis issue moved some files logic SegmentInfo. The -1/1/0 logic and especially with norms is confusing, and at least I have to re-read the code carefully each time again and again to be convinced that it is correct. It would be nice when we can get rid of some of the backward compatibility cases here. Anyhow I added some documentation and also replaced the -1/1/0 with constants, I think this makes it easier to understand.\n\nRegards,\nDoron\n",
            "date": "2007-03-15T10:08:50.112+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks for the review Doron!\n\nYour added comments & improvements to the variable names are\nexcellent.  I especially like the new constants (YES, NO, CHECK_DIR,\netc.) in SegmentInfo.  I've tweaked your patch here and there, and\nattached a modified patch (710.review.take2.diff).  If you're happy\nwith that then go ahead and commit it?\n\n> IndexFileDeleter - it is nice that this became non public and somewhat\n> simpler.\n\nI especially like that this class now has very little knowledge of\nwhat files \"belong\" to an index, especially compared to before.  That\nknowledge has now been consolidated under SegmentInfo which I think is\nthe right place for it.\n\n> I don't understand yet why we allow a deletion policy to delete\n> *all* commits (including the most recent) - TestDeletionPolicy\n> explains this as: \"This is useful for adding to a big index w/\n> autoCommit =false when you know readers are not using it.\" - so,\n> would I risk losing the big index should uncommited additions fail?\n> what does one earn by this? I first thought we should prevent\n> (exception) deleting the most recent commit, but I must be missing\n> something - could you elaborate on this?\n\nThe use case I was thinking of is: say you already have a large index\nand then you need to add a bunch more docs to it.  If you are not\nallowed to delete the starting commit, then, you will consume\nsubstantially more disk space as you are building your index because\nthe large segments at the start can't be removed.  This would have\nmade the \"autoCommit false\" case unnecessarily worse than the\n\"autoCommit true\" case.  If for a given application the developer is\nconcerned about safety (losing index due to crash), then the normal\ndefault policy should be used.\n\n> This issue moved some files logic SegmentInfo. The -1/1/0 logic and\n> especially with norms is confusing, and at least I have to re-read\n> the code carefully each time again and again to be convinced that it\n> is correct. It would be nice when we can get rid of some of the\n> backward compatibility cases here. Anyhow I added some documentation\n> and also replaced the -1/1/0 with constants, I think this makes it\n> easier to understand.\n\nYes the backwards compatibility code (for pre-2.1 indices) is complex.\nThe good news is by the time we release this in 2.2, most indices that\nupgrade to 2.2 will be 2.1.",
            "date": "2007-03-15T12:14:57.213+0000",
            "id": 20
        },
        {
            "author": "Doron Cohen",
            "body": "> If for a given application the developer is concerned \n> about safety (losing index due to crash), then the \n> normal default policy should be used. \n\nSpooky... what if onCommit() also deletes all commits?  \n(Might this be a pit for users to fall in...?)\n\nI added warnings about this in IndexDeletionPolicy methods.\n\nJust commiited review.take2 + these comments.\n",
            "date": "2007-03-15T19:30:03.673+0000",
            "id": 21
        },
        {
            "author": "Michael McCandless",
            "body": "> I added warnings about this in IndexDeletionPolicy methods.\n\nI think that's good.  Thanks!",
            "date": "2007-03-15T22:31:12.751+0000",
            "id": 22
        }
    ],
    "component": "core/index",
    "description": "This was touched on in recent discussion on dev list:\n\n  http://www.gossamer-threads.com/lists/lucene/java-dev/41700#41700\n\nand then more recently on the user list:\n\n  http://www.gossamer-threads.com/lists/lucene/java-user/42088\n\nLucene's \"point in time\" searching currently relies on how the\nunderlying storage handles deletion files that are held open for\nreading.\n\nThis is highly variable across filesystems.  For example, UNIX-like\nfilesystems usually do \"close on last delete\", and Windows filesystem\ntypically refuses to delete a file open for reading (so Lucene retries\nlater).  But NFS just removes the file out from under the reader, and\nfor that reason \"point in time\" searching doesn't work on NFS\n(see LUCENE-673 ).\n\nWith the lockless commits changes (LUCENE-701 ), it's quite simple to\nre-implement \"point in time searching\" so as to not rely on filesystem\nsemantics: we can just keep more than the last segments_N file (as\nwell as all files they reference).\n\nThis is also in keeping with the design goal of \"rely on as little as\npossible from the filesystem\".  EG with lockless we no longer re-use\nfilenames (don't rely on filesystem cache being coherent) and we no\nlonger use file renaming (because on Windows it can fails).  This\nwould be another step of not relying on semantics of \"deleting open\nfiles\".  The less we require from filesystem the more portable Lucene\nwill be!\n\nWhere it gets interesting is what \"policy\" we would then use for\nremoving segments_N files.  The policy now is \"remove all but the last\none\".  I think we would keep this policy as the default.  Then you\ncould imagine other policies:\n\n  * Keep past N day's worth\n\n  * Keep the last N\n\n  * Keep only those in active use by a reader somewhere (note: tricky\n    how to reliably figure this out when readers have crashed, etc.)\n\n  * Keep those \"marked\" as rollback points by some transaction, or\n    marked explicitly as a \"snaphshot\".\n\n  * Or, roll your own: the \"policy\" would be an interface or abstract\n    class and you could make your own implementation.\n\nI think for this issue we could just create the framework\n(interface/abstract class for \"policy\" and invoke it from\nIndexFileDeleter) and then implement the current policy (delete all\nbut most recent segments_N) as the default policy.\n\nIn separate issue(s) we could then create the above more interesting\npolicies.\n\nI think there are some important advantages to doing this:\n\n  * \"Point in time\" searching would work on NFS (it doesn't now\n    because NFS doesn't do \"delete on last close\"; see LUCENE-673 )\n    and any other Directory implementations that don't work\n    currently.\n\n  * Transactional semantics become a possibility: you can set a\n    snapshot, do a bunch of stuff to your index, and then rollback to\n    the snapshot at a later time.\n\n  * If a reader crashes or machine gets rebooted, etc, it could choose\n    to re-open the snapshot it had previously been using, whereas now\n    the reader must always switch to the last commit point.\n\n  * Searchers could search the same snapshot for follow-on actions.\n    Meaning, user does search, then next page, drill down (Solr),\n    drill up, etc.  These are each separate trips to the server and if\n    searcher has been re-opened, user can get inconsistent results (=\n    lost trust).  But with, one series of search interactions could\n    explicitly stay on the snapshot it had started with.\n\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-710",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Implement \"point in time\" searching without relying on filesystem semantics",
    "systemSpecification": true,
    "version": "2.1"
}