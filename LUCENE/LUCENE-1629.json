{
    "comments": [
        {
            "author": "Xiaoping Gao",
            "body": "Here is all the source code of intelligent analyzer for Chinese. About 2500 lines\nThe unit TestCase contains a main method, which needs lexical dictionary to run, so I will post the binary lexical dictionary soon.",
            "date": "2009-05-07T05:53:49.970+0000",
            "id": 0
        },
        {
            "author": "Xiaoping Gao",
            "body": "Lexical dictionary files, unzip it to somewhere, run TestSmartChineseAnalyzer with this command:\njava org.apache.lucene.analysis.cn.TestSmartChineseAnalyzer -Danalysis.data.dir=/path/to/analysis-data/\n",
            "date": "2009-05-07T06:21:57.098+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good -- thanks Xiaoping!\n\nOne problem is that contrib/analyzers is currently limited to Java 1.4, and I don't think we should change that at this point (though in 3.0, we will change it to 1.5).  How hard would it be to switch your sources to use only Java 1.4?\n\nA couple other issues:\n\n  * Each copyright header is missing the starting 'S' in the sentence 'ee the License for the specific language governing permissions and'\n\n  * Can you remove the @author tags?  (Lucene sources don't include author tags anymore)",
            "date": "2009-05-07T09:45:49.978+0000",
            "id": 2
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Xiaoping,\n\nlooks good, but I have some suggestions:\n- Making the datafile only readable from a RandomAccessFile makes it hard to e.g. move the data file directly into the jar file. I would like to put the data file directly into the package directory  and load it with Class.getResourceAsStream(). In this case, the binary Lucene analyzer jar would be ready-to-use and the analyzer would run out of the box. Often configuring external files in e.g. web applications is complicated. An automatism to load the file from the JAR would be fine.\n- I have seen some singleton implementations, where the getInstance() static method is not synchronized. Without it there may be more than one instance, if different threads call getInstance() at the same time or close together.\n- Do we compile the source files with a fixed encoding of UTF-8 (build.xml?). If not, there may be problems, if the Java compiler uses another encoding (because platform default).",
            "date": "2009-05-07T13:32:44.019+0000",
            "id": 3
        },
        {
            "author": "Xiaoping Gao",
            "body": "to McCandless:\nThere is lots of code depending on Java 1.5, I use enum, generalization frequently. Because I saw these points on apache wiki:\n    * All core code to be included in 2.X releases should be compatible with Java 1.4.\n    * All contrib code should be compatible with *either Java 5 or 1.4*.\nI have corrected the copyright header and @author tags, thank you.\n\nto Schindler:\n1. This is really a good idea, I wanna to move the data file into jar in next develop cycle, but now I need to make some changes to the data files independently, can I just commit the codes now?\n2. I have changed the getInstance() method to synchronized\n3. All the source files are fixed encoded using UTF-8, and I had put a notice in package.html,  Should I do something else?\n\nThank you all!",
            "date": "2009-05-07T15:00:38.372+0000",
            "id": 4
        },
        {
            "author": "Xiaoping Gao",
            "body": "New patch in reply to Michael McCandless and Uwe Schindler 's comments.",
            "date": "2009-05-07T15:09:57.520+0000",
            "id": 5
        },
        {
            "author": "Robert Muir",
            "body": "Hi,\n\nI see in the paper that lexical resources were also developed for Big5 (traditional chinese). Are you able to acquire these resources with BSD license as well?",
            "date": "2009-05-07T15:49:02.659+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "bq. There is lots of code depending on Java 1.5, I use enum, generalization frequently. Because I saw these points on apache wiki:\n\nWell... \"in general\" contrib packages can be 1.5, but the analyzers contrib package is widely used, and is not 1.5 now, so it's a biggish change to force it to 1.5 with this.  We should at least separate discuss in on java-dev if we want to consider allowing 1.5 code into contrib-analyzers.\n\nWe could hold off on committing this until 3.0?",
            "date": "2009-05-07T19:04:03.500+0000",
            "id": 7
        },
        {
            "author": "Xiaoping Gao",
            "body": "I have ported the code to Java1.4 today, fortunately there were not so much problems.\n\n\"Lucene-1629-java1.4.patch\"  is all the code working on Java 1.4, I have just changed it to fit Java1.4 code style.Data structures and algorithms are not modified. \nIt has been tested that it would produce the very same result, just with a slight affection on speed.",
            "date": "2009-05-08T05:05:12.043+0000",
            "id": 8
        },
        {
            "author": "Xiaoping Gao",
            "body": "all the code working on java1.4",
            "date": "2009-05-08T05:05:50.353+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "bq. all the code working on java1.4\n\nFabulous, thanks Xiaoping!",
            "date": "2009-05-08T08:47:07.555+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "When I apply the patch and then run \"ant test\" in contrib/analyzers, I'm hitting this compilation error:\n{code}\ncompile-core:\n    [mkdir] Created dir: /lucene/src/cn.1629/build/contrib/analyzers/classes/java\n    [javac] Compiling 88 source files to /lucene/src/cn.1629/build/contrib/analyzers/classes/java\n    [javac] /lucene/src/cn.1629/contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/AnalyzerProfile.java:98: load(java.io.InputStream) in java.util.Properties cannot be applied to (java.io.FileReader)\n    [javac]       prop.load(reader);\n    [javac]           ^\n    [javac] Note: Some input files use or override a deprecated API.\n    [javac] Note: Recompile with -Xlint:deprecation for details.\n    [javac] 1 error\n{code}\n",
            "date": "2009-05-08T08:52:38.536+0000",
            "id": 11
        },
        {
            "author": "Xiaoping Gao",
            "body": "new patch for java1.4, I have corrected the bug \"java.util.Property.load(Reader)\".\nThe new code can now be compiled now.",
            "date": "2009-05-08T15:39:53.317+0000",
            "id": 12
        },
        {
            "author": "Michael McCandless",
            "body": "Xiaoping, could you turn the TestSmartChineseAnalyzer into a real JUnit test case?  (Ie, invoke that sample method from the testChineseAnalyzer method)?\n\nAlso, it looks like you didn't switch to Class.getResourceAsStream() (Uwe's suggestion above) -- are you planning on doing that?\n\nFinally, Robert asked a question above (about Big5) that maybe you missed?\n\nbq. Do we compile the source files with a fixed encoding of UTF-8 (build.xml?). If not, there may be problems, if the Java compiler uses another encoding (because platform default).\n\nLucene's common-build.xml already sets the encoding (for javac) to utf-8.  So I think we're good here...",
            "date": "2009-05-09T10:15:29.525+0000",
            "id": 13
        },
        {
            "author": "Xiaoping Gao",
            "body": "to Robert Muir:\nThe dictionary only supports GB2312 encoding now, which has about 6800 characters, so I don't think it can support big5 encoding with this dictionary. \nYou can ask the author about the big5 issue. May be he has another lexical dictionary.\n\nNow I will switch to Class.getResourceAsStream() to load the dictionary, so the user don't have to download the dictionary independently.\nAfter that I can write a real JUnit test case.\n\n",
            "date": "2009-05-09T15:55:02.538+0000",
            "id": 14
        },
        {
            "author": "Robert Muir",
            "body": "Xiaoping, thanks. I see they didn't get great performance with big5 tests but just curious.\n\nMaybe mention somewhere in the javadocs that this analyzer is for simplified chinese text, just so its clear? \n",
            "date": "2009-05-09T19:07:49.979+0000",
            "id": 15
        },
        {
            "author": "Xiaoping Gao",
            "body": "changes\n1. Add two binary dictionary files into the java package: coredict.mem(1.6M) bigramdict.mem(4.7M), I'll post them after this\n2. Using Class.getResourceAsStream() to load the dictionary, so users don't need to download dictionaries manually.\n3. Switch TestSmartChineseAnalyzer into a real JUnit test case\n",
            "date": "2009-05-11T05:11:54.264+0000",
            "id": 16
        },
        {
            "author": "Xiaoping Gao",
            "body": "two binary dictionary files, please put them into contrib/analyzers/src/java/org/apache/lucene/analysis/cn/smart/hhmm/",
            "date": "2009-05-11T05:48:33.268+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "When I run \"ant test\" in contrib/analyzers, SmartChineseAnalyzer is unable to locate the stopwords.txt:\n{code}\n    [junit] Testcase: testChineseAnalyzer(org.apache.lucene.analysis.cn.TestSmartChineseAnalyzer):\tCaused an ERROR\n    [junit] null\n    [junit] java.lang.NullPointerException\n    [junit] \tat java.io.Reader.<init>(Reader.java:61)\n    [junit] \tat java.io.InputStreamReader.<init>(InputStreamReader.java:80)\n    [junit] \tat org.apache.lucene.analysis.cn.SmartChineseAnalyzer.loadStopWords(SmartChineseAnalyzer.java:112)\n    [junit] \tat org.apache.lucene.analysis.cn.SmartChineseAnalyzer.<init>(SmartChineseAnalyzer.java:71)\n    [junit] \tat org.apache.lucene.analysis.cn.TestSmartChineseAnalyzer.testChineseAnalyzer(TestSmartChineseAnalyzer.java:36)\n{code}\n",
            "date": "2009-05-11T10:56:59.166+0000",
            "id": 18
        },
        {
            "author": "Xiaoping Gao",
            "body": "On Mon, May 11, 2009 at 6:57 PM, Michael McCandless (JIRA)\n\n\nstopwords.txt should be in the same package as\norg.apache.lucene.analysis.cn.SmartChineseAnalyzer , can you find it there?\n\n",
            "date": "2009-05-11T12:51:47.658+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "I do have the file, but at runtime the JRE cannot locate it using Class.getResourceAsStream().\n\nAre you able to run \"ant test -Dtestcase=TestSmartChineseAnalyzer\" from the command line in contrib/analzyers successfully?",
            "date": "2009-05-11T13:46:06.258+0000",
            "id": 20
        },
        {
            "author": "Uwe Schindler",
            "body": "Did the <jar> ANT task also adds the non *.class files? During compilation, the additional files must be copied to the build directory, this is normally done by an additional copy task (I do it in this way). The Packager then packs all files below build into the jar file. Maybe the build script must be modified?\nI will try this out later.",
            "date": "2009-05-11T14:43:56.862+0000",
            "id": 21
        },
        {
            "author": "Xiaoping Gao",
            "body": "I think Schindler should be right.\nI modified the code to skip loading stopwords.txt, but NullPointerException\npop out again when loading coredict.mem file. When I run\nTestSmartChineseAnalyzer using eclipse, it just run successfully.\nSo the problem might exist in the ant build script.\n\n\n",
            "date": "2009-05-11T14:59:48.866+0000",
            "id": 22
        },
        {
            "author": "Uwe Schindler",
            "body": "I did some checks now, it is the problem of the ant script. Because of this, e.g. ArabicAnalyzer throws an IOException (but this is not tested, and so no test failures occur).\nThe ant script should copy all the data files to the build/classes directory after compiling and before jaring.\n\nI do not know, how to fix this correctly, because I do not fully understand all the parts of the build files and how maven and common-build.xml works together with contrib-build and so on.\nThe simpliest would be to customize the \"compile\" target for the analyzers package and list there all files that must be copied during the compilation step.\n\nShould I open an additional bug report for the ArabicAnalyzer, or should we fix the build.xml for analyzers with this case?",
            "date": "2009-05-11T18:10:37.312+0000",
            "id": 23
        },
        {
            "author": "Michael McCandless",
            "body": "bq. The simpliest would be to customize the \"compile\" target for the analyzers package and list there all files that must be copied during the compilation step.\n\nLet's just do this fix, under this issue, for all contrib/analyzers that need to load a resource?",
            "date": "2009-05-11T20:05:32.458+0000",
            "id": 24
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Mike,\n\nhere is a patch that adds a maven-like resources directory. It patches the build script in two ways:\n- The junit test classpath is extended to include src/resources\n- The jarify macro is changed to also add src/resources to the jar file\n\nSo all resource files mut be put into the corresponding subdirectory under src/resources. The patch contains this for the stopword.txt file af the arabic analyzer. The data files should be removed from src/java.\n\nThe cn analyzers stopwords must be put in the top-level cn directory, the mem files into cn/smart/hhmm (I took me some time to find this out).\n\nThe patch also includes some src/resources directory additions. For the compilation to work, every src/ directory now needs at least an empty resources folder. I found no way to make the jarify macro work without this?\n\nIf somebody has an idea, it would be good.",
            "date": "2009-05-11T21:39:50.977+0000",
            "id": 25
        },
        {
            "author": "Xiaoping Gao",
            "body": "I think it is unacceptable to ask every package to have a resources folder,  \ncan we write the build script to test whether the resources file exists,  \nlike this:\n<available property=\"resources.exists\" file=\"${resources.dir}\" type=\"dir\"/>\n<target name=\"index\" depends=\"compile\" description=\"Build WordNet index\">\n<do_something if=\"sources.exists\">\npackage the reources.\n</do_something>\n",
            "date": "2009-05-12T04:51:46.850+0000",
            "id": 26
        },
        {
            "author": "Uwe Schindler",
            "body": "I know this, the problem with th lucene build is that JAR ing is done using a macro called <jarify>. And here this is not possible. From ANT 1.7.1 on there is the possibility to specify a \"erroronmissingdir\" when using <fileset/>: http://ant.apache.org/manual/CoreTypes/fileset.html\n\nI do not know what version of ant we require, but using it, the error can be avoided.",
            "date": "2009-05-12T06:18:43.854+0000",
            "id": 27
        },
        {
            "author": "Michael McCandless",
            "body": "(Shooting in the dark, here, since I'm no ant expert...)\n\nLucene's common-build.xml has this:\n\n{code}\n<!-- Copy any data files present to the classpath -->\n<copy todir=\"@{destdir}\">\n  <fileset dir=\"@{srcdir}\" excludes=\"**/*.java\"/>\n</copy>\n{code}\n\nWhich for all tests will copy any resources (any file that's not *.java) into the corresponding build/classes directory; eg contrib/xml-query-parser's tests rely on this.  This approach doesn't cause any errors when a given contrib module doesn't have resources.  Is there some way to use a similar approach here (and not bump up the minimum ant version required)?",
            "date": "2009-05-13T10:40:16.028+0000",
            "id": 28
        },
        {
            "author": "Uwe Schindler",
            "body": "I wonder, why this build fragment did not work for contrib? The only problem is, that this also copies the package. and overview javadoc files. They should also be excluded.",
            "date": "2009-05-13T11:21:35.914+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "That fragment is under \"compile-test-macro\", which is run only on src/test/*.  I agree, we should fix it to not copy package/javadoc files.",
            "date": "2009-05-13T11:48:17.972+0000",
            "id": 30
        },
        {
            "author": "Uwe Schindler",
            "body": "I will look into it this evening and provide a patch.\n\nBecause of the file exclusion problematics, I thought, the approach to have a separate resources directory (like Maven does it), would be a great new invention. We could also do this for the tests. In my opinion, data files should be separated from source files. And by adding the resources folder to classpath during tests saves a lot of disk space during compilation and testing (ok, thats not important). By this compilation/test class path and building the jar files are separate tasks.\nThe problem with my current approach is only, that the JAR packager fails, when the directory is not available :( - Is it so bad to just add an empty resources folder to every compilation unit? This would be similar to Maven.",
            "date": "2009-05-13T11:59:53.269+0000",
            "id": 31
        },
        {
            "author": "Michael McCandless",
            "body": "OK, I agree, separation of resources from source code is good.\n\nCan we limit the required addition of src/resources/org/apache/lucene/* to just contrib/analyzers?  Ie, somehow only override its jarify macro?",
            "date": "2009-05-13T12:08:00.847+0000",
            "id": 32
        },
        {
            "author": "Uwe Schindler",
            "body": "Its only needed to have the src/resources folder, no subfolders, I think it would be no problem to add this folder to every compilation unit (I added it to my svn in minutes). The good thing is, that future developments then know, where to put the resource files. But I agree, there should be a better way to automatically detect the resources folder before ANT 1.7.1.\n\nMaybe we should ask Erik Hatcher as the ANT specialist...!",
            "date": "2009-05-13T12:50:20.331+0000",
            "id": 33
        },
        {
            "author": "Erik Hatcher",
            "body": "My initial thought is to move the <copy> excluding {noformat} **/*.java and **/*.html{noformat}  to the \"compile\" macro.   In the ancient past, Ant actually used to do this automatically with <javac>.\n\n",
            "date": "2009-05-13T12:57:06.565+0000",
            "id": 34
        },
        {
            "author": "Uwe Schindler",
            "body": "Here another try with Erik's suggestion:\nI moved the <copy> task to the compile macro and extended the list of exclusions. With some work and verbose=true, I added all \"source\" files to the exclusion (also .jj and so on).\n\nUsing this patch, you can compile Xiaoping Gao patch, add the resources to cn/ and cn/smart/hhmm/ and they appear in classpath for testing and the final jar file.\n\nMy problem with this is the messy exclusion list. During reading ANT docs, I dound out that there is the possibility with the <copy> task to not stop on errors. The idea is now again to put the data files into a maven-like resources folder and just copy them to the classpath (if the folder does not exist, copy would simply do nothing).\n\nI post a patch/test later.",
            "date": "2009-05-14T07:05:26.680+0000",
            "id": 35
        },
        {
            "author": "Uwe Schindler",
            "body": "This is a second try, again with the resources folder. It is now optional, to have a src/resources folder, if it exists, all files from inside are copied to the build destination.\n\nThe trick was, that the copy task can additionally use a globmapping, and by that, does the following:\n- The source fileset of the copy task uses the src/ folder directly\n- The fileset only includes resources/**\n- Because then the target folder would get an additional sub-folder \"resources\" (because the base dir of the copy operation is \"src/\"), the filenames are replaced by a globmapping, stripping the \"resources/\" from the relative path\n\nThis patch also adds a simple test case, that shows, that ArabicAnalyzer does not start correctly, when the stopwords.txt file is not in the classpath. The test fails, if the stopwords.txt file stays at the original location and/or the copy operation is commented out.\n\nThe patch does not contain the deletion of the arabic stopwords file from the sources folder (was binary), so remove it by hand or simply move it after aplying the patch.",
            "date": "2009-05-14T09:07:34.633+0000",
            "id": 36
        },
        {
            "author": "Michael McCandless",
            "body": "Awesome!  I've applied your patch, Uwe, and moved ArabicAnalyzer's stopwords.txt, as well as SmartChineseAnalyzer's stopwords.txt, bigramdict.mem, coredict.mem, under their respective subdirs under src/resources/*.  I confirmed TestArabicAnalyzer passes (and verified it really did instantiate ArabicAnalyzer).  All tests pass.\n\nI will commit shortly.\n\nThis issue is a delightful example of the collaboration that makes open source development work so well.  Thanks Xiaoping, Uwe and Erik!",
            "date": "2009-05-14T10:07:02.647+0000",
            "id": 37
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks everyone!",
            "date": "2009-05-14T10:16:38.187+0000",
            "id": 38
        },
        {
            "author": "Uwe Schindler",
            "body": "Fine!\nShould I commit the ArabicAnalyzer test, too? But I think the test is not really needed, as the new chinese analyzer already tests for the resources implicit.\n\nOne thing: The change is in the main changes.txt, normally it should be in contrib's changes.txt, or not? If it should stay there, we should also add Spatial and TrieRange to main changes.txt.\n\nAnd one other thing: The analyzer (and many more) use the old TokenStream API at the moment, we should change this before 2.9 for all contrib analyzers, see LUCENE-1460?",
            "date": "2009-05-14T10:39:07.925+0000",
            "id": 39
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Should I commit the ArabicAnalyzer test, too?\n\nWoops, I missed it -- I'll commit it.  The more tests the better!\n\nbq. The change is in the main changes.txt, normally it should be in contrib's changes.txt, or not?\n\nWoops -- you're right.  I'll move this to contrib's CHANGES.txt.",
            "date": "2009-05-14T10:51:10.505+0000",
            "id": 40
        },
        {
            "author": "Michael McCandless",
            "body": "bq. The analyzer (and many more) use the old TokenStream API at the moment, we should change this before 2.9 for all contrib analyzers, see LUCENE-1460?\n\nYes -- we need to resolve LUCENE-1460 (and a great many more; the list keeps growing!) before 2.9.",
            "date": "2009-05-14T10:51:51.853+0000",
            "id": 41
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Mike,\na small patch: The HTML files generated by Javadoc do not contain the charset header and are displayed as ISO-8859-1. This breaks the docs for the chinese analyzer. The attached patch sets the output encoding correctly to UTF-8 using the <meta/> html tag.",
            "date": "2009-05-14T14:13:58.245+0000",
            "id": 42
        },
        {
            "author": "Xiaoping Gao",
            "body": "Test successful on my laptop now! Thank all of you for your patience and hard work!\nI will continue to maintain this analyzer and develop new features.\n\nBest Wishes! \n",
            "date": "2009-05-14T14:39:58.198+0000",
            "id": 43
        },
        {
            "author": "Michael McCandless",
            "body": "OK, I just committed that fix (javadocs encoding == UTF-8) Uwe.  Thanks.",
            "date": "2009-05-14T14:49:13.893+0000",
            "id": 44
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Xiaoping,\n\nThanks! The code is now committed.\n\nOnly for the understanding (as I do not know chinese and cannot read some comments), some questions/comments:\nThe .mem files are serializations of the dictionaries. They are created by loading from the random access file (these dct files) and then serialized to the mem files. But for developers and further updates you need to have the dct files and rerun these steps (that are all these private methods).\nAn interesting addition would be to create a custom build step, that uses the dct files and builds the .mem files from it. How could I invoke that? So maybe you could extract the useless dct file loaders from the current classes and create a separate tool from it, that could be invoked from ant, that builds that mem files.\n\nUwe\n\nP.S.: By the way: In these private conversation methods (that are never called from the library code) you have these default try-catch blocks, which is bad programming practice. So the proposed separate conversion tool should correctly handle the exceptions or better just not catch them at all and pass up (side note: I hate eclipse for generating these auto-catch blocks, better would be to auto-add throws-clauses to the method signatures!)",
            "date": "2009-05-14T15:04:01.609+0000",
            "id": 45
        },
        {
            "author": "Mingfai Ma",
            "body": "hi Xiaoping,\n\nI'm interested to get the Chinese analyzer work for Traditional Chinese (UTF-8/Big5).  Just wonder if your coredict.dct comes from ICTCLAS? (http://ictclas.org/Down_share.html) if yes, is it 2009 or 2008?\n\nThe ICTCLAS has traditional chinese edition for its 2008 release. But the distribution are not in .dct. I wonder if we have a simple specification for the .dct so I could find a way to convert the ICTCLAS's lexical dictionary to the .dct format to work with your library? ",
            "date": "2009-05-15T10:19:40.436+0000",
            "id": 46
        },
        {
            "author": "Xiaoping Gao",
            "body": "Hello Mingfai!\n\ncoredict.mem is converted from coredict.dct which come from ICTCLAS1.0,  \nneither 2008 nor 2009.\nThe author authorized me to release just the lexical dictionary from  \nICTCLAS1.0 under APLv2, but he didn't authorize the dictionary of  \nictclas2008~2009.\nAs far as I know, coredict.dct just contain GB2312 characters, so it cannot  \nsupport Big5.\n\nI think we should find the proper big5 dictionary first, then I will help  \nyou to convert to dct file.\n\n\nOn May 15, 2009 6:20pm, \"Mingfai Ma (JIRA)\" <jira@apache.org> wrote:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
            "date": "2009-05-15T15:15:46.884+0000",
            "id": 47
        },
        {
            "author": "Xiaoping Gao",
            "body": "Hello Mingfai!\n\ncoredict.mem is converted from coredict.dct which come from ICTCLAS1.0,  \nneither 2008 nor 2009.\nThe author authorized me to release just the lexical dictionary from  \nICTCLAS1.0 under APLv2, but he didn't authorize the dictionary of  \nictclas2008~2009.\nAs far as I know, coredict.dct just contain GB2312 characters, so it cannot  \nsupport Big5.\n\nI think we should find the proper big5 dictionary first, then I will help  \nyou to convert to dct file.\n\n\nOn May 15, 2009 6:20pm, \"Mingfai Ma (JIRA)\" <jira@apache.org> wrote:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
            "date": "2009-05-15T15:15:47.380+0000",
            "id": 48
        },
        {
            "author": "Robert Muir",
            "body": "if you acquire the big5 resources, do you think it would be possible to create a single dictionary that works with both Simplified & Traditional?\n\n(i.e. merge the big5 resources with the gb resources)\n\nThe reason I say this, is the existing chinese analyzers, although they tokenize in a less intelligent way, they are agnostic to Simplified/Traditional issues...\n",
            "date": "2009-05-15T15:25:19.374+0000",
            "id": 49
        },
        {
            "author": "Robert Muir",
            "body": "another potential issue with big5 i want to point out is that many of the big5 character sets such as HKSCS have characters that are mapped into regions of unicode outside of the BMP.\n\njust glancing at the code, some things will need to be modified for this to work correctly with surrogate pairs, various functions that take char will need to take codepoint (int), etc. \n",
            "date": "2009-05-15T15:33:30.147+0000",
            "id": 50
        },
        {
            "author": "Mingfai Ma",
            "body": "could we use CC-CEDICT's dictionary instead? it is using Creative Commons Attribution-Share Alike 3.0 license\n\nhttp://www.mdbg.net/chindict/chindict.php?page=cc-cedict",
            "date": "2009-05-15T21:18:00.406+0000",
            "id": 51
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Just an FYI. There have been a working for mapping between simplified and traditional chinese characters in Solr 1.4. (but you need to define mapping rules in mapping.txt)\nSee SOLR-822 and the attached JPG for chinese mapping sample.\nI opened LUCENE-1466 for Lucene. :)",
            "date": "2009-05-16T00:05:19.192+0000",
            "id": 52
        },
        {
            "author": "Robert Muir",
            "body": "koji, have you considered using icu transforms for this behavior?\nNot only is the rule-based language very nice (you can define variables, use context, etc), but many transformations such as \"Traditional-Simplified\" are already defined.\n\nhttp://userguide.icu-project.org/transforms/general\n",
            "date": "2009-05-16T05:16:06.307+0000",
            "id": 53
        },
        {
            "author": "Koji Sekiguchi",
            "body": "bq. koji, have you considered using icu transforms for this behavior?\n\nNot yet.\n\nbq. Not only is the rule-based language very nice (you can define variables, use context, etc), but many transformations such as \"Traditional-Simplified\" are already defined. \n\nRight. CharFilter framework that I introduced in SOLR-822 is not only for rule-based mapping, but it can use existing library like ICU to transform/normalize characters.",
            "date": "2009-05-16T14:47:09.646+0000",
            "id": 54
        },
        {
            "author": "Mingfai Ma",
            "body": "i'm not sure if the character mapping is a feasible approach. The CC-CEDICT dictionary has terms in both simplified and traditional chinese already and we need not to define any rule. how much effort is needed to define all mapping rules for the simplified and traditional chinese thesaurus?\n\nBesides, simplified and traditional Chinese conversion is not as simple as mapping the code. For the sample meaning, it may use different words in the SC and TC. \n\nif the approach accepted in this issue is ok, I just need to figure out how to convert CC-CEDICT to the dct / mem format, and i suppose it is doable.",
            "date": "2009-05-16T22:54:36.578+0000",
            "id": 55
        },
        {
            "author": "Xiaoping Gao",
            "body": "The dictionary is loaded in to 2 classes:\nBigramDictionary.java\nWordDictionary.java\n\nso you can read from the loading section to get \"dct\" format\n\nmem files are just arrays that has been serialized to object files, you can get the format from the code and comments.",
            "date": "2009-05-19T11:47:51.123+0000",
            "id": 56
        },
        {
            "author": "Otis Gospodnetic",
            "body": "I just got to look at this code and I only scanned it quickly.  Is all of the code really Chinese-specific? \nWould any of it be applicable to other languages, say Japanese or Korean? (assuming we have dictionaries in suitable format)\n\n",
            "date": "2009-05-29T03:35:11.149+0000",
            "id": 57
        },
        {
            "author": "Xiaoping Gao",
            "body": "I think the algorithm of Hidden Markov Model is applicable,\nbut I doubt that Japanese and Korean may need some language specific\nanalysis, such as \"\u7247\u5047\u540d\u304b\u305f\u304b\u306a.\u5e73\u5047\u540d\u3072\u3089\u304c\u306a\" in Japanese, as far as I know, the same\nword can have different spelling. This may be a problem in the application?",
            "date": "2009-05-29T04:09:42.028+0000",
            "id": 58
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Hm, my Japanese is a little weak, so I'm not sure what exactly that means and what exactly\ndifferent spelling means in this context... :)\n\nGoogle says \"\u7247\u5047\u540d\u304b\u305f\u304b\u306a.\u5e73\u5047\u540d\u3072\u3089\u304c\u306a\" means \"How do\u5047\u540dpiece. Hiragana\u5047\u540dflat\"\n\nWhich word in the above Japanese text is the same as another word, yet with a different spelling?  Is this a question of synonyms,\nsuch as \"auto\", \"automobile\", and \"car\", and even \"sedan\" in English?\n",
            "date": "2009-05-29T20:38:46.300+0000",
            "id": 59
        },
        {
            "author": "Robert Muir",
            "body": "otis if you are interested in japanese/korean you might find this link interesting:\n\nhttp://bugs.icu-project.org/trac/ticket/2229\n\nsimilar to the thai approach (in contrib) but with log probabilities.\n",
            "date": "2009-05-29T20:53:36.111+0000",
            "id": 60
        },
        {
            "author": "Mingfai Ma",
            "body": "re. \u5e73\u5047\u540d and  \u7247\u5047\u540d in Japanese\nhttp://en.wikipedia.org/wiki/Kana\nhttp://en.wikipedia.org/wiki/Hiragana\nhttp://en.wikipedia.org/wiki/Katakana\n\n",
            "date": "2009-05-30T05:54:55.546+0000",
            "id": 61
        },
        {
            "author": "Kuro Kurosaka",
            "body": "WordTokenizer extends Tokenizer, but it's constructor takes a TokenStream rather than a Reader.  \nShouldn't WordTokenizer rather extends TokenFilter, and if so, shouldn't it be named WordTokenFilter?\n",
            "date": "2009-07-09T18:32:35.094+0000",
            "id": 62
        },
        {
            "author": "Robert Muir",
            "body": "bq. Shouldn't WordTokenizer rather extends TokenFilter, and if so, shouldn't it be named WordTokenFilter? \n\nyes, you are correct. see LUCENE-1728 where we have proposed correcting this.",
            "date": "2009-07-09T18:45:13.367+0000",
            "id": 63
        }
    ],
    "component": "modules/analysis",
    "description": "I wrote a Analyzer for apache lucene for analyzing sentences in Chinese language. it's called \"imdict-chinese-analyzer\", the project on google code is here: http://code.google.com/p/imdict-chinese-analyzer/\n\nIn Chinese, \"\u6211\u662f\u4e2d\u56fd\u4eba\"(I am Chinese), should be tokenized as \"\u6211\"(I)   \"\u662f\"(am)   \"\u4e2d\u56fd\u4eba\"(Chinese), not \"\u6211\" \"\u662f\u4e2d\" \"\u56fd\u4eba\". So the analyzer must handle each sentence properly, or there will be mis-understandings everywhere in the index constructed by Lucene, and the accuracy of the search engine will be affected seriously!\n\nAlthough there are two analyzer packages in apache repository which can handle Chinese: ChineseAnalyzer and CJKAnalyzer, they take each character or every two adjoining characters as a single word, this is obviously not true in reality, also this strategy will increase the index size and hurt the performance baddly.\n\nThe algorithm of imdict-chinese-analyzer is based on Hidden Markov Model (HMM), so it can tokenize chinese sentence in a really intelligent way. Tokenizaion accuracy of this model is above 90% according to the paper \"HHMM-based Chinese Lexical analyzer ICTCLAL\" while other analyzer's is about 60%.\n\nAs imdict-chinese-analyzer is a really fast and intelligent. I want to contribute it to the apache lucene repository.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1629",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "contrib intelligent Analyzer for Chinese",
    "systemSpecification": true,
    "version": "2.4.1"
}