{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "Possible patch fixing the issue.  I'm not yet certain there is no other place where we use an int...",
            "date": "2010-02-09T19:46:46.219+0000",
            "id": 0
        },
        {
            "author": "Tom Burton-West",
            "body": "Thanks for the patch Michael,\n\nThe patch worked fine with CheckIndex.  Checkindex worked with an index with 2.49 billion terms.\nI added commas to the output below:\n test: terms, freq, prox...OK [2,487,224,745 terms; 23,573,976,855 terms/docs pairs; 97,223,318,067 tokens]\n\nWe are working on determining how to test it with Solr.  The ArrayIndexOutOfBounds exception appears in the logs about for about 1 in every 100 queries.   We haven't been able to determine which queries trigger the problem.\n\nWe are using an older version of Solr with lucene 2.9-dev 779312 - 2009-05-27 17:19:55 .  I'm not sure if we can just drop in a later version of lucene with the patch or if I need to patch the older 2.9 dev lucene version that came with our Solr.   What do you suggest?\n\nWhat I'm thinking of is to run 10,000 queries against our dev server pointing at one of the large segment indexes  with and without the patch.\n\nTom\n\n\n",
            "date": "2010-02-10T16:35:00.337+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "OK, I'm glad to hear that.\n\nThe attached patch applies to 2.9, and I think should apply fine to the revision of Lucene you're using (779312) that you're using within Solr.  I'd recommend checking out that exact revision of Lucene (svn co -r779312 ...), applying this patch, building a JAR, and replacing Solr's Lucene JAR with this one.\n\nIt's only queries that contain terms above the 2.1B mark (your last ~390 M terms) that will hit the exception.  Once you find such a query it should always hit the exception on this large segment.",
            "date": "2010-02-10T17:14:59.378+0000",
            "id": 2
        },
        {
            "author": "Tom Burton-West",
            "body": "Hi Michael,\n\nThanks for your help. We mounted one of the indexes with 2.4 billion terms on our dev server and tested with and without the patch. (I discovered that queries containing Korean characters would consistently trigger the bug).   With the patch, we don't see any ArrayIndexOutOfBounds exceptions.  We are going to do a bit more testing before we put this into production. (We rolled back our production indexes temporarily to indexes that split the terms over 2 segments and therefore didn't trigger the bug).\n\nOther than walking though the code in the debugger, is there some systematic way of looking for any other places where an int is used that might also have problems when we have over 2.1x billion terms?\n\nTom",
            "date": "2010-02-11T21:28:31.598+0000",
            "id": 3
        },
        {
            "author": "Robert Muir",
            "body": "bq. (I discovered that queries containing Korean characters would consistently trigger the bug).\n\nthis makes sense because Hangul is sorted towards the end of the term dictionary\n\nyou can see this visually here: http://unicode.org/roadmaps/bmp/\n",
            "date": "2010-02-11T22:00:10.116+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "bq. With the patch, we don't see any ArrayIndexOutOfBounds exceptions.\n\nGreat!  And the results look correct?\n\nbq. Other than walking though the code in the debugger, is there some systematic way of looking for any other places where an int is used that might also have problems when we have over 2.1x billion terms?\n\nNot that I know of!  The code that handles the term dict lookup is\nfairly contained, in TermInfosReader and SegmentTermEnum.  I think\nscrutinizing the code and testing (as you're doing) is the only way.\n\nI just looked again -- there are a few places where int is still being used.\n\nFirst is two methods (get(int position) and scanEnum), in\nTermInfosReader, that are actually dead code (package private &\nunused).  Second is int SegmentTermEnum.scanTo, but this is fine\nbecause it's never asked to can more than termIndexInterval terms.\n\nI'll attach patch that additionally just removes that dead code.\n",
            "date": "2010-02-11T23:25:20.027+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "Committed for 2.9.2, 3.0.1, 3.1.",
            "date": "2010-02-12T10:57:07.089+0000",
            "id": 6
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Hello,\nI'd like to confirm what the \"term\" means in \"unique terms\". Is the term Term? (unique terms in whole fields in a single segment?) or word? (unique terms in each field in a single segment?). Thanks.",
            "date": "2010-05-18T02:33:30.723+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "Yes, the limit is number of unique terms per-segment.\n\nFlex actually increases the limit (the limit is per-field, per-segment; but in trunk, the limit is across all fields).",
            "date": "2010-05-18T09:20:27.593+0000",
            "id": 8
        }
    ],
    "component": "",
    "description": "Lucene can't handle more than 2.1B (limit of signed 32 bit int) unique terms in a single segment.\n\nBut I think we can improve this to termIndexInterval (default 128) * 2.1B.  There is one place (internal API only) where Lucene uses an int but should use a long.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2257",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "relax the per-segment max unique term limit",
    "systemSpecification": true,
    "version": ""
}