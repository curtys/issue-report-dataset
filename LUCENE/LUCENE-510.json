{
    "comments": [
        {
            "author": "Doug Cutting",
            "body": "Link to LUCENE-510.",
            "date": "2006-03-03T03:32:00.000+0000",
            "id": 0
        },
        {
            "author": "Marvin Humphrey",
            "body": "The following patch...\n\n  * Changes Lucene to use bytecounts as the prefix to all written Strings\n  * Changes Lucene to write standard UTF-8 rather than Modified UTF-8 \n  * Adds the new test classes MockIndexOutput and TestIndexOutput\n  * Increases the number of tests in TestIndexInput\n\nIt also slows Lucene down -- indexing takes around a 20% speed hit.  It would be possible to submit a patch which had a smaller impact on performance, but this one is already over 700 lines long, and it's goal is to achieve standard UTF-8 compliance and modify the definition of Lucene strings as simply and reliably as possible.  Optimization patches can now be submitted which build upon this one.\n\nMarvin Humphrey\nRectangular Research\nhttp://www.rectangular.com/\n\n",
            "date": "2006-05-09T04:02:36.000+0000",
            "id": 1
        },
        {
            "author": "Marvin Humphrey",
            "body": "Greets,\n\nI've ported KinoSearch's external sorting module to java, along with its tests.  This class is the linchpin for the KinoSearch merge model, as it allows serialized postings to be dumped into a sort pool of effectively unlimited size.\n\nAt some point, I'll submit patches implementing the KinoSearch merge model in Lucene.  I'm reasonably confident that it will more than make up for the index-time performance hit caused by using bytecounts as string headers.\n\nThematically, this class belongs in org.apache.lucene.util, and that's where I've put it for now.  The classes that will use it are in org.apache.lucene.index, so if it stays in util, it will have to be public.  However, it shouldn't be part of Lucene's documented public API.  The process by which Lucene's docs are generated is not clear to me, so access control advice would be appreciated.\n\nThere are a number of other areas where this code could stand review, especially considering my relatively limited experience using Java.  I'd single out exception handling and thread safety, but of course anything else is fair game.\n\nMarvin Humphrey\nRectangular Research\nhttp://www.rectangular.com/\n",
            "date": "2006-06-05T08:54:50.000+0000",
            "id": 2
        },
        {
            "author": "Grant Ingersoll",
            "body": "Hi Marvin,\n\nThis no longer applies cleanly to trunk, care to update? \n\nThanks,\nGrant",
            "date": "2007-01-04T02:05:40.268+0000",
            "id": 3
        },
        {
            "author": "Chuck Williams",
            "body": "Has an improvement been made to eliminate the reported 20% indexing hit?  That would be a big price to pay.\n\nTo me the performance benefits in algorithms that scan for selected fields (e.g., FieldsReader.doc() with a FieldSelector) are much more important than standard UTF-8 compliance.\n\nA 20% hit seems suprising.  The pre-scan over the string to be written shouldn't cost much compared to the cost of tokenizing and indeixng that string (assuming it is in an indexed field).\n\nIn case it is relevant, I had a related issue in my bulk updater, a case where a vint required at the beginning of a record by the lucene index format was not known until after the end.  I solved this with a fixed length vint record that was estimated up front and revised if necessary after the whole record was processed.  The vint representation still works if more bytes than necessary are written.\n",
            "date": "2007-01-04T03:15:49.434+0000",
            "id": 4
        },
        {
            "author": "Yonik Seeley",
            "body": "I'd like to see everything kept as bytes for as long as possible (right up into Term).\nA nice bonus would be to reduce the size of things like the FieldCache, and to allow true binary data.\n",
            "date": "2007-01-04T05:46:13.446+0000",
            "id": 5
        },
        {
            "author": "Marvin Humphrey",
            "body": "Grant... At the moment I am completely consumed by the task of getting a devel release of KinoSearch version 0.20 out the door.  Once that is taken care of, I will be glad to update this patch, and to explore how to compensate for the performance hit it causes.\n\nChuck... If bytecount-based strings are adopted, standard UTF-8 probably comes along for the ride.  There's actually a 1-2% performance gain to be had using standard over modified because of simplified conditionals.  What holds us back is backwards compatibility -- but we'll have wrecked backwards compat with the bytecounts.  However, I no longer have a strong objection to using Modified UTF-8 (for Lucene, that is -- Modified UTF-8 would be a deal-breaker for Lucy), so if somewhere along the way we find a compelling reason to stick with modified UTF-8, so be it.\n\nIf bytecount-based strings get adopted, it will be because they hold up on their own merits.  They're required for KinoSearch merge model; once KS 0.20 is out, I'll port the new benchmarking stuff, we can study the numbers, and assess whether the significant effort needed to pry that algo into Lucene would be worthwhile.\n\nYonik... yes, I agree.  Even better for indexing time, leave postings in serialized form for the entire indexing session.  :)",
            "date": "2007-01-04T19:01:43.283+0000",
            "id": 6
        },
        {
            "author": "Grant Ingersoll",
            "body": "I don't have time at the moment",
            "date": "2007-06-01T17:07:37.763+0000",
            "id": 7
        },
        {
            "author": "Michael Busch",
            "body": "I think it makes total sense to change this. And this issue seems to be\nvery popular with 5 votes.\n\nMike, you've done so much performance & indexing work recently. Are\nyou interested in taking this?",
            "date": "2008-01-11T09:07:22.321+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "Yup, I'll take this!",
            "date": "2008-01-11T10:03:23.709+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "Attached patch.\n\nI modernized Marvin's original patch and added full backwards\ncompatibility to it so that old indices can be opened for reading or\nwriting.  New segments are written in the new format.\n\nAll tests pass.  I think it's close, but, I need to run performance\ntests now to measure the impact to indexing throughput.\n\nI think future optimizations can keep the byte[] further, eg, into\nTerm and FieldCache, as Yonik mentioned.  We could also fix\nDocumentsWriter to use byte[] for its terms storage which would\nimprove RAM efficiency for single-byte (ascii) content.\n\nI also updated the TestBackwardsCompatibility testcase to properly\ntest non-ascii terms.\n\n",
            "date": "2008-03-04T20:55:49.877+0000",
            "id": 10
        },
        {
            "author": "Grant Ingersoll",
            "body": "So, with this we should be able to skip ahead in the FieldsReader, right?  I will try to update your patch with that.  Should improve lazy loading, etc.",
            "date": "2008-03-07T14:09:38.962+0000",
            "id": 11
        },
        {
            "author": "Michael McCandless",
            "body": "Yes, exactly.  But I think the current patch is already doing this? -- ie, using seek instead of skipChars, if the fdt is new.\n",
            "date": "2008-03-07T16:10:15.540+0000",
            "id": 12
        },
        {
            "author": "Grant Ingersoll",
            "body": "Cool.  I just downloaded and applied, but hadn't looked at it yet.  :-)",
            "date": "2008-03-07T18:23:47.683+0000",
            "id": 13
        },
        {
            "author": "Michael McCandless",
            "body": "New rev of the patch.  I think it's ready to commit.  I'll wait a few\ndays.\n\nI made some performance improvements by factoring out a new\nUnicodeUtil class that does not allocate new objects for every\nconversion to/from UTF8.\n\nOne new issue I fixed is the handling of invalid UTF-16 strings.\nSpecifically if the UTF16 text has invalid surrogate pairs, UTF-8 is\nunable to represent it (unlike the current modified UTF-8 Lucene\nformat).  I changed DocumentsWriter & UnicodeUtil to substitute the\nreplacement char U+FFFD for such invalid surrogate characters.  This\naffects terms, stored String fields and term vectors.\n\nIndexing performance has a small slowdown (3.5%); details are below.\n\nUnfortunately, time to enumerate terms was more affected.  I made a\nsimple test that enumerates all terms from the index (= ~3.3 million\nterms) created below:\n\n  public class TestTermEnum {\n    public static void main(String[] args) throws Exception {\n      IndexReader r = IndexReader.open(args[0]);\n      TermEnum terms = r.terms();\n      int count = 0;\n      long t0 = System.currentTimeMillis();\n      while(terms.next())\n        count++;\n      long t1 = System.currentTimeMillis();\n      System.out.println(count + \" terms in \" + (t1-t0) + \" millis\");\n      r.close();\n    }\n  }\n\nOn trunk with current index format this takes 3104 msec (best of 5).\nWith the patch with UTF8 index format it takes 3443 msec = 10.9%\nslower.  I don't see any further ways to make this faster.\n\nDetails on the indexing performance test:\n\n  analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer\n  \n  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n  \n  docs.file=/Volumes/External/lucene/wiki.txt\n  doc.stored = true\n  doc.term.vector = true\n  doc.add.log.step=2000\n  \n  directory=FSDirectory\n  autocommit=false\n  compound=false\n  \n  ram.flush.mb=64\n  \n  { \"Rounds\"\n    ResetSystemErase\n    { \"BuildIndex\"\n      CreateIndex\n      { \"AddDocs\" AddDoc > : 200000\n      - CloseIndex\n    }\n    NewRound\n  } : 5\n  \n  RepSumByPrefRound BuildIndex\n\nI ran it on a quad-core Intel Mac Pro, with 4 drive RAID 0 array,\nrunning OS 10.4.11, java 1.5, run with these command-line args:\n\n  -server -Xbatch -Xms1024m -Xmx1024m\n\nBest of 5 with current trunk is 921.2 docs/sec and with patch it's\n888.7 = 3.5% slowdown.\n\n",
            "date": "2008-03-17T20:04:12.356+0000",
            "id": 14
        },
        {
            "author": "Hiroaki Kawai",
            "body": "I'm wondering why the patch doesn't utilize java.nio.charset.CharsetEncoder, CharsetDecoder....?",
            "date": "2008-03-18T02:28:19.989+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nI'm wondering why the patch doesn't utilize java.nio.charset.CharsetEncoder, CharsetDecoder....?\n{quote}\n\nI think there are two reasons for rolling our own instead of using\nCharsetEncoder/Decoder.  First is performance.  If I use\nCharsetEncoder, like this:\n\n  CharsetEncoder encoder = Charset.forName(\"UTF-8\").newEncoder();\n  CharBuffer cb = CharBuffer.allocate(5000);\n  ByteBuffer bb = ByteBuffer.allocate(5000);\n  byte[] bbArray = bb.array();\n  UnicodeUtil.UTF8Result utf8Result = new UnicodeUtil.UTF8Result();\n\n  t0 = System.currentTimeMillis();\n  for(int i=0;i<count;i++) {\n    cb.clear();\n    cb.put(strings[i]);\n    cb.flip();\n    bb.clear();\n    encoder.reset();\n    encoder.encode(cb, bb, true);\n  }\n\nThen it takes 676 msec to convert ~3.3 million strings from the terms\nfrom indexing first 200K Wikipedia docs.  If I replace for loop with:\n\n  UnicodeUtil.UTF16toUTF8(strings[i], 0, strings[i].length(), utf8Result);\n\nIt's 441 msec.\n\nSecond reason is some API mismatch.  EG we need to convert char[] that\nend in the 0xffff character.  Also, we need to do incremental\nconversion (only convert changed bytes), which is used by TermEnum.\nCharsetEncoder/Decoder doesn't do this.\n\n",
            "date": "2008-03-19T22:39:40.546+0000",
            "id": 16
        },
        {
            "author": "Hiroaki Kawai",
            "body": "To the first reason of performance issue,\n\n{quote}\nCharsetEncoder encoder = Charset.forName(\"UTF-8\").newEncoder();\nCharBuffer cb = CharBuffer.allocate(5000);\nByteBuffer bb = ByteBuffer.allocate(5000);\nbyte[] bbArray = bb.array();\nUnicodeUtil.UTF8Result utf8Result = new UnicodeUtil.UTF8Result();\n\nt0 = System.currentTimeMillis();\nfor(int i=0;i<count;i++) { cb.clear(); cb.put(strings[i]); cb.flip(); bb.clear(); encoder.reset(); encoder.encode(cb, bb, true); }\n{quote}\n\nHow about this code?\n\nCharsetEncoder encoder = Charset.forName(\"UTF-8\").newEncoder();\nCharBuffer cb;\nByteBuffer bb = ByteBuffer.allocate(5000);\nbyte[] bbArray = bb.array();\nUnicodeUtil.UTF8Result utf8Result = new UnicodeUtil.UTF8Result();\n\nt0 = System.currentTimeMillis();\nfor(int i=0;i<count;i++) {\n  bb.clear();\n  encoder.reset();\n  encoder.encode(CharBuffer.wrap(strings[i]), bb, true); \n}\n\n",
            "date": "2008-03-20T12:43:47.158+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "That version takes 1067 msec (best of 3).",
            "date": "2008-03-20T13:49:41.479+0000",
            "id": 18
        },
        {
            "author": "Hiroaki Kawai",
            "body": "Ah interesting !\n\nI'm not tried yet, but...\n\nCharBuffer cb = ByteBuffer.allocateDirect(5000).asCharBuffer();\nByteBuffer bb = ByteBuffer.allocateDirect(5000);\n\nwill improve the performance.",
            "date": "2008-03-20T15:41:53.310+0000",
            "id": 19
        },
        {
            "author": "Hiroaki Kawai",
            "body": "{quote}\nSecond reason is some API mismatch. EG we need to convert char[] that\nend in the 0xffff character. Also, we need to do incremental\nconversion (only convert changed bytes), which is used by TermEnum.\nCharsetEncoder/Decoder doesn't do this.\n{quote}\n\nIMHO, the char 0xffff is not telling that the string is the end. The marker is not to be used to dermine the termination, but is for assertion, I think (in DocumentsWriterThreadState.java). And we should rewrite DocumentsWriterFieldMergeState.java to provide \"private int postingUpto\" outside via public method, to use it in DocumentsWriter.java and DocumentsWriterField.java.",
            "date": "2008-03-20T16:11:47.694+0000",
            "id": 20
        },
        {
            "author": "Michael McCandless",
            "body": "Using allocateDirect is surprisingly slower: 1050 msec.",
            "date": "2008-03-20T16:18:02.756+0000",
            "id": 21
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nIMHO, the char 0xffff is not telling that the string is the end. The marker is not to be used to dermine the termination, but is for assertion, I think (in DocumentsWriterThreadState.java). \n{quote}\n\nThe 0xffff is used for more than assertion.  It marks the end of the text for each term.\n\n{quote}\nAnd we should rewrite DocumentsWriterFieldMergeState.java to provide \"private int postingUpto\" outside via public method, to use it in DocumentsWriter.java and DocumentsWriterField.java.\n{quote}\nThe \"private int postingUpto\" in DocumentsWriterFieldMergeState is very different from the usage of 0xffff marker; I'm not sure what you're suggesting here?",
            "date": "2008-03-20T16:25:00.721+0000",
            "id": 22
        },
        {
            "author": "Hiroaki Kawai",
            "body": "I'm sorry I mistook something in DocumentsWriterFieldMergeState.java.\n\nBut, my suggestion here, is use 0xffff only for assertion.\n\nTo achive this, we should add textLength property to DocumentsWriterFieldMergeState and use it in DocumentsWriter.java, modify DocumentsWriter#compareText.\n\nI'll submit a patch.",
            "date": "2008-03-20T16:58:46.916+0000",
            "id": 23
        },
        {
            "author": "Michael McCandless",
            "body": "OK.  Can you open a separate issue for that?",
            "date": "2008-03-20T17:21:43.910+0000",
            "id": 24
        },
        {
            "author": "Hiroaki Kawai",
            "body": "OK. I'll open another ticket. :-)\n\nI'll try measuring the performance, too. I still believe that we should use java.nio.charset for our code maintainance.",
            "date": "2008-03-20T17:34:09.708+0000",
            "id": 25
        },
        {
            "author": "Michael McCandless",
            "body": "I think using java.nio.charset is too much of a performance hit.  I think it will especially further slow down enumerating terms since we can't convert the characters incrementally if we use java.nio.charset.",
            "date": "2008-03-20T17:40:48.811+0000",
            "id": 26
        },
        {
            "author": "Hiroaki Kawai",
            "body": "I think performance issue is yet another issue, and I believe we can find the way to speed up.",
            "date": "2008-03-21T02:11:23.689+0000",
            "id": 27
        },
        {
            "author": "Mark Miller",
            "body": "I suspect that this issue is not reading old indexes perfectly. I am very suspicious that this patch caused what I thought was index corruption the other day. I didn't notice it was a format change and should have paid more attention.\n\nI seem to be able to replicate the issue so hopefully I will have more to report soon.\n\nIf my thoughts are unlikely, please let me know.",
            "date": "2008-05-08T23:43:29.595+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "Whoa, I think you are correct Mark!\n\nOn inspecting my changes here, I think the bulk-merging of stored fields is to blame.  Specifically, when we bulk merge the stored fields we fail to check whether the segments being merged are the pre-UTF8 format.  And so that code bulk-copies stored fields in the older format into a file that claims it's using the newer format.\n\nThis only affects trunk, not 2.3.\n\nThanks for being such a brave early-adopter trunk tester, Mark.  And, sorry :(",
            "date": "2008-05-09T09:59:25.154+0000",
            "id": 29
        },
        {
            "author": "Mark Miller",
            "body": "No problem Michael. Brave/Foolish, I certainly am one. Can't keep myself away from these fresh issues :) My fault for not understanding the release process really though. I thought I was roughly getting 2.3.2 ahead of time. Live and learn :) I got a little lax inspecting all the issues for someone grabbing from trunk...once all my tests pass I usually feel pretty good about it. Time to add some upgrade index tests :)\n\nThanks so much for checking into this so quick - certainly saves me some time today.\n\n",
            "date": "2008-05-09T11:15:03.087+0000",
            "id": 30
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Time to add some upgrade index tests\nIn fact Lucene already has TestBackwardsCompatibility ... so my first step is to add a test case there that shows this bug...",
            "date": "2008-05-09T11:28:07.669+0000",
            "id": 31
        },
        {
            "author": "Michael McCandless",
            "body": "OK, I've fixed TestBackwardsCompatibility to catch this, and then fixed merging of stored fields to work properly.  Will commit shortly...\n\nThanks again Mark!",
            "date": "2008-05-09T12:04:18.635+0000",
            "id": 32
        }
    ],
    "component": "core/store",
    "description": "We should change the format of strings written to indexes so that the length of the string is in bytes, not Java characters.  This issue has been discussed at:\n\nhttp://www.mail-archive.com/java-dev@lucene.apache.org/msg01970.html\n\nWe must increment the file format number to indicate this change.  At least the format number in the segments file should change.\n\nI'm targetting this for 2.1, i.e., we shouldn't commit it to trunk until after 2.0 is released, to minimize incompatible changes between 1.9 and 2.0 (other than removal of deprecated features).",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-510",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "IndexOutput.writeString() should write length in bytes",
    "systemSpecification": true,
    "version": "2.1"
}