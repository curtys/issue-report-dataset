{
    "comments": [
        {
            "author": "Robert Muir",
            "body": "Attached is my implementation",
            "date": "2008-09-26T11:57:17.046+0000",
            "id": 0
        },
        {
            "author": "Grant Ingersoll",
            "body": "Very cool.  I've used a modified version of the light 8 before, and it is indeed pretty good.\n\nCan you provide it as a patch?  Also, unit tests would be good.  See the How To Contribute section on the Lucene wiki: http://wiki.apache.org/lucene-java/HowToContribute\n\n",
            "date": "2008-09-26T12:35:14.076+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "attached is patch",
            "date": "2008-09-26T15:34:29.433+0000",
            "id": 2
        },
        {
            "author": "Robert Muir",
            "body": "Thought I would add the following comments:\n\nI tried to stick to basics to start. Some things that kept bugging me just for the record:\n\n1) the rules for stemming only require stemmed token to have 2 characters in many places. This seems incorrect... triliteral root anyone? Seems to be too aggresive. Yet at the same time, many common \"prefix\"/suffix combinations are not stemmed by light8 algorithm...  But its trec tested... \n\n2) there is no decomposition of unicode presentation forms. These characters show up (typically when text is extracted out of PDF). The easiest way to deal with this is Unicode normalization, but that requires Java 6 or ICU.\n\n3) there is no enhanced parsing. Typically academics index high quality news text but in other less perfect text often you see much text without spaces between words when the characters do not join (to the human reader there is a space!). to really solve this you need a lot of special stuff including morphological data, but you can partially solve some of the common cases by splitting words when you see 100% certain cases such as medial teh marbuta, medial alef maksura, double alef, ... I didnt do this because I wanted to keep it simple, but its important, see here: http://papers.ldc.upenn.edu/COLING2004/Buckwalter_Arabic-orthography-morphology.pdf\n \n4) it is simply a stemmer, but I read in lucene docs where it is possible to inject synonym-like information (multiple tokens for one word) and boost the score for certain ones. Seems like this would be better than simply stemming, at least indexing and boosting the normalized surface form for better precision. I'd want to setup TREC tests to actually measure this though.\n",
            "date": "2008-09-26T17:45:20.459+0000",
            "id": 3
        },
        {
            "author": "Grant Ingersoll",
            "body": "I'll commit once 2.4 is released.",
            "date": "2008-09-30T11:35:42.405+0000",
            "id": 4
        },
        {
            "author": "Grant Ingersoll",
            "body": "Committed revision 706342.\n\nI made some small changes to reuse Tokens, also added in some comments into the stopwords list and added to WordListLoader to accommodate this\n\nThanks Robert!",
            "date": "2008-10-20T17:22:41.882+0000",
            "id": 5
        }
    ],
    "component": "modules/analysis",
    "description": "I've noticed there is no Arabic analyzer for Lucene, most likely because Tim Buckwalter's morphological dictionary is GPL.\n\nHowever, it is not necessary  to have full morphological analysis engine for a quality arabic search. \nThis implementation implements the light-8s algorithm present in the following paper: http://ciir.cs.umass.edu/pubfiles/ir-249.pdf\n\nAs you can see from the paper, improvement via this method over searching surface forms (as lucene currently does) is significant, with almost 100% improvement in average precision.\n\nWhile I personally don't think all the choices were the best, and some easily improvements are still possible, the major motivation for implementing it exactly the way it is presented in the paper is that the algorithm is TREC-tested, so the precision/recall improvements to lucene are already documented.\n\nFor a stopword list, I used a list present at http://members.unine.ch/jacques.savoy/clef/index.html simply because the creator of this list documents the data as BSD-licensed.\n\nThis implementation (Analyzer) consists of above mentioned stopword list plus two filters:\n ArabicNormalizationFilter: performs orthographic normalization (such as hamza seated on alif, alif maksura, teh marbuta, removal of harakat, tatweel, etc)\n ArabicStemFilter: performs arabic light stemming\n\nBoth filters operate directly on termbuffer for maximum performance. There is no object creation in this Analyzer.\n\nThere are no external dependencies. I've indexed about half a billion words of arabic text and tested against that.\n\nIf there are any issues with this implementation I am willing to fix them. I use lucene on a daily basis and would like to give something back. Thanks.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1406",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "new Arabic Analyzer (Apache license)",
    "systemSpecification": true,
    "version": ""
}