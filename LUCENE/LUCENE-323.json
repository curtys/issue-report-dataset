{
    "comments": [
        {
            "author": "Chuck Williams",
            "body": "Created an attachment (id=13747)\nTest case to demonstrate problem, and fix to fix it.\n\nRun TestQuery with Lucene on classpath.  All test output to System.out.\nMaxDisjunctionQuery is ready for use (alhthough provides no QueryParser\nintegration).  DistributedMultiFieldQueryParser is not complete as it contains\nonly the functionality I've needed.\n",
            "date": "2004-12-14T06:13:24.000+0000",
            "id": 0
        },
        {
            "author": "Daniel Naber",
            "body": "Unfortunately for this to become part of Lucene the dependencies to Java 1.5 \nwill need to be removed I think. Anyway, I finally understand the problem now. \nThe reason that nobody complained so far might be that people use mostly AND \nqueries today (and for cases where AND queries don't make sense, e.g. synonym \nexpansion, the current scoring implementation is okay I think). ",
            "date": "2004-12-14T07:31:00.000+0000",
            "id": 1
        },
        {
            "author": "Chuck Williams",
            "body": "Oops, forgot I use Java 1.5 and forgot I had improved this class by taking\nadvatnage of it.  I don't believe the original version of MaxDisjunctionQuery\nthat I sent in email sometime ago had this issue, but it seems moot nonetheless\nas Paul Elschot's DisjunctionQuery is going into Lucene and can be used to get\nthe same effect.  This can be done easily if it is factored such that\nDisjunctionQuery provides a general mechanism for subclasses to initialize\nstate, update state, and produce the final score as it combines the subscorers,\nwhile DisjunctionSumQuery overrides this to implement an optimized version that\ndoesn't require the method calls.\n\nSo my recommendation, would be to factor DisjunctionQuery to make overriding for\ndifferent combining operators easy and to include a version of\nDisjunctionMaxQuery that uses this to implement the MaxDisjunctionQuery scoring\nsemantics (i.e., combining operator = max plus constant times sum of other\nterms).  I'd be happy to write that version of DisjunctionMaxQuery once\nDisjunctionQuery comes out in a released version of Lucene if it's not done for\nthe release.\n",
            "date": "2004-12-14T07:51:24.000+0000",
            "id": 2
        },
        {
            "author": "Chuck Williams",
            "body": "(From update of attachment 13747)\nAbout to upload a new version that fixes a bug.\n",
            "date": "2004-12-20T14:57:41.000+0000",
            "id": 3
        },
        {
            "author": "Chuck Williams",
            "body": "Created an attachment (id=13791)\nTestRanking, with MaxDisjunctionQuery and DistributingMultiFieldQueryParser\n\nNew version of attachment that fixes a bug in MaxDisjunctionScorer.skipTo()\n(didn't want to leave a buggy version posted here).  MaxDisjunctionQuery\nrequires Java 1.5, although it would be easy to elimiante the dependencies.\n",
            "date": "2004-12-20T15:02:55.000+0000",
            "id": 4
        },
        {
            "author": "Miles Barr",
            "body": "Created an attachment (id=13883)\nPrevious attachment ported to Java 1.4\n\nI've modified MaxDisjunctionQuery and MaxDisjunctionScorer so they compile\nagainst Java 1.4.",
            "date": "2005-01-04T20:22:42.000+0000",
            "id": 5
        },
        {
            "author": "Chuck Williams",
            "body": "Created an attachment (id=14131)\nInitial Similarity for use with Wikipedia benchmark relevance test\n\nThis is the untuned Similarity to initially try on the Wikipedia relevanc\nbenchmark.  It assumes there are two fields called \"title\" and \"body\" (actually\nown \"body\" is referenced).  It is designed for use with\nDistributingMultiFieldQueryParser using these initial settings:\n\n  private static final String[] DEFAULT_FIELDS = {\"title\", \"body\"};\n  private static final float[] DEFAULT_BOOSTS = {3.0f, 1.0f};\n\nDEFAULT_BOOSTS may need tuning as well.\n",
            "date": "2005-01-29T06:14:17.000+0000",
            "id": 6
        },
        {
            "author": "Chuck Williams",
            "body": "Created an attachment (id=14132)\nRevised WikipediaSimilarity for Java 1.4\n\nWikipediaSimilarity revised to not use Math.log10 since it isn't in Java 1.4\nplatform.  Also generalized to make the log base a tunable parameter.\n",
            "date": "2005-01-29T08:04:54.000+0000",
            "id": 7
        },
        {
            "author": "Chuck Williams",
            "body": "Created an attachment (id=14136)\nWikipediaSimilarity for Java 1.4 refactored for interactive parameter tuning\n\nThis is the same WikipediaSimilarity as the previous, but parameterized so that\nthe tf & idf logarithm bases can be interactively tuned if required.\n",
            "date": "2005-01-30T10:44:34.000+0000",
            "id": 8
        },
        {
            "author": "Hoss Man",
            "body": "2cents:\n\nEven if the DistributingMultiFieldQueryParser isn't deemd \"core worthy\" (the comments suggest it should be treated more as an example then a complete parser) I definitely think MaxDisjunctionQuery.java should be added to the core.  I've been playing with it for a few weeks now in progromaticaly constructed queries, and I love it -- I can't remember how i lived with out it.\n",
            "date": "2005-09-27T03:56:28.000+0000",
            "id": 9
        },
        {
            "author": "Chuck Williams",
            "body": "Thanks Hoss, that's nice to hear.  I've been out of the community for a while doing other things, but am about to start a large Lucene-based project that I hope will lead to some interesting contributions along the way.\n",
            "date": "2005-09-27T10:53:33.000+0000",
            "id": 10
        },
        {
            "author": "Hoss Man",
            "body": "In the interest of encouraging commitment, i've written a UnitTest to demonstrate/prove the expected behavior of a MaxDisjunctionQuery, with and without a tiebreaker, and in combination with a BooleanQuery wrapper.",
            "date": "2005-10-05T08:10:21.000+0000",
            "id": 11
        },
        {
            "author": "Hoss Man",
            "body": "Be advised: in 1.9, Query.createWeight is declared to throw IOException (it didn't used to in 1.4) so in order for MaxDisjunctionQuery.java to compile against 1.9, the MaxDisjunctionWeight constructor and MaxDisjunctionQuery.createWeight must be declared to throw IOException as well.",
            "date": "2005-10-05T08:13:40.000+0000",
            "id": 12
        },
        {
            "author": "Yonik Seeley",
            "body": "I'd love to see MaxDisjunctionQuery committed before lucene 1.9 is final.\nI'd vote to commit the current version, as I think Chuck's recommendations would not change the MaxDisjunctionQuery public interface, correct?\n\nI assume that the DisjunctionQuery that Chuck mentions would actually be DisjunctionScorer or DisjunctionSumScorer?\n\nQueryParser support can also be handled later.",
            "date": "2005-11-12T07:41:04.000+0000",
            "id": 13
        },
        {
            "author": "Paul Elschot",
            "body": "There is an issue with the MaxDisjunctionScorer in the .zip attachment, I'm\nsorry I did not see this earlier when I posted on java-dev about this.\n\nThe problem is that MaxDisjunctionScorer uses bubble sort to keep the subscorer\nsorted over the documents in the next() method (line 103), and this does not scale nicely\nwhen the number of subscorers increases.\nSupposing the number of subscores that match the document is N,\nthe amount of work to be done is proportional to (N*N) per document.\nIn DisjunctionSumScorer a priority queue is used, and there the amount of work is\nproportional to (N log(N)) per document.\nSo I would recommend to rewrite MaxDisjunctionScorer to inherit from a new common\nsuper class with DisjunctionSumScorer, sharing everything except the\nadvanceAfterCurrent() method (which could be abstract in the new superclass).\nIt's possible to be more aggressive in refactoring by initializing and adapting\nthe score per index document using different methods, but this would take N\nextra method calls per document.\n\nAt the same time the name could be changed to DisjunctionMaxScorer\nfor consistency in the org.lucene.search package.\n\nRegards,\nPaul Elschot\n",
            "date": "2005-11-15T04:24:06.000+0000",
            "id": 14
        },
        {
            "author": "Yonik Seeley",
            "body": "Changes:\n- renamed MaxDisjunction* to DisjunctionMax*\n- added DisjunctionMaxQuery.getClauses()\n- fixed DisjunctionMaxQuery.hashCode()  & equals()\n- made DisjunctionMaxScorer package protected (for now at least)\n",
            "date": "2005-11-15T04:50:25.000+0000",
            "id": 15
        },
        {
            "author": "Yonik Seeley",
            "body": "I'd rather have something right now  that worked well for a small number clauses, even if it didn't scale to a large number of clauses.  All of my usecases consist of small numbers of clauses.\n\nSince the scorer isn't public, a rewrite can easily be dropped in later when it's done, right?\n\nFor the very common two clause case, will the rewrite you have in mind be as fast as the current version?",
            "date": "2005-11-15T04:59:18.000+0000",
            "id": 16
        },
        {
            "author": "Chuck Williams",
            "body": "The code only uses bubble sort for the incremental resorting of an already-sorted list.  The initial sort is done with Arrays.sort() which is O(n*logn).  The incremental resort is O(k*n) where k is the number of clauses that match the document last generated.  Even if n is large, k will usually be small.  Theoretically this is O(n^2) because k could be as high as n, but this is extremely unlikely especially when n is large.    More likely is that k is bounded by a small constant, in which case the algorithm is O(n).  It's like Quicksort in that regard -- there are outlier cases where it won't perform well, but it will perform better than most alternatives for the vast majority of cases.\n\nResorting the whole list every time would perform worse.  The best algorithm would probably be to use the standard insert and delete operations on a heap (as in heap sort):\n\n    while top element generated last doc\n        heap remove it\n        generate it\n        heap insert it\n\nThis would yield total time O(k*logn), as with a PriorityQueue.\n\nI don't think this is much of an issue to worry about, but the algorithm could be revised to use the heap sort operations if others think it is important.\n\nChuck\n",
            "date": "2005-11-15T07:55:22.000+0000",
            "id": 17
        },
        {
            "author": "Paul Elschot",
            "body": "The ScorerDocQueue.java here has a single operation for something\nvery similar to the heap-remove/generate/heap-insert:\n\nhttp://issues.apache.org/jira/browse/LUCENE-365\n\nThere is also a test class for testing performance of disjunction scorers\nwhich could be used to find out which k is big enough to warrant the use\nof a heap (priority queue).\n\nRegards,\nPaul Elschot\n",
            "date": "2005-11-16T04:12:46.000+0000",
            "id": 18
        },
        {
            "author": "Yonik Seeley",
            "body": "Added Iterable to DisjunctionMaxQuery as a semi Java5 friendly way to iterate over the disjuncts.  Added ability to add all disjuncts from an Iterable (Collection, List, another DisjunctionMaxQuery, etc).\n\nI Committed DisjunctionMaxQuery/Scorer/Test since the Interface should be stable, and the implementation seems to work fine for the common cases.  I'll be happy to evaluate & commit performance updates when they become available.\n\nI'll leave this bug open since it contains multiple issues.\n\n",
            "date": "2005-11-17T03:54:12.000+0000",
            "id": 19
        },
        {
            "author": "Chuck Williams",
            "body": "The attached archive contains a revised DisjunctionMaxScorer that maintains the disjunct scorers as a min heap instead of a sorted list.  This reduces the time per next() to O(k*log(n)) instead of O(k*n) per Paul's earlier comment.  Most of the class changed, so I included both a patch and the new class.  This is only lightly tested; the junit test passes, along with the entire Lucene test suite.  I'm not working on the project anymore that led to the original class and so have not tested it on that.  I'm working on a new project that will use this and so it will get thoroughly tested there, but am not yet to an appropriate point.  I thought it was best to post the patch now as I believe it is correct and the unit test does pass.  Perhaps others would like to try it out.  E.g., it would be interesting to run the performance test that Paul mentions.\n\nAlso, I found and fixed another bug while updating the class.  In the current committed version, there is a problem if skipTo() exhausts all the scorers.  It did not set more to false, and so a subsequent call to next() would attempt to access the non-existent first scorer.\n\nIt would be nice to get some form of DistributingMultiFieldQueryParser in so that this is easy to use.\n\nThanks to Yonik for committing this functionality!\n\nChuck\n",
            "date": "2005-11-30T12:43:06.000+0000",
            "id": 20
        },
        {
            "author": "Yonik Seeley",
            "body": "Thanks for the changes Chuck!\n\nYour patch was backwards, BTW :-)\n\nI haven't had a chance to run any benchmarks, but I committed this because it also fixes a bug.\nSince it also looks like the uses of /2 and *2 were all unsigned, I replaced them with shifts.  The multiply doesn't matter much, but IDIV is horribly slow (between 20 and 80 cycles, depending on the arch and operands).  Not that I thought it was a bottleneck, but I have problems avoiding that \"root of all evil\", premature optimization ;-)\n",
            "date": "2005-12-13T11:20:27.000+0000",
            "id": 21
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Yonik - you committed this?  The case is still showing as open, so you may want to close it if you're done with it.\n",
            "date": "2005-12-21T09:30:19.000+0000",
            "id": 22
        },
        {
            "author": "Yonik Seeley",
            "body": "> Yonik - you committed this? The case is still showing as open, so you may want to close it if you're done with it. \n\nThis bug also contains a different Similarity implementation, as well as a DistributingMultiFieldQueryParser.  I only committed the DisjunctionMaxQuery part of it  and that's why I left it open.",
            "date": "2005-12-21T10:44:40.000+0000",
            "id": 23
        },
        {
            "author": "Chuck Williams",
            "body": "FYI, I've recently noticed the new implementation of MultiFieldQueryParser (new to me since I was out of touch for about 6 months or so).  This now does the distribution, so I'm no longer of the opinion that DistributingMultiFieldQueryParser should be committed.  A better approach would be to generalize MultiFieldQueryParser to be able use MaxDisjunctionQuery as its container instead of BooleanQuery in the appropriate places.  FYI, I'll look at this soon and will submit a revised patch unless there is some reason the details don't sort out.  Even it that case, the same approach used in MultiFieldQueryParser of specializing certain methods of QueryParser could be used for DistributingMultiFieldQueryParser, rather than the post parsing traversal/transformation it currently does.\n",
            "date": "2005-12-21T13:57:28.000+0000",
            "id": 24
        },
        {
            "author": "Hoss Man",
            "body": "The WIkipediaSimilarity seems to only have been included as an example for the purposes of comparison testing, not as an item to be commited.\n\nGiven Chuck's comment on 21/Dec/05 I'm of the opinion this issue should be closed.",
            "date": "2006-02-10T17:40:44.000+0000",
            "id": 25
        },
        {
            "author": "Yonik Seeley",
            "body": "OK, closing this bug.\nWe can open separate bugs for any alternate Similarity, or any query parser enhancements.",
            "date": "2006-03-03T04:30:25.000+0000",
            "id": 26
        }
    ],
    "component": "core/queryparser",
    "description": "The attached test case demonstrates this problem and provides a fix:\n  1.  Use a custom similarity to eliminate all tf and idf effects, just to \nisolate what is being tested.\n  2.  Create two documents doc1 and doc2, each with two fields title and \ndescription.  doc1 has \"elephant\" in title and \"elephant\" in description.  \ndoc2 has \"elephant\" in title and \"albino\" in description.\n  3.  Express query for \"albino elephant\" against both fields.\nProblems:\n      a.  MultiFieldQueryParser won't recognize either document as containing \nboth terms, due to the way it expands the query across fields.\n      b.  Expressing query as \"title:albino description:albino title:elephant \ndescription:elephant\" will score both documents equivalently, since each \nmatches two query terms.\n  4.  Comparison to MaxDisjunctionQuery and my method for expanding queries \nacross fields.  Using notation that () represents a BooleanQuery and ( | ) \nrepresents a MaxDisjunctionQuery, \"albino elephant\" expands to:\n        ( (title:albino | description:albino)\n          (title:elephant | description:elephant) )\nThis will recognize that doc2 has both terms matched while doc1 only has 1 \nterm matched, score doc2 over doc1.\n\nRefinement note:  the actual expansion for \"albino query\" that I use is:\n        ( (title:albino | description:albino)~0.1\n          (title:elephant | description:elephant)~0.1 )\nThis causes the score of each MaxDisjunctionQuery to be the score of highest \nscoring MDQ subclause plus 0.1 times the sum of the scores of the other MDQ \nsubclauses.  Thus, doc1 gets some credit for also having \"elephant\" in the \ndescription but only 1/10 as much as doc2 gets for covering another query term \nin its description.  If doc3 has \"elephant\" in title and both \"albino\" \nand \"elephant\" in the description, then with the actual refined expansion, it \ngets the highest score of all (whereas with pure max, without the 0.1, it \nwould get the same score as doc2).\n\nIn real apps, tf's and idf's also come into play of course, but can affect \nthese either way (i.e., mitigate this fundamental problem or exacerbate it).",
    "hasPatch": false,
    "hasScreenshot": false,
    "id": "LUCENE-323",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "[PATCH] MultiFieldQueryParser and BooleanQuery do not provide adequate support for queries across multiple fields",
    "systemSpecification": true,
    "version": "1.4"
}