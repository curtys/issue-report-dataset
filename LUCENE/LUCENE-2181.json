{
    "comments": [
        {
            "author": "Steve Rowe",
            "body": "Attached .zip'd patch (over 10MB because of the 4 languages' LineDocs) integrated into the Ant build for the ICU contrib, rather than integrated into the Benchmark build.\n\nInvoke using {{ant benchmark}} from the {{contrib/icu/}} directory.\n",
            "date": "2010-01-02T23:37:24.889+0000",
            "id": 0
        },
        {
            "author": "Robert Muir",
            "body": "Hi Steven, one idea we can do here is to put the large files online (zipped) and have ant download them when we run the benchmark... \n\nI can put them on people.apache.org for this purpose, we already do the same with the huge wikipedia file... what do you think?\n",
            "date": "2010-01-03T09:13:42.735+0000",
            "id": 1
        },
        {
            "author": "Steve Rowe",
            "body": "Works for me.\n\nI do have one concern, though: the LineDocSource parser doesn't know how to handle comments, so these four files don't have Apache2 license declarations in them.  We should put a README (or something like it) with these files to indicate the license.\n\nDifferent subject: I'm not sure where it would go, but the code I used to produce these top-TF wikipedia files may be useful to other people - where do you think it could live?  An example, maybe?",
            "date": "2010-01-03T19:13:35.039+0000",
            "id": 2
        },
        {
            "author": "Robert Muir",
            "body": "bq. I do have one concern, though: the LineDocSource parser doesn't know how to handle comments, so these four files don't have Apache2 license declarations in them. We should put a README (or something like it) with these files to indicate the license.\n\nAre they really apache license? or derived from wikipedia content? if these files are only being downloaded when you run 'ant benchmark' for collation, then it is just like the enwiki task in benchmark, it downloads some huge wikipedia data and runs it. So someone please correct me if I am wrong, but I don't think we should be putting apache license headers in these files anyway, its just like the benchmark enwiki task, we are not shipping it with our source distribution.\n\nbq. Different subject: I'm not sure where it would go, but the code I used to produce these top-TF wikipedia files may be useful to other people - where do you think it could live? An example, maybe?\n\nhmm I will have to think about this... anyone got ideas?  I think this would be useful too (I admit to not having yet looked at the implementation), here are two examples:\n* Karl could use this to evaluate his swedish stemming improvements, taking frequency into account.\n* the obvious use of when you need to build a stopword list, these top terms are where you want to start.",
            "date": "2010-01-04T02:43:10.691+0000",
            "id": 3
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nbq. ... these four files don't have Apache2 license declarations in them. We should put a README (or something like it) with these files to indicate the license.\n\nAre they really apache license? or derived from wikipedia content?... I don't think we should be putting apache license headers in these files\n{quote}\n\nHmm, I just assumed that since these files were not (anything even close to) verbatim copies that they were independently licensable new works, but it's definitely more complicated than that...\n\nThis looks like the place to start where licensing is concerned:\n\nhttp://en.wikipedia.org/wiki/Wikipedia_Copyright\n\nMy (way non-expert) reading of this is that Wikipedia-derived works (and I'm pretty sure these frequency lists qualify as such) must be licensed under the [Creative Commons Attribution-Share Alike 3.0 Unported license|http://creativecommons.org/licenses/by-sa/3.0/], which does not appear to me to be entirely compatible with the Apache2 license.\n\nSo I agree with you :) - with the caveat that some form of attribution and a pointer to licensing info should be included with these files.\n",
            "date": "2010-01-04T14:09:17.768+0000",
            "id": 4
        },
        {
            "author": "Steve Rowe",
            "body": "Hi Robert, \n\nIn the new version of the patch, {{ant benchmark}} from the {{contrib/icu/}} directory attempts to download the attached {{tar.bz2}} file from {{http://people.apache.org/~rmuir/wikipedia}} (*please change this to the location where you end up putting the file*), then unpacks the archive to the {{contrib/icu/src/benchmark/work/}} directory, then compiles and runs the benchmark.\n\nIn addition to the top 100K word lists, the {{tar.bz2}} file contains {{LICENSE.txt}}, which contains links to the Wikipedia dumps from which the lists were extracted, along with a link to the license that Wikipedia uses.",
            "date": "2010-01-09T22:14:51.592+0000",
            "id": 5
        },
        {
            "author": "Robert Muir",
            "body": "Hi Steve, thanks a lot for your work here, this is nice.\n\nI put the files in my apache directory, but modified your patch somewhat\n* I moved it to the benchmark package proper\n* I created two tasks: NewLocale and NewCollationAnalyzer\n\nThe NewLocale task sets a Locale in the PerfRunData, in my opinion this is reusable beyond collation since it could be used for testing Localized sorting and range searching as well, it supports all Locale params (lang,country,variant)\n\nThe NewCollationAnalyzer creates a [ICU]CollationKeyAnalyzer with the collator defaults depending upon this Locale, right now i only added options to specify impl (impl:jdk or impl:icu), but in the future we can add strength, decomposition, etc.\n\nTake a look and let me know what you think, surely I made some mistakes but it appears to work fine.\n",
            "date": "2010-01-10T04:39:56.463+0000",
            "id": 6
        },
        {
            "author": "Steve Rowe",
            "body": "Looks good.  I like the way you've integrated it into the benchmark suite, and as you say the NewLocaleTask should prove useful elsewhere.\n\nbq. I put the files in my apache directory, but modified your patch somewhat\n\nOne major thing you changed but didn't mention above is that rather than applying the collation key transform only to the LineDoc body field, it's now applied also to the title and date fields.  Given the nature of the top 100k words files -- the title is an integer representing term frequency, and the date is essentially meaningless (the date on which I created the file) -- I don't think this makes sense (and that's why I made analyzers that only applied collation to the body field).",
            "date": "2010-01-10T17:11:30.466+0000",
            "id": 7
        },
        {
            "author": "Robert Muir",
            "body": "Steve ahh i was wondering about the per-field analyzer wrapper (sorry i neglected to mention this, i just forgot about it)... there are likely other problems too, and the new stuff needs tests.\n\nWhat about this per-field thing, what if in the data files, title and date were simply blank?\n\nOr should we worry, I agree its stupid, does it skew the results though?\nOne way to look at it is that its also fairly realistic (even though its meaningless, you see numbers and dates everywhere).\n\nThe downside to doing per-analyzer wrapper is that it introduces some complexity, in all honesty this is not really specific to this collation task, right? (i.e. the existing analysis/tokenization benchmarks have this same problem)\n",
            "date": "2010-01-10T17:24:06.128+0000",
            "id": 8
        },
        {
            "author": "Robert Muir",
            "body": "Steven, another idea: what if we simply added the options to DocMaker so we could turn off the tokenization of title and date fields?\n\nright now you can only control this stuff for the body fields. imo this would be the best solution.\n\nedit: ok this is already there, just a little strange. we can use doc.tokenized to turn it off for these other fields, and doc.body.tokenized turned on so that we are only analyzing the field we want.\n\ni'll update the alg file and produce a new patch",
            "date": "2010-01-10T17:34:00.921+0000",
            "id": 9
        },
        {
            "author": "Steve Rowe",
            "body": "bq. What about this per-field thing, what if in the data files, title and date were simply blank?\n\nHmm, although the date field value is meaningless, I like the TF-in-title-field thing.\n\n{quote}\nOr should we worry, I agree its stupid, does it skew the results though?\nOne way to look at it is that its also fairly realistic (even though its meaningless, you see numbers and dates everywhere).\n{quote}\n\nI was thinking that it would, and that it's not really a meaningful test of collation - who's going to bother running collation over integers and dates? - but since the comparison here is between two implementations of collation, I think you're right that there is no skew in doing this comparison:\n{panel}\nicu(kiwi) + icu(apple) + icu(orange) : jdk(kiwi) + jdk(apple) + jdk(orange)\n{panel}\ninstead of this one:\n{panel}\nkeyword(kiwi) + keyword(apple) + icu(orange) : keyword(kiwi) + keyword(apple) + jdk(orange)\n{panel}\n(where the icu(X) transform = keyword(X) + icu-collation(X), and similarly for the jdk(X) transform)\n\nbq. The downside to doing per-analyzer wrapper is that it introduces some complexity, in all honesty this is not really specific to this collation task, right? (i.e. the existing analysis/tokenization benchmarks have this same problem)\n\nYup, you're right.  A general facility to do this will end up looking (modulo syntax) like Solr's per-field analysis specification.",
            "date": "2010-01-10T17:54:59.029+0000",
            "id": 10
        },
        {
            "author": "Steve Rowe",
            "body": "bq. Steven, another idea: what if we simply added the options to DocMaker so we could turn off the tokenization of title and date fields?\n\nGood idea!\n\nbq. i'll update the alg file and produce a new patch\n\nExcellent, thanks!",
            "date": "2010-01-10T17:58:17.350+0000",
            "id": 11
        },
        {
            "author": "Robert Muir",
            "body": "Steven I also havent forgotten about your other contribution, the thing that creates the benchmark corpus in the first place from wikipedia.\n\nOne idea I had would be that such a thing wouldn't be too out of place in the open relevance project... (munging corpora etc)",
            "date": "2010-01-10T18:38:34.603+0000",
            "id": 12
        },
        {
            "author": "Robert Muir",
            "body": "ok i think we might be close to something committable now:\n* wrote tests for NewLocaleTask and NewCollationAnalyzerTask\n* set doc.stored=false, doc.tokenized=false, doc.body.tokenized=true in the collation.alg file\n* i moved the two scripts into a 'scripts' directory, i thought this made more sense? \n* I also renamed the bm2jira.pl script to collation.bm2jira.pl\n\nhere is the output from 'ant collation' from the benchmark package:\n\n||Language||java.text||ICU4J||KeywordAnalyzer||ICU4J Improvement||\n|English|10.78s|7.32s|1.58s|60%|\n|French|11.48s|7.52s|1.59s|67%|\n|German|11.19s|7.52s|1.61s|62%|\n|Ukrainian|13.03s|8.68s|1.66s|62%|\n\ni think its more accurate relative to KeywordAnalyzer now that we aren't storing the body text in a stored field and things like that, but of course you can change the .alg file to see if the differences matter in the context of overall indexing by turning these back on.\n",
            "date": "2010-01-10T23:13:47.797+0000",
            "id": 13
        },
        {
            "author": "Robert Muir",
            "body": "fix a bug in testCollator/assertEqualCollation, so its actually testing correct behavior rather than being a no-op :)",
            "date": "2010-01-10T23:22:15.729+0000",
            "id": 14
        },
        {
            "author": "Robert Muir",
            "body": "ok, somehow it completely bypassed my brain you are using ReadTokens task :)\n\nso this is a problem, because ReadTokens does not respect the DocMaker configuration. In my opinion it should not tokenize fields unless they are configured to be tokenized. \n\nSo I added the following in this patch to fix this:\n{noformat}\n     for(final Fieldable field : fields) {\n+      if (!field.isTokenized()) continue;\n+\n{noformat}\n\nnow we get the results we expect:\n||Language||java.text||ICU4J||KeywordAnalyzer||ICU4J Improvement||\n|English|3.43s|2.21s|1.15s|115%|\n|French|3.78s|2.37s|1.17s|117%|\n|German|3.84s|2.42s|1.18s|115%|\n|Ukrainian|5.81s|3.67s|1.24s|88%|\n\nif you comment out the doc.tokenized=false, then you get the other results i just posted instead, as it will analyze the other fields too.\n",
            "date": "2010-01-11T00:20:21.290+0000",
            "id": 15
        },
        {
            "author": "Steve Rowe",
            "body": "Works for me:\n\nJAVA:\njava version \"1.5.0_15\"\nJava(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_15-b04)\nJava HotSpot(TM) 64-Bit Server VM (build 1.5.0_15-b04, mixed mode)\n\nOS:\ncygwin\nWinVistaService Pack 2\nService Pack 26060022202561\n\n||Language||java.text||ICU4J||KeywordAnalyzer||ICU4J Improvement||\n|English|5.53s|2.03s|1.20s|422%|\n|French|6.41s|2.13s|1.19s|455%|\n|German|6.36s|2.19s|1.22s|430%|\n|Ukrainian|8.92s|3.62s|1.21s|220%|\n",
            "date": "2010-01-11T06:11:03.696+0000",
            "id": 16
        },
        {
            "author": "Steve Rowe",
            "body": "I just ran the contrib/benchmark tests, and I got one test failure:\n\n{noformat}\n    [junit] Testcase: testReadTokens(org.apache.lucene.benchmark.byTask.TestPerfTasksLogic):    FAILED\n    [junit] expected:<3108> but was:<3128>\n    [junit] junit.framework.AssertionFailedError: expected:<3108> but was:<3128>\n    [junit]     at org.apache.lucene.benchmark.byTask.TestPerfTasksLogic.testReadTokens(TestPerfTasksLogic.java:480)\n    [junit]     at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:212)\n    [junit] \n    [junit] \n    [junit] Test org.apache.lucene.benchmark.byTask.TestPerfTasksLogic FAILED\n{noformat}\n",
            "date": "2010-01-11T06:39:10.764+0000",
            "id": 17
        },
        {
            "author": "Steve Rowe",
            "body": "I think NewCollationAnalyzerTask should be a little more careful about parsing its parameters - here's a slightly modified version of your setParams() that understands \"impl:jdk\" and complains about unrecognized params:\n\n{code:java}\n@Override\n  public void setParams(String params) {\n    super.setParams(params);\n    \n    StringTokenizer st = new StringTokenizer(params, \",\");\n    while (st.hasMoreTokens()) {\n      String param = st.nextToken();\n      StringTokenizer expr = new StringTokenizer(param, \":\");\n      String key = expr.nextToken();\n      String value = expr.nextToken();\n      // for now we only support the \"impl\" parameter.\n      // TODO: add strength, decomposition, etc\n      if (key.equals(\"impl\")) {\n        if (value.equalsIgnoreCase(\"icu\"))\n          impl = Implementation.ICU;\n        else if (value.equalsIgnoreCase(\"jdk\"))\n          impl = Implementation.JDK;\n        else\n          throw new RuntimeException(\"Unknown parameter \" + param);\n      } else {\n        throw new RuntimeException(\"Unknown parameter \" + param);\n      }\n    }\n  }\n{code}",
            "date": "2010-01-11T06:44:37.789+0000",
            "id": 18
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nSteven I also havent forgotten about your other contribution, the thing that creates the benchmark corpus in the first place from wikipedia.\n\nOne idea I had would be that such a thing wouldn't be too out of place in the open relevance project... (munging corpora etc)\n{quote}\n\nInteresting idea, thanks - I'll take a look at what's there now and see how my stuff would fit in.",
            "date": "2010-01-11T06:46:08.326+0000",
            "id": 19
        },
        {
            "author": "Robert Muir",
            "body": "bq. I just ran the contrib/benchmark tests, and I got one test failure: \n\nOK, I think this is from adjusting the ReadTokensTask...\n\nI looked at the test and i think it .... should be improved.... :)",
            "date": "2010-01-11T12:55:07.239+0000",
            "id": 20
        },
        {
            "author": "Robert Muir",
            "body": "corrected this testReadTokens(), it tests by adding up token freq across the index and comparing it to the number of tokens read, but there is a non-tokenized, but indexed field (DocMaker.ID_FIELD), the keywords from this should not add to the expected count.\n\nadded your addt'l parameter checking for NewCollationAnalyzerTask\n",
            "date": "2010-01-11T13:14:11.723+0000",
            "id": 21
        },
        {
            "author": "Steve Rowe",
            "body": "+1, tests all pass, and \"ant collation\" produced expected output.\n\nOne minor detail, though - shouldn't the output files be renamed to identify their purpose, similarly to how you renamed bm2jira.pl?  Here's the relevant section in {{contrib/benchmark/build.txt}}:\n\n{code:xml}\n<property name=\"collation.output.file\" value=\"${working.dir}/benchmark.output.txt\"/>\n<property name=\"collation.jira.output.file\"  value=\"${working.dir}/bm2jira.output.txt\"/>\n{code}\n",
            "date": "2010-01-11T14:49:17.935+0000",
            "id": 22
        },
        {
            "author": "Robert Muir",
            "body": "Steven thanks, in addition to your comments I also changed the config to download the top100k files to the temp directory, and expand to work/top100k-out, consistent with the other benchmark datasets.\n\nI think this one is ready. If there is no objection I will commit in a day or two.",
            "date": "2010-01-11T18:43:39.526+0000",
            "id": 23
        },
        {
            "author": "Steve Rowe",
            "body": "+1, once again, tests all pass, and \"ant collation\" produced expected output. ",
            "date": "2010-01-12T03:38:02.782+0000",
            "id": 24
        },
        {
            "author": "Robert Muir",
            "body": "Committed revision 898491.\n\nThanks Steven!",
            "date": "2010-01-12T20:08:04.954+0000",
            "id": 25
        }
    ],
    "component": "modules/benchmark",
    "description": "Steven Rowe attached a contrib/benchmark-based benchmark for collation (both jdk and icu) under LUCENE-2084, along with some instructions to run it... \n\nI think it would be a nice if we could turn this into a committable patch and add it to benchmark.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2181",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "benchmark for collation",
    "systemSpecification": true,
    "version": ""
}