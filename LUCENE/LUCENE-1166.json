{
    "comments": [
        {
            "author": "Thomas Peuss",
            "body": "A preliminary version of the token filter.",
            "date": "2008-02-06T11:09:00.321+0000",
            "id": 0
        },
        {
            "author": "Thomas Peuss",
            "body": "A hyphenation grammar. You can download them from: http://downloads.sourceforge.net/offo/offo-hyphenation.zip?modtime=1168687306&big_mirror=0",
            "date": "2008-02-06T11:10:44.012+0000",
            "id": 1
        },
        {
            "author": "Thomas Peuss",
            "body": "The DTD describing the hyphenation grammar XML files.",
            "date": "2008-02-06T11:11:12.449+0000",
            "id": 2
        },
        {
            "author": "Steve Rowe",
            "body": "Hi Thomas,\n\nLooking at [http://offo.sourceforge.net/hyphenation/licenses.html], which seems to be the same information as in the off-hyphenation.zip file you attached to this issue, the license issue may be a problem - the hyphenation data is covered by different licenses on a per-language basis.  For example, there are two German data files, and both are licensed under a LaTeX license, as is the Danish file, and these two languages are the most likely targets for your TokenFilter.  IANAL, but unless Apache licenses can be secured for this data, I don't think the files can be incorporated directly into an Apache project.\n\nAlso, I don't see Swedish among the hyphenation data licenses - is it covered in some other way?",
            "date": "2008-02-06T16:39:21.371+0000",
            "id": 3
        },
        {
            "author": "Thomas Peuss",
            "body": "bq. Looking at http://offo.sourceforge.net/hyphenation/licenses.html, which seems to be the same information as in the off-hyphenation.zip file you attached to this issue, the license issue may be a problem - the hyphenation data is covered by different licenses on a per-language basis. For example, there are two German data files, and both are licensed under a LaTeX license, as is the Danish file, and these two languages are the most likely targets for your TokenFilter. IANAL, but unless Apache licenses can be secured for this data, I don't think the files can be incorporated directly into an Apache project.\n\nThis is true. And that's why I uploaded the two files without the ASF license grant. The FOP project does not have the files in the code base as well because of the licensing problem.\n\nbq. Also, I don't see Swedish among the hyphenation data licenses - is it covered in some other way?\nOFFO has no Swedish grammar file. We can generate a Swedish grammar file out of the LaTeX grammar files. I have a look into this tonight.\n\nAll other hyphenation implementations I have found so far use them either directly or in an converted variant like the FOP code. What we can do of course is to ask the authors of the LaTeX files if they want to license their work under the ASF license as well. It is worth a try. But I suppose that many email addresses in the LaTeX files are not used anymore. I try to contact the authors of the German grammar files tomorrow.\n\nBTW: an example for those that don't want to try the patch:\n+Input token stream:+\nRindfleisch\u00fcberwachungsgesetz Drahtschere abba\n\n+Output token stream:+\n(Rindfleisch\u00fcberwachungsgesetz,0,29)\n(Rind,0,4,posIncr=0)\n(fleisch,4,11,posIncr=0)\n(\u00fcberwachung,11,22,posIncr=0)\n(gesetz,23,29,posIncr=0)\n(Drahtschere,30,41)\n(Draht,30,35,posIncr=0)\n(schere,35,41,posIncr=0)\n(abba,42,46)",
            "date": "2008-02-06T17:33:46.886+0000",
            "id": 4
        },
        {
            "author": "Thomas Peuss",
            "body": "A Swedish hyphenation grammar is available at http://www.peuss.de/node/64",
            "date": "2008-02-07T12:22:51.394+0000",
            "id": 5
        },
        {
            "author": "Thomas Peuss",
            "body": "Changes:\n* added unittest\n* minor tweaks for getting the encoding of the XML files right",
            "date": "2008-02-12T11:09:19.819+0000",
            "id": 6
        },
        {
            "author": "Thomas Peuss",
            "body": "Updated version:\n* new dumb decomposition filter\n** uses a brute-force approach by generating substrings and checking them against the dictionary\n** seems to work better for languages that have no patterns file with a lot of special cases\n** Is roughly 3 times slower than the decomposition filter using hyphenation patterns\n** No licensing problems because of the hyphenation pattern files\n* Refactoring to have all methods used by both decomposition filters in one place\n* Minor performance improvements",
            "date": "2008-02-14T16:22:12.856+0000",
            "id": 7
        },
        {
            "author": "Otis Gospodnetic",
            "body": "I haven't looked at the patch.\nBut I'm wondering if a similar approach could be used for, say, word segmentation in Chinese?\nThat is, iterate through a string of Chinese characters, buffering them and looking up the buffered string in a Chinese dictionary.  Once there is a dictionary match, and the addition of the following character results in a string that has no entry in the dictionary, that previous buffered string can be considered a word/token.\n\nI'm not sure if your patch does something like this, but if it does, I am wondering if it is general enough that what you did can be used (as the basis of) word segmentation for Chinese, and thus for a Chinese Analyzer that's not just a dump n-gram Analyzer (which is what we have today).\n",
            "date": "2008-02-17T05:48:12.966+0000",
            "id": 8
        },
        {
            "author": "Thomas Peuss",
            "body": "bq. But I'm wondering if a similar approach could be used for, say, word segmentation in Chinese? That is, iterate through a string of Chinese characters, buffering them and looking up the buffered string in a Chinese dictionary. Once there is a dictionary match, and the addition of the following character results in a string that has no entry in the dictionary, that previous buffered string can be considered a word/token. I'm not sure if your patch does something like this, but if it does, I am wondering if it is general enough that what you did can be used (as the basis of) word segmentation for Chinese, and thus for a Chinese Analyzer that's not just a dump n-gram Analyzer (which is what we have today).\n\nCurrently the code adds a token to the stream when an n-gram from the current token in the token stream matches a word in the dictionary (I am only speaking about the DumbCompoundWordTokenFilter because I doubt that there exist hyphenation patterns for Chinese languages). I don't know much about the structure of Chinese characters to answer this questions in detail. You can have a look at the test-case in the patch to see how the filters work.\n\n\n",
            "date": "2008-02-17T15:24:40.327+0000",
            "id": 9
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Thomas, I think that might work for Chinese - going through the \"string\" of Chinese characters, one at a time, and looking up a dictionary after each additional character.  One you find a dictionary match, you look at one more character.  If that matches a dictionary entry, keep doing that until you keep matching dictionary entries (in order to grab the longest dictionary-matching string of characters).  If the next character does not match, then the previous/last character was the end of the dictionary entry.\nThat would work, no?\n\nAs for the license info, I think you could take the approach where the required libraries are not included in the contribution in the ASF repo, but are downloaded on the fly, at build time, much like some other contributions.  Could you do that?\n",
            "date": "2008-02-28T23:11:14.680+0000",
            "id": 10
        },
        {
            "author": "Thomas Peuss",
            "body": "bq. Thomas, I think that might work for Chinese - going through the \"string\" of Chinese characters, one at a time, and looking up a dictionary after each additional character. One you find a dictionary match, you look at one more character. If that matches a dictionary entry, keep doing that until you keep matching dictionary entries (in order to grab the longest dictionary-matching string of characters). If the next character does not match, then the previous/last character was the end of the dictionary entry. That would work, no?\n\nI have started to look into this. I will add the constructor parameter \"onlyLongestMatch\" (default is false).\n\nbq. As for the license info, I think you could take the approach where the required libraries are not included in the contribution in the ASF repo, but are downloaded on the fly, at build time, much like some other contributions. Could you do that?\n\nI pull the grammar files for the tests already. But I don't know if it makes sense to pull them on build time because the end-user can easily download them. I need the XML versions now - so the jar-file from Sourceforge does not help anymore (I have included the needed classes from the FOP project - they use the ASF license as well).",
            "date": "2008-03-03T10:33:40.650+0000",
            "id": 11
        },
        {
            "author": "Thomas Peuss",
            "body": "Updated patch according to Otis suggestions for longest match.\n\nNext step: move to contrib",
            "date": "2008-03-03T16:35:35.403+0000",
            "id": 12
        },
        {
            "author": "Thomas Peuss",
            "body": "Moved the compound word tokenfilter stuff to contrib.",
            "date": "2008-03-25T12:43:39.766+0000",
            "id": 13
        },
        {
            "author": "Thomas Peuss",
            "body": "Moved compound word token filter to contrib.",
            "date": "2008-03-25T12:49:16.249+0000",
            "id": 14
        },
        {
            "author": "Thomas Peuss",
            "body": "Dropped Java5 dependencies.",
            "date": "2008-03-25T12:56:27.532+0000",
            "id": 15
        },
        {
            "author": "Thomas Peuss",
            "body": "Fixed a compilation bug in the testcase.",
            "date": "2008-03-29T11:04:16.212+0000",
            "id": 16
        },
        {
            "author": "Grant Ingersoll",
            "body": "{quote}\nI pull the grammar files for the tests already. But I don't know if it makes sense to pull them on build time because the end-user can easily download them. I need the XML versions now - so the jar-file from Sourceforge does not help anymore (I have included the needed classes from the FOP project - they use the ASF license as well).\n{quote}\n\nI think they have to download automatically, otherwise the automated tests, etc. will not run.  I applied the patch and ran \"ant test\" and it fails b/c I didn't download the files.\n\nAlso, much of the code has author tags that are not you, I am assuming you got it from FOP per your comments above, but can you explicitly mark all the files as to there origin?",
            "date": "2008-04-24T01:14:00.982+0000",
            "id": 17
        },
        {
            "author": "Thomas Peuss",
            "body": "All files in the package _org.apache.lucene.analysis.compound.hyphenation_ are from the FOP project (as well ASF licensed). Should I add a comment to them to state from where they are? All other files are from me. I have to check why it fails when you run \"ant test\" by downloading a fresh copy of Lucene-trunk.",
            "date": "2008-04-24T06:13:16.061+0000",
            "id": 18
        },
        {
            "author": "Thomas Peuss",
            "body": "The error is\n{code}\n    [junit] Testsuite: org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter\n    [junit] Tests run: 4, Failures: 0, Errors: 2, Time elapsed: 2,139 sec\n    [junit]\n    [junit] Testcase: testHyphenationCompoundWordsDE(org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter):  Caused an ERROR\n    [junit] File not found: /home/thomas/projects/lucene-trunk-compound/hyphenation.dtd (No such file or directory)\n    [junit] org.apache.lucene.analysis.compound.hyphenation.HyphenationException: File not found: /home/thomas/projects/lucene-trunk-compound/hyphenation.dtd (No such file or directory)\n    [junit]     at org.apache.lucene.analysis.compound.hyphenation.PatternParser.parse(PatternParser.java:123)\n    [junit]     at org.apache.lucene.analysis.compound.hyphenation.HyphenationTree.loadPatterns(HyphenationTree.java:138)\n    [junit]     at org.apache.lucene.analysis.compound.HyphenationCompoundWordTokenFilter.getHyphenationTree(HyphenationCompoundWordTokenFilter.java:142)\n    [junit]     at org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter.testHyphenationCompoundWordsDE(TestCompoundWordTokenFilter.java:70)\n    [junit]\n    [junit]\n    [junit] Testcase: testHyphenationCompoundWordsDELongestMatch(org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter):      Caused an ERROR\n    [junit] File not found: /home/thomas/projects/lucene-trunk-compound/hyphenation.dtd (No such file or directory)\n    [junit] org.apache.lucene.analysis.compound.hyphenation.HyphenationException: File not found: /home/thomas/projects/lucene-trunk-compound/hyphenation.dtd (No such file or directory)\n    [junit]     at org.apache.lucene.analysis.compound.hyphenation.PatternParser.parse(PatternParser.java:123)\n    [junit]     at org.apache.lucene.analysis.compound.hyphenation.HyphenationTree.loadPatterns(HyphenationTree.java:138)\n    [junit]     at org.apache.lucene.analysis.compound.HyphenationCompoundWordTokenFilter.getHyphenationTree(HyphenationCompoundWordTokenFilter.java:142)\n    [junit]     at org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter.testHyphenationCompoundWordsDELongestMatch(TestCompoundWordTokenFilter.java:96)\n    [junit]\n    [junit]\n    [junit] Test org.apache.lucene.analysis.compound.TestCompoundWordTokenFilter FAILED\n{code}\n\nSo it does not find the hyphenation.dtd. I have to investigate how I can make that DTD know to the parser without copying the hyphenation.dtd to Lucene's base directory.",
            "date": "2008-04-24T06:51:40.545+0000",
            "id": 19
        },
        {
            "author": "Thomas Peuss",
            "body": "* Fixed the problem with the hyphenation.dtd file that was not found\n* Removed all @author tags\n* Added a note to all files I copied from the FOP project\n* Added package.html files (not very much in there - but credits for the FOP project)",
            "date": "2008-04-24T10:11:00.324+0000",
            "id": 20
        },
        {
            "author": "Grant Ingersoll",
            "body": "So, why would I ever want to use a \"Dumb\" compound filter?  Any suggestions for a better name?  No need for a patch, I can just make the change.",
            "date": "2008-04-30T01:12:00.567+0000",
            "id": 21
        },
        {
            "author": "Grant Ingersoll",
            "body": "This looks pretty good Thomas.  I think the last bit that would be good is to add to the package docs an example of start to finish using it, kind of like in the test case.  You might want to explain a little bit about where to get the hyphenation files, etc. (if I am understanding them correctly). \n\nI think if we can finish that up, we can look to commit.\n\nThe other interesting thing here, as an aside, is the Ternary Tree might be worth pulling up to a \"util\" package (no need to do so now, just thinking out loud), as it could be used for other interesting things.  For instance, see http://www.javaworld.com/javaworld/jw-02-2001/jw-0216-ternary.html   The version we have needs a little work, but I have been thinking about how it might be used to improve spelling, etc.",
            "date": "2008-04-30T01:20:44.153+0000",
            "id": 22
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Ah, TST was in there, lovely!  +1 to what Grant said about getting it into util later.\nNoticed a misspelling in javadoc while glancing at TST: hibrid -> hybrid\n",
            "date": "2008-04-30T03:16:53.686+0000",
            "id": 23
        },
        {
            "author": "Grant Ingersoll",
            "body": "\n\n\ncool.  It needs some work, IMO to add more features, per that article  \nI sent, but no biggie.\n\n\nI saw some of those, but purposely left in the FOP typos... There were  \nmore than just that one.\n\n\n--------------------------\nGrant Ingersoll\n\nLucene Helpful Hints:\nhttp://wiki.apache.org/lucene-java/BasicsOfPerformance\nhttp://wiki.apache.org/lucene-java/LuceneFAQ\n\n\n\n\n\n\n",
            "date": "2008-04-30T03:28:57.133+0000",
            "id": 24
        },
        {
            "author": "Thomas Peuss",
            "body": "bq. So, why would I ever want to use a \"Dumb\" compound filter? Any suggestions for a better name? No need for a patch, I can just make the change.\n\nA better name would be _DictionaryCompoundWordTokenFilter_. I called it \"Dumb\" because it uses a brute-force approach. But _DictionaryCompoundWordTokenFilter_ characterizes it better.",
            "date": "2008-04-30T06:49:40.374+0000",
            "id": 25
        },
        {
            "author": "Thomas Peuss",
            "body": "* Renamed _DumbCompoundWordTokenFilter_ to _DictionaryCompoundWordTokenFilter_\n* Added more text to the package description file (package.html)\n* Removed some code that was necessary because of LUCENE-1163 (in HyphenationCompoundWordTokenFilter and DictionaryCompoundWordTokenFilter\n)",
            "date": "2008-04-30T09:17:56.033+0000",
            "id": 26
        },
        {
            "author": "Thomas Peuss",
            "body": "* Minor bugfix in DictionaryCompoundWordFilter: it was not using the _maxSubwordSize_ parameter\n* Major performance improvement for the DictionaryCompoundWordTokenFilter: we now convert all dictionary strings to lower case before adding them to the CharArraySet and set the _ignoreCase_ parameter of CharArraySet to false. The filter makes a lower case copy of the token before it starts working on it. This avoids many _toLowerCase()_ calls in CharArraySet.\n* Minor performance improvement for the HyphenationCompoundWordTokenFilter: see above",
            "date": "2008-04-30T14:26:05.474+0000",
            "id": 27
        },
        {
            "author": "Fran\u00e7ois Terrier",
            "body": "Is there any plan of integrating this patch in the official lucene libraries in the short term ? ",
            "date": "2008-05-07T09:48:16.188+0000",
            "id": 28
        },
        {
            "author": "Grant Ingersoll",
            "body": "Yes.\n\n\n\n--------------------------\nGrant Ingersoll\n\nLucene Helpful Hints:\nhttp://wiki.apache.org/lucene-java/BasicsOfPerformance\nhttp://wiki.apache.org/lucene-java/LuceneFAQ\n\n\n\n\n\n\n",
            "date": "2008-05-07T10:42:57.170+0000",
            "id": 29
        },
        {
            "author": "Grant Ingersoll",
            "body": "I'm now getting:\n ..../lucene/java/lucene-clean/contrib/analyzers/src/test/org/apache/lucene/analysis/compound/TestCompoundWordTokenFilter.java:60: warning: unmappable character for encoding utf-8\n    [javac]         \"Aufgabe\", \"\u00dcberwachung\" };\n\n\nCan you convert the classes in question to UTF-8 for the source?",
            "date": "2008-05-14T10:43:50.963+0000",
            "id": 30
        },
        {
            "author": "Thomas Peuss",
            "body": "UTF-8 problem fixed...",
            "date": "2008-05-16T11:32:39.858+0000",
            "id": 31
        },
        {
            "author": "Grant Ingersoll",
            "body": "Committed revision 657027.",
            "date": "2008-05-16T12:28:41.610+0000",
            "id": 32
        }
    ],
    "component": "modules/analysis",
    "description": "A tokenfilter to decompose compound words you find in many germanic languages (like German, Swedish, ...) into single tokens.\n\nAn example: Donaudampfschiff would be decomposed to Donau, dampf, schiff so that you can find the word even when you only enter \"Schiff\".\n\nI use the hyphenation code from the Apache XML project FOP (http://xmlgraphics.apache.org/fop/) to do the first step of decomposition. Currently I use the FOP jars directly. I only use a handful of classes from the FOP project.\n\nMy question now:\nWould it be OK to copy this classes over to the Lucene project (renaming the packages of course) or should I stick with the dependency to the FOP jars? The FOP code uses the ASF V2 license as well.\n\nWhat do you think?",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1166",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "A tokenfilter to decompose compound words",
    "systemSpecification": true,
    "version": ""
}