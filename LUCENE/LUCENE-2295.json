{
    "comments": [
        {
            "author": "Shai Erera",
            "body": "This will open the door for more extensible field-length limit - today MFL dictates the field length for all fields, while this Analyzer (TokenFilter actually) someone could wrap the TokenStream only for specific fields, and/or have fields with different limits. Just a thought.",
            "date": "2010-03-05T11:40:15.306+0000",
            "id": 0
        },
        {
            "author": "Uwe Schindler",
            "body": "The TokenFilter is quite easy, only few lines of code:\n- no attributes to be registered\n- use a counter which is 0\n- override incrementToken() to update counter on true, return false when counter reaches limit or input exhausted\n- reset() resets counter\n- no other methods need to be overridden (this emulates the original behaviour of MaxFieldLength)\n\nThe Analyzer is more complicated as it should respect reusable streams. It should work like QueryAutoStopWordAnalyzer and maintain a Map of field names to chached streams. To detect if reusableTokenStream has reused a stream compare with cache. If new stream wrap.",
            "date": "2010-03-05T14:10:47.657+0000",
            "id": 1
        },
        {
            "author": "Uwe Schindler",
            "body": "-In the indexer, the backwards code is hairy: If a max limit is set, the indexer just plugs our Filter around the TokenStream. This would limit nr. of tokens for all cases:-\n\n# -An ANALYZED field (wrapping the TokenStream got from Analyzer)-\n# -A Field with ctor taking TokenStream: wrap the Field's TokenStream-\n\n-We need some cache here, too :( -- The analyzer cannot be used because of (2).-\n\nFor backwards we leave the code as it is and just remove the limiting code in 4.0. We just add coments, what needs to be removed.",
            "date": "2010-03-05T14:17:06.084+0000",
            "id": 2
        },
        {
            "author": "Uwe Schindler",
            "body": "After some discussion with rmuir, we realized, that an explicit reuse of the filter does not make sense. The maintenance of the Map<String,ReuseableStream> is more resource and maintenance than simply creating a class instance without any initialization cost.\nThe simple implementation of the Analyzer would be:\n\n- override reusableTokenStream that delegates to the inner analyzer and wrap it with the filter. The cost of creating the filter is neglectible, as the filter has no initialization cost (it uses no attributes, does not create attribute maps,...)\n- override tokenStream that does the same, but instead delegates to inner analyzers tokenStream method. \n- Make this analyzer final, else we need VirtualMethod (also the TokenFilter, of course)\n- Override the rest of the methods in Analyzer and simply delegate. Don't forget the posIncr Gap methods and so on!\n\nI will supply a patch with filter and analyzer later.",
            "date": "2010-03-05T18:02:55.059+0000",
            "id": 3
        },
        {
            "author": "Uwe Schindler",
            "body": "Here is a first patch.\n\nRobert & me found a bug in CharTokenizer that it not correctly sets the endOffset when the underlying reader is not exhausted. This is fixed in the patch, too. The bug in CharTokenizer was also there when sombody used the MaxFieldLength with IW -- possible highlighting problems :-(\n\nNevertheless, CharTokenizer should be rewritten, the code is not easy understandable. Too many states and branches and possibly uninitialized variables (i fixed by two asserts).\n\nDeprecating IW.MaxFieldLength is not yet added, this is just the new Filter/Analyzer.",
            "date": "2010-03-06T14:01:05.347+0000",
            "id": 4
        },
        {
            "author": "Robert Muir",
            "body": "Add this to the list of potential future checks in BaseTokenStreamTestcase.\n\nPerhaps we can use this new limiting analyzerwrapper in assertAnalyzesToReuse\nto help determine if tokenstreams implement reset() correctly, versus just assuming\nthe entire stream will be consumed completely.\n",
            "date": "2010-03-06T14:14:51.514+0000",
            "id": 5
        },
        {
            "author": "Uwe Schindler",
            "body": "Further investigantions showed, that there is some difference between using this filter/analyzer and the current setting in IndexWriter. IndexWriter uses the given MaxFieldLength as maximum value for all instances of the same field name. So if you add 100 fields \"foo\" (with each 1,000 terms) and have the default of 10,000 tokens, DocInverter will index 10 of these field instances (10,000 terms in total) and the rest will be supressed.\n\nIf you use the Filter, the limit is per TokenStream, so the above example will index all field instances and produce 100,000 terms.\n\nBut the current IndexWriter code has a bug, too: The check for too many terms is done after the first token of each input stream is indexed, so in the abovce example, IW will index 10,089 terms, because once the limit is reached, each stream left will index one term. This could be fixed (if really needed, as the MaxFieldLength in IW should be deprecated) by moving the check up and dont even try to index the field and create the TokenStream.\n\nI just wanted to add this difference here for further discussing.",
            "date": "2010-03-07T09:49:51.111+0000",
            "id": 6
        },
        {
            "author": "Uwe Schindler",
            "body": "Updated patch for trunk.",
            "date": "2010-05-27T12:51:01.381+0000",
            "id": 7
        },
        {
            "author": "Uwe Schindler",
            "body": "Committed revision: 949445 (trunk), 949446 (3.x)\n",
            "date": "2010-05-29T23:20:11.321+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Further investigantions showed, that there is some difference between using this filter/analyzer and the current setting in IndexWriter. IndexWriter uses the given MaxFieldLength as maximum value for all instances of the same field name. So if you add 100 fields \"foo\" (with each 1,000 terms) and have the default of 10,000 tokens, DocInverter will index 10 of these field instances (10,000 terms in total) and the rest will be supressed.\n\nIn LUCENE-2450 I'm experimenting with having multi-valued fields be handled entirely by an analyzer stage, ie, the logical concatenation of tokens (with gaps) would \"hidden\" to IW, and IW would think its dealing with a single token stream.  In this model, if you then appended the new LimitTokenCountFilter to the end, I think it'd result in the same behavior as maxFieldLength today.\n\nBut, even before we eventually switch to that model... can't we still deprecate (on 3x) IW's maxFieldLength (remove from trunk) now?  I realize the limiting is different (applying the limit pre vs post concatenation), but I think the javadocs can explain this difference?  I think it's unlikely apps are relying on this specific interaction of truncation and multi-valued fields...",
            "date": "2010-05-30T10:06:14.394+0000",
            "id": 9
        },
        {
            "author": "Robert Muir",
            "body": "I'm reopening this for discussion (both because it came up on the mailing list and Mike's question was never answered).\n\nI think the functionality added here is more flexible than the IW setting and we should consider deprecating/removing.\n",
            "date": "2010-08-26T23:10:03.102+0000",
            "id": 10
        },
        {
            "author": "Uwe Schindler",
            "body": "+1, I missed Mike's comment after resolving this issue!",
            "date": "2010-08-27T07:44:43.908+0000",
            "id": 11
        },
        {
            "author": "Shai Erera",
            "body": "I think the changes to 3x are less complicated than they seem - we don't need to deprecate anything, more than we already did. IndexWriterConfig is introduced in 3.1 and all IW ctors are already deprecated. So we can just remove the get/setMaxFieldLength from IWC and be done with it + some jdocs.\n\nIs that the intention behind the reopening of the issue?",
            "date": "2011-01-17T08:58:22.023+0000",
            "id": 12
        },
        {
            "author": "Robert Muir",
            "body": "Hi Shai, that sounds like the right solution to me!\n",
            "date": "2011-01-17T11:10:59.876+0000",
            "id": 13
        },
        {
            "author": "Shai Erera",
            "body": "Patch against 3x. Removed the get/set from IWC and changed code which used it. I also added some clarifying notes to the deprecation note in IW.setMaxFieldLength.\n\nI will post a separate patch for trunk where this setting will be removed altogether.",
            "date": "2011-01-18T09:52:54.371+0000",
            "id": 14
        },
        {
            "author": "Shai Erera",
            "body": "Patch against trunk - removes maxFieldLength handling from all the code.",
            "date": "2011-01-18T11:15:13.666+0000",
            "id": 15
        },
        {
            "author": "Robert Muir",
            "body": "Shai, the patch looks good to me. \n\nI would also say that with your trunk patch we can resolve SOLR-2086,\nbecause then the maxFieldLength is totally implemented in the analyzer,\nso tools like analysis.jsp will automatically work with it.\n",
            "date": "2011-01-18T11:21:21.055+0000",
            "id": 16
        },
        {
            "author": "Shai Erera",
            "body": "Committed revision 1060340 (trunk).\nCommitted revision 1060342 (3x).",
            "date": "2011-01-18T12:03:35.720+0000",
            "id": 17
        },
        {
            "author": "Grant Ingersoll",
            "body": "Bulk close for 3.1",
            "date": "2011-03-30T15:50:12.183+0000",
            "id": 18
        }
    ],
    "component": "modules/analysis",
    "description": "A spinoff from LUCENE-2294. Instead of asking the user to specify on IndexWriter his requested MFL limit, we can get rid of this setting entirely by providing an Analyzer which will wrap any other Analyzer and its TokenStream with a TokenFilter that keeps track of the number of tokens produced and stop when the limit has reached.\n\nThis will remove any count tracking in IW's indexing, which is done even if I specified UNLIMITED for MFL.\n\nLet's try to do it for 3.1.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2295",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Create a MaxFieldLengthAnalyzer to wrap any other Analyzer and provide the same functionality as MaxFieldLength provided on IndexWriter",
    "systemSpecification": true,
    "version": ""
}