{
    "comments": [
        {
            "author": "Karl Wettin",
            "body": "Regarding data and user queries, I have a 150 000 document corpus with 4 000 000 queries that I might be able to convince the owners to release. It is great data, but a bit politically incorrect (torrents). \n\nThere is some simple Wikipedia harvesting in LUCENE-826, and I'm in the middle of rewriting it to a more general Wikipedia library for text mining purposes. Perhaps you have some ideas you want to put in there? I plan something like this:\n\npublic class WikipediaCorpus {  \n  Map<String, String> wikipediaDomainPrefixByLanguageISO\n  Map<URL, WikipediaArticle> harvestedArticle\n\n  public WikipediaArticle getArticle(String languageISO, String title) {\n    ..\n  }\n}\n\npublic class WikipediaArticle {\n  WikipediaArticle(URL url) {\n    ..\n  }\n \n  String languageISO;\n  String title;\n  String[] contentParagraphs\n\n  Date[] modified; \n\n  Map<String, String> articleInOtherLanguagesByLanguageISO\n\n}\n\n",
            "date": "2007-03-20T10:57:15.149+0000",
            "id": 0
        },
        {
            "author": "Doron Cohen",
            "body": "lucene-836.benchmark.quality.patch adds a new package \"quality\" under o.a.l.benchmark. \n\nThis is also followup to some of http://www.mail-archive.com/java-dev@lucene.apache.org/msg10851.html\n\nPatch is based on trunk folder. \nFastest way to test it: \"ant test\" from contrib/benchmark dir.\nTo see more output in this run, try \"ant test -Dtests.verbose=true\".\n\nThis is early code, not ready to commit - wanted to show it sooner for feedback, especially the API. \n\nFor a quick view of the API see benchmark.quality at http://people.apache.org/~doronc/api (note that not much javadocs yet - I would wait with that for API closure.)\n\nCode in this patch is:\n  - extendable.\n  - can run a quality benchmark.\n  - report quality results, comparing to given judgements (optional).\n  - create a submission log (optional).\n  - format of submission log can be modified, by extending a logger class.\n  - format of inputs - queries, judgments - can be modified, by extending \n    default readers, or by providing pre-read ones.\n\nThere is a general \"Judge\" interface - answering if a given doc name is valid for a given \"QualityQuery\". And one implementation of it, based on Trec's QRels. The alternative of TRels, for instance, would mean another implementation of the \"Judge\" interface. (I would love a better name for it, btw...)\n\nA new TestQualityRun tests this package on the Reuters collection - so that test source is a good place to start, to see how to run a quality test.",
            "date": "2007-06-29T21:36:59.548+0000",
            "id": 1
        },
        {
            "author": "Doron Cohen",
            "body": "Updated patch is cleaner and almost ready to commit: interfaces are more clean now, and most javadocs is in place. Package javadocs still missing. ",
            "date": "2007-07-24T01:20:09.151+0000",
            "id": 2
        },
        {
            "author": "Doron Cohen",
            "body": "A ready to commit patch for search quality benchmarking. \n\nJavadocs can be reviewed in http://people.apache.org/~doronc/api/ - see the benchmark.quality package for a code sample to run the quality benchmark with your input index, queries, judgments, etc.\n\nI would like to commit this in a day or two, to make it easier to proceed with LUCENE-965 and the other search quality ideas - comments (especially on the API) are most welcome...\n",
            "date": "2007-07-27T05:20:17.658+0000",
            "id": 3
        },
        {
            "author": "Grant Ingersoll",
            "body": "+1\n\nApplies clean and I like the API, but I think you should have a Jury object too...  \n\nI can't actually run it w/o TREC but the tests pass.  I think I might have TREC Arabic lying around somewhere, maybe I will give a run w/ that some day, but don't wait on me to apply this.",
            "date": "2007-07-27T10:58:51.963+0000",
            "id": 4
        },
        {
            "author": "Doron Cohen",
            "body": "Thanks for the review Grant!\n\nNote that you can see the output by the default Logger and default SubmissionReport by running the TestQualityRun Junit with -Dtests.verbose=true. The submission report and the log both go to stdio so they will be intermixed, but you'll at least get to see what gets printed.\n",
            "date": "2007-07-27T19:49:57.282+0000",
            "id": 5
        },
        {
            "author": "Doron Cohen",
            "body": "Committed. \n",
            "date": "2007-07-28T23:29:17.338+0000",
            "id": 6
        }
    ],
    "component": "core/other",
    "description": "Would be great if the benchmark contrib had a way of providing precision/recall benchmark information ala TREC.  I don't know what the copyright issues are for the TREC queries/data (I think the queries are available, but not sure about the data), so not sure if the is even feasible, but I could imagine we could at least incorporate support for it for those who have access to the data.  It has been a long time since I have participated in TREC, so perhaps someone more familiar w/ the latest can fill in the blanks here.\n\nAnother option is to ask for volunteers to create queries and make judgments for the Reuters data, but that is a bit more complex and probably not necessary.  Even so, an Apache licensed set of benchmarks may be useful for the community as a whole.  Hmmm.... \n\nWikipedia might be another option instead of Reuters to setup as a download for benchmarking, as it is quite large and I believe the licensing terms are quite amenable.  Having a larger collection would be good for stressing Lucene more and would give many users a demonstration of how Lucene handles large collections.\n\nAt any rate, this kind of information could be useful for people looking at different indexing schemes, formats, payloads and different query strategies.\n\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-836",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Benchmarks Enhancements (precision/recall, TREC, Wikipedia)",
    "systemSpecification": true,
    "version": ""
}