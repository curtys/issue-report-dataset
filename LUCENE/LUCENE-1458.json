{
    "comments": [
        {
            "author": "Mark Miller",
            "body": "Hmmm...I think something is missing -  FormatPostingsPositionsReader?",
            "date": "2008-11-18T14:40:35.112+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "Woops, sorry... I was missing a bunch of files.  Try this one?",
            "date": "2008-11-18T15:41:06.616+0000",
            "id": 1
        },
        {
            "author": "Marvin Humphrey",
            "body": "The work on streamlining the term dictionary is excellent, but perhaps we can do better still.  Can we design a format that allows us rely upon the operating system's virtual memory and avoid caching in process memory altogether?  \n\nSay that we break up the index file into fixed-width blocks of 1024 bytes.  Most blocks would start with a complete term/pointer pairing, though at the top of each block, we'd need a status byte indicating whether the block contains a continuation from the previous block in order to handle cases where term length exceeds the block size.  \n\nFor Lucy/KinoSearch our plan would be to mmap() on the file, but accessing it as a stream would work, too.  Seeking around the index term dictionary would involve seeking the stream to multiples of the block size and performing binary search, rather than performing binary search on an array of cached terms.  There would be increased processor overhead; my guess is that since the second stage of a term dictionary seek -- scanning through the primary term dictionary -- involves comparatively more processor power than this, the increased costs would be acceptable.\n\nAdvantages:\n\n* Multiple forks can all share the same system buffer, reducing per-process memory footprint.\n* The cost to read in the index term dictionary during IndexReader startup drops to zero.\n* The OS caches for the index term dictionaries can either be allowed to warm naturally, or can be nudged into virtual memory via e.g. \"cat /path/to/index/*.tis > /dev/null\".",
            "date": "2008-11-18T19:52:41.116+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Can we design a format that allows us rely upon the operating system's virtual memory and avoid caching in process memory altogether?\n\nInteresting!  I've been wondering what you're up to over on KS, Marvin :)\n\nI'm not sure it'll be a win in practice: I'm not sure I'd trust the\nOS's IO cache to \"make the right decisions\" about what to cache.  Plus\nduring that binary search the IO system is loading whole pages into\nthe IO cache, even though you'll only peak at the first few bytes of\neach.\n\nWe could also explore something in-between, eg it'd be nice to\ngenericize MultiLevelSkipListWriter so that it could index arbitrary\nfiles, then we could use that to index the terms dict.  You could\nchoose to spend dedicated process RAM on the higher levels of the skip\ntree, and then tentatively trust IO cache for the lower levels.\n\nI'd like to eventually make the TermsDict index pluggable so one could\nswap in different indexers like this (it's not now).\n",
            "date": "2008-11-18T20:25:49.709+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "\n[Attached patch]\n\nTo test whether the new pluggable codec approach is flexible enough, I\ncoded up \"pulsing\" (described in detail in\nhttp://citeseer.ist.psu.edu/cutting90optimizations.html), where\nfreq/prox info is inlined into the terms dict if the term freq is < N.\n\nIt was wonderfully simple :) I just had to create a reader & a writer,\nand then switch the places that read (SegmentReader) and write\n(SegmentMerger, FreqProxTermsWriter) to use the new pulsing codec\ninstead of the default one.\n\nThe pulsing codec can \"wrap\" any other codec, ie, when a term is\nwritten, if the term's freq is < N, then it's inlined into the terms\ndict with the pulsing writer, else it's fed to the other codec for it\nto do whatever it normally would.  The two codecs are strongly\ndecoupled, so we can mix & match pulsing with other codecs like pfor.\n\nAll tests pass with this pulsing codec.\n\nAs a quick test I indexed first 1M docs from Wikipedia, with N=2 (ie\nterms that occur only in one document are inlined into the terms\ndict).  5.4M terms get inlined (only 1 doc) and 2.2M terms are not (>\n1 doc).  The final size of the index (after optimizing) was a bit\nsmaller with pulsing (1120 MB vs 1131 MB).\n",
            "date": "2008-11-18T22:10:13.722+0000",
            "id": 4
        },
        {
            "author": "Michael Busch",
            "body": "I'll look into this patch soon.\n\nJust wanted to say: I'm really excited about the progress here, this is cool stuff!\nGreat job...",
            "date": "2008-11-18T22:18:47.525+0000",
            "id": 5
        },
        {
            "author": "Marvin Humphrey",
            "body": "> I'm not sure I'd trust the OS's IO cache to \"make the right decisions\" about what to cache.\n\nIn KS and Lucy, at least, we're focused on optimizing for the use case of dedicated search clusters where each box has enough RAM to fit the entire index/shard -- in which case we won't have to worry about the OS swapping out those pages.\n\nI suspect that in many circumstances the term dictionary would be a hot file even if RAM were running short, but I don't think it's important to worry about maxing out performance on such systems -- if the term dictionary isn't hot the posting list files are definitely not hot and search-time responsiveness is already compromised.\n\nIn other words...\n\n* I trust the OS to do a decent enough job on underpowered systems.\n* High-powered systems should strive to avoid swapping entirely. To aid in that endeavor, we minimize per-process RAM consumption by maximizing our use of mmap and treating the system IO cache backing buffers as interprocess shared memory.\n\nMore on designing with modern virtual memory in mind at <http://varnish.projects.linpro.no/wiki/ArchitectNotes>.\n\n> Plus during that binary search the IO system is loading whole pages into\n> the IO cache, even though you'll only peak at the first few bytes of each.\n\nI'd originally been thinking of mapping only the term dictionary index files. Those are pretty small, and the file itself occupies fewer bytes than the decompressed array of term/pointer pairs. Even better if you have several search app forks and they're all sharing the same memory mapped system IO buffer.\n\nBut hey, we can simplify even further! How about dispensing with the index file? We can just divide the main dictionary file into blocks and binary search on that.\n\nKilling off the term dictionary index yields a nice improvement in code and file specification simplicity, and there's no performance penalty for our primary optimization target use case.\n\n> We could also explore something in-between, eg it'd be nice to\n> genericize MultiLevelSkipListWriter so that it could index arbitrary\n> files, then we could use that to index the terms dict. You could\n> choose to spend dedicated process RAM on the higher levels of the skip\n> tree, and then tentatively trust IO cache for the lower levels.\n\nThat doesn't meet the design goals of bringing the cost of opening/warming an IndexReader down to near-zero and sharing backing buffers among multiple forks. It's also very complicated, which of course bothers me more than it bothers you. ;) So I imagine we'll choose different paths.\n\n> I'd like to eventually make the TermsDict index pluggable so one could\n> swap in different indexers like this (it's not now).\n\nIf we treat the term dictionary as a black box, it has to accept a term and return... a blob, I guess.  Whatever calls the lookup needs to know how to handle that blob.",
            "date": "2008-11-19T00:19:11.192+0000",
            "id": 6
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nWe could also explore something in-between, eg it'd be nice to\ngenericize MultiLevelSkipListWriter so that it could index arbitrary\nfiles, then we could use that to index the terms dict.\n{quote}\n\nHmm, +1 for generalizing the MultiLevelSkipListWriter/Reader so that we can re-use it for different (custom) posting-list formats easily.\nHowever, I'm not so sure if it's the right approach for a dictionary. A skip list is optimized for skipping forward (as the name says), so excellent for positing lists, which are always read from \"left to right\". \nHowever, in the term dictionary you do a binary search for the lookup term. So something like a B+Tree would probably work better. Then you can decide similar to the MultiLevelSkipListWriter how many of the upper levels you want to keep in memory and control memory consumption.",
            "date": "2008-11-19T00:37:09.965+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "bq. So something like a B+Tree would probably work better.\n\nI agree, btree is a better fit, though we don't need insertion & deletion operations since each segment is write once.",
            "date": "2008-11-19T10:28:39.023+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nIn KS and Lucy, at least, we're focused on optimizing for the use case of dedicated search clusters where each box has enough RAM to fit the entire index/shard - in which case we won't have to worry about the OS swapping out those pages.\n\nI suspect that in many circumstances the term dictionary would be a hot file even if RAM were running short, but I don't think it's important to worry about maxing out performance on such systems - if the term dictionary isn't hot the posting list files are definitely not hot and search-time responsiveness is already compromised.\n\nIn other words...\n\n    * I trust the OS to do a decent enough job on underpowered systems.\n    * High-powered systems should strive to avoid swapping entirely. To aid in that endeavor, we minimize per-process RAM consumption by maximizing our use of mmap and treating the system IO cache backing buffers as interprocess shared memory.\n{quote}\n\nThese are the two extremes, but, I think most common are all the apps\nin between.  Take a large Jira instance, where the app itself is also\nconsuming alot of RAM, doing alot of its own IO, etc., where perhaps\nsearching is done infrequently enough relative to other operations\nthat the OS may no longer think the pages you hit for the terms index\nare hot enough to keep around.\n\nbq. More on designing with modern virtual memory in mind at <http://varnish.projects.linpro.no/wiki/ArchitectNotes>.\n\nThis is a good read, but I find it overly trusting of VM.\n\nHow can the VM system possibly make good decisions about what to swap\nout?  It can't know if a page is being used for terms dict index,\nterms dict, norms, stored fields, postings.  LRU is not a good policy,\nbecause some pages (terms index) are far far more costly to miss than\nothers.\n\nFrom Java we have even more ridiculous problems: sometimes the OS\nswaps out garbage... and then massive swapping takes place when GC\nruns, swapping back in the garbage only to then throw it away.  Ugh!\n\nI think we need to aim for *consistency*: a given search should not\nsuddenly take 10 seconds because the OS decided to swap out a few\ncritical structures (like the term index).  Unfortunately we can't\nreally achieve that today, especially from Java.\n\nI've seen my desktop OS (Mac OS X 10.5.5, based on FreeBSD) make\nstupid VM decisions: if I run something that does a single-pass\nthrough many GB of on-disk data (eg re-encoding a video), it then\nswaps out the vast majority of my apps even though I have 6 GB RAM.  I\nhit tons (many seconds) of swapping just switching back to my mail\nclient.  It's infuriating.  I've seen Linux do the same thing, but at\nleast Linux let's you tune this behavior (\"swappiness\"); I had to\ndisable swapping entirely on my desktop.\n\nSimilarly, when a BG merge is burning through data, or say backup\nkicks off and moves many GB, or the simple act of iterating through a\nbig postings list, the OS will gleefully evict my terms index or norms\nin order to populate its IO cache with data it will need again for a\nvery long time.\n\nI bet the VM system fails to show graceful degradation: if I don't\nhave enough RAM to hold my entire index, then walking through postings\nlists will evict my terms index and norms, making all searches slower.\n\nIn the ideal world, an IndexReader would be told how much RAM to use.\nIt would spend that RAM wisely, eg first on the terms index, second on\nnorms, third maybe on select column-stride fields, etc.  It would pin\nthese pages so the OS couldn't swap them out (can't do this from\njava... though as a workaround we could use a silly thread).  Or, if\nthe OS found itself tight on RAM, it would ask the app to free things\nup instead of blindly picking pages to swap out, which does not happen\ntoday.\n\nFrom Java we could try using WeakReference but I fear the\ncommunication from the OS -> JRE is too weak.  IE I'd want my\nWeakReference cleared only when the OS is threatening to swap out my\ndata structure.\n\n{quote}\n> Plus during that binary search the IO system is loading whole pages into\n> the IO cache, even though you'll only peak at the first few bytes of each.\n\nI'd originally been thinking of mapping only the term dictionary index files. Those are pretty small, and the file itself occupies fewer bytes than the decompressed array of term/pointer pairs. Even better if you have several search app forks and they're all sharing the same memory mapped system IO buffer.\n\nBut hey, we can simplify even further! How about dispensing with the index file? We can just divide the main dictionary file into blocks and binary search on that.\n{quote}\n\nI'm not convinced this'll be a win in practice.  You are now paying an\neven higher overhead cost for each \"check\" of your binary search,\nespecially with something like pulsing which inlines more stuff into\nthe terms dict.  I agree it's simpler, but I think that's trumped by\nthe performance hit.\n\nIn Lucene java, the concurrency model we are aiming for is a single\nJVM sharing a single instance of IndexReader.  I do agree, if fork()\nis the basis of your concurrency model then sharing pages becomes\ncritical.  However, modern OSs implement copy-on-write sharing of VM\npages after a fork, so that's another good path to sharing?\n\nbq. Killing off the term dictionary index yields a nice improvement in code and file specification simplicity, and there's no performance penalty for our primary optimization target use case.\n\nHave you tried any actual tests swapping these approaches in as your\nterms index impl?  Tests of fully hot and fully cold ends of the\nspectrum would be interesting, but also tests where a big segment\nmerge or a backup is running in the background...\n\nbq. That doesn't meet the design goals of bringing the cost of opening/warming an IndexReader down to near-zero and sharing backing buffers among multiple forks.\n\nThat's a nice goal.  Our biggest cost in Lucene is warming the\nFieldCache, used for sorting, function queries, etc.  Column-stride\nfields should go a ways towards improving this.\n\nbq. It's also very complicated, which of course bothers me more than it bothers you. So I imagine we'll choose different paths.\n\nI think if we make the pluggable API simple, and capture the\ncomplexity inside each impl, such that it can be well tested in\nisolation, it's acceptable.\n\nbq. If we treat the term dictionary as a black box, it has to accept a term and return... a blob, I guess. Whatever calls the lookup needs to know how to handle that blob. \n\nIn my approach here, the blob is opaque to the terms dict reader: it\nsimply seeks to the right spot in the tis file, and then asks the\ncodec to decode the entry.  TermsDictReader is entirely unaware of\nwhat/how is stored there.\n",
            "date": "2008-11-19T13:12:41.489+0000",
            "id": 9
        },
        {
            "author": "Marvin Humphrey",
            "body": "> Take a large Jira instance, where the app itself is also\n> consuming alot of RAM, doing alot of its own IO, etc., where perhaps\n> searching is done infrequently enough relative to other operations\n> that the OS may no longer think the pages you hit for the terms index\n> are hot enough to keep around.\n\nSearch responsiveness is already compromised in such a situation, because we\ncan all but guarantee that the posting list files have already been evicted\nfrom cache.  If the box has enough RAM for the large JIRA instance including\nthe Lucene index, search responsiveness won't be a problem.  As soon as you\nstart running a little short on RAM, though, there's no way to stop infrequent\nsearches from being sluggish.  \n\nNevertheless, the terms index isn't that big in comparison to, say, the size\nof a posting list for a common term, so the cost of re-heating it isn't\nastronomical in the grand scheme of things.\n\n> Similarly, when a BG merge is burning through data, or say backup kicks off\n> and moves many GB, or the simple act of iterating through a big postings\n> list, the OS will gleefully evict my terms index or norms in order to\n> populate its IO cache with data it will need again for a very long time.\n\nWhen that background merge finishes, the new files will be hot.  So, if we\nopen a new IndexReader right away and that IndexReader uses mmap() to get at\nthe file data, new segments be responsive right away.  \n\nEven better, any IO caches for old segments used by the previous IndexReader\nmay still be warm.  All of this without having to decompress a bunch of stream\ndata into per-process data structures at IndexReader startup.\n\nThe terms index could indeed get evicted some of the time on busy systems, but\nthe point is that the system IO cache usually works in our favor, even under\nload.\n\nAs far as backup daemons blowing up everybody's cache, that's stupid,\npathological behavior: <http://kerneltrap.org/node/3000#comment-8573>.  Such\napps ought to be calling madvise(ptr, len, MADV_SEQUENTIAL) so that the kernel\nknows it can recycle the cache pages as soon as they're cleared.\n\n>> But hey, we can simplify even further! How about dispensing with the index\n>> file? We can just divide the main dictionary file into blocks and binary\n>> search on that.\n> \n> I'm not convinced this'll be a win in practice. You are now paying an\n> even higher overhead cost for each \"check\" of your binary search,\n> especially with something like pulsing which inlines more stuff into\n> the terms dict. I agree it's simpler, but I think that's trumped by\n> the performance hit.\n\nI'm persuaded that we shouldn't do away with the terms index.  Even if we're\noperating on a dedicated search box with gobs of RAM, loading entire cache\npages when we only care about the first few bytes of each is poor use of\nmemory bandwidth.  And, just in case the cache does get blown, we'd like to\nkeep the cost of rewarming down.\n\nNathan Kurz and I brainstormed this subject in a phone call this morning, and\nwe came up with a three-file lexicon index design:\n\n  * A file which is a solid stack of 64-bit file pointers into the lexicon\n    index term data.  Term data UTF-8 byte length can be determined by\n    subtracting the current pointer from the next one (or the file length at\n    the end).\n  * A file which is contains solid UTF-8 term content.  (No string lengths, no\n    file pointers, just character data.)\n  * A file which is a solid stack of 64-bit file pointers into the primary\n    lexicon.\n\nSince the integers are already expanded and the raw UTF-8 data can be compared\nas-is, those files can be memory-mapped and used as-is for binary search.\n\n> In Lucene java, the concurrency model we are aiming for is a single JVM\n> sharing a single instance of IndexReader. \n\nWhen I mentioned this to Nate, he remarked that we're using the OS kernel like\nyou're using the JVM.  \n\nWe don't keep a single IndexReader around, but we do keep the bulk of its data\ncached so that we can just slap a cheap wrapper around it.\n\n> I do agree, if fork() is the basis of your concurrency model then sharing\n> pages becomes critical.  However, modern OSs implement copy-on-write sharing\n> of VM pages after a fork, so that's another good path to sharing?\n\nLucy/KS can't enforce that, and we wouldn't want to.  It's very convenient to\nbe able to launch a cheap search process.\n\n> Have you tried any actual tests swapping these approaches in as your\n> terms index impl? \n\nNo -- changing something like this requires a lot of coding, so it's better to\ndo thought experiments first to winnow down the options.\n\n> Tests of fully hot and fully cold ends of the\n> spectrum would be interesting, but also tests where a big segment\n> merge or a backup is running in the background...\n\n>> That doesn't meet the design goals of bringing the cost of opening/warming\n>> an IndexReader down to near-zero and sharing backing buffers among\n>> multiple forks.\n> \n> That's a nice goal. Our biggest cost in Lucene is warming the FieldCache, used\n> for sorting, function queries, etc.\n\nExactly. It would be nice to add a plug-in indexing component that writes sort\ncaches to files that can be memory mapped at IndexReader startup.  There would\nbe multiple files: both a solid array of 32-bit integers mapping document\nnumber to sort order, and the field cache values.  Such a component would\nallow us to move the time it takes to read in a sort cache from\nIndexReader-startup-time to index-time.\n\nHmm, maybe we can conflate this with a column-stride field writer and require\nthat sort fields have a fixed width?\n\n> In my approach here, the blob is opaque to the terms dict reader: it\n> simply seeks to the right spot in the tis file, and then asks the\n> codec to decode the entry. TermsDictReader is entirely unaware of\n> what/how is stored there.\n\nSounds good.  Basically, a hash lookup.\n\nIn KS, the relevant IndexReader methods no longer take a Term object.  (In\nfact, there IS no Term object any more -- KinoSearch::Index::Term has been\nremoved.)  Instead, they take a string field and a generic \"Obj\".  \n\n    Lexicon*\n    SegReader_lexicon(SegReader *self, const CharBuf *field, Obj *term)\n    {\n        return (Lexicon*)LexReader_Lexicon(self->lex_reader, field, term);\n    }\n\nI suppose we genericize this by adding a TermsDictReader/LexReader argument to\nthe IndexReader constructor?  That way, someone can supply a custom subclass\nthat knows how to decode custom dictionary files.\n",
            "date": "2008-11-21T01:48:57.357+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "\nOK I created another codec, SepCodec (for lack of a better name) that\nstores doc & frq & skip in 3 separate files (vs 1 for Lucene today),\nas well as positions & payloads in 2 separate files (vs 1 for Lucene\ntoday).\n\nThe code is still messy -- lots of nocommits all over the place.  I'm\nstill iterating.\n\nFinally, this gets us one step closer to using PFOR!  With this codec,\nthe .frq, .doc and .prx are now \"pure\" streams of ints.\n\nThis codec was more interesting because it adds new files to the file\nformat, which required fixing the various interesting places where we\nassume which file extensions belong to a segment.\n\nIn this patch I also created a PostingCodec class, with the 3\nsubclasses (so far):\n \n  * DefaultCodec: new terms dict format, but same back-compatible\n    prx/frq format\n\n  * PulsingCodec: new terms dict format, but inlines rare terms into\n    terms dict\n\n  * SepCodec: new terms dict format, and splits doc/frq/skip into\n    3 separate files, and prox/payload into 2 separate files\n\nBy editing the PostingCodec.getCodec method you can switch all tests\nto use each codec; all tests pass using each codec.\n\nI built the 1M Wikipedia index, using SepCodec.  Here's the ls -l:\n\n{code}\n-rw-rw-rw-  1 mike  admin    4000004 Nov 20 17:16 _0.fdt\n-rw-rw-rw-  1 mike  admin    8000004 Nov 20 17:16 _0.fdx\n-rw-rw-rw-  1 mike  admin  303526787 Nov 20 17:34 _n.doc\n-rw-rw-rw-  1 mike  admin         33 Nov 20 17:30 _n.fnm\n-rw-rw-rw-  1 mike  admin  220470670 Nov 20 17:34 _n.frq\n-rw-rw-rw-  1 mike  admin    3000004 Nov 20 17:34 _n.nrm\n-rw-rw-rw-  1 mike  admin  651670377 Nov 20 17:34 _n.prx\n-rw-rw-rw-  1 mike  admin          0 Nov 20 17:30 _n.pyl\n-rw-rw-rw-  1 mike  admin   84963104 Nov 20 17:34 _n.skp\n-rw-rw-rw-  1 mike  admin     666999 Nov 20 17:34 _n.tii\n-rw-rw-rw-  1 mike  admin   87551274 Nov 20 17:34 _n.tis\n-rw-rw-rw-  1 mike  admin         20 Nov 20 17:34 segments.gen\n-rw-rw-rw-  1 mike  admin         64 Nov 20 17:34 segments_2\n{code}\n\nSome initial observations for SepCodec:\n\n  * Merging/optimizing was noticeably slower... I think there's some\n    pending inefficiency in my changes, but it could also simply be\n    that having to step through 3 (.frq, .doc, .prx) files instead of\n    2 (.frq, .prx) for each segment is that much more costly.  (With\n    payloads it'd be 4 files instead of 2).\n\n  * Net index size is quite a bit larger (1300 MB vs 1139 MB), I think\n    because we are not efficiently encoding the frq=1 case anymore.\n    PFOR should fix that.\n\n  * Skip data is just about as large as the terms dict, which\n    surprises me (I had intuitively expected it to be smaller I\n    guess).\n",
            "date": "2008-11-21T11:40:56.957+0000",
            "id": 11
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nNevertheless, the terms index isn't that big in comparison to, say, the size\nof a posting list for a common term, so the cost of re-heating it isn't\nastronomical in the grand scheme of things.\n{quote}\n\nBe careful: it's the seeking that kills you (until we switch to SSDs\nat which point perhaps most of this discussion is moot!).  Even though\nthe terms index net size is low, if re-heating the spots you touch\nincurs 20 separate page misses, you lose.\n\nPotentially worse than the terms index are norms, if the search hits\nalot of docs.\n\n{quote}\n> Take a large Jira instance...\n\nSearch responsiveness is already compromised in such a situation, because we\ncan all but guarantee that the posting list files have already been evicted\nfrom cache. If the box has enough RAM for the large JIRA instance including\nthe Lucene index, search responsiveness won't be a problem. As soon as you\nstart running a little short on RAM, though, there's no way to stop infrequent\nsearches from being sluggish.\n{quote}\n\nIf the term index and norms are pinned (or happen to still be hot), I\nwould expect most searches to be OK with this \"in the middle\" use case\nbecause the number of seeks you'll hit should be well contained\n(assuming your posting list isn't unduly fragmented by the\nfilesystem).  Burning through the posting list is a linear scan.\nQueries that simply hit too many docs will always be slow anyways.\n\nI think at both extremes (way too litle RAM and tons of RAM) both\napproaches (pinned in RAM vs mmap'd) should perfom the same.  It's the\ncases in between where I think letting VM decide whether critical\nthings (terms index, norms) get to stay hot is dangerous.\n\n{quote}\nThe terms index could indeed get evicted some of the time on busy systems, but\nthe point is that the system IO cache usually works in our favor, even under\nload.\n{quote}\n\nI think you're just more trusting of the IO/VM system.  I think LRU is\na poor metric.\n\n{quote}\nAs far as backup daemons blowing up everybody's cache, that's stupid,\npathological behavior: <http://kerneltrap.org/node/3000#comment-8573>. Such\napps ought to be calling madvise(ptr, len, MADV_SEQUENTIAL) so that the kernel\nknows it can recycle the cache pages as soon as they're cleared.\n{quote}\n\nExcellent!  If only more people knew about this.  And, if only we\ncould do this from javaland.  EG SegmentMerger should do this for all\nsegment data it's reading & writing.\n\n{quote}\nNathan Kurz and I brainstormed this subject in a phone call this morning, and\nwe came up with a three-file lexicon index design:\n{quote}\n\nI don't fully understand this approach.  Would the index file pointers\npoint into the full lexicon's packed utf8 file, or a separate \"only\nterms in the index\" packed utf8 file?\n\nWe currently materialize individual Strings when we load our index,\nwhich is bad because of the GC cost, added RAM overhead (& swapping)\nand because for iso8859-1 only terms we are using 2X the space over\nutf8.  So I'd love to eventually do something similar (in RAM) for\nLucene.\n\n{quote}\n> Have you tried any actual tests swapping these approaches in as your\n> terms index impl?\n\nNo - changing something like this requires a lot of coding, so it's better to\ndo thought experiments first to winnow down the options.\n{quote}\n\nAgreed.  But once you've got the mmap-based solution up and running\nit'd be nice to meaure net time doing terms lookup / norms reading,\nfor a variety of search use cases, and plot that on a histogram.\n\n{quote}\nWhen I mentioned this to Nate, he remarked that we're using the OS kernel like\nyou're using the JVM.\n{quote}\n\nTrue!\n\n{quote}\nLucy/KS can't enforce that, and we wouldn't want to. It's very convenient to\nbe able to launch a cheap search process.\n{quote}\n\nIt seems like the ability to very quickly launch brand new searchers\nis/has become a strong design goal of Lucy/KS.  What's the driver\nhere?  Is it for near-realtime search?  (Which I think may be better\nachieved by having IndexWriter export a reader, rather than using IO\nsystem as the intermediary).\n\nIf we fix terms index to bulk load arrays (it's not now) then the cost\nof loading norms & terms index on instantiating a reader should be\nfairly well contained, though not as near zero as Lucy/KS will be.\n\n{quote}\n> That's a nice goal. Our biggest cost in Lucene is warming the\n> FieldCache, used for sorting, function queries, etc.\n\nExactly. It would be nice to add a plug-in indexing component that\nwrites sort caches to files that can be memory mapped at IndexReader\nstartup. There would be multiple files: both a solid array of 32-bit\nintegers mapping document number to sort order, and the field cache\nvalues. Such a component would allow us to move the time it takes to\nread in a sort cache from IndexReader-startup-time to index-time.\n{quote}\n\nExcept I would have IndexReader use its RAM budget to pick & choose\nwhich of these will be hot, and which would be mmap'd.\n\n{quote}\nHmm, maybe we can conflate this with a column-stride field writer\nand require that sort fields have a fixed width?\n{quote}\n\nYes I think column-stride fields writer should write the docID -> ord\npart of StringIndex to disk, and MultiRangeQuery in LUCENE-1461 would\nthen use it.  With enumerated type of fields (far fewer unique terms\nthan docs), bit packing will make them compact.\n\n{quote}\nIn KS, the relevant IndexReader methods no longer take a Term\nobject. (In fact, there IS no Term object any more -\nKinoSearch::Index::Term has been removed.) Instead, they take a\nstring field and a generic \"Obj\".\n{quote}\n\nBut you must at least require these Obj's to know how to compareTo one\nanother?  Does this mean using per-field custom sort ordering\n(collator) is straightforward for KS?\n\n{quote}\nI suppose we genericize this by adding a TermsDictReader/LexReader\nargument to the IndexReader constructor? That way, someone can\nsupply a custom subclass that knows how to decode custom dictionary\nfiles.\n{quote}\n\nRight; that's what let me create the PulsingCodec here.\n\nThe biggest problem with the \"load important stuff into RAM\" approach,\nof course, is we can't actually pin VM pages from java, which means\nthe OS will happily swap out my RAM anyway, at which point of course\nwe should have used mmap.  Though apparently at least Windows has an\noption to \"optimize for services\" (= \"don't swap out my RAM\" I think)\nvs \"optimize for applications\", and Linux lets you tune swappiness.\nBut both are global.\n",
            "date": "2008-11-22T19:11:37.803+0000",
            "id": 12
        },
        {
            "author": "Marvin Humphrey",
            "body": ">     Nathan Kurz and I brainstormed this subject in a phone call this morning, and\n>     we came up with a three-file lexicon index design:\n> \n> I don't fully understand this approach. Would the index file pointers\n> point into the full lexicon's packed utf8 file, or a separate \"only\n> terms in the index\" packed utf8 file?\n\nJust the index terms (i.e. every 128th term).  We're trying to fake up an\narray of strings without having to load anything into process memory.  The\ncomparison would go something like this:\n\n{code}\n  /* self->text_lengths, self->char_data, and self->lex_file_ptrs are all\n   * memory mapped buffers.\n   */\n  while (hi >= lo) {\n    const i32_t mid        = lo + ((hi - lo) / 2);\n    const i64_t offset     = self->text_lengths[mid];\n    const i64_t mid_len    = self->text_lengths[mid + 1] - offset;\n    char *const mid_text   = self->char_data + offset;\n    const i32_t comparison = StrHelp_string_diff(target_text, target_len, \n                                                 mid_text, mid_len);\n    if      (comparison < 0) { hi = mid - 1; }\n    else if (comparison > 0) { lo = mid + 1; }\n    else { \n      result = mid; \n      break;\n    }\n  }\n  offset_into_main_lexicon = self->lex_file_ptrs[result]\n  ...\n{code}\n\nHowever, perhaps some sort of a B-tree with string prefix compression would be\nbetter, as per recent suggestions.\n\n\n",
            "date": "2008-11-24T20:41:24.228+0000",
            "id": 13
        },
        {
            "author": "Marvin Humphrey",
            "body": ">> Hmm, maybe we can conflate this with a column-stride field writer\n>> and require that sort fields have a fixed width?\n> \n> Yes I think column-stride fields writer should write the docID -> ord\n> part of StringIndex to disk, and MultiRangeQuery in LUCENE-1461 would\n> then use it. With enumerated type of fields (far fewer unique terms\n> than docs), bit packing will make them compact.\n\nHow do you plan on dealing with the ord values changing as segments get \nadded?  The addition of a single document triggers the rewriting of the\nentire mapping.\n\nI was planning on having SortCacheWriter write the out the docID -> ord\nmapping, but with the understanding that there was a relatively high cost so\nthe module couldn't be core.   The idea was to take the cost of iterating over\nthe field caches during IndexReader startup, move that to index time, and write\nout a file that could be memory mapped and shared among multiple search apps.\n\nIn theory, if we were to have only per-segment docID -> ord maps, we could\nperform inter-segment collation the same way that it's handled at the\nMultiSearcher level -- by comparing the original strings.  It wouldn't be that\nexpensive in the grand scheme of things, because most of the work would be\ndone by comparing ord values within large segments.\n\nUnfortunately, that won't work because segment boundaries are hidden from\nScorers.\n\n>> In KS, the relevant IndexReader methods no longer take a Term\n>> object. (In fact, there IS no Term object any more -\n>> KinoSearch::Index::Term has been removed.) Instead, they take a\n>> string field and a generic \"Obj\".\n> \n> But you must at least require these Obj's to know how to compareTo one\n> another? \n\nYes.\n\n> Does this mean using per-field custom sort ordering (collator) is\n> straightforward for KS?\n\nThat's one objective.  The implementation is incomplete.\n\nAnother objective is to allow non-string term types, e.g. TimeStamp,\nFloat... Hmm... how about FixedWidthText?",
            "date": "2008-11-24T23:18:46.523+0000",
            "id": 14
        },
        {
            "author": "Marvin Humphrey",
            "body": ">> I suppose we genericize this by adding a TermsDictReader/LexReader\n>> argument to the IndexReader constructor? That way, someone can\n>> supply a custom subclass that knows how to decode custom dictionary\n>> files.\n> \n> Right; that's what let me create the PulsingCodec here.\n\nI'm running into an OO design problem because of the SegmentReader/MultiReader\nbifurcation.  If IndexReader were an ordinary class, and we expected all of\nits component parts to perform their own collation of data from multiple\nsegments, then the API for overriding individual components would be\nstraightforward:\n\n{code}\n  reader = new IndexReader(termsDictReader, postingsReader, fieldsReader);\n{code}\n\nWe can't do that, though, because there's logic in IndexReader.open() which\nguards against race conditions with regards to file deletion and index\nmodification, and the initialization of the auxiliary reader components would\nhappen outside those guards -- possibly resulting in sub-components within an\nIndexReader object reading from different versions of the index.\n\nUsing setters a la reader.setTermsDictReader(termsDictReader) is problematic\nfor the same reason.\n\nAre factory methods the only way to handle adding or replacing components\nwithin IndexReader?\n\nKS forces people to subclass Schema to define their index, but up till now\nthere hasn't been anything that would affect the complement of major\nsub-components within IndexReader or InvIndexer (=IndexWriter).  I suppose\nSchema is the right place to put stuff like this, but it seems a lot more\nelaborate than the factory method which returns the index's default Analyzer.",
            "date": "2008-11-24T23:27:44.088+0000",
            "id": 15
        },
        {
            "author": "Marvin Humphrey",
            "body": "\n> Be careful: it's the seeking that kills you (until we switch to SSDs\n> at which point perhaps most of this discussion is moot!). Even though\n> the terms index net size is low, if re-heating the spots you touch\n> incurs 20 separate page misses, you lose.\n\nPerhaps for such situations, we can make it possible to create custom\nHotLexiconReader or HotIndexReader subclasses that slurp term index files and\nwhat-have-you into process memory.  Implementation would be easy, since we can\njust back the InStreams with malloc'd RAM buffers rather than memory mapped\nsystem buffers.\n\nConsider the tradeoffs.  On the one hand, if we rely on memory mapped buffers,\nbusy systems may experience sluggish search after long lapses in a worst case\nscenario.  On the other hand, reading a bunch of stuff into process memory\nmakes IndexReader a lot heavier, with large indexes imposing consistently \nsluggish startup and a large RAM footprint on each object.\n\n> It seems like the ability to very quickly launch brand new searchers\n> is/has become a strong design goal of Lucy/KS. What's the driver\n> here? Is it for near-realtime search? \n\nNear-realtime search is one of the motivations.  But lightweight IndexReaders\nare more convenient in all sorts of ways.\n\nElaborate pre-warming rituals are necessary with heavy IndexReaders whenever\nindexes get modified underneath a persistent search service.  This is\ncertainly a problem when you are trying to keep up with real-time insertions,\nbut it is also a problem with batch updates or optimization passes.\n\nWith lightweight IndexReaders, you can check whether the index has been\nmodified as requests come in, launch a new Searcher if it has, then deal with\nthe request after a negligible delay.  You have to warm the system io caches\nwhen the service starts up (\"cat /path/to/index/* > /dev/null\"), but after\nthat, there's no more need for background warming.\n\nLightweight IndexReaders can also be sprinkled liberally around source code in \na way that heavy IndexReaders cannot.  For instance, each thread in a \nmulti-threaded server can have its own Searcher.\n\nLaunching cheap search processes is also important when writing tools akin to\nthe Unix command line 'locate' app.  The first time you invoke locate it's\nslow, but subsequent invocations are nice and quick.  You can only mimic that\nwith a lightweight IndexReader.\n\nAnd so on... The fact that segment data files are never modified once written\nmakes the Lucene/Lucy/KS file format design particularly well suited for\nmemory mapping and sharing via the system buffers.  In addition to the reasons\ncited above, intuition tells me that this is the right design decision and\nthat there will be other opportunities not yet anticipated.  I don't see how Lucy\ncan deny such advantages to most users for the sake of those few for whom\nterm dictionary cache eviction proves to be a problem, especially when we can\noffer those users a remedy.\n\n> The biggest problem with the \"load important stuff into RAM\" approach,\n> of course, is we can't actually pin VM pages from java, which means\n> the OS will happily swap out my RAM anyway, at which point of course\n> we should have used mmap. \n\nWe can't realistically pin pages from C, either, at least on Unixen.  Modern\nUnixen offer the mlock() command, but it has a crucial limitation -- you have to\nrun it as root.  \n\nAlso, there aren't any madvise() flags that hint to the OS that the mapped\nregion should stay hot.  The closest thing is MADV_WILLNEED, which\ncommunicates \"this will be needed soon\" -- not \"keep this around\".",
            "date": "2008-11-25T00:34:07.489+0000",
            "id": 16
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nJust the index terms (i.e. every 128th term). We're trying to fake up an\narray of strings without having to load anything into process memory. The\ncomparison would go something like this:\n{quote}\n\nOK this makes sense.  We could do something similar in Lucene.  Not\ncreating String objects is nice.  I wonder in practice how much time\nwe are \"typically\" spending loading the terms index...\n\n{quote}\nHowever, perhaps some sort of a B-tree with string prefix compression would be\nbetter, as per recent suggestions.\n{quote}\n\nB-tree or FST/trie or ... something.\n\nActually: I just realized the terms index need not store all suffixes\nof the terms it stores.  Only unique prefixes (ie a simple letter\ntrie, not FST).  Because, its goal is to simply find the spot in the\nmain lexicon file to seek to and then scan from.  This makes it even\nsmaller!\n\nThough, if we want to do neat things like respelling, wildcard/prefix\nsearching, etc., which reduce to graph-intersection problems, we would\nneed the suffix and we would need the entire lexicon (not just every\n128th index term) compiled into the FST.\n",
            "date": "2008-11-25T11:23:40.047+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nHow do you plan on dealing with the ord values changing as segments get\nadded? The addition of a single document triggers the rewriting of the\nentire mapping.\n\n...\n\nUnfortunately, that won't work because segment boundaries are hidden from\nScorers.\n\n{quote}\n\nThis is a big challenge -- presenting a merged docID->ord map for a\nMultiSegmentReader is very costly.\n\nI think, just like we are pushing for column-stride / FieldCache to be\n\"per segment\" instead of one big merged array, we should move in the\nsame direction for searching?\n\nIe, if one did all searching with MultiSearcher, it should work well.\nEach segment uses its pre-computed (during indexing) docID->ord\nmapping.  Merge-sorting the results from each searcher ought to be low\ncost since you only need to lookup the string values for the top N\ndocs (though care must be taken to not incur N seeks for this... eg\nperhaps each reader, on hitting a doc that makes it into the pqueue,\nshould then seek&load the String value from column-stride store?).  An\noptimized index wouldn't need to read any of the actual string values\nsince no results merging is needed.\n\nFor the RangeFilter impl in LUCENE-1461 (which'd use the docID->order\nper segment, using MultiSearcher), string values are never needed.\n\n{quote}\n> Does this mean using per-field custom sort ordering (collator) is\n> straightforward for KS?\n\nThat's one objective. The implementation is incomplete.\n\nAnother objective is to allow non-string term types, e.g. TimeStamp,\nFloat... Hmm... how about FixedWidthText?\n{quote}\n\nNeat!",
            "date": "2008-11-25T11:43:41.824+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nIf IndexReader were an ordinary class, and we expected all of\nits component parts to perform their own collation of data from multiple\nsegments, then the API for overriding individual components would be\nstraightforward:\n\nreader = new IndexReader(termsDictReader, postingsReader, fieldsReader);\n\nWe can't do that, though, because there's logic in IndexReader.open() which\nguards against race conditions with regards to file deletion and index\nmodification, and the initialization of the auxiliary reader components would\nhappen outside those guards - possibly resulting in sub-components within an\nIndexReader object reading from different versions of the index.\n{quote}\n\nI think you \"just\" have to have \"index version data\" that's\ncollectively read/written, atomically, and is then used to init all\nthe components.  This is what segments_N is in Lucene (and I think\n\"Schema\" is in KS/Lucy?): it contains all details that all\nsub-components need.\n\nIf init'ing each sub-component is then costly (opening files,\nslurping things in, etc.) its OK because they are all still loading a\nconsistent commit point.\n",
            "date": "2008-11-25T11:52:39.781+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\n> Be careful: it's the seeking that kills you (until we switch to SSDs\n> at which point perhaps most of this discussion is moot!). Even though\n> the terms index net size is low, if re-heating the spots you touch\n> incurs 20 separate page misses, you lose.\n\nPerhaps for such situations, we can make it possible to create custom\nHotLexiconReader or HotIndexReader subclasses that slurp term index files and\nwhat-have-you into process memory. Implementation would be easy, since we can\njust back the InStreams with malloc'd RAM buffers rather than memory mapped\nsystem buffers.\n\nConsider the tradeoffs. On the one hand, if we rely on memory mapped buffers,\nbusy systems may experience sluggish search after long lapses in a worst case\nscenario. On the other hand, reading a bunch of stuff into process memory\nmakes IndexReader a lot heavier, with large indexes imposing consistently\nsluggish startup and a large RAM footprint on each object.\n{quote}\n\nI think this is a fabulous solution.  If you make things so pluggable\nthat you can choose to swap in \"mmap this thing\" vs \"slurp in this\nthing\" and it's the same interface presented to the consumer, then, we\ndon't need to resolve this debate now ;)  Put both out in the field and\ngather data...\n\n{quote}\nElaborate pre-warming rituals are necessary with heavy IndexReaders whenever\nindexes get modified underneath a persistent search service. This is\ncertainly a problem when you are trying to keep up with real-time insertions,\nbut it is also a problem with batch updates or optimization passes.\n\nWith lightweight IndexReaders, you can check whether the index has been\nmodified as requests come in, launch a new Searcher if it has, then deal with\nthe request after a negligible delay. You have to warm the system io caches\nwhen the service starts up (\"cat /path/to/index/* > /dev/null\"), but after\nthat, there's no more need for background warming.\n{quote}\n\nWell ...that cat command can be deadly for a large index, too?  You've\nreplaced the elaborate pre-warming ritual (= run certain queries that\nyou know will populate variou caches) with a cat command that doesn't\ndistinguish what's important (norms, terms index, certain docID->ord\nmaps, certain column-stride-fields, etc.) from what's less important.\n\n{quote}\nLightweight IndexReaders can also be sprinkled liberally around source code in\na way that heavy IndexReaders cannot. For instance, each thread in a\nmulti-threaded server can have its own Searcher.\n\nLaunching cheap search processes is also important when writing tools akin to\nthe Unix command line 'locate' app. The first time you invoke locate it's\nslow, but subsequent invocations are nice and quick. You can only mimic that\nwith a lightweight IndexReader.\n{quote}\n\nThis is indeed nice.  I think the two approaches boil down to \"pay up\nfront & reuse\" (Lucene, slurping) vs \"pay as you go & discard\"\n(KS/Lucy, mmap'ing).\n\n{quote}\nAnd so on... The fact that segment data files are never modified once written\nmakes the Lucene/Lucy/KS file format design particularly well suited for\nmemory mapping and sharing via the system buffers. In addition to the reasons\ncited above, intuition tells me that this is the right design decision and\nthat there will be other opportunities not yet anticipated. I don't see how Lucy\ncan deny such advantages to most users for the sake of those few for whom\nterm dictionary cache eviction proves to be a problem, especially when we can\noffer those users a remedy.\n{quote}\n\n[BTW the ZFS filesystem gets many of its nice properties for the same\nreason -- \"write once\", at the file block level.]\n\nLucene java takes advantage of that 'write once' nature during\nIndexReader.reopen().  If we can finally push FieldCache, norms,\ndocID->ord to be per-reader then the reopen of a MultiSearcher should\nbe alot better than it is today.\n\n{quote}\n> The biggest problem with the \"load important stuff into RAM\" approach,\n> of course, is we can't actually pin VM pages from java, which means\n> the OS will happily swap out my RAM anyway, at which point of course\n> we should have used mmap.\n\nWe can't realistically pin pages from C, either, at least on Unixen. Modern\nUnixen offer the mlock() command, but it has a crucial limitation - you have to\nrun it as root.\n\nAlso, there aren't any madvise() flags that hint to the OS that the mapped\nregion should stay hot. The closest thing is MADV_WILLNEED, which\ncommunicates \"this will be needed soon\" - not \"keep this around\".\n{quote}\n\nAlas.\n\nThe only fallback is gross system-level tunings (\"swappiness\" on Linux\nand \"Adjust for best performance of: Programs/System Cache\" on Windows\nServer 2003, at least).\n\nOr also a silly \"keep warm\" thread...\n",
            "date": "2008-11-25T13:20:33.916+0000",
            "id": 20
        },
        {
            "author": "Marvin Humphrey",
            "body": "> I think you \"just\" have to have \"index version data\" that's\n> collectively read/written, atomically, and is then used to init all\n> the components. This is what segments_N is in Lucene (and I think\n> \"Schema\" is in KS/Lucy?): it contains all details that all\n> sub-components need.\n\nThe equivalent to segments_N in KinoSearch is snapshot_N.meta, which is\nencoded as JSON.  There's a KinoSearch::Index::Snapshot class that's\nresponsible for reading/writing it.\n\nKinoSearch::Schema is for defining your index: global field properties,\ndefault Analyzer, etc.  It's similar to Solr's schema.xml, but implemented as\nan abstract class that users are required to subclass.  Translated to Java,\nthe subclassing might look something like this:\n\n{code}\n  class MySchema extends Schema {\n    class URLField extends TextField {\n        boolean analyzed() { return false; }\n        boolean indexed() { return false; }\n    }\n\n    void initFields() {\n      addField(\"title\", \"text\");\n      addField(\"content\", \"text\");\n      addField(\"url\", new URLField());\n    }\n\n    Analyzer analyzer() {\n      return new PolyAnalyzer(\"en\");\n    }\n  }\n{code}\n\nI anticipate that Lucy will adopt both Schema and Snapshot in some form, but\nafter discussion.  \n\n> If init'ing each sub-component is then costly (opening files,\n> slurping things in, etc.) its OK because they are all still loading a\n> consistent commit point.\n\nSo, something like this prospective Lucy code? (Lucy with Java bindings, that is.)\n\n{code}\n  MySchema schema = new MySchema();\n  Snapshot snapshot = new Snapshot((Schema)schema);\n  snapShot.readSnapShot(\"/path/to/index\");\n  MyTermsDictReader termsDictReader = new MyTermsDictReader(schema, snapshot);\n  IndexReader reader = new IndexReader(schema, snapshot, null, null,\n                                       (TermsDictReader)termsDictReader);\n{code}\n\nWhat if index files get deleted out from under that code block?  The user will\nhave to implement retry logic.\n",
            "date": "2008-11-25T14:47:57.315+0000",
            "id": 21
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nThe equivalent to segments_N in KinoSearch is snapshot_N.meta, which is\nencoded as JSON. There's a KinoSearch::Index::Snapshot class that's\nresponsible for reading/writing it.\n\nKinoSearch::Schema is for defining your index: global field properties,\ndefault Analyzer, etc. It's similar to Solr's schema.xml, but implemented as\nan abstract class that users are required to subclass. Translated to Java,\nthe subclassing might look something like this:\n{quote}\n\nOK got it.\n\n{quote}\nWhat if index files get deleted out from under that code block? The\nuser will have to implement retry logic.\n{quote}\n\nI would think this \"openReader\" method would live inside Lucy/KS, and\nwould in fact implement its own retry logic (to load the next snapshot\nand try again).  I must be missing some part of the question here...\n",
            "date": "2008-11-25T15:51:38.860+0000",
            "id": 22
        },
        {
            "author": "Marvin Humphrey",
            "body": "> I would think this \"openReader\" method would live inside Lucy/KS, and\n> would in fact implement its own retry logic (to load the next snapshot\n> and try again). I must be missing some part of the question here...\n\nIf the retry code lives inside of IndexReader, then the only way to get the\nIndexReader to use e.g. a subclassed TermsDictReader is to subclass\nIndexReader and override a factory method:\n\n{code}\n  class MyIndexReader extends IndexReader {\n    TermsDictReader makeTermsDictReader() {\n      return (TermsDictReader) new MyTermsDictReader(invindex, snapshot);\n    }\n  }\n\n  InvIndex invindex = MySchema.open(\"/path/to/index\");\n  IndexReader reader = (IndexReader) new MyIndexReader(invindex);\n{code}\n\nI was hoping to avoid forcing the user to subclass IndexReader, but I think\nthe need for retry logic during open() precludes that possibility.",
            "date": "2008-11-25T16:39:45.101+0000",
            "id": 23
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nI was hoping to avoid forcing the user to subclass IndexReader, but I\nthink the need for retry logic during open() precludes that\npossibility.\n{quote}\n\nHow about the caller provides a codec instance which when asked will\nreturn a TermsDictReader \"matching\" the codec that had been used to\nwrite the index?\n\nThen open() implements the retry logic, asking the codec to load each\npart of the index?\n\nThat's roughly the approach I'm taking here (on next iteriaton of the\npatch, hopefully soon), though I'm only tackling the postings now (not\nyet norms, stored fields, term vectors, fields infos).\n",
            "date": "2008-11-25T17:45:32.415+0000",
            "id": 24
        },
        {
            "author": "Marvin Humphrey",
            "body": ">> We're trying to fake up an array of strings without having to load anything\n>> into process memory.\n\n> We could do something similar in Lucene. Not creating String objects is\n> nice. \n\nOK, assume that you slurp all three files.  Here's the code from above, ported\nfrom C to Java.  \n\n{code}\nwhile (hi >= lo) {\n  int  mid           = lo + ((hi - lo) / 2);\n  long midTextOffset = textLengths[mid];\n  long midTextLength = textLengths[mid + 1] - midTextOffset;\n  int comparison     = StringHelper.compareUTF8Bytes(\n                          targetUTF8Bytes, 0, targetLength, \n                          termUTF8bytes, midTextOffset, midTextLength);\n  if      (comparison < 0) { hi = mid - 1; }\n  else if (comparison > 0) { lo = mid + 1; }\n  else { \n    result = mid; \n    break;\n  }\n}\nlong offsetIntoMainTermDict = mainTermDictFilePointers[result];\n...\n{code}\n\nOther than the slurping, the only significant difference is the need for the\ncomparison routine to take a byte[] array and an offset, rather than a char*\npointer.\n\nYou can also use FileChannels to memory map this stuff, right?  (Have to be\ncareful on 32-bit systems, though.)\n\n> B-tree or FST/trie or ... something.\n\nMuch to my regret, my tree algorithm vocabulary is limited -- I haven't spent\nenough time coding such projects that I can intuit sophisticated solutions.\nSo I'll be counting on you, Jason Rutherglen, and Eks Dev to suggest\nappropriate algorithms based on your experience.\n\nOur segment-based inverted index term dictionary has a few defining\ncharacteristics.\n\nFirst, a lot of tree algorithms are optimized to a greater or lesser extent\nfor insertion speed, but we hardly care about that at all.  We can spend all\nthe cycles we need at index-time balancing nodes within a segment, and once\nthe tree is written out, it will never be updated.\n\nSecond, when we are writing out the term dictionary at index-time, the raw\ndata will be fed into the writer in sorted order as iterated values, one\nterm/term-info pair at a time.  Ideally, the writer would be able to serialize\nthe tree structure during this single pass, but it could also write a\ntemporary file during the terms iteration then write a final file afterwards.\nThe main limitation is that the writer will never be able to \"see\" all\nterms at once as an array.\n\nThird, at read-time we're going to have one of these trees per segment.  We'd\nreally like to be able to conflate them somehow.  KinoSearch actually\nimplements a MultiLexicon class which keeps SegLexicons in a PriorityQueue;\nMultiLexicon_Next() advances the queue to the next unique term.  However,\nthat's slow, unwieldy, and inflexible.  Can we do better?\n\n> Actually: I just realized the terms index need not store all suffixes\n> of the terms it stores. Only unique prefixes (ie a simple letter\n> trie, not FST). Because, its goal is to simply find the spot in the\n> main lexicon file to seek to and then scan from. This makes it even\n> smaller!\n\nIt would be ideal if we could separate the keys from the values and put all\nthe keys in a single file.\n\n> Though, if we want to do neat things like respelling, wildcard/prefix\n> searching, etc., which reduce to graph-intersection problems, we would\n> need the suffix and we would need the entire lexicon (not just every\n> 128th index term) compiled into the FST.\n\nThe main purpose of breaking out a separate index structure is to avoid binary\nsearching over the large primary file.  There's nothing special about the\nextra file -- in fact, it's a drawback that it doesn't include all terms.  If\nwe can jam all the data we need to binary search against into the front of the\nfile, but include the data for all terms in an infrequently-accessed tail, we\nwin.",
            "date": "2008-11-25T17:46:54.528+0000",
            "id": 25
        },
        {
            "author": "Marvin Humphrey",
            "body": "> How about the caller provides a codec instance which when asked will\n> return a TermsDictReader \"matching\" the codec that had been used to\n> write the index?\n\nOK, it makes sense to have the user access these capabilities via a single\nhandle at both index-time and search-time.  However, for Lucy/KS, the handle\nshould definitely be specified via the Schema subclass rather than via\nconstructor argument.\n\n\"Codec\" isn't really the right name for this, though.  \"IndexComponent\",\nmaybe?  Lucy would have three main index components by default:\nLexiconComponent, PostingsComponent, StorageComponent.  \n\n{code}\n// If Lucy's Schema class were implemented in Java instead of C...\nabstract class Schema extends Obj {\n  LexiconComponent lexiconComponent() { return new LexiconComponent(); }\n  PostingsComponent postingsComponent() { return new PostingsComponent(); }\n  StorageComponent storageComponent() { return new StorageComponent(); }\n  ...\n}\n{code}\n\nAuxiliary IndexComponents might include TermVectorsComponent,\nSortCacheComponent, ColumnStrideComponent, RTreeComponent, etc.\n\nHere's example code for overriding the default LexiconComponent:\n\n{code}\n// Implements term dictionary as a hash table with term texts as keys.\nclass HashLexiconComponent extends LexiconComponent {\n  LexiconReader makeReader(InvIndex invindex, Snapshot snapshot) {\n    SegInfos segInfos = Snapshot.getSegInfos();\n    if (segInfos.size == 1) { \n      return (LexiconReader) new SegHashLexiconReader(invindex, snapshot);\n    }\n    else {\n      return (LexiconReader) new MultiHashLexiconReader(invindex, snapshot);\n    }\n  }\n\n  LexiconWriter makeWriter(InvIndex invindex, SegInfo segInfo) {\n    return (LexiconWriter) new HashLexiconWriter(invindex, segInfo);\n  }\n}\n\n// [User code]\nclass MySchema extends Schema {\n  LexiconComponent lexiconComponent() {\n    return (LexiconComponent) new HashLexiconComponent();\n  }\n  ...\n}\n{code}\n",
            "date": "2008-11-25T20:27:36.144+0000",
            "id": 26
        },
        {
            "author": "Marvin Humphrey",
            "body": "> I think, just like we are pushing for column-stride / FieldCache to be\n> \"per segment\" instead of one big merged array, we should move in the\n> same direction for searching?\n\nAlgorithmically speaking, it would definitely help this specific task, and\nthat's a BIG FAT PLUS.  This, plus memory mapping and writing the DocID -> ord\nmap at index-time, allows us to totally eliminate the current cost of loading\nsort caches at IndexReader startup.  The question is, how easy is it to\nrefactor our search OO hierarchy to support it?\n\nIf our goal is minimal impact to the current model, we worry only about the\nTopFieldDocs search() method.  We can hack in per-segment bookending via doc\nnumber to the hit collection routine, initializing the TopFieldDocCollector\neach segment (either creating a new one or popping all the collected docs).\n\nBut does it make sense to be more aggressive?  Should Searchers run hit\ncollection against individual segments?  Should Scorers only be compiled\nagainst single segments?\n\nMaybe so.  I implemented pruning (early termination) in KS, and it had to be\ndone per segment.  This is because you have to sort the documents within a\nsegment according to the primary criteria you want to prune on (typically doc\nboost).  I've since ripped out that code because it was adding too much\ncomplexity, but maybe there would have been less complexity if segments were\ncloser to the foreground.",
            "date": "2008-11-26T03:37:57.793+0000",
            "id": 27
        },
        {
            "author": "Marvin Humphrey",
            "body": "> Well ...that cat command can be deadly for a large index, too? \n\nIt will be costly for a large index, and it wouldn't be appropriate in all\ncases.  The use case I was thinking of was: dedicated server with gobs of RAM.\nThe index could either be updated often or not updated at all.  Pre-existing\nsegments stay warm on such a box, and the writer would leave the latest\nsegment hot, so the cat command would only be needed once, at the startup of\nthe persistent service.",
            "date": "2008-11-26T03:40:57.035+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nOK, assume that you slurp all three files. Here's the code from above, ported\nfrom C to Java.\n{quote}\nLooks good!\n\n{quote}\nYou can also use FileChannels to memory map this stuff, right? (Have to be\ncareful on 32-bit systems, though.)\n{quote}\nYes.\n\n{quote}\nFirst, a lot of tree algorithms are optimized to a greater or lesser extent\nfor insertion speed, but we hardly care about that at all. We can spend all\nthe cycles we need at index-time balancing nodes within a segment, and once\nthe tree is written out, it will never be updated.\n{quote}\n\nRight, neither inserts nor deletes matter to us.\n\n{quote}\nSecond, when we are writing out the term dictionary at index-time, the raw\ndata will be fed into the writer in sorted order as iterated values, one\nterm/term-info pair at a time. Ideally, the writer would be able to serialize\nthe tree structure during this single pass, but it could also write a\ntemporary file during the terms iteration then write a final file afterwards.\nThe main limitation is that the writer will never be able to \"see\" all\nterms at once as an array.\n{quote}\n\nLucene differs from Lucy/KS in this.  For Lucene, when flushing a\nnew segment, we can assume you can see all Terms in RAM at once.  We\ndon't make use of this today (it's a simple iteration that's given to\nthe consumer), but we could.  In Lucene, when RAM is full, we flush a\nreal segment (but KS flushes a \"run\" which I think is more of a raw\ndump, ie, you don't build lexicon trees during that?).\n\nHowever, for both Lucene and Lucy/KS, during merging one cannot assume\nthe entire lexicon can be in RAM at once.  But then, during merging\nyou could in theory merge trees not expanded terms.\n\nI think for starters at least we should stick with the simple\nshared-prefix-compression we have today.\n\n{quote}\nThird, at read-time we're going to have one of these trees per segment. We'd\nreally like to be able to conflate them somehow. KinoSearch actually\nimplements a MultiLexicon class which keeps SegLexicons in a PriorityQueue;\nMultiLexicon_Next() advances the queue to the next unique term. However,\nthat's slow, unwieldy, and inflexible. Can we do better?\n{quote}\n\nContinuing the move towards pushing searching closer to the segments\n(ie, using MultiSearcher instead of MultiReader), I think we should\nnot try to conflate the terms dict?\n\n{quote}\nIt would be ideal if we could separate the keys from the values and put all\nthe keys in a single file.\n{quote}\n\nWhy not inline the value with the key?  The pointer to the value just\nconsumes extra space.  I think \"value\" in this context is the long\noffset into the main terms dict file, which then stores the \"real\n[opaque] value\" for each term.\n\n{quote}\n> Though, if we want to do neat things like respelling, wildcard/prefix\n> searching, etc., which reduce to graph-intersection problems, we would\n> need the suffix and we would need the entire lexicon (not just every\n> 128th index term) compiled into the FST.\n\nThe main purpose of breaking out a separate index structure is to avoid binary\nsearching over the large primary file. There's nothing special about the\nextra file - in fact, it's a drawback that it doesn't include all terms. If\nwe can jam all the data we need to binary search against into the front of the\nfile, but include the data for all terms in an infrequently-accessed tail, we\nwin.\n{quote}\nAnd... if your terms index is in RAM, to minimize its net size and\ndecode cost on loading.\n",
            "date": "2008-11-26T11:22:36.713+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nOK, it makes sense to have the user access these capabilities via a single\nhandle at both index-time and search-time. However, for Lucy/KS, the handle\nshould definitely be specified via the Schema subclass rather than via\nconstructor argument.\n\n\"Codec\" isn't really the right name for this, though. \"IndexComponent\",\nmaybe? Lucy would have three main index components by default:\nLexiconComponent, PostingsComponent, StorageComponent.\n{quote}\n\nWell, maybe both?  Ie, each of these IndexComponents could have many\ndifferent codecs to write/read the data to/from the index.  So when I\nimplement PostingsComponent, when writing a segment I could choose my\nown codec; when reading it, I retrieve the matching codec to decode\nit.\n\nSubclassing Schema seems like the right approach.\n",
            "date": "2008-11-26T11:31:51.591+0000",
            "id": 30
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\n> I think, just like we are pushing for column-stride / FieldCache to be\n> \"per segment\" instead of one big merged array, we should move in the\n> same direction for searching?\n\nAlgorithmically speaking, it would definitely help this specific task, and\nthat's a BIG FAT PLUS. This, plus memory mapping and writing the DocID -> ord\nmap at index-time, allows us to totally eliminate the current cost of loading\nsort caches at IndexReader startup. The question is, how easy is it to\nrefactor our search OO hierarchy to support it?\n\nIf our goal is minimal impact to the current model, we worry only about the\nTopFieldDocs search() method. We can hack in per-segment bookending via doc\nnumber to the hit collection routine, initializing the TopFieldDocCollector\neach segment (either creating a new one or popping all the collected docs).\n\nBut does it make sense to be more aggressive? Should Searchers run hit\ncollection against individual segments? Should Scorers only be compiled\nagainst single segments?\n\nMaybe so. I implemented pruning (early termination) in KS, and it had to be\ndone per segment. This is because you have to sort the documents within a\nsegment according to the primary criteria you want to prune on (typically doc\nboost). I've since ripped out that code because it was adding too much\ncomplexity, but maybe there would have been less complexity if segments were\ncloser to the foreground.\n{quote}\n\nI think the plus's are substantial here.  Not having to materialize\none massive array of norms, of FieldCache/column-stride values, of\ndocID->ord values, is very important because these are at least linear\ncost (more for the docID->ord) in # docs in the index.  Reopening a\nsearcher on a large index is very costly in Lucene now because of\nthese materializations.\n\nWe need to think more about the tradeoffs here...\n",
            "date": "2008-11-26T11:38:58.603+0000",
            "id": 31
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\n> Well ...that cat command can be deadly for a large index, too?\n\nIt will be costly for a large index, and it wouldn't be appropriate in all\ncases. The use case I was thinking of was: dedicated server with gobs of RAM.\nThe index could either be updated often or not updated at all. Pre-existing\nsegments stay warm on such a box, and the writer would leave the latest\nsegment hot, so the cat command would only be needed once, at the startup of\nthe persistent service.\n{quote}\n\nAhh OK.  But that cat command is basically just a different, more\nglobal, implemenation of \"warming\".\n\nSo eg you'd still need to coordinate so that the new searcher isn't\nused until warming finishes, right?  In Lucene, since warming is explicit\nand under direct programmatic control, we know when warming is done.\nI guess you could also do a system call to do the cat command,\nblocking cutover to the new searcher until it completes.\n",
            "date": "2008-11-26T11:45:45.761+0000",
            "id": 32
        },
        {
            "author": "Marvin Humphrey",
            "body": "> Not having to materialize one massive array of norms, of\n> FieldCache/column-stride values, of docID->ord values, is very important\n> because these are at least linear cost (more for the docID->ord) in # docs\n> in the index. Reopening a searcher on a large index is very costly in Lucene\n> now because of these materializations.\n> \n> We need to think more about the tradeoffs here...\n\nLet's continue the discussion of segment-centric searching on java-dev, since it it's \nonly tangentially related to flexible indexing.",
            "date": "2008-11-26T14:04:56.255+0000",
            "id": 33
        },
        {
            "author": "Marvin Humphrey",
            "body": "> So eg you'd still need to coordinate so that the new searcher isn't\n> used until warming finishes, right? \n\n...\n\n> I guess you could also do a system call to do the cat command,\n> blocking cutover to the new searcher until it completes.\n\n\nWarming is only needed once, at search service startup.  The idea is to get \nthe whole index into the system IO cache.\n\nOnce all segment data is in the IO cache, we assume that it stays there,\nbecause this is a beefy dedicated search box with more than enough RAM to fit\nthe entire index/shard.\n\nSay that we add a new segment to the index, either by running an\nindex-writing process locally, or via rsync.  (Assume for the purposes of\nargument that the local indexing process doesn't require much RAM -- which \nis true with KS -- and so won't have the side effect of nudging existing\nsegments out of IO cache.) \n\nNow, say that our search service checks at the beginning of each request to\nsee if the index has been modified.  If it has, it opens a new searcher from\nscratch -- which takes almost no time, because we're memory mapping rather\nthan slurping.\n\n{code}\nwhile (newRequest()) {\n  if (indexHasBeenUpdated()) {\n    searcher = new IndexSearcher(\"/path/to/index\");\n  }\n  ...\n}\n{code}\n\nAfter an abrupt cutover to the new searcher, we process the search request.  \nIs the new search sluggish in any way?  No, because all the segments used \nby the new searcher are \"hot\".  Older segments are hot because they were \nin use by the prior searcher, and the new segment is hot because it was \njust written.\n\nTherefore, we don't need to worry about managing cutover to a new searcher.\nWe can just discard the old one and replace it with the new one.\n",
            "date": "2008-11-26T14:38:25.776+0000",
            "id": 34
        },
        {
            "author": "Marvin Humphrey",
            "body": "> Well, maybe both? Ie, each of these IndexComponents could have many\n> different codecs to write/read the data to/from the index. So when I\n> implement PostingsComponent, when writing a segment I could choose my\n> own codec; when reading it, I retrieve the matching codec to decode\n> it.\n\nYes, both -- that sounds good.  However, I'm not sure whether you're proposing\nthe creation of a class named \"Codec\", which I think we should avoid unless\nall of our \"codecs\" can descend from it.  So: PostingsCodec, TermsDictCodec\n(or LexiconCodec, for Lucy/KS), and so on would be base classes.\n\n> Subclassing Schema seems like the right approach.\n\nGroovy. How are you going to handle it in Lucene?  I think you just have to\nrequire the end user to be consistent about supplying the necessary arguments\nto the IndexReader and IndexWriter constructors.\n\nHow do we handle auxiliary IndexComponents?  I've long wanted to implement an\nRTreeComponent for geographic searching, so I'll use that as an example.\n\nAt index-time, I think we just create an array of SegDataWriter objects and\nfeed each document to each writer in turn.  The SegDataWriter abstract base\nclass will define all the necessary abstract methods: addDoc(),\naddSegment(SegReader) (for Lucy/KS), various commands related to merging (for\nLucene), finish()/close(), and so on.  RTreeWriter would simply subclass\nSegDataWriter.\n\nAt search-time, things get a little trickier.  Say we hand our Searcher object\nan RTreeRadiusQuery.  At some point, the RTreeRadiusQuery will need to be\ncompiled to an RTreeRadiusScorer, which will involve accessing an RTreeReader\nwhich presumably resides within an IndexReader.  However, right now,\nIndexReader hides all of its inner readers and provides access through\nspecific methods, e.g. IndexReader.document(int docNum), which ultimately\nhands off to FieldsReader internally.  This model doesn't scale with the\naddition of arbitrary IndexComponents.\n\nThe only thing I can thing of is an IndexReader.getReader(String name) method.",
            "date": "2008-11-26T17:18:52.101+0000",
            "id": 35
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Warming is only needed once, at search service startup.\n\nAhh, got it.  Lucene must warm for each reopened searcher (though that warming cost will eventually be in proportion to what's changed in the index), but KS/Lucy should be fine doing zero warming except for the very first searcher startup (eg after rebooting the machine).",
            "date": "2008-11-26T17:57:50.683+0000",
            "id": 36
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\n> So: PostingsCodec, TermsDictCodec (or LexiconCodec, for Lucy/KS), and\n> so on would be base classes.\n{quote}\n\nRight: separate codec base classes for each component.  Back to the\nvideo analogy: a typical video has a \"audio\" component and a \"video\"\ncomponent.  AudioCodec would be the base class for all the various\naudio codecs, and likewise for VideoCodec.\n\n{quote}\n> I think you just have to require the end user to be consistent about\n> supplying the necessary arguments to the IndexReader and IndexWriter\n> constructors.\n{quote}\n\nRight.\n\n{quote}\n> How do we handle auxiliary IndexComponents? I've long wanted to implement an\n> RTreeComponent for geographic searching, so I'll use that as an example.\n\n> At index-time, I think we just create an array of SegDataWriter objects and\n> feed each document to each writer in turn.\n{quote}\n\nI think that's right.  In Lucene we now have an indexing chain\n(package private), so that you can \"tap in\" at whatever point is\nappropriate -- you could handle the whole doc yourself (like\nSegDataWriter); you could be fed one field at a time; you could tap in\nafter inversion so you get one token at a time, etc.\n\n{quote}\n> At search-time, things get a little trickier.\n> ...\n> The only thing I can thing of is an IndexReader.getReader(String\n> name) method.\n{quote}\n\nI haven't thought enough about how to handle this at search time.\nIR.getReader seems fine, though, you'd need to open each\nIndexComponent up front inside the retry loop, right?\n",
            "date": "2008-11-26T18:10:35.286+0000",
            "id": 37
        },
        {
            "author": "Marvin Humphrey",
            "body": "> In Lucene we now have an indexing chain\n> (package private), so that you can \"tap in\" at whatever point is\n> appropriate - you could handle the whole doc yourself (like\n> SegDataWriter); you could be fed one field at a time; you could tap in\n> after inversion so you get one token at a time, etc.\n\nThat's pretty nice.  It occurred to me to try something like that, but I got a\nlittle lost.  \n\nThe fact that the Doc object in KS uses the host language's native hashtable\nand string implementations for field data complicates an already complicated\nmatter.  It's hard to abstract out access to field data so that the KS/Lucy\ncore, which knows nothing about the host language, can see it, yet still\nmaintain peak performance in the addDoc() loop.\n\nIn any case, I don't anticipate intractable implementation troubles with\nadding IndexComponents at index-time.\n\n> IR.getReader seems fine, though, you'd need to open each\n> IndexComponent up front inside the retry loop, right?\n\nSure, startup's easy.  I think we just add Schema.auxiliaryComponents(),\nwhich returns an array of IndexComponents.  The default would be to return\nnull or an empty array, but subclasses could override it.\n\nWhere we have problems, though, is with remote searching or multi-searching.\nYou can't ask a Searchable for its inner IndexReader, because it might not\nhave one.  That means that you can't \"see\" information pertaining to a custom\nIndexComponent until you're at the level of the individual machine --\naggregate information, like docFreq across an entire collection spanning\nmultiple indexes, wouldn't be available to searches which use custom\ncomponents.\n\nThe only remedy would be to subclass all your Searchables -- the local\nIndexSearcher, the RemoteSearchable that wraps it, and the MultiSearcher that\naggregates results -- to drill down into the correct IndexReader and pass data\nback up the chain.  Basically, you'd have to duplicate e.g. the call chain\nthat fetches documents.",
            "date": "2008-11-26T20:50:56.849+0000",
            "id": 38
        },
        {
            "author": "Michael McCandless",
            "body": "\nNew patch attached (still plenty more to do...):\n\n  * Updated to current trunk (747391).\n\n  * All tests pass, but back-compat tests don't compile...\n\n  * Switched the new \"4d iteration API\" (Fields -> Terms -> Docs ->\n    Positions) to subclass AttributeSource; this way codecs can add in\n    their own attrs.\n\n  * Added PostingsCodecs class, that holds all PostingCodec instances\n    your index may make use of, and changed segments_N format to\n    record which codec was used per segment.  So, an index can have\n    mixed codecs (though for a single IndexWriter session, the same\n    codec is used when writing new segments).\n\n  * I cutover TermScorer to use the new API; I still need to cutover\n    other queries, segment merging, etc.\n\n",
            "date": "2009-02-24T14:22:22.772+0000",
            "id": 39
        },
        {
            "author": "Michael McCandless",
            "body": "Clearing fix version.",
            "date": "2009-03-12T12:51:59.372+0000",
            "id": 40
        },
        {
            "author": "Michael Busch",
            "body": "I took Mike's latest patch and updated it to current trunk.\nIt applies cleanly and compiles fine.\n\nSome test cases fail. The problem is in SegmentReader in termsIndexIsLoaded() and loadTermsIndex(). I'll take a look tomorrow, I need to understand the latest changes we made in the different IndexReaders better (and now it's getting quite late here...)",
            "date": "2009-08-12T10:13:48.392+0000",
            "id": 41
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks for modernizing the patch Michael!  I'll get back to this one soon... I'd really love to get PForDelta working as a codec.  It's a great test case since it's block-based, ie, very different from the other codecs.",
            "date": "2009-08-12T16:47:02.077+0000",
            "id": 42
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nSwitches to a new more efficient terms dict format. \n{quote}\n\nThis is nice! Maybe we should break this whole issue into smaller pieces? We could start with the dictionary. The changes you made here are really cool already. We could further separate the actual TermsDictReader from the terms index with a clean API (I think you put actually a TODO comment into your patch). Then we can have different terms index implementations in the future, e.g. one that uses a tree. We could also make SegmentReader a bit cleaner: if opened just for merging it would not create a terms index reader at all; only if cloned for an external reader we would instantiate the terms index lazily. Currently this is done by setting the divisor to -1.\n",
            "date": "2009-08-27T19:51:20.247+0000",
            "id": 43
        },
        {
            "author": "Michael Busch",
            "body": "In the current patch the choice of the Codec is index-wide, right? So I can't specify different codecs for different fields. Please correct me if I'm wrong.",
            "date": "2009-08-27T23:29:54.996+0000",
            "id": 44
        },
        {
            "author": "Michael Busch",
            "body": "Oups, didn't want to steal this from you, Mike. Wanted to hit the \"Watch\" button instead...",
            "date": "2009-08-28T04:06:53.833+0000",
            "id": 45
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Maybe we should break this whole issue into smaller pieces? We could start with the dictionary. The changes you made here are really cool already.\n\nYeah the issue is very large now.  I'll think about how to break it\nup.\n\nI agree: the new default terms dict codec is a good step forward.\nRather than load a separate TermInfo instance for every indexed term\n(costly in object overhead, and, because we store Term[] as well we\nare wasting space storing many duplicate String field pointers in a\nrow), we only store the String and the long offset into the index file\nas two arrays.  It's a sizable memory savings for indexes with many\nterms.\n\nThis was a nice side-effect of genericizing things, because the\nTermInfo class had to be made private to the codec since it's storing\nthings like proxOffset, freqOffset, etc., which is particular to how\nthe Lucene's default codec stores postings.\n\nBut, it's somewhat tricky to break out only this change... eg it's\nalso coupled with the change to strongly separate field from term\ntext, and, to remove TermInfo reliance.  Ie, the new terms dict has a\nseparate per-field class, and within that per-field class it has the\nString[] termText and long[] index offsets.  I guess we could make a\ndrop-in class that tries to emulate TermInfosReader/SegmentTermEnum\neven though it separates into per-field, internally.\n\nbq. We could further separate the actual TermsDictReader from the terms index with a clean API (I think you put actually a TODO comment into your patch).\n\nActually the whole terms dict writing/reading is itself pluggable, so\nyour codec could provide its own.  Ie, Lucene \"just\" needs a\nFieldsConsumer (for writing) and a FieldsProducer (for reading).\n\nBut it sounds like you're proposing making a strong decoupling of\nterms index from terms dict?\n\nbq. Then we can have different terms index implementations in the future, e.g. one that uses a tree.\n\n+1\n\nOr, an FST.  FST is more compelling than tree since it also compresses\nsuffixes.  FST is simply a tree in the front plus a tree in the back\n(in reverse), where the \"output\" of a given term's details appears in\nthe middle, on an edge that is \"unique\" to each term, as you traverse\nthe graph.\n\nbq. We could also make SegmentReader a bit cleaner: if opened just for merging it would not create a terms index reader at all; only if cloned for an external reader we would instantiate the terms index lazily. Currently this is done by setting the divisor to -1.\n\nRight.  Somehow we should genericize the \"I don't need the terms\nindex at all\" when opening a SegmentReader.  Passing -1 is sort of\nhackish.  Though I do prefer passing up front your intentions, rather\nthan loading lazily (LUCENE-1609).\n\nWe could eg pass \"requirements\" when asking the codec for the terms\ndict reader.  EG if I don't state that RANDOM_ACCESS is required (and\nonly say LINEAR_SCAN) then the codec internally can make itself more\nefficient based on that.\n\nbq. In the current patch the choice of the Codec is index-wide, right? So I can't specify different codecs for different fields. Please correct me if I'm wrong.\n\nThe Codec is indeed index-wide, however, because the field vs term\ntext are strongly separated, it's completely within a Codec's control\nto return a different reader/writer for different fields.  So it ought\nto work fine... eg one in theory could make a \"PerFieldCodecWrapper\".\nBut, I haven't yet tried this with any codecs.  It would make a good\ntest case though... I'll write down to make a test case for this.\n\nAlso, it's fine if an index has used different codecs over time when\nwriting, as long as when reading you provide a PostingsCodecs\ninstance that's able to [correctly] retrieve those codecs to read those\nsegments.\n\n",
            "date": "2009-08-28T09:36:00.582+0000",
            "id": 46
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nBut it sounds like you're proposing making a strong decoupling of\nterms index from terms dict?\n{quote}\n\nRight.\n\n{quote}\nRight. Somehow we should genericize the \"I don't need the terms\nindex at all\" when opening a SegmentReader. Passing -1 is sort of\nhackish. Though I do prefer passing up front your intentions, rather\nthan loading lazily (LUCENE-1609).\n{quote}\n\nI'm a bit confused. Doesn't the IndexWriter open SegmentReaders\nusually with termsIndexDivisor=-1 for merge, and maybe later with\na termsIndexDivisor>0 when IndexWriter#getReader() is called?\nThat's what I meant with loading lazily. \n\nI thought that's why it'd be good to separate the terms index from\nthe terms dict. For merge we'd open the dict reader only, and then\nif getReader() is called we'd open the terms index reader and give\nits reference to the dict reader.\n\nI admit that I didn't follow the NRT changes as closely as I should\nhave, so I might be missing things here.",
            "date": "2009-08-31T08:45:08.199+0000",
            "id": 47
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nThe Codec is indeed index-wide, however, because the field vs term\ntext are strongly separated, it's completely within a Codec's control\nto return a different reader/writer for different fields. So it ought\nto work fine... eg one in theory could make a \"PerFieldCodecWrapper\".\nBut, I haven't yet tried this with any codecs. It would make a good\ntest case though... I'll write down to make a test case for this.\n{quote}\n\nOK I see now. Did you think about possibly extending the field API\nto specify the codec? And then to store the Codec name in the \nfieldinfos (which we'd want to make extensible too, as briefly \ndiscussed in LUCENE-1597) instead of the dictionary?",
            "date": "2009-08-31T08:58:09.393+0000",
            "id": 48
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nI'm a bit confused. Doesn't the IndexWriter open SegmentReaders\nusually with termsIndexDivisor=-1 for merge, and maybe later with\na termsIndexDivisor>0 when IndexWriter#getReader() is called?\nThat's what I meant with loading lazily.\n{quote}\n\nRight, it does.  This is the one case (internal to Lucene, only) where\nloading lazily is still necessary.\n\n{quote}\nI thought that's why it'd be good to separate the terms index from\nthe terms dict. For merge we'd open the dict reader only, and then\nif getReader() is called we'd open the terms index reader and give\nits reference to the dict reader.\n{quote}\n\nOK got it.  I think this makes sense.\n\nThe separation in the current approach is already quite strong, in\nthat the terms dict writer/reader maintains its own String[] indexText\nand long[] indexOffset and then \"defers\" to its child component just\nwhat is stored in each terms dict entry.  So each child can store\nwhatever it wants in the terms dict entry (eg the pulsing codec\ninlines low-freq postings).\n\nIf we make pluggable how the indexText/indexOffset is stored/loaded in\nmemory/used, then we have a stronger separation/pluggability on the\nindex.  EG even before FST for the index we should switch to blocks of\nchar[] instead of separate Strings, for indexText.\n",
            "date": "2009-08-31T09:18:48.349+0000",
            "id": 49
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nEG even before FST for the index we should switch to blocks of\nchar[] instead of separate Strings, for indexText.\n{quote}\n\nI totally agree. I made a similar change (from String objects to \nchar[] blocks) on some other code (not Lucene) and the savings \nin memory and garbage collection were tremendous!",
            "date": "2009-08-31T09:22:41.691+0000",
            "id": 50
        },
        {
            "author": "Michael McCandless",
            "body": "I attached a .tar.bz2 of src/* with my current state -- too hard to\nkeep svn in sync / patchable right now.  Changes:\n\n  * Factored out the terms dict index, so it's now \"pluggable\" (though\n    I've only created one impl, so far)\n\n  * Cutover SegmentMerger to flex API\n\n  * Changed terms to be stored in RAM as byte[] (not char[]), when\n    reading.  These are UTF8 bytes, but in theory eventually we could\n    allow generic bytes here (there are not that many places that try\n    to decode them as UTF8).  I think this is a good step towards\n    allowing generic terms.  It also saves 50% RAM for simple ascii\n    terms w/ the terms index.\n\n  * Changed terms index to use shared byte[] blocks\n\n  * Broke sources out into \"codecs\" subdir of oal.index.  Right now I\n    have \"preflex\" (only provides reader, to read old index format),\n    \"standard\" (new terms dict & index, but otherwise same\n    freq/prox/skip/payloads encoding), \"pulsing\" (inlines low-freq\n    terms directly into terms dict) and \"sep\" (seperately stores docs,\n    frq, prox, skip, payloads, as a pre-cursor to using pfor to encode\n    doc/frq/prox).\n\nThe patch is very rough... core & core-test compile, but most tests\nfail.  It's very much still a work in progress...\n",
            "date": "2009-09-04T00:10:19.857+0000",
            "id": 51
        },
        {
            "author": "Michael McCandless",
            "body": "New patch & src.tar.bz2 attached.  All tests, including back-compat, pass.\n\nThere are still zillions of nocommits to resolve.\n\nSome of the changes:\n\n  - Got all tests to pass.\n\n  - Separated out a non-enum Fields/Terms API.\n\n  - Improved byte[] block allocation in the new terms index so that\n    the blocks are shared across fields (important when there are\n    zillions of fields each of which has few index terms)\n\n  - Changed TermsEnum.docs() API to accept a new bit set interface\n    (currently called Bits) skipDocs.  This is towards eventual\n    support for random access filters.  I also added Bits\n    IndexReader.getDeletedDocs().\n\nNext step is to get the other codecs (sep, pulsing) to pass all tests,\nthen to make a pfor codec!  I also need to perf test all of these\nchanges...\n",
            "date": "2009-09-11T13:49:31.573+0000",
            "id": 52
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. Changed terms to be stored in RAM as byte[] (not char[]),\n\nYay!  This will be important for NumericField too since it uses 7 bits per char and will probably account for the majority of terms in the index in many applications.\n\nbq. I attached a .tar.bz2 of src/* with my current state - too hard to keep svn in sync / patchable right now.\n\nCould a git branch make things easier for mega-features like this?",
            "date": "2009-09-11T16:27:48.024+0000",
            "id": 53
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nbq. Changed terms to be stored in RAM as byte[] (not char[]),\n\nYay! This will be important for NumericField too since it uses 7 bits per char and will probably account for the majority of terms in the index in many applications.\n{quote}\n\nIt's actually byte[] both in how the terms dict index stores the terms\nin RAM (using shared byte[] blocks) and also in how terms are\nrepresented throughout the flex API.  EG TermsEnum API returns\na TermRef from its next() method.  TermRef holds byte[]/offset/length.\n\nbq. Could a git branch make things easier for mega-features like this?\n\nMaybe -- though I don't have much experience w/ git.  If people are\ninterested in working together on this then I think it'd be worth\nexploring?\n",
            "date": "2009-09-11T17:45:39.019+0000",
            "id": 54
        },
        {
            "author": "Michael McCandless",
            "body": "Attached patch.\n\nAll tests pass with all 3 codecs (standard = just like today's index format; pulsing = terms that occur in only 1 doc are inlined into terms dict; sep = separate files for doc, freq, prx, payload, skip data).",
            "date": "2009-09-12T16:41:41.761+0000",
            "id": 55
        },
        {
            "author": "Jason Rutherglen",
            "body": "Mike,\n\nMaybe a directed acyclic word graph would work well as an alternative dictionary implementation?  ",
            "date": "2009-09-14T00:17:43.549+0000",
            "id": 56
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Maybe a directed acyclic word graph would work well as an alternative dictionary implementation?\n\nI think that'd be great.  In particular, an FST (DAG that shares prefix & suffix and \"outputs\" the per-term data in the middle of the graph) should be a good savings in most normal term distributions.\n\nFlexible indexing makes the terms dict & terms dict index pluggable, so we are free to experiment with alternative impls.  I've only taken some baby steps to improve on the current terms dict index (by switching to shared byte[] blocks, instead of a separate TermInfo / String instance per indexed term).",
            "date": "2009-09-14T09:24:08.921+0000",
            "id": 57
        },
        {
            "author": "Michael McCandless",
            "body": "New patch attached.  All tests pass.\n\nI haven't quite made it to PForDelta yet, but it's very close!\n\nThe sep codec was the first step (uses separate files for doc, frq,\npos, payload, skip).\n\nThen, in this patch, the big change was to create new\nIntIndexInput/Output abstract classes, that only expose reading &\nwriting ints.  I then fixed the sep codec to use this class for doc,\nfrq and pos files.\n\nThe trickiest part was abstracting away just what a \"file pointer\"\nis.  In Lucene we assume in many places this is the long file offset,\nbut I needed to change this to file-offset plus within-block-offset,\nfor int-block based files.\n\nOnce I did that, I created a FixedIntBlockIndexInput/Output, which\nreads & writes the ints in blocks of a specified size.  They are\nabstract classes and require a subclass to do the actual encode/decode\nof a given block.  To test it I created a simple class that just\nwrites multiple vInts.  All tests also pass with this newly added\n(\"intblock\") codec.\n\nSo the next step is to hook up PforDelta...\n",
            "date": "2009-09-23T17:10:59.155+0000",
            "id": 58
        },
        {
            "author": "John Wang",
            "body": "This is awesome!\nFeel free to take code from Kamikaze for the p4delta stuff.\nThe impl in Kamikaze assumes no decompression at load time, e.g. the Docset can be traversed in compressed form.",
            "date": "2009-09-24T01:57:47.555+0000",
            "id": 59
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nFeel free to take code from Kamikaze for the p4delta stuff.\nThe impl in Kamikaze assumes no decompression at load time, e.g. the Docset can be traversed in compressed form.\n{quote}\n\nThanks John.  I've been starting with LUCENE-1410 for now, but we can easily swap in any p4 impl, or any other int compression method.  All that's needed in the subclass is to implement encodeBlock (in the writer) and decodeBlock (in the reader).  The intblock codec takes care of the rest.\n\nKamikaze looks like great stuff!\n\nWhat variation on p4 is Kamikaze using?\n\nKeeping the p4 data compressed is interesting... when you implement AND/OR/NOT on p4, do you have a shortcut that traverses the compressed form while applying the operator?  Or do you do the full decode and then 2nd pass to apply the operator?",
            "date": "2009-09-24T10:11:23.507+0000",
            "id": 60
        },
        {
            "author": "John Wang",
            "body": "Hi Mike:\n\n     We have been using Kamikaze in our social graph engine in addition to our search system. A person's network can be rather large, decompressing it in memory some network operation is not feasible for us, hence we made the requirement for the DocIdSetIterator to be able to walk to DocIdSet's P4Delta implementation in compressed form.\n\n     We do not decode the p4delta set and make a second pass for boolean set operations, we cannot afford it in both memory cost and latency. The P4Delta set adheres to the DocIdSet/Iterator api, and the And/Or/Not is performed on that level of abstraction using next() and skipTo methods.\n\n-John\n",
            "date": "2009-09-24T13:09:08.094+0000",
            "id": 61
        },
        {
            "author": "John Wang",
            "body": "Just a FYI: Kamikaze was originally started as our sandbox for Lucene contributions until 2.4 is ready. (we needed the DocIdSet/Iterator abstraction that was migrated from Solr) \n\nIt has three components:\n\n1) P4Delta\n2) Logical boolean operations on DocIdSet/Iterators (I have created a jira ticket and a patch for Lucene awhile ago with performance numbers. It is significantly faster than DisjunctionScorer)\n3) algorithm to determine which DocIdSet implementations to use given some parameters, e.g. miniD,maxid,id count etc. It learns and adjust from the application behavior if not all parameters are given.\n\nSo please feel free to incorporate anything you see if or move it to contrib.\n",
            "date": "2009-09-24T13:13:45.487+0000",
            "id": 62
        },
        {
            "author": "John Wang",
            "body": "Hi Uwe:\n\n     Thanks for the pointer to the isCacheable method. We will defn incorporate it.\n\n-John\n",
            "date": "2009-09-24T13:28:57.564+0000",
            "id": 63
        },
        {
            "author": "Michael McCandless",
            "body": "Attached patch.  This includes the pfor impl from LUCENE-1410.\n\nPforDelta is working!  I added another codec (pfordelta).  It uses the\nsep codec to separately store freq, doc, pos, and then uses PforDelta\nto encode the ints (as fixed-size blocks).\n\nHowever, there are a couple test failures\n(TestIndexWriter.testNegativePositions,\nTestPositionIncrement.testPayloadsPos0) due to PforDelta not properly\nencoding -1 (it's returned as 255).  Lucene normally doesn't write\nnegative ints, except for the special case of a 0 position increment\nin the initial token(s), in which case due to the bug in LUCENE-1542\nwe write a -1 if you've called IndexWriter.setAllowMinus1Position.\nHowever, that's deprecated and will be removed shortly at which point\nthe pfordelta codec will pass all tests.\n",
            "date": "2009-09-25T17:58:47.419+0000",
            "id": 64
        },
        {
            "author": "Michael McCandless",
            "body": "I wrote up first cut of the toplevel design of this patch, in the wiki: http://wiki.apache.org/lucene-java/FlexibleIndexing.",
            "date": "2009-09-26T12:52:31.395+0000",
            "id": 65
        },
        {
            "author": "John Wang",
            "body": "Hi Mike:\n\n     Truly awesome work!\n\n     Quick question, are codecs per index or per field? From the wiki, it seems to be per index, if so, is it possible to make it per field?\n\nThanks\n\n-John",
            "date": "2009-09-26T15:32:49.827+0000",
            "id": 66
        },
        {
            "author": "Michael McCandless",
            "body": "The codec is per segment.  However, we ask the codec for\nTerms/TermsEnum by fields, so it should be simple to make a Codec that\ndispatches to field-specific Codecs.\n",
            "date": "2009-09-27T14:19:38.663+0000",
            "id": 67
        },
        {
            "author": "Michael McCandless",
            "body": "Attached current patch.  All tests pass:\n\n  * Cutover merging to flex API.\n\n  * Cutover FieldCache to flex API.  This got tricky, because terms\n    are now UTF8 byte[].  First, we have a back-compat issue (I\n    changed FieldCache's parsers to take TermRef not String).  Second,\n    parsing float/double from byte[] is tricky.  I just punted and\n    made a new String(), and then called parseDouble/parseFloat, which\n    is slow (but, NumericFields don't do this -- they are easy to\n    parse straight from byte[], I think).  Net/net this should be\n    faster loading the FieldCache now.  Also, later we can make a\n    String/StringIndex FieldCache variant that keeps things as byte[].\n\n  * Cutover CheckIndex to flex API.\n\n  * Removed the codec-owned extensions from IndexFileNames; added\n    methods to quey a Codec for all file extensions it may write.  As\n    part of this there is a minor (I think) runtime change whereby\n    Directory.copy or new RamDirectory(Directory) will now copy all\n    files not just index-related files.\n\nI'm now working towards getting this committable.  While PforDelta\nworks, I think we should move its codec over to LUCENE-1410 and get it\nworking well, separately, after this is committed.\n\nStill need to cutover more stuff (queries, AllTermDocs, etc.) to flex\nAPI, get the ThreadLocal cache carried over, fix a bunch of nocommits,\nremove debugging, do perf testing & fix issues, add some more tests,\netc.\n",
            "date": "2009-10-02T00:32:58.675+0000",
            "id": 68
        },
        {
            "author": "Michael McCandless",
            "body": "New patch attached.  All tests pass.  The changes are mostly cutting\nmany things over to the flex API.  Still many nocommits to address,\nbut I'm getting closer!\n\nI haven't \"svn up\"d to all the recent the deprecations removals /\ngenerics additions.  Kinda dreading doing so :) I think I'll wait\nuntil all deprecations are gone and then bite the bullet...\n\nCutting over all the MultiTermQuery subclasses was nice because all\nthe places where we get a TermEnum & iterate, checking if .field() is\nstill our field, are now cleaner because with the flex API the\nTermsEnum you get is already only for your requested field.\n",
            "date": "2009-10-05T12:19:44.217+0000",
            "id": 69
        },
        {
            "author": "Yonik Seeley",
            "body": "Sounding cool!  I haven't had time to look at the code too much... but I j ust wanted to mention two features I've had in the back of my mind for a while that seem to have multiple use cases.\n\n1) How many terms in a field?\n- If the tii/TermInfos were exposed, this could be estimated.\n- Perhaps this could just be stored in FieldInfos... should be easy to track during indexing?\n- MultiTermQuery could also use this to switch impls\n\n2) Convert back and forth between a term number and a term.\nSolr has code to do this... stores every 128th term in memory as an index, and uses that to convert back and forth.  This is very much like the internals of TermInfos... would be nice to expose some of that.",
            "date": "2009-10-05T14:59:38.129+0000",
            "id": 70
        },
        {
            "author": "John Wang",
            "body": "Hi Yonik:\n\n     These are indeed useful features. LUCENE-1922 addresses 1), perhaps, we can add 2) to the same issue to track?\n\nThanks\n\n-John",
            "date": "2009-10-05T15:48:40.585+0000",
            "id": 71
        },
        {
            "author": "Michael McCandless",
            "body": "bq. 1) How many terms in a field?\n\nActually I've already added this one (Terms.getUniqueTermCount), but I\ndidn't punch it through to IndexReader.  I'll do that.  The standard\ncodec (new \"default\" codec when writing segments) already records this\nper field, so it's trivial to expose.\n\nHowever, some impls may throw UOE (eg a composite IndexReader).\n\nbq. 2) Convert back and forth between a term number and a term.\n\nI agree this would be useful.  I did have ord() in early iterations of\nthe TermsEnum API, but it wasn't fully implemented and I stripped it\nwhen I switched to \"just finish it already\" mode :) We could think\nabout adding it back, though you'd also presumably need seek(int ord)\nas well?  (And docFreq(String field, int ord) sugar exposed in\nIndexReader?).\n",
            "date": "2009-10-05T18:05:02.343+0000",
            "id": 72
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. I agree this would be useful. I did have ord() in early iterations of the TermsEnum API, but it wasn't fully implemented and I stripped it when I switched to \"just finish it already\" mode\n\nA \"complete\" implementation seems hard (i.e. across multiple segments also)... but it still seems useful even if it's only at the segment level.  So perhaps just on SegmentTermEnum, and uses would have to cast to access?\n\nExposing the term index array (i.e. every 128th term) as an expert-subject-to-change warning would let people implement variants themselves at least.\n\nbq. you'd also presumably need seek(int ord)\n\nYep.",
            "date": "2009-10-05T18:27:50.569+0000",
            "id": 73
        },
        {
            "author": "Grant Ingersoll",
            "body": "I haven't followed too closely (even though it is one of my favorite issues) but I figured while Yonik was throwing out ideas, I'd add that one of the obvious use cases for flexible indexing is altering scoring.  One of the common statistics one needs to implement some more advanced scoring approaches is the average document length.  Is this patch far enough along that I could take a look at it and think about how one might do this?",
            "date": "2009-10-05T21:59:13.620+0000",
            "id": 74
        },
        {
            "author": "Mark Miller",
            "body": "bq. I haven't \"svn up\"d to all the recent the deprecations removals / generics additions. Kinda dreading doing so  :)'\n\nCome on old man, stop clinging to emacs ;) I've got a meditation technique for that :)\n\nSounds like some annoyance, and I think I made a comment there - and I'm a man of my word... or child of my word - take your pick.\n\nTo trunk. Since you likely have moved on, don't worry - this was good practice - I'll do it again sometime if you'd like. I may have mis merged something little or something. I went fairly quick (I think it took like 30 or 40 min - was hoping to do it faster, but eh - sometimes I like to grind).\n\nI didn't really look at the code, but some stuff I noticed:\n\njava 6 in pfor Arrays.copy\n\nskiplist stuff in codecs still have package of index - not sure what is going on there - changed them\n\nin IndexWriter: \n+          // Mark: read twice?\n           segmentInfos.read(directory);\n+        segmentInfos.read(directory, codecs);\n\nCore tests pass, but I didn't wait for contrib or back compat.",
            "date": "2009-10-05T23:58:39.030+0000",
            "id": 75
        },
        {
            "author": "Mark Miller",
            "body": "eh - even if you have moved on, if I'm going to put up a patch, might as well do it right - here is another:\n\n* removed a boatload of unused imports\n* removed DefaultSkipListWriter/Reader - I accidently put them back in\n* removed an unused field or two (not all)\n* paramaterized LegacySegmentMergeQueue.java\n* Fixed the double read I mentioned in previous comment in IndexWriter\n* TermRef defines an equals (that throws UOE) and not hashCode - early stuff I guess but odd since no class extends it. Added a hashCode that throws UOE anyway.\n* fixed bug in TermRangeTermsEnum: lowerTermRef = new TermRef(lowerTermText); to lowerTermRef = new TermRef(this.lowerTermText);\n* Fixed Remote contrib test to work with TermRef for fieldcache parser (since you don't include contrib in the tar)\n* Missed a StringBuffer to StringBuilder in MultiTermQuery.toString\n* had missed removing deprecated IndexReader.open(final Directory directory) and deprecated IndexReader.open(final IndexCommit commit)\n* Paramertized some stuff in ParrallelReader that made sense - what the heck\n* added a nocommit or two on unread fields with a comment that made it look like they were/will be used\n* Looks like SegmentTermPositions.java may have been screwy in last patch - ensure its now a deleted file - same with TermInfosWriter.java\n* You left getEnum(IndexReader reader) in the MultiTerm queries, but no in PrefixQuery - just checkin'.\n* Missed removing listAll from FileSwitchDirectory - gone\n* cleaned up some white space nothings in the patch\n* I guess TestBackwardsCompatibility.java has been removed from trunk or something? kept it here for now.\n* looks like i missed merging in a change to TestIndexWriter.java#assertNoUnreferencedFiles - done\n* doubled checked my merge work\n\ncore and contrib tests pass\n\n\n",
            "date": "2009-10-06T04:06:49.179+0000",
            "id": 76
        },
        {
            "author": "Michael McCandless",
            "body": "Whoa thanks for the sudden sprint Mark!\n\nbq. Come on old man, stop clinging to emacs\n\nHey!  I'm not so old :) But yeah I still cling to emacs.  Hey, I know\npeople who still cling to vi!\n\n{quote}\nI didn't really look at the code, but some stuff I noticed:\n\njava 6 in pfor Arrays.copy\n\nskiplist stuff in codecs still have package of index - not sure what is going on there - changed them\n\nin IndexWriter: \n+ // Mark: read twice?\nsegmentInfos.read(directory);\n+ segmentInfos.read(directory, codecs);\n{quote}\n\nExcellent catches!  All of these are not right.\n\nbq. (since you don't include contrib in the tar)\n\nGak, sorry.  I have a bunch of mods there, cutting over to flex API.\n\nbq. You left getEnum(IndexReader reader) in the MultiTerm queries, but no in PrefixQuery - just checkin'.\n\nWoops, for back compat I think we need to leave it in (it's a\nprotected method), deprecated.  I'll put it back if you haven't.\n\nbq. I guess TestBackwardsCompatibility.java has been removed from trunk or something? kept it here for now.\n\nEek, it shouldn't be -- indeed it is.  When did that happen?  We\nshould fix this (separately from this issue!).\n\nDo you have more fixes coming?  If so, I'll let you sprint some more; else, I'll merge in, add contrib & back-compat branch, and post new patch!  Thanks :)\n",
            "date": "2009-10-06T09:53:48.083+0000",
            "id": 77
        },
        {
            "author": "Michael McCandless",
            "body": "bq.  One of the common statistics one needs to implement some more advanced scoring approaches is the average document length. Is this patch far enough along that I could take a look at it and think about how one might do this?\n\nWell, thinking through how you'd do this... likely you'd want to store\nthe avg length (in tokens), eg as a single float per field per\nsegment, right?  The natural place to store this would be in the\nFieldInfos, I think?.  Unfortunately, this patch doesn't yet add\nextensibility to FieldInfos.\n\nAnd you'd need a small customization to the indexing chain to\ncompute this when indexing new docs, which is already doable today\n(though, package private).\n\nBut then on merging segments, you'd need an extensions point, which we\ndon't have today, to recompute the avg.  Hmm: how would you handle\ndeleted docs?  Would you want to go back to the field length for every\ndoc & recompute the average?  (Which'd mean you need to per doc per\nfield length, not just the averages).\n\nUnfortunately, this patch doesn't yet address things like customizing\nwhat's stored in FieldInfo or SegmentInfo, nor customizing what\nhappens during merging (though it takes us a big step closer to this).\nI think we need both of these to \"finish\" flexible indexing, but I'm\nthinking at this point that these should really be tackled in followon\nissue(s).  This issue is already ridiculously massive.\n",
            "date": "2009-10-06T10:08:55.631+0000",
            "id": 78
        },
        {
            "author": "Uwe Schindler",
            "body": "{quote}\nbq. I guess TestBackwardsCompatibility.java has been removed from trunk or something? kept it here for now.\n\nEek, it shouldn't be - indeed it is. When did that happen? We\nshould fix this (separately from this issue!).\n{quote}\n\nMy fault, I removed it during the remove backwards tests on Saturday. If we do not remove DateTools/DateField for 3.0 (we may need to leave it in for index compatibility), I will restore, these tests, too. It's easy with TortoiseSVN and you can also preserve the history (using svn:mergeinfo prop).\n\nI have this on my list when going forward with removing the old TokenStream API.",
            "date": "2009-10-06T11:43:04.182+0000",
            "id": 79
        },
        {
            "author": "Michael McCandless",
            "body": "bq. It's easy with TortoiseSVN and you can also preserve the history (using svn:mergeinfo prop).\n\nAhh -- can you do this for TestBackwardsCompatibility?  I restored it, but, lost all history.  Thanks.",
            "date": "2009-10-06T12:01:35.670+0000",
            "id": 80
        },
        {
            "author": "Uwe Schindler",
            "body": "Done. I also did it for the BW branch, but didn't create a tag yet. The next tag creation for the next bigger patch is enough (no need to do it now).\n\nWhat I have done: svn copy from the older revision to the same path :-)",
            "date": "2009-10-06T12:31:46.820+0000",
            "id": 81
        },
        {
            "author": "Michael McCandless",
            "body": "bq. What I have done: svn copy from the older revision to the same path\n\nExcellent, thanks!  It had a few problems (was still trying to deprecated APIs, some of which were gone) -- I just committed fixes.",
            "date": "2009-10-06T14:14:28.929+0000",
            "id": 82
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. likely you'd want to store the avg length (in tokens), eg as a single float per field per segment, right?\n\nI think we might want to store fundamentals instead:\n - total number of tokens indexed for that field in the entire segment\n - total number of documents that contain the field in the entire segment\n\nBoth of these seem really easy to keep track of?\nI also think we'd just ignore deleted docs (i.e. don't change the stats) just as idf does today.\n\nbq. The natural place to store this would be in the FieldInfos, I think?\n\nyep.\n\n",
            "date": "2009-10-06T14:26:27.993+0000",
            "id": 83
        },
        {
            "author": "Michael McCandless",
            "body": "Uber-patch attached: started from Mark's patch (thanks!), added my contrib & back-compat branch changes.  All tests pass.\n\nAlso, I removed pfor from this issue.  I'll attach the pfor codec to LUCENE-1410.\n\nNote that I didn't use \"svn move\" in generating the patch, so that the patch can be applied cleanly.  When it [finally] comes time to commit for real, I'll svn move so we preserve history.",
            "date": "2009-10-06T15:06:34.236+0000",
            "id": 84
        },
        {
            "author": "Mark Miller",
            "body": "bq. Hey! I'm not so old  But yeah I still cling to emacs. \n\ncan you say both of those things in the same breath? Just how long did it take to get that phd...\n\nI'd look it up and guestimate your age, but I think MIT still has my ip blocked from back when I was applying to colleges. So I'm going with the \"uses emacs\" guestimate.\n\nbq. Hey, I know people who still cling to vi!\n\nvi is the only one I can half way use - I know 3 commands - edit mode, leave edit mode, and save. And every now and then I accidently delete a whole line. When I make a change that I don't want to save, I have to kill the power.\n\nThe patch is in a bit of an unpatchable state ;) I think I know what editor to blame...Pico!\n\nOur old friend, the $id is messing up WildcardTermEnum - no problem, I can fix that...\n\nBut also, NumericUtils is unpatched, Codec is missing, along with most of the classes from the codecs packages! This looks like my work :)\n\nMy only conclusion is that your one of those guys that can write the whole program once without even running it - and then it works perfectly on the first go. Thats the only way I can explain those classes in the wrong package previously as well :) No bug hunting tonight :(",
            "date": "2009-10-07T01:01:20.695+0000",
            "id": 85
        },
        {
            "author": "Mark Miller",
            "body": "nope - something else - looking through the patch I see the files I want - a second attempt at patching has gone over better.\n\nA couple errors still, but stuff I think I can fix so that I can at least look over. False alarm. My patcher wonked out or something. I can resolve the few errors that popped up this time. Sweet.\n\n*edit*\n\nJust for reference - not sure what happened the first time - my patch preview looked the same both times (was only complaining about the $id), but completely failed on attempt one and worked on attempt two - the only issue now appears to be you have half switch deletedDocs to Bits from BitVector - but only have way, so its broken in a dozen places. Not sure what you are doing about size() and what not, so I'm just gonna read around.\n\n*edit*\n\nYes - I found it - BitVector was supposed to implement Bits - which was in the patch ... this patch just did not want to apply. I guess it was right, but Eclipse just did not want it to take ...",
            "date": "2009-10-07T01:15:32.405+0000",
            "id": 86
        },
        {
            "author": "Mark Miller",
            "body": "Bah - all this huffing an puffing over the patch and I'm too sick to stay up late anyway.\n\nHave you started benching at all? I'm seeing like a 40-50% drop in same reader search benches with standard, sep, and pulsing. Like 80% with intblock.",
            "date": "2009-10-07T02:02:55.459+0000",
            "id": 87
        },
        {
            "author": "Michael McCandless",
            "body": "Mark is there anything wrong w/ the patch?  Did you get it working?\n\nbq. Have you started benching at all? I'm seeing like a 40-50% drop in same reader search benches with standard, sep, and pulsing. Like 80% with intblock.\n\nI haven't but it sounds like you have!  I'll get to it soon... but one thing I know is missing is the equivalent of the \"terminfo cache\" so that when a query 1) looks up docFreq of the term (to compute its weight), and 2) looks up the freq/prox offsets, that 2nd lookup is cached.\n\nIntBlock is expected to be slow -- it naively encodes one int at a time using vInt.  Ie, it's just a \"test\" codec, meant to be the base for real block-based codecs like pfor.\n",
            "date": "2009-10-07T09:28:10.997+0000",
            "id": 88
        },
        {
            "author": "Mark Miller",
            "body": "bq. Mark is there anything wrong w/ the patch? Did you get it working?\n\nI got it working - it didn't apply cleanly, but perhaps that was just me. It was a weird situation - I get a preview of whats going to happen with complaints, and it only complained about the $id issue in wildcardtermenum - the half the patch failed. A second attempt and it only complained about that again - but then it missed making BitVector implement Bits - could just be ghosts in my machine. I wouldn't worry about it till someone else complains. In any case, I got it working in my case by just fixing the $id issue and adding implements Bits to BitVector.",
            "date": "2009-10-07T12:01:13.437+0000",
            "id": 89
        },
        {
            "author": "Mark Miller",
            "body": "bq. I haven't but it sounds like you have!\n\nNothing serious ;) Just began trying to understand the code a bit more, so started with playing around with the different Codecs. Which lead to just quickly trying out the micro bench with each of em.",
            "date": "2009-10-07T15:30:39.496+0000",
            "id": 90
        },
        {
            "author": "Michael McCandless",
            "body": "New patch attached.  All tests pass.\n\nI simplified the TermsEnum.seek API, and added ord to the API.  The\nord is a long, but the standard codec (and, I think, Lucene today)\ninternally use an int...\n",
            "date": "2009-10-07T16:01:50.835+0000",
            "id": 91
        },
        {
            "author": "Yonik Seeley",
            "body": "Another for theTermsEnum wishlist: the ability to seek to the term *before* the given term... useful for finding the largest value in a field, etc.\n\nI imagine \"at or before\" semantics would also work (like the current semantics of TermEnum in reverse)",
            "date": "2009-10-08T21:28:59.057+0000",
            "id": 92
        },
        {
            "author": "Mark Miller",
            "body": "Okay, I just tried a toy cache with standard - its not perfect because the tests have a bunch that end up finding one doc short, and I don't turn off the cache for any reason (the old one terns it off when returning the segmenttermenum, but I didn't even try to understand that with the new stuff). But that appears to get the majority of the perf back. Went from about 3500 r/s to 7500 - the old is 8400.\n\nThis stuff is so cool by the way.\n\n*edit*\n\nwhew - emphasis on toy - its hard to do this right with docsreader ",
            "date": "2009-10-08T21:53:31.297+0000",
            "id": 93
        },
        {
            "author": "Michael McCandless",
            "body": "bq. its hard to do this right with docsreader\n\nI was thinking something along the lines of adding a \"captureState\" to DocsProducer.Reader, that returns an opaque object, and then adding a corresponding seek that accepts that object.  It would chain to the positions reader.\n\nThen StandardTermsDictReader would hold the thread private cache, using this API.",
            "date": "2009-10-09T00:06:31.463+0000",
            "id": 94
        },
        {
            "author": "Mark Miller",
            "body": "Well thats reassuring - I think I was on the right path then. I've got the thread private cache, and I was initially just capturing in's position so I could set it before calling readTerm after pulling from the cache - so I knew I had an issue with the positions reader in there too (the position of it in readTerm) - but didn't see the cleanest path to set and capture that without modifying the reader like you said - but I wasn't even sure I was on the right path, so thats about where I gave up :)\n\nYour comment makes me feel a little less dumb about it all though.",
            "date": "2009-10-09T00:47:15.306+0000",
            "id": 95
        },
        {
            "author": "Michael McCandless",
            "body": "No problem :)  Please post the patch once you have it working!  We'll need to implement captureState/seek for the other codes too.  The pulsing case will be interesting since it's state will hold the actual postings for the low freq case.\n\nBTW I think an interesting codec would be one that pre-loads postings into RAM, storing them uncompressed (eg docs/positions as simple int[]) or slightly compressed (stored as packed bits).  This should be a massive performance win at the expense of sizable RAM consumption, ie it makes the same tradeoff as contrib/memory and contrib/instantiated.",
            "date": "2009-10-09T09:54:23.041+0000",
            "id": 96
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nAnother for theTermsEnum wishlist: the ability to seek to the term before the given term... useful for finding the largest value in a field, etc.\nI imagine \"at or before\" semantics would also work (like the current semantics of TermEnum in reverse)\n{quote}\n\nRight now seek(TermRef seekTerm) stops at the earliest term that's >=\nseekTerm.\n\nIt sounds like you're asking for a variant of seek that'd stop at the\nlatest term that's <= seekTerm?\n\nHow would you use this to seek to the last term in a field?  With the\nflex API, the TermsEnum only works with a single field's terms.  So I\nguess we'd need TermRef constants, eg TermRef.FIRST and TermRef.LAST,\nthat \"act like\" -infinity / +infinity.\n",
            "date": "2009-10-09T10:26:53.946+0000",
            "id": 97
        },
        {
            "author": "Michael McCandless",
            "body": "Actually, FIRST/LAST could be achieved with seek-by-ord (plus getUniqueTermCount()).  Though that'd only work for TermsEnum impls that support ords.",
            "date": "2009-10-09T12:07:16.020+0000",
            "id": 98
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. How would you use this to seek to the last term in a field?\n\nIt's not just last in a field, since one may be looking for last out of any given term range (the highest value of a trie int is not the last value encoded in that field).\nSo if you had a trie based field, one would find the highest value via seekAtOrBefore(triecoded(MAXINT))\n\nbq. Actually, FIRST/LAST could be achieved with seek-by-ord (plus getUniqueTermCount()).\n\nAhhh... right, prev could be implemented like so:\n\nint ord = seek(triecoded(MAXINT))).ord\nseek(ord-1)\n\nbq. Though that'd only work for TermsEnum impls that support ords. \n\nAs long as ord is supported at the segment level, it's doable.",
            "date": "2009-10-09T13:39:24.117+0000",
            "id": 99
        },
        {
            "author": "Mark Miller",
            "body": "hmm - I think I'm close. Everything passes except for omitTermsTest, LazyProxTest, and for some odd reason the multi term tests. Getting close though.\n\nMy main concern at the moment is the state capturing. It seems I have to capture the state before readTerm in next() - but I might not use that state if there are multiple next calls before the hit. So thats a lot of wasted capturing. Have to deal with that somehow.\n\nDoing things more correctly like this, the gain is much less significant. What really worries me is that my hack test was still slower than the old - and that skipped a bunch of necessary work, so its almost a better than best case here - I think you might need more gains elsewhere to get back up to speed.\n\n*edit*\n\nHmm - still no equivalent of the cached enum for one I guess.\nAnd at the least, since you only cache when the scan is great than one, you can at least skip one capture there...",
            "date": "2009-10-09T15:24:34.330+0000",
            "id": 100
        },
        {
            "author": "Michael McCandless",
            "body": "bq. It seems I have to capture the state before readTerm in next() \n\nWait, how come?  It seems like we should only cache if we find exactly the requested term (ie, where we return SeekStatus.FOUND)?  So you should only have to capture the state once, there?\n\nHmm I wonder whether we should also cache the seek(ord) calls?",
            "date": "2009-10-09T15:54:23.366+0000",
            "id": 101
        },
        {
            "author": "Mark Miller",
            "body": "Hmm - I must have something off then. I've never been into this stuff much before.\n\non a cache hit, I'm still calling docs.readTerm(entry.freq, entry.isIndex) - I'm just caching the freq, isIndex, and the positions with a CurrentState object. The captureCurrentState now telescopes down capturing the state of each object \n\nPerhaps I'm off there - because if I do that, it seems I have to capture the state right before the call to readTerm in next() - otherwise readTerm will move everything forward before I can grab it when I actually put the state into the cache - when its FOUND.\n\nI may be all wet though - no worries - I'm really just playing around trying to learn some of this - only way I learn to is to code.\n\nbq. Hmm I wonder whether we should also cache the seek(ord) calls?\n\nI was wondering about that, but hand't even got to thinking about it :)",
            "date": "2009-10-09T16:06:31.884+0000",
            "id": 102
        },
        {
            "author": "Michael Busch",
            "body": "I added this cache originally because it seemed the easiest to improve the term lookup performance. \n\nNow we're adding the burden of implementing such a cache to every codec, right? Maybe instead we should improve the search runtime to not call idf() twice for every term?",
            "date": "2009-10-09T16:24:47.121+0000",
            "id": 103
        },
        {
            "author": "Michael McCandless",
            "body": "bq. on a cache hit, I'm still calling docs.readTerm(entry.freq, entry.isIndex) \n\nHmm... I think your cache might be one level too low?  I think we want the cache to live in StandardTermsDictReader.  Only the seek(TermRef) method interacts with the cache for now (until we maybe add ord as well).\n\nSo, seek first checks if that term is in cache, and if so pulls the opaque state and asks the docsReader to restore to that state.  Else, it does the normal seek, but then if the exact term is found, it calls docsReader.captureState and stores it in the cache.\n\nMake sure the cache lives high enough to be shared by different TermsEnum instances.  I think it should probably live in StandardTermsDictReader.FieldReader.  There is one instance of that per field.",
            "date": "2009-10-09T16:25:23.836+0000",
            "id": 104
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Now we're adding the burden of implementing such a cache to every codec, right?\n\nI suspect most codecs will reuse the StandardTermsDictReader, ie, they will usually only change the docs/positions/payloads format.  So each codec will only have to implement capture/restoreState.\n\nbq. Maybe instead we should improve the search runtime to not call idf() twice for every term?\n\nOh I didn't realize we call idf() twice per term -- we should separately just fix that.  Where are we doing that?\n\n(I thought the two calls were first for idf() and then 2nd when it's time to get the actual TermDocs/Positions to step through).\n",
            "date": "2009-10-09T16:30:14.815+0000",
            "id": 105
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nOh I didn't realize we call idf() twice per term\n{quote}\n\nHmm I take that back. I looked in LUCENE-1195 again:\n\n{quote}\nCurrently we have a bottleneck for multi-term queries: the dictionary lookup is being done\ntwice for each term. The first time in Similarity.idf(), where searcher.docFreq() is called.\nThe second time when the posting list is opened (TermDocs or TermPositions). \n{quote}\n\nHmm something's wrong with my memory this morning! Maybe the lack of caffeine :)",
            "date": "2009-10-09T16:41:08.831+0000",
            "id": 106
        },
        {
            "author": "Mark Miller",
            "body": "Ah - okay - that helps. I think the cache itself is currently around the right level (StandardTermsDictReader, and it gets hit pretty hard), but I thought it was funky I still had to make that read call - I think I see how it should work without that now, but just queuing up the docsReader to where it should be correctly. We will see. Vacation till Tuesday - don't let me stop you from doing it correctly if its on your timeline. Just playing over here - and I don't have a lot of time to play really.",
            "date": "2009-10-09T17:40:41.374+0000",
            "id": 107
        },
        {
            "author": "Michael McCandless",
            "body": "New patch attached.  All tests pass.\n\nA few small changes (eg sync'd to trunk) but the biggest change is a\nnew test case (TestExternalCodecs) that contains two new codecs:\n\n  * RAMOnlyCodec -- like instantiated, it writes and reads all\n    postings into RAM in dedicated classes\n\n  * PerFieldCodecWrapper -- dispatches by field name to different\n    codecs (this was asked about a couple times)\n\nThe test indexes one field using the standard codec, and the other\nusing the RAMOnlyCodec.  It also verifies one can in fact make a\ncustom codec external to oal.index.\n",
            "date": "2009-10-09T22:46:52.971+0000",
            "id": 108
        },
        {
            "author": "Mark Miller",
            "body": "Okay, after all that poking around in the dark, tonight I decided to actually try turning on the DEBUG stuff you have and figuring out how things actually work ;) Always too lazy to open that instruction manual till I've wasted plenty of time spinning in circles.\n\nSo I've got it working -\n\nWhen it was working like 99% I benched the speed at 6300-6500 r/s with the samerdr bench as compared to 9500-11000 with the trunk version I had checked out.\n\nBut that last 1% meant adding two TermRef clones, and that dropped things to about 5800 or so.\n\nI'm sure I might have a few wasteful instructions and/or there can be a little more eeked out, but I think it will still come up short.\n\nI dont see seek(ord) being called using eclipse (other than in tests), but it may be missing it? So I'm not really sure if it needs to be cached or not - no code to test it with at the moment.",
            "date": "2009-10-12T02:34:30.792+0000",
            "id": 109
        },
        {
            "author": "Michael Busch",
            "body": "Shall we create a flexible-indexing branch and commit this? \n\nThe downside of course is that we'd have to commit patches to trunk and this branch until 3.0 is out. Or we could use svn's new branch merging capabilities, which I haven't tried out yet.",
            "date": "2009-10-12T16:19:36.107+0000",
            "id": 110
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Shall we create a flexible-indexing branch and commit this?\n\nI think this is a good idea.\n\nBut I haven't played heavily w/ svn & branching.  EG if we branch now, and trunk moves fast (which it still is w/ deprecation removals), are we going to have conflicts?  Or... is svn good about merging branches?",
            "date": "2009-10-12T19:32:27.155+0000",
            "id": 111
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I dont see seek(ord) being called using eclipse (other than in tests), but it may be missing it?\n\nYeah this won't be used yet -- we only just added it (and only to the flex API).  I guess wait on caching it for now?\n\nbq. I'm sure I might have a few wasteful instructions and/or there can be a little more eeked out, but I think it will still come up short.\n\nOK we've got some work to do :)  Which queries in particular are slower?",
            "date": "2009-10-12T19:34:23.897+0000",
            "id": 112
        },
        {
            "author": "Mark Miller",
            "body": "Havn't gotten that far yet ;) Still just doing quick standard micro benches of each. I think I've got it around 6500 now - perhaps a little higher.\n\nI'll post the patch fairly soon - still struggling merging with your latest and trunk.\n\nI think I've got it all except an issue with one of the contribs - must have gotten a little mis merge. Also your new external codecs test through a monkey wrench in - pulsing isn't setup to work with the cache yet - I'm punting on that for now.",
            "date": "2009-10-12T20:44:30.544+0000",
            "id": 113
        },
        {
            "author": "Michael McCandless",
            "body": "bq. pulsing isn't setup to work with the cache yet - I'm punting on that for now.\n\nOK that's fine for now.  The cache should gracefully handle codecs that don't implement \"captureState\" by simply not caching them.\n",
            "date": "2009-10-12T20:59:07.641+0000",
            "id": 114
        },
        {
            "author": "Mark Miller",
            "body": "Here is my patch. I won't say its 100% polished and done, but I believe its in initial working order. This is a good check point time for me for various reasons.\n\nSimple LRU cache for Standard Codec - meant to replace TermInfo cache.\n\nMerged with latest patch from Mike + to trunk\n\nSome other little random stuff that I remember:\n\nPrefixTermsEnum is deprecated - sees itself - fixed\n\nWildcardQuery should have @see WildcardTermsEnums - fixed\n\nsome stuff in preflex is already deprecated but not all?\n\nStandardDocsReader - freqStart is always 0 - left it in, but doesn't do anything at the moment\n\nbackcompattests missing termref - fixed\n\nnote: currently, with the testThreadSafety test in TestIndexReaderReopen appears to have some Garbage Collection issues with Java6 - not really seeing them with Java5 though - will investigate more.\n\nI've got the latest tag updated too - but there appear to be some odditties with it (unrelated to this patch), so leaving out for now.",
            "date": "2009-10-13T05:54:41.786+0000",
            "id": 115
        },
        {
            "author": "Mark Miller",
            "body": "Latest to trunk - still issues with GC and the reopen thread safety test (unless the test is run in isolation).\n\nMust be a tweak needed, but I'm not sure what. I'm closing the thread locals when the StandardTermsDictReader is closed - I don't see a way to improve on that yet.",
            "date": "2009-10-13T16:11:23.840+0000",
            "id": 116
        },
        {
            "author": "Mark Miller",
            "body": "Whoops - double check the wrong index splitter test - the multi pass one is throwing a null pointer exception for me - don't think its related to this patch, but I havn't checked.\n\n*edit*\n\nOkay, just checked - it is this patch. Looks like perhaps something to do with LegacyFieldsEnum? Something that isnt being hit by core tests at the moment (I didnt run through all the backcompat tests with this yet, since that failed)",
            "date": "2009-10-13T17:00:43.856+0000",
            "id": 117
        },
        {
            "author": "Mark Miller",
            "body": "Looks pretty simple - the field is not getting set with LegacyFieldsEnum.",
            "date": "2009-10-13T19:52:53.594+0000",
            "id": 118
        },
        {
            "author": "Michael McCandless",
            "body": "OK I think I've committed Mark's last patch onto this branch:\n\n  https://svn.apache.org/repos/asf/lucene/java/branches/flex_1458\n\nand I also branched the 2.9 back-compat branch and committed the last back compat patch:\n\n  https://svn.apache.org/repos/asf/lucene/java/branches/flex_1458_2_9_back_compat_tests\n\nMark can you check it out & see if I missed anything?",
            "date": "2009-10-13T21:07:01.527+0000",
            "id": 119
        },
        {
            "author": "Uwe Schindler",
            "body": "By the way, a lot of these PriorityQueues can be generified like in trunk to remove the unneeded casts in lessThan, pop, insert,... everywhere.",
            "date": "2009-10-13T21:22:35.311+0000",
            "id": 120
        },
        {
            "author": "Michael McCandless",
            "body": "I just committed some small improvements to the ThreadLocal cache; all\ntests pass at 512M heap limit again.\n\nI think the reason why TestIndexReaderReopen was hitting the limit is\nbecause its testThreadSafety test opens many (344) IndexReaders at\nonce, without closing them until the very end, and the standard codec\nis now using more starting RAM per reader because 1) the terms index\nuses a fixed minimal block size for the byte[], and 2) the new terms\ninfo cache is less RAM efficient.\n\nI've made some progress to \"scale down\" better:\n\n  * Don't create a 1024 sized cache when total # terms is less than\n    that\n\n  * Cache a single thread-private TermsEnum, to re-use for docFreq\n    lookups\n\n  * Reduced what's stored in each cache entry\n\n  * Made StandardDocsReader subclass CacheEntry to store its own\n    stuff; saves one extra object per entry.\n",
            "date": "2009-10-16T00:08:34.909+0000",
            "id": 121
        },
        {
            "author": "Mark Miller",
            "body": "{quote}// nocommit -- why scanCnt > 1?\n            //if (docs.canCaptureState() && scanCnt > 1) {{quote}\n\nMy mistake - an early mess up when I was copying from preflix caching code - I saw it doing this - but its doing it with the cached enum - I should have been looking below where it doesn't do that. Just a left over from early on when I was kind of shooting in the dark.\n\n*edit*\n\nI also had messed with it a bit - tried 0 and 2 - neither appeared to affect the micro bench samerdrsearch results. Seemed odd. Adding the cache did help those results, so I'd expect that changing that would affect things more.",
            "date": "2009-10-16T00:38:51.033+0000",
            "id": 122
        },
        {
            "author": "Mark Miller",
            "body": "{code}\n    // nocommit -- not needed?  we don't need to sync since\n    // only one thread works with this?\n\n    /*\n    @Override\n    public synchronized Object put(Object key, Object value) {\n      // TODO Auto-generated method stub\n      return super.put(key, value);\n    }\n    \n    @Override\n    public synchronized Object get(Object key) {\n      // TODO Auto-generated method stub\n      return super.get(key);\n    }\n    */\n{code}\n\nWhoops! I'm sorry! I wondered why I didn't have to replace all to get rid of that when I updated - I didn't mean to commit that! That was just part of my experimenting with the RAM blowout issue - was just making sure everything still worked without each thread having its own cache. That means the ThreadResources was out of whack too - I did have it as a member of the SegmentTermsEnum - I'm sorry - totally didn't mean to commit that!\n\n*edit* Also the stuff with the threadResourceSet and setting to null - just trying to figure out the mem issue - I did a bunch of debugging things and they all got caught up in a merge. Yuck.",
            "date": "2009-10-16T00:56:39.434+0000",
            "id": 123
        },
        {
            "author": "Mark Miller",
            "body": "  // nocommit -- wonder if simple double-barrel LRU cache\n  // would be better\n\n  Yeah - haven't considered anything about the cache being used - really just took the same cache that was being used to cache terminfos. The only reason I changed to my own impl over SimpleLRUCache was that I wanted to reuse the removed entry.\n\n\n  // nocommit -- we should not init cache w/ full\n  // capacity?  init it at 0, and only start evicting\n  // once #entries is over our max\n\n  Same here - I took the same thing the old cache was doing.\n  Do we want to start it at 0 though? Perhaps a little higher? Doesn't it keep rehashing to roughly double the size? That could be a lot of resizing ...",
            "date": "2009-10-16T01:08:20.637+0000",
            "id": 124
        },
        {
            "author": "Mark Miller",
            "body": "Hmm - I'm still getting the heap space issue I think - its always been somewhat intermittent - sometimes it doesn't happen - usually it happens when you run all the tests - sometimes not though. Same when you run the test class individually - usually to sometimes it doesn't happen - and then usually to sometimes it does.",
            "date": "2009-10-16T01:28:34.978+0000",
            "id": 125
        },
        {
            "author": "Michael McCandless",
            "body": "OK thank for addressing the new nocommits -- you wanna remove them & commit as you find/comment on them?  Can be our means of communicating through the branch :)\n\nFor now, I don't think we need to explore improvements to the TermInfo cache (starting @ smaller size, simplistic double barrel LRU cache) -- we can simply mimic trunk for now; such improvements are orthogonal here.  Maybe switch those nocommits to TODOs instead?\n\nbq. Hmm - I'm still getting the heap space issue I think\n\nSigh.  I think we have more work to do to \"scale down\" RAM used by IndexReader for a smallish index.",
            "date": "2009-10-16T09:21:30.387+0000",
            "id": 126
        },
        {
            "author": "Michael McCandless",
            "body": "bq. you wanna remove them & commit as you find/comment on them?\n\nWoops, I see you already did!  Thanks.",
            "date": "2009-10-16T09:23:04.089+0000",
            "id": 127
        },
        {
            "author": "Mark Miller",
            "body": "just committed an initial stab at pulsing cache support - could prob use your love again ;)\n\nOddly, the reopen test passed no problem and this adds more to the cache - perhaps I was seeing a ghost last night ...\n\nI'll know before too long.",
            "date": "2009-10-16T15:39:59.744+0000",
            "id": 128
        },
        {
            "author": "Mark Miller",
            "body": "Almost got an initial rough stab at the sep codec cache done - just have to get two more tests to pass involving the payload's state.",
            "date": "2009-10-17T04:30:18.816+0000",
            "id": 129
        },
        {
            "author": "Mark Miller",
            "body": "Hey Mike: you tweaked a couple little things with the standard cache capture state (showing that I'm a cheater and getting stuff to work that I haven't yet fully understood ;) My specialty ) - what worries me is that they look like important little pieces if they are correct, but all tests passed without them. Hopefully we can get some tests in that catch these little off bys.",
            "date": "2009-10-17T13:36:14.726+0000",
            "id": 130
        },
        {
            "author": "Mark Miller",
            "body": "Okay, first pass for sep cache support is in - def needs to be trimmed down - heap issue with reopen everytime - I'm using a state object with the Index objects though, and I'm sure that can be done away with - though I guess a clone is not really much better and there is no access to their guts at the moment. Works for a first pass though.",
            "date": "2009-10-17T14:07:15.657+0000",
            "id": 131
        },
        {
            "author": "Michael McCandless",
            "body": "bq. you tweaked a couple little things with the standard cache capture state\n\nActually I think I just moved things around?  EG I made it the StandardTermsDictReader's job to seek the termsIn file, I moved docCount \"up\", and I made a single cache entry.  I think I also removed a few attrs that we didn't need to store... and downgraded skipOffset from long -> int (it's int on trunk).",
            "date": "2009-10-17T14:34:29.954+0000",
            "id": 132
        },
        {
            "author": "Mark Miller",
            "body": "bq. Actually I think I just moved things around? EG I made it the StandardTermsDictReader's job to seek the termsIn file, I moved docCount \"up\", and I made a single cache entry. I think I also removed a few attrs that we didn't need to store... and downgraded skipOffset from long -> int (it's int on trunk).\n\nOkay - that makes me feel a little better - I knew there was some unneccessary stuff, just hadn't gone through and figured out what could be stripped yet (there is likely the same thing with the new caches, but I don't think as much).\n\nThey main thing I saw that made me worry that I didn't think I had was:\n{code}\n          posReader.positions.seekPending = true;\n          posReader.positions.skipOffset = posReader.proxOffset;\n{code}\nBut perhaps I was just accomplishing the same thing in a different manner? I'd have to go back and look - I just don't think I knew enough to set either of those correctly - but seeing it helped me figure out what the heck was wrong with the final payloads piece in Sep ;)",
            "date": "2009-10-17T14:41:30.672+0000",
            "id": 133
        },
        {
            "author": "Michael McCandless",
            "body": "Ahh, I just changed your seek to be a lazy seek, in case the caller won't use the positions; though I think setting skipPosCount=0 (which I also added) should have been necessary even with the non-lazy seek.  Probably we could get the TestCodecs test to tickle that bug, if we get a DocsEnum, get PositionsEnum, read a few docs but NOT the positions, then seek to a term we had already seeked to (so it uses the cache) then try to read positions.  The positions should be wrong because skipPosCount will carry over a non-zero value.",
            "date": "2009-10-17T17:08:04.603+0000",
            "id": 134
        },
        {
            "author": "Michael McCandless",
            "body": "I just committed fix for a major memory cost during TestIndexReaderReopen.\n\nThe new  terms dict index uses fixed byte[] blocks to hold the UTF8 bytes, of size 32 KB currently.  But for a tiny segment this is very wasteful.  So I fixed it to trim down the last byte[] block to free up the unused space.  I think TestIndexReaderReopen should no longer hit OOMs.",
            "date": "2009-10-18T13:05:24.334+0000",
            "id": 135
        },
        {
            "author": "Mark Miller",
            "body": "Nice! Sep and Pulsing still need to be trimmed down though - or we consider their bloat acceptable (they still don't pass). Sep especially should be pretty trimable I think. Pulsing is more of an issue because of the Document caching...",
            "date": "2009-10-18T14:44:02.657+0000",
            "id": 136
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Pulsing is more of an issue because of the Document caching...\n\nYeah, we probably need to measure cache size by RAM usage not shear count.  And, make it settable when you instantiate the codec.\n\nbq. Sep and Pulsing still need to be trimmed down though \n\nAre they causing OOMs with TestIndexReaderReopen?  (I haven't tried yet).",
            "date": "2009-10-18T15:12:20.437+0000",
            "id": 137
        },
        {
            "author": "Mark Miller",
            "body": "bq. Are they causing OOMs with TestIndexReaderReopen? (I haven't tried yet).\n\nYes - they both def need polish too - I just got them working (passing all the tests), but havn't really finished them.",
            "date": "2009-10-18T15:15:14.289+0000",
            "id": 138
        },
        {
            "author": "Michael McCandless",
            "body": "I just committed contrib/benchmark/sortBench.py on the branch, to run\nperf tests comparing trunk to flex.\n\nYou have to apply patches from LUCENE-2042 and LUCENE-2043 (until we\nresync branch).\n\nFirst edit the TRUNK_DIR and FLEX_DIR up top, and WIKI_FILE (it\nrequires wiki export -- all tests run against it), then run with \"-run\nXXX\" to test performance.\n\nIt first creates the 5M doc index, for trunk and for flex, with\nmultiple commit points holding higher pctg of deletions (0, 0.1%, 1%,\n10%), and then tests speed of various queries against it.\n\nI also fixed a bug in the standard codec's terms index reader.\n",
            "date": "2009-11-07T20:50:23.472+0000",
            "id": 139
        },
        {
            "author": "Michael McCandless",
            "body": "Initial results.  Performance is quite catastrophically bad for the MultiTermQueries!  Something silly must be up....\n\nJAVA:\njava version \"1.5.0_19\"\nJava(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_19-b02)\nJava HotSpot(TM) Server VM (build 1.5.0_19-b02, mixed mode)\n\n\nOS:\nSunOS rhumba 5.11 snv_111b i86pc i386 i86pc Solaris\n\n||Query||Deletes %||Tot hits||QPS old||QPS new||Pct change||\n|body:[tec TO tet]|0.0|body:[tec TO tet]|3.06|0.23|{color:red}-92.5%{color}|\n|body:[tec TO tet]|0.1|body:[tec TO tet]|2.87|0.22|{color:red}-92.3%{color}|\n|body:[tec TO tet]|1.0|body:[tec TO tet]|2.85|0.22|{color:red}-92.3%{color}|\n|body:[tec TO tet]|10|body:[tec TO tet]|2.83|0.23|{color:red}-91.9%{color}|\n|1|0.0|1|22.15|23.87|{color:green}7.8%{color}|\n|1|0.1|1|19.89|21.72|{color:green}9.2%{color}|\n|1|1.0|1|19.47|21.55|{color:green}10.7%{color}|\n|1|10|1|19.82|21.13|{color:green}6.6%{color}|\n|2|0.0|2|23.54|25.97|{color:green}10.3%{color}|\n|2|0.1|2|21.12|23.56|{color:green}11.6%{color}|\n|2|1.0|2|21.37|23.27|{color:green}8.9%{color}|\n|2|10|2|21.55|23.10|{color:green}7.2%{color}|\n|+1 +2|0.0|+1 +2|7.13|6.97|{color:red}-2.2%{color}|\n|+1 +2|0.1|+1 +2|6.40|6.77|{color:green}5.8%{color}|\n|+1 +2|1.0|+1 +2|6.41|6.64|{color:green}3.6%{color}|\n|+1 +2|10|+1 +2|6.65|6.98|{color:green}5.0%{color}|\n|+1 -2|0.0|+1 -2|7.78|7.95|{color:green}2.2%{color}|\n|+1 -2|0.1|+1 -2|7.11|7.31|{color:green}2.8%{color}|\n|+1 -2|1.0|+1 -2|7.18|7.27|{color:green}1.3%{color}|\n|+1 -2|10|+1 -2|7.11|7.70|{color:green}8.3%{color}|\n|1 2 3 -4|0.0|1 2 3 -4|5.03|4.91|{color:red}-2.4%{color}|\n|1 2 3 -4|0.1|1 2 3 -4|4.62|4.39|{color:red}-5.0%{color}|\n|1 2 3 -4|1.0|1 2 3 -4|4.72|4.67|{color:red}-1.1%{color}|\n|1 2 3 -4|10|1 2 3 -4|4.78|4.74|{color:red}-0.8%{color}|\n|real*|0.0|real*|28.40|0.19|{color:red}-99.3%{color}|\n|real*|0.1|real*|26.23|0.20|{color:red}-99.2%{color}|\n|real*|1.0|real*|26.04|0.20|{color:red}-99.2%{color}|\n|real*|10|real*|26.83|0.20|{color:red}-99.3%{color}|\n|\"world economy\"|0.0|\"world economy\"|18.82|17.83|{color:red}-5.3%{color}|\n|\"world economy\"|0.1|\"world economy\"|18.64|17.99|{color:red}-3.5%{color}|\n|\"world economy\"|1.0|\"world economy\"|18.97|18.35|{color:red}-3.3%{color}|\n|\"world economy\"|10|\"world economy\"|19.59|18.12|{color:red}-7.5%{color}|\n",
            "date": "2009-11-08T00:03:08.669+0000",
            "id": 140
        },
        {
            "author": "Michael McCandless",
            "body": "Committed fixes addressing silly slowness.  You also need LUCENE-2044 patch, until we sync up with trunk again, to run sortBench.py.\n\nPart of the slowness was from MTQ queries incorrectly running the TermsEnum to exhaustion, instead of stopping when they hit their upperTerm.  But, another part of the slowness was because sortBench.py was actually incorrectly testing flex branch against a trunk index.  This is definitely something we have to test (it's what people will see when they use flex to search existing indexes -- flex API emulated on the current index format), so, we'll have to address that slowness as well, but for now I want to test pure flex (flex API on a flex index).",
            "date": "2009-11-08T13:12:04.181+0000",
            "id": 141
        },
        {
            "author": "Michael McCandless",
            "body": "OK new numbers after the above commits:\n\nJAVA:\njava version \"1.5.0_19\"\nJava(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_19-b02)\nJava HotSpot(TM) Server VM (build 1.5.0_19-b02, mixed mode)\n\n\nOS:\nSunOS rhumba 5.11 snv_111b i86pc i386 i86pc Solaris\n\n\n||Query||Deletes %||Tot hits||QPS old||QPS new||Pct change||\n|body:[tec TO tet]|0.0|1934684|3.13|3.96|{color:green}26.5%{color}|\n|body:[tec TO tet]|0.1|1932754|2.98|3.62|{color:green}21.5%{color}|\n|body:[tec TO tet]|1.0|1915224|2.97|3.62|{color:green}21.9%{color}|\n|body:[tec TO tet]|10|1741255|2.96|3.61|{color:green}22.0%{color}|\n|real*|0.0|389378|27.80|28.73|{color:green}3.3%{color}|\n|real*|0.1|389005|26.74|28.93|{color:green}8.2%{color}|\n|real*|1.0|385434|26.61|29.04|{color:green}9.1%{color}|\n|real*|10|350404|26.32|29.29|{color:green}11.3%{color}|\n|1|0.0|1170209|21.81|22.27|{color:green}2.1%{color}|\n|1|0.1|1169068|20.41|21.47|{color:green}5.2%{color}|\n|1|1.0|1158528|20.42|21.41|{color:green}4.8%{color}|\n|1|10|1053269|20.52|21.39|{color:green}4.2%{color}|\n|2|0.0|1088727|23.29|23.86|{color:green}2.4%{color}|\n|2|0.1|1087700|21.67|22.92|{color:green}5.8%{color}|\n|2|1.0|1077788|21.77|22.80|{color:green}4.7%{color}|\n|2|10|980068|21.90|23.04|{color:green}5.2%{color}|\n|+1 +2|0.0|700793|7.25|6.65|{color:red}-8.3%{color}|\n|+1 +2|0.1|700137|6.58|6.33|{color:red}-3.8%{color}|\n|+1 +2|1.0|693756|6.50|6.32|{color:red}-2.8%{color}|\n|+1 +2|10|630953|6.73|6.37|{color:red}-5.3%{color}|\n|+1 -2|0.0|469416|8.11|7.27|{color:red}-10.4%{color}|\n|+1 -2|0.1|468931|7.02|6.61|{color:red}-5.8%{color}|\n|+1 -2|1.0|464772|7.27|6.75|{color:red}-7.2%{color}|\n|+1 -2|10|422316|7.28|6.99|{color:red}-4.0%{color}|\n|1 2 3 -4|0.0|1104704|4.80|4.46|{color:red}-7.1%{color}|\n|1 2 3 -4|0.1|1103583|4.74|4.40|{color:red}-7.2%{color}|\n|1 2 3 -4|1.0|1093634|4.72|4.45|{color:red}-5.7%{color}|\n|1 2 3 -4|10|994046|4.79|4.63|{color:red}-3.3%{color}|\n|\"world economy\"|0.0|985|19.43|16.79|{color:red}-13.6%{color}|\n|\"world economy\"|0.1|984|18.71|16.59|{color:red}-11.3%{color}|\n|\"world economy\"|1.0|970|19.65|16.86|{color:red}-14.2%{color}|\n|\"world economy\"|10|884|19.69|17.25|{color:red}-12.4%{color}|\n\n\nThe term range query & preifx query are now a bit faster; boolean queries are somewhat slower; the phrase query shows the biggest slowdown...\n",
            "date": "2009-11-08T13:16:14.731+0000",
            "id": 142
        },
        {
            "author": "Mark Miller",
            "body": "I'll merge up when I figure out how -\n\nmerge does not like the restoration of RussianLowerCaseFilter or the move of PatternAnalyzer. Not really sure why not yet. I'll try and play with it tonight.",
            "date": "2009-11-08T15:56:59.679+0000",
            "id": 143
        },
        {
            "author": "Michael McCandless",
            "body": "Yikes!  That sounds challenging.",
            "date": "2009-11-08T16:03:57.057+0000",
            "id": 144
        },
        {
            "author": "Mark Miller",
            "body": "Indeed - the merging has been quite challenging - its a bit unfair really - one of these days we will have to switch - I'll write the flexible indexing stuff, and you start doing the hard tasks ;)\n\nI'll commit the merge in a bit when the tests finish - might not get to the back compat branch if its needed till tomorrow night though.",
            "date": "2009-11-09T02:26:46.898+0000",
            "id": 145
        },
        {
            "author": "Mark Miller",
            "body": "I still get OOM's on the reopen test every so often. Many times I don't, then sometimes I do.",
            "date": "2009-11-09T03:05:16.445+0000",
            "id": 146
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I'll write the flexible indexing stuff, and you start doing the hard tasks\n\nDon't you just have to press one button in your IDE? ;)\n\nbq. I still get OOM's on the reopen test every so often. Many times I don't, then sometimes I do.\n\nHmm... I'll try to dig.  This is with the standard codec, or, eg pulsing or intblock?",
            "date": "2009-11-09T10:46:48.012+0000",
            "id": 147
        },
        {
            "author": "Mark Miller",
            "body": "bq. Don't you just have to press one button in your IDE? \n\nOuch - thats like claiming all it takes to drive a porsche carrera gt is pushing the accelerator :)\n\nbq. Hmm... I'll try to dig. This is with the standard codec, or, eg pulsing or intblock?\n\nI'm talking standard - sep and pulsing def blow up - they still need some work in that regard - but you have gotten standard pretty darn close - it usually doesn't blow - but sometimes it still seems to (I guess depending on random factors in the test). intblock is still cachless, so I don't think it ever blows.",
            "date": "2009-11-09T13:15:28.754+0000",
            "id": 148
        },
        {
            "author": "Michael McCandless",
            "body": "I removed all the \"if (Codec.DEBUG)\" lines a local checkout and re-ran sortBench.py -- looks like flex is pretty close to trunk now (on OpenSolaris, Java 1.5, at least):\n\nJAVA:\njava version \"1.5.0_19\"\nJava(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_19-b02)\nJava HotSpot(TM) Server VM (build 1.5.0_19-b02, mixed mode)\n\n\nOS:\nSunOS rhumba 5.11 snv_111b i86pc i386 i86pc Solaris\n\nIndex /x/lucene/wiki.baseline.nd5M already exists...\nIndex /x/lucene/wiki.flex.nd5M already exists...\n\n\n\n||Query||Deletes %||Tot hits||QPS old||QPS new||Pct change||\n|body:[tec TO tet]|0.0|1934684|2.95|4.04|{color:green}36.9%{color}|\n|body:[tec TO tet]|0.1|1932754|2.86|3.73|{color:green}30.4%{color}|\n|body:[tec TO tet]|1.0|1915224|2.88|3.69|{color:green}28.1%{color}|\n|body:[tec TO tet]|10|1741255|2.86|3.74|{color:green}30.8%{color}|\n|real*|0.0|389378|26.85|28.74|{color:green}7.0%{color}|\n|real*|0.1|389005|25.83|26.96|{color:green}4.4%{color}|\n|real*|1.0|385434|25.55|27.15|{color:green}6.3%{color}|\n|real*|10|350404|25.38|28.10|{color:green}10.7%{color}|\n|1|0.0|1170209|21.75|21.80|{color:green}0.2%{color}|\n|1|0.1|1169068|20.39|22.02|{color:green}8.0%{color}|\n|1|1.0|1158528|20.35|21.88|{color:green}7.5%{color}|\n|1|10|1053269|20.48|21.96|{color:green}7.2%{color}|\n|2|0.0|1088727|23.37|23.42|{color:green}0.2%{color}|\n|2|0.1|1087700|21.61|23.49|{color:green}8.7%{color}|\n|2|1.0|1077788|21.85|23.46|{color:green}7.4%{color}|\n|2|10|980068|21.93|23.66|{color:green}7.9%{color}|\n|+1 +2|0.0|700793|7.29|7.32|{color:green}0.4%{color}|\n|+1 +2|0.1|700137|6.58|6.70|{color:green}1.8%{color}|\n|+1 +2|1.0|693756|6.60|6.68|{color:green}1.2%{color}|\n|+1 +2|10|630953|6.73|6.92|{color:green}2.8%{color}|\n|+1 -2|0.0|469416|8.07|7.69|{color:red}-4.7%{color}|\n|+1 -2|0.1|468931|7.02|7.46|{color:green}6.3%{color}|\n|+1 -2|1.0|464772|7.31|7.12|{color:red}-2.6%{color}|\n|+1 -2|10|422316|7.28|7.60|{color:green}4.4%{color}|\n|1 2 3 -4|0.0|1104704|4.83|4.52|{color:red}-6.4%{color}|\n|1 2 3 -4|0.1|1103583|4.73|4.48|{color:red}-5.3%{color}|\n|1 2 3 -4|1.0|1093634|4.75|4.46|{color:red}-6.1%{color}|\n|1 2 3 -4|10|994046|4.87|4.65|{color:red}-4.5%{color}|\n|\"world economy\"|0.0|985|19.50|20.11|{color:green}3.1%{color}|\n|\"world economy\"|0.1|984|18.65|19.76|{color:green}6.0%{color}|\n|\"world economy\"|1.0|970|19.56|18.71|{color:red}-4.3%{color}|\n|\"world economy\"|10|884|19.58|20.19|{color:green}3.1%{color}|\n",
            "date": "2009-11-09T16:27:56.767+0000",
            "id": 149
        },
        {
            "author": "Mark Miller",
            "body": "I've got a big merge coming - after a recent merge I noticed a bunch of things didn't merge at all - so I started looking back and saw a few things that didn't merge properly previously as well. So I'm working on a file by file line by line update that should be ready fairly soon.",
            "date": "2009-11-16T14:51:02.210+0000",
            "id": 150
        },
        {
            "author": "Uwe Schindler",
            "body": "If you are merging, you should simplky replace the old 2.9 BW branch by the new 3.0 one I recently created for trunk.",
            "date": "2009-11-16T15:36:06.562+0000",
            "id": 151
        },
        {
            "author": "Mark Miller",
            "body": "Simply ? :) What about the part where I have to merge in the flexible indexing backward compat changes into the new branch after first figuring out what changes those are :) Okay, its not unsimple, but this backward branch stuff is my least favorite part.",
            "date": "2009-11-16T16:59:35.024+0000",
            "id": 152
        },
        {
            "author": "Mark Miller",
            "body": "Merged up - I've gotto say - that was a nasty one. I think things are more in sync then there were though.",
            "date": "2009-11-16T21:25:51.454+0000",
            "id": 153
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Mark!  Hopefully, once 3.0 is out the door, the merging becomes a little less crazy.  I was dreading carrying this through 3.0 and I'm very glad you stepped in ;)",
            "date": "2009-11-16T22:06:09.454+0000",
            "id": 154
        },
        {
            "author": "Michael McCandless",
            "body": "I just committed a nice change on the flex branch: all term data in\nDocumentsWriter's RAM buffer is now stored as UTF8 bytes.  Previously\nthey were stored as char.\n\nI think this is a good step forward:\n\n  * Single-byte UTF8 characters (ascii, including terms created by\n    NumericField) now take half the RAM, which should lead to faster\n    indexing (better RAM efficiency so less frequent flushing)\n\n  * I now use the 0xff byte marker to mark the end of the term, which\n    never appears in UTF-8; this should mean 0xffff is allowed again\n    (though we shouldn't advertise it)\n\n  * Merging & flushing should be a tad faster since the terms data now\n    remains as UTF8 the whole time\n\nTermsConsumer now takes a TermRef (previously it took a char[] +\noffset), which makes it nicely symmetic with TermsEnum.\n\nAlso I cleaned up the \"nocommit not reads\" -- thanks Mark!\n",
            "date": "2009-11-19T19:10:32.401+0000",
            "id": 155
        },
        {
            "author": "Michael McCandless",
            "body": "I just committed changes to flex branch to make it possible for the\ncodec to override how merging happens.\n\nBasically I refactored SegmentMerger's postings merging code\n(mergeTermInfos, appendPostings) onto Fields/Terms/Docs/PositionsConsumer,\nso that the base class provides a default impl for merging at each\nlevel but the codec can override if it wants.  This should make issues\nlike LUCENE-2082 easy for a codec to implement.\n",
            "date": "2009-11-21T18:36:49.859+0000",
            "id": 156
        },
        {
            "author": "Robert Muir",
            "body": "edit: change supp char to <suppl. char> so erik can index this one too :)\n\nMike, this change to byte[] in TermRef will break backwards compatibility, without some special attention paid to the utf-16 to utf-8 conversion.\n\nimagine FuzzyQuery on a string starting with <suppl. char>, prefix of 1.\nthis will create a prefix of U+D866, which is an unpaired lead surrogate.\nThis is perfectly ok though, because we are not going to write it to UTF-8 form, it is just being used as an intermediary processing.\nbefore, this would work just fine, because everything was an internal unicode string, so startsWith() would work just fine.\n\nnow it will no longer work, because it must be downconverted to UTF-8 byte[]. \nWhether you use getBytes() or UnicodeUtil, it will be replaced by U+FFFD, and the same code will not work.\nthe standard provides that this kind of processing is ok for internal unicode strings, see CH3 D89.\n",
            "date": "2009-11-23T02:49:54.553+0000",
            "id": 157
        },
        {
            "author": "Robert Muir",
            "body": "here is a workaround you will not like.\nin the impl for FuzzyTermsEnum etc, we must not use TermRef.startsWith in its current state due to this issue, if the prefix ends with unpaired surrogate.\nin this case the String must be materialized each time from TermRef for comparison.\n\nthis is an example, where using byte[] will start to make things a bit complicated. It is not really a fault in TermRef, it is due to how the enums are currently implemented,\nthey will either need additional checks or we will need special unicode conversion so we can use things like TermRef.startsWith safely.\n\nedit: actually i do think now this is a fault in TermRef/TermsEnum api. how do i seek to U+D866 in the term dictionary? I can do this with trunk...\nit is not possible with the flex branch, because you cannot represent this in UTF-8 byte[]",
            "date": "2009-11-23T02:58:30.999+0000",
            "id": 158
        },
        {
            "author": "Robert Muir",
            "body": "test that passes on trunk, fails on branch.",
            "date": "2009-11-23T04:08:21.124+0000",
            "id": 159
        },
        {
            "author": "Michael McCandless",
            "body": "bq. how do i seek to U+D866 in the term dictionary? I can do this with trunk...\n\nBut, that's an unpaired surrogate?  Ie, not a valid unicode character?\nIt's nice that the current API let's you seek based on an unpaired\nsurrogate, but that's not valid use of the API, right?\n\nI guess if we want we can assert that the incoming TermRef is actually valid\nunicode...",
            "date": "2009-11-23T09:59:28.605+0000",
            "id": 160
        },
        {
            "author": "Robert Muir",
            "body": "Michael, it is a valid unicode String though, this is ok, and such things are supported by the unicode standard.\n\nalso, perhaps it would help convince you if i instead wrote the code as .terms(\"\ud866\udf05\".charAt(0));\npreviously, naive treatment of text like this would work correctly, now with byte it cannot.\nI hope you can start to see how many east asian applications will break because of this.\n\nhttp://www.unicode.org/notes/tn12/\n",
            "date": "2009-11-23T11:11:55.104+0000",
            "id": 161
        },
        {
            "author": "Robert Muir",
            "body": "same test, coded in a slightly different way, to show how this can commonly happen.\n\nMichael, I urge you to reconsider this. Please read Ch2 and 3 of the unicode standard if you want to do this.\nThe problem is, this substring, it is a valid unicode String. it is true it cannot be converted into valid utf-8, but \nits perfectly reasonable to use code units for internal processing like this, I am not attempting to write this data into the index or anything!\n\nI think data from TermRef for merging or writing to IndexWriter, is completely different from data being used to search!\nI know you want an elegant encapsulation of both, but I think its a broken design.\n\nI don't just make this up to be annoying, i have applications that will break because of this.",
            "date": "2009-11-23T11:28:08.147+0000",
            "id": 162
        },
        {
            "author": "Michael McCandless",
            "body": "bq. perhaps it would help convince you if i instead wrote the code as .terms(\"\ud866\udf05\".charAt(0));\n\nI realize a java String can easily contain an unpaired surrogate (eg,\nyour test case) since it operates in code units not code points, but,\nthat's not valid unicode, right?\n\nI mean you can't in general send such a string off to a library that\nworks w/ unicode (like Lucene) and expect the behavior to be well\ndefined.  Yes, it's neat that Lucene allows that today, but I don't\nsee that it's \"supposed to\".\n\nWhen we encounter an unpaired surrogate during indexing, we replace it\nw/ the replacement char.  Why shouldn't we do the same when\nsearching/reading the index?\n\nWhat should we do during searching if the unpaired surrogate is inside\nthe string (not at the end)?  Why should that be different?\n\nbq. Please read Ch2 and 3 of the unicode standard if you want to do this.\n\nDoesn't this apply here?  In \"3.2 Conformance\"\n(http://www.unicode.org/versions/Unicode5.0.0/ch03.pdf) is this first\nrequirement (C1):\n\n  * A process shall not interpret a high-surrogate code point or a\n    low-surrogate code point as an abstract character.\n\nbq. I hope you can start to see how many east asian applications will break because of this.\n\nBut how would a search application based on an east asian language\nactually create such a term?  In what situation would an unpaired\nsurrogate find its way down to TermEnum?\n\nEg when users enter searches, they enter whole unicode chars (code\npoints) at once (not code units / unpaired surrogates)?  I realize an\napp could programmatically construct eg a PrefixQuery that has an\nunpaired surrogate... but couldn't they just as easily pair it up\nbefore sending it to Lucene?\n\nbq.  i have applications that will break because of this.\n\nOK, can you shed some more light on how/when your apps do this?\n",
            "date": "2009-11-23T13:39:41.596+0000",
            "id": 163
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nI realize a java String can easily contain an unpaired surrogate (eg,\nyour test case) since it operates in code units not code points, but,\nthat's not valid unicode, right?\n{quote}\n\nit is valid unicode. it is a valid \"Unicode String\". This is different than a Term stored in the index, which will be stored as UTF-8, and thus purports to be in a valid unicode encoding form.\n\nHowever,\nthe conformance clauses do not prevent processes from operating on code\nunit sequences that do not purport to be in a Unicode character encoding form.\nFor example, for performance reasons a low-level string operation may simply\noperate directly on code units, without interpreting them as characters. See,\nespecially, the discussion under D89.\n\nD89:\nUnicode strings need not contain well-formed code unit sequences under all conditions.\nThis is equivalent to saying that a particular Unicode string need not be in a Unicode\nencoding form.\n\u2022 For example, it is perfectly reasonable to talk about an operation that takes the\ntwo Unicode 16-bit strings, <004D D800> and <DF02 004D>, each of which\ncontains an ill-formed UTF-16 code unit sequence, and concatenates them to\nform another Unicode string <004D D800 DF02 004D>, which contains a wellformed\nUTF-16 code unit sequence. The first two Unicode strings are not in\nUTF-16, but the resultant Unicode string is.\n\n{quote}\nBut how would a search application based on an east asian language\nactually create such a term? In what situation would an unpaired\nsurrogate find its way down to TermEnum?\n{quote}\nI gave an example already, where they use FuzzyQuery with say a prefix of one. \nwith the current code, even in the flex branch!!! this will create a lead surrogate prefix.\nThere is code in the lucene core that does things like this (which I plan to fix, and also try to preserve back compat!)\nThis makes it impossible to preserve back compat.\n\nThere is also probably a lot of non-lucene east asian code that does similar things.\nFor example, someone with data from Hong Kong almost certainly encounters suppl. characters, because\nthey are part of Big5-HKSCS. They may not be smart enough to know about this situation, i.e. they might take a string, substring(0, 1) and do a prefix query.\nright now this will work!\n\nThis is part of the idea that for most operations (such as prefix), in java, supplementary characters work rather transparently.\nIf we do this, upgrading lucene to support for unicode 4.0 will be significantly more difficult.\n\nbq. OK, can you shed some more light on how/when your apps do this?\n\nYes, see LUCENE-1606. This library uses UTF-16 intervals for transitions, which works fine because for its matching purposes, this is transparent.\nSo there is no need for it to be aware of suppl. characters. If we make this change, I will need to refactor/rewrite a lot of this code, most likely the underlying DFA library itself.\nThis is working in production for me, on chinese text outside of the BMP with lucene right now. With this change, it will no longer work, and the enumerator will most likely go into an infinite loop!\n\nThe main difference here is semantics, before IndexReader.terms() accepted as input any Unicode String. Now it would tighten that restriction to only any interchangeable UTF-8 string. Yet the input being used, will not be stored as UTF-8 anywhere, and most certainly will not be interchanged! The paper i sent on UTF-16 mentions problems like this, because its very reasonable and handy to use code units for processing, since suppl. characters are so rare.\n",
            "date": "2009-11-23T15:08:32.314+0000",
            "id": 164
        },
        {
            "author": "Robert Muir",
            "body": "attached is a patch that provides a workaround for the back compat issue.\nin my opinion it does not hurt performance (though, you should optimize this)\nwhen opening a TermEnum with IndexReader.terms(Term), the deprecated API, \nin LegacyTermEnum(Term t), if the term ends with a lead surrogate, tack on \\uDC00 to emulate the old behavior.\n\nwith this patch, my testcase passes.\n\nwe might be able to workaround these issues in similar ways for better backwards compatibility, at the same time preserving performance.\nI think we should mention somewhere in the docs that the new api behaves a bit differently though, so people know to fix their code.",
            "date": "2009-11-23T19:03:58.622+0000",
            "id": 165
        },
        {
            "author": "Michael McCandless",
            "body": "bq.  if the term ends with a lead surrogate, tack on \\uDC00 to emulate the old behavior.\n\nOK I think this is a good approach, in the \"emulate old on flex\" layer, and then in the docs for TermRef call out that the incoming String cannot contain unpaired surrogates?\n\nCan you commit this, along with your test? Thanks!",
            "date": "2009-11-23T19:45:27.395+0000",
            "id": 166
        },
        {
            "author": "Robert Muir",
            "body": "bq. OK I think this is a good approach, in the \"emulate old on flex\" layer, and then in the docs for TermRef call out that the incoming String cannot contain unpaired surrogates?\n\nJust so you know, its not perfect back compat though. \nFor perfect back compat I would have to iterate thru the string looking for unpaired surrogates.. at which point you truncate after, and tack on \\uDC00 if its a high surrogate.\nIf its an unpaired low surrogate, I am not actually sure what the old API would do? My guess would be to replace with U+F000, but it depends how this was being handled before.\n\nthe joys of UTF-16 vs UTF-8 binary order...\n\nI didnt do any of this, because in my opinion fixing just the \"trailing lead surrogate\" case is all we should worry about, especially since the lucene core itself does this.\n\nI'll commit the patch and test, we can improve it in the future if you are worried about these corner-corner-corner cases, no problem.",
            "date": "2009-11-23T19:58:52.539+0000",
            "id": 167
        },
        {
            "author": "Robert Muir",
            "body": "the patch and test are in revision 883485.\nI added some javadocs to TermRef where it takes a String constructor as well.\n",
            "date": "2009-11-23T20:29:07.656+0000",
            "id": 168
        },
        {
            "author": "Robert Muir",
            "body": "Mike, what to do about MultiTermQueries now?\nthey still have some problems, especially with regards to doing 'startsWith' some constant prefix, which might be unpaired lead surrogate (lucene problem)\n\nI guess we need to specialize this case in their FilteredTermEnum (not TermsEnum), and if they are doing this stupid behavior, return null from getTermsEnum() ?\nand force it to the old TermEnum which has some back compat shims for this case?\n",
            "date": "2009-11-23T21:06:18.094+0000",
            "id": 169
        },
        {
            "author": "Robert Muir",
            "body": "Also, I am curious in general if we support any old index formats that might contain unpaired surrogates or \\uFFFF in the term text.\n\nThis will be good to know when trying to fix unicode 4 issues, especially if we are doing things like compareTo() or startsWith() on the raw bytes.",
            "date": "2009-11-23T21:15:03.851+0000",
            "id": 170
        },
        {
            "author": "Michael McCandless",
            "body": "LUCENE-510 (fixed in 2.4 release) cutover new indexes to UTF8.\n\nBefore 2.4, here's what IndexOutput.writeString looked like:\n\n{code}\n  public void writeChars(String s, int start, int length)\n       throws IOException {\n    final int end = start + length;\n    for (int i = start; i < end; i++) {\n      final int code = (int)s.charAt(i);\n      if (code >= 0x01 && code <= 0x7F)\n\twriteByte((byte)code);\n      else if (((code >= 0x80) && (code <= 0x7FF)) || code == 0) {\n\twriteByte((byte)(0xC0 | (code >> 6)));\n\twriteByte((byte)(0x80 | (code & 0x3F)));\n      } else {\n\twriteByte((byte)(0xE0 | (code >>> 12)));\n\twriteByte((byte)(0x80 | ((code >> 6) & 0x3F)));\n\twriteByte((byte)(0x80 | (code & 0x3F)));\n      }\n    }\n  }\n{code}\n\nwhich I think can represent unpaired surrogates & \\uFFFF just fine?",
            "date": "2009-11-23T21:30:00.009+0000",
            "id": 171
        },
        {
            "author": "Yonik Seeley",
            "body": "In general, I think things like unpaired surrogates should be undefined, giving us more room to optimize.",
            "date": "2009-11-23T21:30:02.771+0000",
            "id": 172
        },
        {
            "author": "Michael McCandless",
            "body": "Also, on the flex branch I believe \\uFFFF is no longer \"reserved\" by Lucene, but we should not advertise that!  Terms data is stored in DocumentsWriter as UTF8 bytes, and I use 0xff byte (an invalid UTF8 byte) as end marker.",
            "date": "2009-11-23T21:35:34.829+0000",
            "id": 173
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nthe patch and test are in revision 883485.\nI added some javadocs to TermRef where it takes a String constructor as well.\n{quote}\n\nThanks Robert!\n\n{quote}\nMike, what to do about MultiTermQueries now?\nthey still have some problems, especially with regards to doing 'startsWith' some constant prefix, which might be unpaired lead surrogate (lucene problem)\n{quote}\nMaybe open a new issue for this?  Or, don't we already have an issue open to fix how various queries handle surrogates?  Or I guess we could fix such queries to pair up the surrogate (add \\uDC00)?",
            "date": "2009-11-23T22:29:35.519+0000",
            "id": 174
        },
        {
            "author": "Robert Muir",
            "body": "bq. In general, I think things like unpaired surrogates should be undefined, giving us more room to optimize. \n\nThis is not an option I feel, when Lucene is the one creating the problem (i.e. our multitermqueries that are unaware of utf-32 boundaries).\n",
            "date": "2009-11-23T22:53:09.125+0000",
            "id": 175
        },
        {
            "author": "Robert Muir",
            "body": "bq. Maybe open a new issue for this? Or, don't we already have an issue open to fix how various queries handle surrogates? Or I guess we could fix such queries to pair up the surrogate (add \\uDC00)?\n\nMike, I have an issue open, for trunk. But it is not such a problem on trunk, because they work \"as expected\" in UTF-16 space\nThe move to byte[] creates the problem really, because then the existing problems in trunk, that happened to work, start to completely fail in UTF-8 space.\nand unfortunately, we can't use the \\uDC00 trick for startsWith :)",
            "date": "2009-11-23T22:56:23.639+0000",
            "id": 176
        },
        {
            "author": "Michael McCandless",
            "body": "Well, for starters can't we just toString() the TermRef on every compare?  Then we're back in UTF16 space.\n\nIt's not as good as flex can be (ie doing the checks in UTF8 space), but it should still be faster than trunk today, so this shouldn't block flex landing, right?",
            "date": "2009-11-24T00:21:19.934+0000",
            "id": 177
        },
        {
            "author": "Robert Muir",
            "body": "this one is more serious.\nthe change to byte[] changes the sort order of lucene (at least TermEnum)\n\nattached is a test that passes on trunk, fails on branch.\nin trunk, things sort in UTF-16 binary order.\nin branch, things sort in UTF-8 binary order.\nthese are different...",
            "date": "2009-11-24T00:44:11.407+0000",
            "id": 178
        },
        {
            "author": "Robert Muir",
            "body": "Mike, if it means anything, I prefer the new behavior... real codepoint order :)\nBut this is a compat problem I think.\n",
            "date": "2009-11-24T00:55:18.919+0000",
            "id": 179
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nin trunk, things sort in UTF-16 binary order.\nin branch, things sort in UTF-8 binary order.\nthese are different...\n{quote}\n\nUgh!  In the back of my mind I almost remembered this... I think this\nwas one reason why I didn't do this back in LUCENE-843 (I think we had\ndiscussed this already, then... though maybe I'm suffering from d\u00e9j\u00e0\nvu).  I could swear at one point I had that fixup logic implemented in\na UTF-8/16 comparison method...\n\nUTF-8 sort order (what flex branch has switched to) is true unicode\ncodepoint sort order, while UTF-16 is not when there are surrogate\npairs as well as high (>= U+E000) unicode chars.  Sigh....\n\nSo this is definitely a back compat problem.  And, unfortunately, even\nif we like the true codepoint sort order, it's not easy to switch to\nin a back-compat manner because if we write new segments into an old\nindex, SegmentMerger will be in big trouble when it tries to merge two\nsegments that had sorted the terms differently.\n\nI would also prefer true codepoint sort order... but we can't break\nback compat.\n\nThough it would be nice to let the codec control the sort order -- eg\nthen (I think?) the ICU/CollationKeyFilter workaround wouldn't be\nneeded.\n\nFortunately the problem is isolated to how we sort the buffered\npostings when it's time to flush a new segment, so I think w/ the\nappropriate fixup logic (eg your comment at\nhttps://issues.apache.org/jira/browse/LUCENE-1606?focusedCommentId=12781746&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12781746)\nwhen comparing terms in oal.index.TermsHashPerField.comparePostings\nduring that sort, we can get back to UTF-16 sort order.\n",
            "date": "2009-11-24T10:38:06.753+0000",
            "id": 180
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nThough it would be nice to let the codec control the sort order - eg\nthen (I think?) the ICU/CollationKeyFilter workaround wouldn't be\nneeded.\n{quote}\n\nI like this idea by the way, \"flexible sorting\".  although i like codepoint order better than code unit order, i hate binary order in general to be honest. \n\nits nice we have 'indexable'/fast collation right now, but its maybe not what users expect either (binary keys encoded into text).\n",
            "date": "2009-11-24T11:22:01.393+0000",
            "id": 181
        },
        {
            "author": "Michael McCandless",
            "body": "bq. i hate binary order in general to be honest.\n\nBut binary order in this case is code point order.",
            "date": "2009-11-24T13:02:51.560+0000",
            "id": 182
        },
        {
            "author": "Robert Muir",
            "body": "Mike, I guess I mean i'd prefer UCA order, which isn't just the order codepoints happened to randomly appear on charts, but is actually designed for sorting and ordering things :)",
            "date": "2009-11-24T13:15:42.209+0000",
            "id": 183
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Mike, I guess I mean i'd prefer UCA order, which isn't just the order codepoints happened to randomly appear on charts, but is actually designed for sorting and ordering things \n\nAhh, gotchya.  Well if we make the sort order pluggable, you could do that...",
            "date": "2009-11-24T13:33:57.190+0000",
            "id": 184
        },
        {
            "author": "Robert Muir",
            "body": "bq. Ahh, gotchya. Well if we make the sort order pluggable, you could do that...\n\nyes, then we could consider getting rid of the Collator/Locale-based range queries / sorts and things like that completely... which have performance problems.\nyou would have a better way to do it... \n\nbut if you change the sort order, any part of lucene sensitive to it might break... maybe its dangerous.\n\nmaybe if we do it, it needs to be exposed properly so other components can change their behavior\n",
            "date": "2009-11-24T13:39:58.698+0000",
            "id": 185
        },
        {
            "author": "Michael McCandless",
            "body": "Yes, this (customizing comparator for termrefs) would definitely be very advanced stuff...  you'd have to create your own codec to do it.  And we'd default to UTF16 sort order for back compat.",
            "date": "2009-11-24T13:43:56.887+0000",
            "id": 186
        },
        {
            "author": "Robert Muir",
            "body": "bq. Yes, this (customizing comparator for termrefs) would definitely be very advanced stuff... you'd have to create your own codec to do it. And we'd default to UTF16 sort order for back compat.\n\nAgreed, changing the sort order breaks a lot of things (not just some crazy seeking around code that I write)\n\ni.e. if 'ch' is a character in some collator and sorts b, before c (completely made up example, there are real ones like this though)\nThen even prefixquery itself will fail!\n\nedit: better example is french collation, where the weight of accent marks is done in reverse order. \nprefix query would make assumptions based on the prefix, which are wrong.",
            "date": "2009-11-24T13:56:59.927+0000",
            "id": 187
        },
        {
            "author": "Uwe Schindler",
            "body": "...not to talk about TermRangeQueries and NumericRangeQueries. They rely on String.compareTo like the current terms dict.",
            "date": "2009-11-24T14:00:40.836+0000",
            "id": 188
        },
        {
            "author": "DM Smith",
            "body": "bq. Yes, this (customizing comparator for termrefs) would definitely be very advanced stuff... you'd have to create your own codec to do it. And we'd default to UTF16 sort order for back compat.\n\nFor those of us working on texts in all different kinds of languages, it should not be very advanced stuff. It should be stock Lucene. A default UCA comparator would be good. And a way to provide a locale sensitive UCA comparator would also be good.\n\nMy use case is that each Lucene index typically has a single language or at least has a dominant language.\n\nbq. ...not to talk about TermRangeQueries and NumericRangeQueries. They rely on String.compareTo like the current terms dict.\nI think that String.compareTo works correctly on UCA collation keys.",
            "date": "2009-11-24T14:21:16.398+0000",
            "id": 189
        },
        {
            "author": "Robert Muir",
            "body": "bq. I think that String.compareTo works correctly on UCA collation keys.\n\nNo, because UCA collation keys are bytes :)\nYou are right that byte comparison on these keys works though.\nBut if we change the sort order like this, various components are not looking at keys, instead they are looking at the term text themselves.\n\nI guess what I am saying is that there is a lot of assumptions in lucene right now, (prefixquery was my example) that look at term text and assume it is sorted in binary order.\n\nbq. It should be stock Lucene\nas much as I agree with you that default UCA should be \"stock lucene\" (with the capability to use an alternate locale or even tailored collator), this creates some practical problems, as mentioned above.\nalso the practical problem that collation in the JDK is poop and we would want ICU for good performance...\n",
            "date": "2009-11-24T14:32:46.179+0000",
            "id": 190
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nSo this is definitely a back compat problem. And, unfortunately, even\nif we like the true codepoint sort order, it's not easy to switch to\nin a back-compat manner because if we write new segments into an old\nindex, SegmentMerger will be in big trouble when it tries to merge two\nsegments that had sorted the terms differently.\n{quote}\n\nMike, I think it goes well beyond this. \nI think sort order is an exceptional low-level case that can trickle all the way up high into the application layer (including user perception itself), and create bugs.\nDoes a non-technical user in Hong Kong know how many code units each ideograph they enter are? \nShould they care? They will just not understand if things are in different order.\n\nI think we are stuck with UTF-16 without a huge effort, which would not be worth it in any case.\n",
            "date": "2009-11-24T16:36:21.551+0000",
            "id": 191
        },
        {
            "author": "Michael McCandless",
            "body": "OK I finally worked out a solution for the UTF16 sort order problem\n(just committed).\n\nI added a TermRef.Comparator class, for comparing TermRefs, and I\nremoved TermRef.compareTo, and fixed all low-level places in Lucene\nthat rely on sort order of terms to use this new API instead.\n\nI changed the Terms/TermsEnum/TermsConsumer API, adding a\ngetTermComparator(), ie, the codec now determines the sort order for\nterms in each field.  For the core codecs (standard, pulsing,\nintblock) I default to UTF16 sort order, for back compat, but you\ncould easily instantiate it yourself and use a different term sort.\n\nI changed TestExternalCodecs to test this new capability, by sorting 2\nof its fields in reversed unicode code point order.\n\nWhile this means your codec is now completely free to define the\nterm sort order per field, in general Lucene queries will not behave\nright if you do this, so it's obviously a very advanced use case.\n\nI also changed (yet again!) how DocumentsWriter encodes the terms\nbytes, to record the length (in bytes) of the term, up front, followed by the\nterm bytes (vs the trailing 0xff that I had switched to).  The length\nis a 1 or 2 byte vInt, ie if it's < 128 it's 1 byte, else 2 bytes.\nThis approach means the TermRef.Collector doesn't have to deal with\n0xff's (which was messy).\n\nI think this also means that, to the flex API, a term is actually\nopaque -- it's just a series of bytes.  It need not be UTF8 bytes.\nHowever, all of analysis, and then how TermsHash builds up these\nbyte[]s, and what queries do with these bytes, is clearly still very\nmuch Unicode/UTF8.  But one could, in theory (I haven't tested this!)\nseparately use the flex API to build up a segment whose terms are\narbitrary byte[]'s, eg maybe you want to use 4 bytes to encode int\nvalues, and then interact with those terms at search time\nusing the flex API.\n",
            "date": "2009-11-29T20:39:04.363+0000",
            "id": 192
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Mike,\n\nI looked into your commit, looks good. You are right with your comment in NRQ, it will only work with UTF-8 or UTF-16. Ideally NRQ would simply not use string terms at all and work directly on the byte[], which should then be ordered in binary order.\n\nTwo things:\n- The legacy NumericRangeTermEnum can be removed completely and the protected getEnum() should simply throw UOE. NRQ cannot be subclassed and nobody can call this method (maybe only classes in same package, but thats not supported). So the enum with the nocommit mark can be removed\n- I changed the logic in the TermEnum in trunk and 3.0 (it no longer works recursive, see LUCENE-2087). We  should change this here, too. This makes also the enum simplier (and it looks more like the Automaton one). The methods in trunk 3.0 setEnum() and endEnum() both throw now UOE.\n\nI will look into these two changes tomorrow and change the code.\n\nUwe",
            "date": "2009-11-29T21:20:05.883+0000",
            "id": 193
        },
        {
            "author": "Robert Muir",
            "body": "bq. Ideally NRQ would simply not use string terms at all and work directly on the byte[], which should then be ordered in binary order.\n\nbut isn't this what it does already with the TermsEnum api? the TermRef itself is just byte[], and NRQ precomputes all the TermRef's it needs up front, there is no unicode conversion there.\n\nedit: btw Uwe, and the comparator is be essentially just comparing bytes, the 0xee/0xef \"shifting\" should never take place with NRQ because those bytes will never be in a numeric field...\n",
            "date": "2009-11-29T21:29:53.039+0000",
            "id": 194
        },
        {
            "author": "Uwe Schindler",
            "body": "Robert: I know, because of that I said it works with UTF-8/UTF-16 comparator. It would *not* work with a reverse comparator as Mike uses in the test.\n\nWith directly on bytes[] I meant that it could not use chars at all and directly encode the numbers into byte[] with the full 8 bits per byte. The resulting byte[] would be never UTF-8, but if the new TermRef API would be able to handle this and also the TokenStreams, it would be fine. Only the terms format would change.",
            "date": "2009-11-29T21:39:02.066+0000",
            "id": 195
        },
        {
            "author": "Robert Muir",
            "body": "bq. With directly on bytes[] I meant that it could not use chars at all and directly encode the numbers into byte[] with the full 8 bits per byte. The resulting byte[] would be never UTF-8, but if the new TermRef API would be able to handle this and also the TokenStreams, it would be fine. Only the terms format would change.\n\nUwe, it looks like you can do this now (with the exception of tokenstreams). \n\nA partial solution for you which does work with tokenstreams, you could use indexablebinarystring which won't change between any unicode sort order... (it will not encode in any unicode range where there is a difference between the UTF-8/UTF32 and UTF-16). With this you could just compare bytes also, but you still would not have the \"full 8 bits per byte\"\n",
            "date": "2009-11-29T22:04:50.359+0000",
            "id": 196
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. A partial solution for you which does work with tokenstreams, you could use indexablebinarystring which won't change between any unicode sort order... (it will not encode in any unicode range where there is a difference between the UTF-8/UTF32 and UTF-16). With this you could just compare bytes also, but you still would not have the \"full 8 bits per byte\"\n\nThis would not change anything, only would make the format incompatible. With 7bits/char the currently UTF-8 coded index is the smallest possible one (even IndexableBinaryString would cost more bytes in the index, because if you would use 14 of the 16 bits/char, most chars would take 3 bytes in index because of UTF-8 vs. 2 bytes with the current encoding. Only the char[]/String representation would take less space than currently. See the discussion with Yonik about this and why we have choosen 7 bits/char. Also en-/decoding is much faster).\n\nFor the TokenStreams: The idea is to create an additional Attribute: BinaryTermAttribute that holds byte[]. If some tokenstream uses this attribute instead of TermAttribute, the indexer would choose to write the bytes directly to the index. NumericTokenStream could use this attribute and encode the numbers directly to byte[] with 8 bits/byte. -- the new AttributeSource API was created just because of such customizations (not possible with Token).",
            "date": "2009-11-29T22:15:04.867+0000",
            "id": 197
        },
        {
            "author": "Robert Muir",
            "body": "Uwe you are right that the terms would be larger but they would have a more distinct alphabet (byte range) and might compare faster... I don't know which one is most important to NRQ really.\n\nyeah I agree that encoding directly to byte[] is the way to go though, this would be nice for collation too...",
            "date": "2009-11-29T22:17:53.146+0000",
            "id": 198
        },
        {
            "author": "Uwe Schindler",
            "body": "As the codec is per field, we could also add an Attribute to TokenStream that holds the codec (the default is Standard). The indexer just uses the codec for the field from the TokenStream. NTS would use a NumericCodec (just thinking...) - will go sleeping now.",
            "date": "2009-11-29T22:18:53.721+0000",
            "id": 199
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Uwe you are right that the terms would be larger but they would have a more distinct alphabet (byte range) and might compare faster... I don't know which one is most important to NRQ really. \n\nThe new TermsEnum directly compares the byte[] arrays. Why should they compare faster when encoded by IndexableBinaryStringTools? Less bytes are faster to compare (it's one CPU instruction if optimized a very native x86/x64 loop). It may be faster if we need to decode to char[] but thats not the case (in flex branch).",
            "date": "2009-11-29T22:23:29.761+0000",
            "id": 200
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I changed the logic in the TermEnum in trunk and 3.0 (it no longer works recursive, see LUCENE-2087). We should change this here, too.\n\nMark has been periodically re-syncing changes down from trunk... we should probably just let this change come in through his process (else I think we cause more conflicts).\n\nbq. The legacy NumericRangeTermEnum can be removed completely and the protected getEnum() should simply throw UOE. NRQ cannot be subclassed and nobody can call this method (maybe only classes in same package, but thats not supported). So the enum with the nocommit mark can be removed\n\nAhh excellent.  Wanna commit that when you get a chance?\n\nbq.  Ideally NRQ would simply not use string terms at all and work directly on the byte[], which should then be ordered in binary order.\n\nThat'd be great!\n\nbq. With directly on bytes[] I meant that it could not use chars at all and directly encode the numbers into byte[] with the full 8 bits per byte. The resulting byte[] would be never UTF-8, but if the new TermRef API would be able to handle this and also the TokenStreams, it would be fine. Only the terms format would change.\n\nRight, this is a change in analysis -> DocumentsWriter -- somehow we have to allow a Token to carry a byte[] and that is directly indexes as the opaque term.  At search time NRQ is all byte[] already (unlike other queries, which are new String()'ing for every term on the enum).",
            "date": "2009-11-29T22:27:53.643+0000",
            "id": 201
        },
        {
            "author": "Robert Muir",
            "body": "bq. Why should they compare faster when encoded by IndexableBinaryStringTools?\n\nbecause it compares from left to right, so even if the terms are 10x as long, if they differ 2x as quick its better? \n\nI hear what you are saying about ASCII-only encoding, but if NRQ's model is always best, why do we have two separate \"encode byte[] into char[]\" models in lucene, one that NRQ is using, and one that collation is using!?\n",
            "date": "2009-11-29T22:28:22.457+0000",
            "id": 202
        },
        {
            "author": "Michael McCandless",
            "body": "bq. The idea is to create an additional Attribute: BinaryTermAttribute that holds byte[]. If some tokenstream uses this attribute instead of TermAttribute, the indexer would choose to write the bytes directly to the index. NumericTokenStream could use this attribute and encode the numbers directly to byte[] with 8 bits/byte. - the new AttributeSource API was created just because of such customizations (not possible with Token).\n\nThis sounds like an interesting approach!  We'd have to work out some details... eg you presumably can't mix char[] term and byte[] term in the same field.",
            "date": "2009-11-29T22:30:11.536+0000",
            "id": 203
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. because it compares from left to right, so even if the terms are 10x as long, if they differ 2x as quick its better? \nIt would not compare faster because in UTF-8 encoding, only 7 bits are used for encoding the chars. The 8th bit is just a marker (simply spoken). If this marker is always 0 or always 1 does not make a difference, in UTF-8 only 7 bits/byte are used for data. And with UTF-8 in the 3rd byte more bits are unused!\n\nbq. I hear what you are saying about ASCII-only encoding, but if NRQ's model is always best, why do we have two separate \"encode byte[] into char[]\" models in lucene, one that NRQ is using, and one that collation is using!?\n\nI do not know who made this IndexableBinaryStrings encoding, but it would not work for NRQ at all with current trunk (too complicated during indexing and decoding, because for NRQ, we also need to decode such char[] very fast for populating the FieldCache). But as discussed with Yonik (do not know the issue), the ASCII only encoding should always perform better (but needs more memory in trunk, as char[] is used during indexing -- I think because of that it was added). So the difference is not speed, its memory consumption.",
            "date": "2009-11-29T22:39:48.714+0000",
            "id": 204
        },
        {
            "author": "Robert Muir",
            "body": "bq. It would not compare faster because in UTF-8 encoding, only 7 bits are used for encoding the chars\n\nyeah you are right I dont think it will be faster on average (i was just posing the question because i dont really know NRQ), and you will waste 4 bits by using the first bit at the minimum.\n\ni am just always trying to improve collation too, so that's why I am bugging you. I guess hopefully soon we have byte[] and can do it properly, and speed up both.",
            "date": "2009-11-29T23:04:12.012+0000",
            "id": 205
        },
        {
            "author": "Robert Muir",
            "body": "fwiw here is a patch to use the algorithm from the unicode std for utf8 in utf16 sort order.\nthey claim it is fast because there is no conditional branching... who knows\n",
            "date": "2009-11-30T00:28:36.324+0000",
            "id": 206
        },
        {
            "author": "Uwe Schindler",
            "body": "I rewrote the NumericRangeTermsEnum, see revision 885360.\n\nChanged: Simplify and optimize NumericRangeTermEnum:\n- the range split logic only seeks forward (an assert verifies this), so the iterator can be reused (like Automaton)\n- removed the iteration by not using setEnum() [throws UOE], see LUCENE-2087\n- removed TermEnum, as class cannot be subclassed - so no BW break!!!; getEnum() throws UOE.\n- seek() cannot work for this TermsEnum, so throw UOE (is not needed for MTQ at the moment)\n",
            "date": "2009-11-30T09:14:38.347+0000",
            "id": 207
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Uwe!",
            "date": "2009-11-30T10:26:59.342+0000",
            "id": 208
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nfwiw here is a patch to use the algorithm from the unicode std for utf8 in utf16 sort order.\nthey claim it is fast because there is no conditional branching... who knows\n{quote}\nWe could try to test to see if we see a difference in practice...\n\nFor term text without surrogate content, the branch always goes one way, so the CPU ought to predict it well and it may turn out to be faster using branching.\n\nWith surrogates, likely the lookup approach is faster since the branch has good chance of going either way.\n\nHowever, the lookup approach adds 256 bytes to CPUs memory cache, which I'm not thrilled about.  We have other places that do the same (NORM_TABLE in Similarity, scoreCache in TermScorer), that I think are much more warranted to make the time vs cache line tradeoff since they deal with a decent amount of CPU.\n\nOr maybe worrying about cache lines from way up in javaland is just silly ;)\n\nI guess at this point I'd lean towards keeping the branch based comparator.",
            "date": "2009-11-30T10:36:11.003+0000",
            "id": 209
        },
        {
            "author": "Robert Muir",
            "body": "bq. We could try to test to see if we see a difference in practice...\n\nit is also very wierd to me that the method you are using is the one being used in ICU... if this one is faster why isnt ICU using it?\nits also sketchy that the table as described in the unicode std doesn't even work anyway as described... so is anyone using it?\n\nI like your reasoning, lets leave it alone for now... other things to work on that will surely help.\n",
            "date": "2009-11-30T13:41:05.698+0000",
            "id": 210
        },
        {
            "author": "Uwe Schindler",
            "body": "To prevent problems like yesterday, he is the patch I applied yesterday to the flex branch (for completeness).",
            "date": "2009-12-01T07:54:54.409+0000",
            "id": 211
        },
        {
            "author": "Mark Miller",
            "body": "I'm going to commit the latest merge to trunk in a bit.\n\nIn a recent commit, NumericRangeQuery was changed to return UnsupportedOperationException for getEnum - I think thats going to be a back compat break? For now I've commented out the back compat test and put a nocommit comment:\n\n{code}\n  @Override\n  // nocommit: I think this needs to be implemented for back compat? When done, \n  // the back compat test for it in TestNumericRangeQuery32 should be uncommented.\n  protected FilteredTermEnum getEnum(final IndexReader reader) throws IOException {\n    throw new UnsupportedOperationException(\"not implemented\");\n  }\n{code}\n\nI think we need to go back to returning the Enum? But I'm not sure why this change was made, so ...",
            "date": "2009-12-01T23:07:56.634+0000",
            "id": 212
        },
        {
            "author": "Uwe Schindler",
            "body": "It is not a break: you cannot extend NumericRangeQuery (it's final), so you can never call that method (protected). Only if you put your class that may call this method into the same package, but that's illegal and not backed by bw compatibility (The BW test is exactly such a case, just comment it out in BW branch - I added this test for explicit enum testing, we should have this in flex trunk, too).\n\n(I explained that in the commit and Mike already wrote that in the comment). So please keep the code clean and do not re-add this TE.",
            "date": "2009-12-01T23:14:12.421+0000",
            "id": 213
        },
        {
            "author": "Mark Miller",
            "body": "bq.  Mike already wrote that in the comment\n\nIn what comment? Would be helpful to have it in a comment above getEnum.\n\nbq.  just comment it out in BW branch\n\nThats what I'll do. Did the BW branch pass when you did it? If not, it would be helpful to commit that fix too, or call out the break loudly in this thread - its difficult to keep up on everything and track all of this down for these merges.\n\nbq.  So please keep the code clean and do not re-add this TE.\n\nOh, I had no plans to do it myself ;) I just commented out the BW compat test and put the comment you see above.",
            "date": "2009-12-01T23:23:03.675+0000",
            "id": 214
        },
        {
            "author": "Mark Miller",
            "body": "Though I do wonder ... if its not a break, why do we have the method there throwing UnsupportedExceptionOperation ... why isn't it just removed?",
            "date": "2009-12-01T23:24:57.509+0000",
            "id": 215
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. In what comment? Would be helpful to have it in a comment above getEnum.\n\nWill do! It's in the log message not comment.\n\nbq. Did the BW branch pass when you did it?\n\nI think so, at least in my checkout. I think the TermEnum test was added after 3.0?\n\nbq. Though I do wonder ... if its not a break, why do we have the method there throwing UnsupportedExceptionOperation ... why isn't it just removed? \n\nI did not look into the super class, which just returns null. I thought it was abstract.",
            "date": "2009-12-01T23:31:37.143+0000",
            "id": 216
        },
        {
            "author": "Uwe Schindler",
            "body": "Mark: The updated backwards branch does not pass because of this (I did not update my checkout, the Enum test was added before 3.0). So the test should be commented out there, too (but you said, you would do this). Else, I will do tomorrow, I am tired, I would produce to many errors - sorry.",
            "date": "2009-12-01T23:35:47.501+0000",
            "id": 217
        },
        {
            "author": "Uwe Schindler",
            "body": "I updated my commit comment above, so it's clear what I have done (copied from commit log message).",
            "date": "2009-12-01T23:49:13.835+0000",
            "id": 218
        },
        {
            "author": "Mark Miller",
            "body": "bq. Else, I will do tomorrow, I am tired, I would produce to many errors - sorry.\n\nNo problem - I got it now - just wasn't sure. Thats why I brought it up :)\n\nbq. It's in the log message not comment.\n\nYup - thats fine, no big deal. Was just saying it would be easier on me if there was a comment over it - I've got it now though - I'll just remove that method.",
            "date": "2009-12-01T23:49:57.458+0000",
            "id": 219
        },
        {
            "author": "Uwe Schindler",
            "body": "bq.  I'll just remove that method.\n\nIn my opinion the super method should throw UOE. If somebody misses to override either getTermsEnum() or getEnum() he will get a good message describing the problem, not just an NPE. The default impl of getTermsEnum() to return null is fine, because rewrite then delegates to getEnum(). If that also returns null, you get NPE.\n\nWe had the same problem with Filter.bits() after deprecation in 2.x - it was not solved very good. In the 2.9 TS BW layer / DocIdSetIterator bw layer it was done correctly.",
            "date": "2009-12-02T00:01:59.048+0000",
            "id": 220
        },
        {
            "author": "Uwe Schindler",
            "body": "This is what I am thinking about for BW and delegation between getEnum() and getTermsEnum().",
            "date": "2009-12-02T00:08:53.911+0000",
            "id": 221
        },
        {
            "author": "Mark Miller",
            "body": "Okay - thats sounds like a good idea - I'll leave it for after the merge is done though.",
            "date": "2009-12-02T00:28:01.167+0000",
            "id": 222
        },
        {
            "author": "Mark Miller",
            "body": "I've put the merge on hold for a bit - will try and come back to it tonight. Ive got to figure out why this BW compat test is failing, and haven't seen an obvious reason yet:\n\n{code}\njunit.framework.AssertionFailedError: expected:<> but was:<>\n\tat org.apache.lucene.search.TestWildcard.testEmptyTerm(TestWildcard.java:108)\n\tat org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:208)\n{code}\n\nPipe in if you know. Hard to debug or run this test singular in Eclipse (because of how BW compat tests work), so its a slow slog to trouble shoot and I haven't had time yet.",
            "date": "2009-12-02T13:20:09.461+0000",
            "id": 223
        },
        {
            "author": "Michael McCandless",
            "body": "I think that test failure was from my fix of BooleanQuery to take coord into account in equals & hashCode (LUCENE-2092)?  I hit exactly that same failure, and it required a fix on back-compat branch to just pass in \"true\" to the \"new BooleanQuery()\" done just before the assert.  Does that explain it?",
            "date": "2009-12-02T13:37:25.941+0000",
            "id": 224
        },
        {
            "author": "Michael McCandless",
            "body": "And, thanks for taking over on merging trunk down!  I'm especially looking forward to getting the faster unit tests (LUCENE-1844).",
            "date": "2009-12-02T13:38:09.085+0000",
            "id": 225
        },
        {
            "author": "Uwe Schindler",
            "body": "I have seen your change in the tests, too. The test just checks that no clauses are generated. In my opinion, it should  not compare to a empty BQ instance, instead just assert bq.clauses().size()==0.",
            "date": "2009-12-02T13:51:16.248+0000",
            "id": 226
        },
        {
            "author": "Michael McCandless",
            "body": "bq. In my opinion, it should not compare to a empty BQ instance, instead just assert bq.clauses().size()==0.\n\n+1, that'd be a good improvement -- I'll do that.",
            "date": "2009-12-02T13:54:57.070+0000",
            "id": 227
        },
        {
            "author": "Uwe Schindler",
            "body": "I rewrote to:\n{code}\npublic void testEmptyTerm() throws IOException {\n\tRAMDirectory indexStore = getIndexStore(\"field\", new String[]{\"nowildcard\", \"nowildcardx\"});\n\tIndexSearcher searcher = new IndexSearcher(indexStore, true);\n\n\tMultiTermQuery wq = new WildcardQuery(new Term(\"field\", \"\"));\n\twq.setRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE);\n\tassertMatches(searcher, wq, 0);\n\tQuery q = searcher.rewrite(wq);\n\tassertTrue(q instanceof BooleanQuery);\n\tassertEquals(0, ((BooleanQuery) q).clauses().size());\n}\n{code}",
            "date": "2009-12-02T14:06:00.610+0000",
            "id": 228
        },
        {
            "author": "Michael McCandless",
            "body": "Looks great -- can/did you commit?",
            "date": "2009-12-02T15:22:48.649+0000",
            "id": 229
        },
        {
            "author": "Mark Miller",
            "body": "bq. Does that explain it?\n\nThat was my initial guess and try - but neither true nor false fixed it.\n\nLooks like Uwes fix with side step the issue though? Sounds good to me :)",
            "date": "2009-12-02T15:27:57.736+0000",
            "id": 230
        },
        {
            "author": "Uwe Schindler",
            "body": "I can do this, but according to Mark, only with a new issue and patch... Just joking :-) ",
            "date": "2009-12-02T15:28:44.877+0000",
            "id": 231
        },
        {
            "author": "Mark Miller",
            "body": "Interesting ... after many, many runs without seeing that testreopen gc overhead limit exceeded, I just hit it again randomly.",
            "date": "2009-12-02T17:08:27.420+0000",
            "id": 232
        },
        {
            "author": "Mark Miller",
            "body": "bq. I can do this, but according to Mark, only with a new issue and patch... Just joking \n\nI put it in the BW branch, but not the flex branch yet.\n\nYeah, I'm a hardass, but I'm not in charge - just giving my opinion :) And I like how most things are fairly loose - I just worry about going to far down a road it will be hard to come back from - usually its so easy to get consensus, its easy to ignore it - but I think thats dangerous.\n\nAnd yes, I get that your just kidding, but for good reason - I don't mean to come off as the abrasive one, but sometimes I think someone has to, and since I'm already in that hole anyway ...",
            "date": "2009-12-02T17:12:29.643+0000",
            "id": 233
        },
        {
            "author": "Uwe Schindler",
            "body": "I put the better test into trunk/trunk BW. I could also put it into 3.0 and 2.9, but I do not think that is needed :)",
            "date": "2009-12-02T23:14:03.655+0000",
            "id": 234
        },
        {
            "author": "Uwe Schindler",
            "body": "Mike: When fixing the NRQ test Mark merged, I found a problem/inconsistency with FilteredTermsEnum:\n\nNormal usage of a termsEnum is that it is positioned on the first term (e.g. after calling getTermsEnum()). Normally you have a do-while-loop and call next() at the end, which is fine. Most code using TermsEnums first checks inside the do-while \"if (term()==null)\" and then break (incorrect positioned or exhausted termsenum). As the call to term() does not check the returned term, it may contain an term, that should normally be filtered. The same happens if you call term() after it is exhausted. The FilteredTermsEnum should return null for term() and docFreq() if the enum is empty or exhausted. I have seen that you added empty() to it, but for consistency the FilteredTermsEnum should return null/-1.\n\nI fixed the test to check for empty() (sorry for two commits, the assertNull check was wrong, I changed before committing).\n\nOpinions?",
            "date": "2009-12-02T23:36:33.000+0000",
            "id": 235
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Interesting ... after many, many runs without seeing that testreopen gc overhead limit exceeded, I just hit it again randomly.\n\nSheesh this one is annoying :)\n\nOh, I see -- we still need to cutover the standard codec's terms dict hash to use DBLRU instead of LinkedHashMap; that should fix it.  And actually after we do that we should re-run perf tests of the MTQs -- LinkedHashMap caused serious GC problems when I was testing automaton query.",
            "date": "2009-12-03T13:02:44.847+0000",
            "id": 236
        },
        {
            "author": "Mark Miller",
            "body": "Cool - was actually thinking about looking if you had done that yet last night (unrelatedly)\n\n*edit*\n\nHeh - though I should have known you handn't considering those classes came in on the merge - just confused about what has gotten down outside of merging I guess - I know there is an issue or two and for some reason thought this was one of them.",
            "date": "2009-12-03T13:12:40.448+0000",
            "id": 237
        },
        {
            "author": "Uwe Schindler",
            "body": "One thing I came along long time ago, but now with a new API it get's interesting again:\n\nDocsEnum should extend DocIdSetIterator, that would make it simplier to use and implement e.g. in MatchAllDocQuery.Scorer, FieldCacheRangeFilter and so on. You could e.g. write a filter for all documents that simply returns the docs enumeration from IndexReader.\n\nSo it should be an abstract class that extends DocIdSetIterator. It has the same methods, only some methods must be a little bit renamed. The problem is, because java does not support multiple inheritace, we cannot also extends attributesource :-( Would DocIdSetIterator be an interface it would work (this is one of the cases where interfaces for really simple patterns can be used, like iterators).\n\n*EDIT*\n\nMaybe an idea would be to provide a method asDocIdSetIterator(), if the multiple inheritance cannot be fixed. Or have the AttributeSource as a member field, which would be good, as it only needs to be created on first access then (because constructing an AttributeSource is costly). getAttributes() returning it and dynamically instantiating would be an idea. The same applies for TermsEnum, it should be separated for lazy init.",
            "date": "2009-12-03T13:20:28.467+0000",
            "id": 238
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Cool - was actually thinking about looking if you had done that yet last night (unrelatedly)\n\nFeel free to fix it!",
            "date": "2009-12-03T13:23:14.221+0000",
            "id": 239
        },
        {
            "author": "Michael McCandless",
            "body": "bq. DocsEnum should extend DocIdSetIterator\n\nIt'd be great if we could find a way to do this without a big hairball of back compat code ;)  They are basically the same, except DocsEnum lets you get freq() for each doc, get the PositionsEnum positions(), and also provides a bulk read API (w/ default impl).",
            "date": "2009-12-03T13:26:03.980+0000",
            "id": 240
        },
        {
            "author": "Michael McCandless",
            "body": "bq. getAttributes() returning it and dynamically instantiating would be an idea. The same applies for TermsEnum, it should be separated for lazy init.\n\nThat's a good point (avoid cost of creating the AttributeSource) -- that makes complete sense.",
            "date": "2009-12-03T13:27:35.437+0000",
            "id": 241
        },
        {
            "author": "Mark Miller",
            "body": "RE: the terms cache\n\nShould we still try and do the reuse stuff, or should we just drop it and use the cache as it is now? (eg reusing the object that is removed, if one is removed) Looks like that would be harder to get done now.",
            "date": "2009-12-03T13:29:32.291+0000",
            "id": 242
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. It'd be great if we could find a way to do this without a big hairball of back compat code\n\nDocsEnum is a new class, why not fit it from the beginning as DocIdSetIterator? In my opinion, as pointed out above, the AttributeSource stuff should go in as a lazy-init member behind getAttributes() / attributes().\n\nSo I would define it as:\n\n{code}\npublic abstract class DocsEnum extends DocIdSetIterator {\n  private AttributeSource atts = null;\n  public int freq()\n  public DontKnowClassName positions()\n  public final AttributeSource attributes() {\n   if (atts==null) atts=new AttributeSource();\n   return atts;\n  }\n  ...default impl of the bulk access using the abstract methods from DocIdSetIterator\n}\n{code}\n",
            "date": "2009-12-03T13:30:53.113+0000",
            "id": 243
        },
        {
            "author": "Uwe Schindler",
            "body": "Here the patch with refactoring DocsEnum.\n\nWith this patch MatchAllDocsQuery is very simple to implement now as a ConstantScoreQuery on top of a Filter that returns the DocsEnum of the supplied IndexReader as iterator. Really cool.",
            "date": "2009-12-03T14:27:12.371+0000",
            "id": 244
        },
        {
            "author": "Uwe Schindler",
            "body": "Updated patch: \n\nI did a search on \"AttributeSource\" in index package. I now also replaced the \"extends AttributeSource\" by a lazy init in in FieldsEnum and PositionsEnum. So all enums have an attributes() method that lazy inits an AttributeSource. When attributes get interesting a custom DocsEnum could just use attributes().addAttribute(XYZ.class) in its ctor and store the reference locally. attributes() is final (to be safe, when called by ctor).\n\nEventually add an Interface AttributeAble *g* that is implemented by all these enums and anywhere else using AttributeSource that may need to be lazy init.",
            "date": "2009-12-03T14:36:29.162+0000",
            "id": 245
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Should we still try and do the reuse stuff, or should we just drop it and use the cache as it is now?\n\nHow about starting w/o reuse but leave a TODO saying we could/should investigate?",
            "date": "2009-12-03T15:28:25.500+0000",
            "id": 246
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good Uwe!\n\nbq.  MatchAllDocsQuery is very simple to implement now as a ConstantScoreQuery on top of a Filter that returns the DocsEnum of the supplied IndexReader as iterator. Really cool.\n\nSweet!  Wait, using AllDocsEnum you mean?",
            "date": "2009-12-03T15:39:01.878+0000",
            "id": 247
        },
        {
            "author": "Michael McCandless",
            "body": "bq. How about starting w/o reuse but leave a TODO saying we could/should investigate?\n\nActually, scratch that -- reuse is too hard in DBLRU -- I would say just no reuse now.  Trunk doesn't reuse either...",
            "date": "2009-12-03T15:40:00.834+0000",
            "id": 248
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Sweet! Wait, using AllDocsEnum you mean?\n\nYes, but this class is package private and unused! AllTermDocs is used by SegmentReader to support termDocs(null), but not AllDocsEnum. There is no method in IndexReader that returns all docs?\n\nThe matchAllDocs was just an example, there are more use cases, e.g. a TermsFilter (that is the non-scoring TermQuery variant): Just use the DocsEnum of this term as the DicIdSetIterator.",
            "date": "2009-12-03T15:52:18.870+0000",
            "id": 249
        },
        {
            "author": "Michael McCandless",
            "body": "bq.  There is no method in IndexReader that returns all docs?\n\nNot yet (in flex API) -- we can add it?  IndexReader.allDocs(Bits skipDocs)?  Or we could make AllDocsEnum public?  Hmm.",
            "date": "2009-12-03T16:12:36.108+0000",
            "id": 250
        },
        {
            "author": "Michael McCandless",
            "body": "We will continue work under new issues -- this one has gotten too big!",
            "date": "2009-12-03T23:30:30.243+0000",
            "id": 251
        },
        {
            "author": "Michael McCandless",
            "body": "Hmm, somehow in the last merge, we lost the fixes for LUCENE-1558 (defaulting readOnly=true for IndexReader)... IndexSearcher looks like it didn't lose the change though.",
            "date": "2009-12-04T16:36:26.254+0000",
            "id": 252
        },
        {
            "author": "Mark Miller",
            "body": "Its not surprising - the merge command sucks from what I can tell :) Which is why I had to go line by line a merge or two ago to catch everything that had been dropped.\n\nI expected I'd have to do it again, but its a lot of effort to do every time.\n\n*edit*\n\nby line by line, I mean I go through a diff of every file comparing trunk and the flex branch - I'll do it again soon.",
            "date": "2009-12-04T16:54:44.629+0000",
            "id": 253
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Mark!  IndexReader.open looks good now.",
            "date": "2009-12-04T17:31:30.765+0000",
            "id": 254
        },
        {
            "author": "Michael McCandless",
            "body": "This issue is \"continuing\" under LUCENE-2111.",
            "date": "2009-12-25T11:15:59.986+0000",
            "id": 255
        }
    ],
    "component": "core/index",
    "description": "I attached a very rough checkpoint of my current patch, to get early\nfeedback.  All tests pass, though back compat tests don't pass due to\nchanges to package-private APIs plus certain bugs in tests that\nhappened to work (eg call TermPostions.nextPosition() too many times,\nwhich the new API asserts against).\n\n[Aside: I think, when we commit changes to package-private APIs such\nthat back-compat tests don't pass, we could go back, make a branch on\nthe back-compat tag, commit changes to the tests to use the new\npackage private APIs on that branch, then fix nightly build to use the\ntip of that branch?o]\n\nThere's still plenty to do before this is committable! This is a\nrather large change:\n\n  * Switches to a new more efficient terms dict format.  This still\n    uses tii/tis files, but the tii only stores term & long offset\n    (not a TermInfo).  At seek points, tis encodes term & freq/prox\n    offsets absolutely instead of with deltas delta.  Also, tis/tii\n    are structured by field, so we don't have to record field number\n    in every term.\n.\n    On first 1 M docs of Wikipedia, tii file is 36% smaller (0.99 MB\n    -> 0.64 MB) and tis file is 9% smaller (75.5 MB -> 68.5 MB).\n.\n    RAM usage when loading terms dict index is significantly less\n    since we only load an array of offsets and an array of String (no\n    more TermInfo array).  It should be faster to init too.\n.\n    This part is basically done.\n\n  * Introduces modular reader codec that strongly decouples terms dict\n    from docs/positions readers.  EG there is no more TermInfo used\n    when reading the new format.\n.\n    There's nice symmetry now between reading & writing in the codec\n    chain -- the current docs/prox format is captured in:\n{code}\nFormatPostingsTermsDictWriter/Reader\nFormatPostingsDocsWriter/Reader (.frq file) and\nFormatPostingsPositionsWriter/Reader (.prx file).\n{code}\n    This part is basically done.\n\n  * Introduces a new \"flex\" API for iterating through the fields,\n    terms, docs and positions:\n{code}\nFieldProducer -> TermsEnum -> DocsEnum -> PostingsEnum\n{code}\n    This replaces TermEnum/Docs/Positions.  SegmentReader emulates the\n    old API on top of the new API to keep back-compat.\n    \nNext steps:\n\n  * Plug in new codecs (pulsing, pfor) to exercise the modularity /\n    fix any hidden assumptions.\n\n  * Expose new API out of IndexReader, deprecate old API but emulate\n    old API on top of new one, switch all core/contrib users to the\n    new API.\n\n  * Maybe switch to AttributeSources as the base class for TermsEnum,\n    DocsEnum, PostingsEnum -- this would give readers API flexibility\n    (not just index-file-format flexibility).  EG if someone wanted\n    to store payload at the term-doc level instead of\n    term-doc-position level, you could just add a new attribute.\n\n  * Test performance & iterate.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1458",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Further steps towards flexible indexing",
    "systemSpecification": true,
    "version": "4.0-ALPHA"
}