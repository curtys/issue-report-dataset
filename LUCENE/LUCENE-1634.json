{
    "comments": [
        {
            "author": "Yasuhiro Matsuda",
            "body": "I posted a patch.",
            "date": "2009-05-12T16:40:41.248+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "Actually, optimize() always merges all segments down to 1,\nirrespective of deletes.  I think you're referring to \"normal\" merges?\n\nI think this approach is reasonable and a good step forward.  Can you\nmake this behaviour get/settable?  I think we should default to the\nold behaviour until 3.0, and then switch it to default to the new one\nat 3.0 (to preserve back compat).\n\nThinking more about this... I think we can further improve how\nLog*MergePolicy takes deletes into account.  Ie, why not explicitly\nmeasure the deletions and bias merge selection to favor merging away\nsegments that have the most deletions?\n\nThis might require relaxing the merge policy so that it's allowed to\npick fewer than mergeFactor segments to merge at once; perhaps it's\ngiven a min/max mergeFactor.\n\nLikely such a change should be a new merge policy...\n",
            "date": "2009-05-13T12:24:49.166+0000",
            "id": 1
        },
        {
            "author": "John Wang",
            "body": "This is actually referring to the optimize(int) call, which selectively merges segments to make sure the total segments in the index is less or equal to the specified number.",
            "date": "2009-05-13T16:12:03.829+0000",
            "id": 2
        },
        {
            "author": "John Wang",
            "body": "Comment on implementing a custom merge policy:\nAs the API current stands, I think the behavior is to assume a subclass of LogMergePolicy. And one cannot subclass LogMergePolicy without injecting the class into the org.apache.lucene.index package, (because the api signature: size(org.apache.lucene.index.SegmentInfo info), SegmentInfo is not an exposed API.",
            "date": "2009-05-13T16:16:35.589+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "bq. This is actually referring to the optimize(int) call\n\nAhh, woops -- sorry, I missed that the first time around.  But I don't think the patch addresses how optimize(int) selects its merges?",
            "date": "2009-05-13T16:20:32.656+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "bq. As the API current stands, I think the behavior is to assume a subclass of LogMergePolicy.\n\nHmm -- that's not intended.  What goes wrong if you use a custom MergePolicy that's not a LogMergePolicy?\n\n(There are utility methods on IndexWriter, eg set/getUseCompoundFile, etc., that will only work if your MergePolicy is a LogMergePolicy, but that shouldn't prevent you from using a non-LogMergePolicy as long as you don't use these utility methods).",
            "date": "2009-05-13T16:23:16.392+0000",
            "id": 5
        },
        {
            "author": "John Wang",
            "body": "The current lucene implementation, optimize(int) selects segments to merge based on the file size of the segment file: say the index has 10 segments, and optmize(6) is called,  Lucene finds 4 smallest segments by number of bytes in the segment files. \n\nThis selection criteria is flawed because you can have a very large segment in terms of bytes but very small in terms of numDocs (if many deleted docs). Having these segment files around impacts performance considerably. \n\nThis is what this patch is trying to fix this in a non-intrusive manner by extending the LogMergePolicy and by normalizing the calculation of the segment size including the delete count.\n",
            "date": "2009-05-13T16:29:36.132+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "bq. say the index has 10 segments, and optmize(6) is called, Lucene finds 4 smallest segments by number of bytes in the segment files.\n\nOh yeah, you're right: for the last (partial) merge, it uses the size() of the segments to pick the merge that's minimal total size.\n\nSo let's proceed with this patch, once you've added setter/getter.\n\nI'd still love to see a merge policy that does a better job explicitly \"targeting\" segments with lots of deletions.",
            "date": "2009-05-13T16:42:08.713+0000",
            "id": 7
        },
        {
            "author": "John Wang",
            "body": "RE: implementing custom MergePolicy\nLet me describe in detail on problems of implementing a custom MergePolicy:\n\n1) In IndexWriter code, such methods on MergePolicy is called, e.g. findMergesForOptimize. I believe that is the contract for implementing your own MergePolicy. However, it is \"hidden\" by the javadoc in terms of documentation, and furthermore, it is hidden because these methods are package protected. So to implement your own MergePolicy, you have to resort back to sneaking the class into the package.\n\n2) Not only seg/getUseCompoundFile is no longer applicable if LogMergePolicy is not used, also popular methods such as set/getMergeFactor etc. are only applicable to LogMergePolicy. (Just to clarify, useCompoundFile is a package-level protected method on the base MergePolicy class, so my guess is that set/getCompoundFile should be applicable to all implementations of MergePolicy.\n\nThis brings up another issue about the practice of having to \"sneak\" classes into a package. We are looking at making our Lucene code, OSGI compliant, and this becomes an issue because we cannot have multiple \"bundles\" exporting the same package. Which means, I would have to repackage lucene to include my classes that I have snuck into some lucene packages. I would like to use a standard distribution of  a lucene jar (as suggested/echoed by some luceners).\n",
            "date": "2009-05-13T16:49:07.777+0000",
            "id": 8
        },
        {
            "author": "John Wang",
            "body": ">> So let's proceed with this patch, once you've added setter/getter.\nCan you please elaborate on this? Add setter/getter on what? The number of target segment is already an input parameter? do you mean some sort of normalization factor on the how much to \"punish\" segments with high deleted docs?",
            "date": "2009-05-13T16:54:03.574+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "I mean a setter/getter to turn on/off \"taking deletions into account\" in Log*MergePolicy.",
            "date": "2009-05-13T17:01:22.380+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "bq. So to implement your own MergePolicy, you have to resort back to sneaking the class into the package.\n\nRight, this is currently necessary for a custom MergePolicy/Scheduler.  It's been discussed before:\n\n  http://www.nabble.com/MergePolicy-public-but-SegmentInfos-package-protected--tt22687527.html\n\nI suppose since merge selection needs so little info about a segment, we could make a public thine wrapper/veneer that exposes a limited number of things.  Or maybe we go whole hog and simply make SegmentInfos/SegmentInfo public.\n\nbq.  it is hidden because these methods are package protected\n\nIf we could javadoc certain package protected classes, that'd give you the javadocs at least.  Or we should simply make these methods public?\n\nbq. Not only seg/getUseCompoundFile is no longer applicable if LogMergePolicy is not used, also popular methods such as set/getMergeFactor etc. are only applicable to LogMergePolicy.\n\nBut the notion of a mergeFactor is very much a LogMergePolicy specific thing.  Other merge policies might not limit themselves to always merging mergeFactor segments at once.  These are convenience methods on IndexWriter (that simply forward the request to the MergePolicy).\n\nbq. my guess is that set/getCompoundFile should be applicable to all implementations of MergePolicy\n\nI think that'd make sense.  (I can't remember exactly why, but way back when, I think there was some reason for not doing so...)",
            "date": "2009-05-13T17:22:44.245+0000",
            "id": 11
        },
        {
            "author": "John Wang",
            "body": ">>I mean a setter/getter to turn on/off \"taking deletions into account\" in Log*MergePolicy.\nMakes sense. What do you suggest the default behavior to be?\nAlso, do you think setter/getter is the right approach, since this is very much hidden to the API, e.g. one would have to do this:\n\nLogMergePolicy policy = (LogMergePolicy)idxWriter.getMergePolicy();\npolicy.setTurnOnSegmentCalcWithDeletes(true);\n\nDO you think instead, we can just add static setter/getter on LogMergePolicy class?\n",
            "date": "2009-05-13T17:41:40.898+0000",
            "id": 12
        },
        {
            "author": "Michael McCandless",
            "body": "bq. What do you suggest the default behavior to be?\n\nI think default to no change, and in 3.0 we flip it?\n\nbq. DO you think instead, we can just add static setter/getter on LogMergePolicy class?\n\nI'd lean towards non-static setter/getters.",
            "date": "2009-05-13T19:19:42.363+0000",
            "id": 13
        },
        {
            "author": "Yasuhiro Matsuda",
            "body": "I added setCalibrateSizeByDeletes/getCalibrateSizeByDeletes. Sorry that the method names are a bit too wordy.",
            "date": "2009-05-13T23:26:24.338+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good, thanks Yasuhiro.  I plan to commit in a day or two.",
            "date": "2009-05-14T15:44:24.501+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Yasuhiro!",
            "date": "2009-05-16T15:59:22.209+0000",
            "id": 16
        }
    ],
    "component": "core/index",
    "description": "I found that IndexWriter.optimize(int) method does not pick up large segments with a lot of deletes even when most of the docs are deleted. And the existence of such segments affected the query performance significantly.\n\nI created an index with 1 million docs, then went over all docs and updated a few thousand at a time.  I ran optimize(20) occasionally. What saw were large segments with most of docs deleted. Although these segments did not have valid docs they remained in the directory for a very long time until more segments with comparable or bigger sizes were created.\n\nThis is because LogMergePolicy.findMergeForOptimize uses the size of segments but does not take the number of deleted documents into consideration when it decides which segments to merge. So, a simple fix is to use the delete count to calibrate the segment size. I can create a patch for this.\n\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1634",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "LogMergePolicy should use the number of deleted docs when deciding which segments to merge",
    "systemSpecification": true,
    "version": ""
}