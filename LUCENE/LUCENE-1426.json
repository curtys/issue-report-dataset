{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "Attached patch.  I think it's ready to commit... I'll wait a few days.\n\nThis factors the writing of postings into separate Format* classes.\nThe approach I took is similar to what I did for DocumentsWriter,\nwhere there is a hierarchical consumer interface (abstract class) for\neach of fields, terms, docs, and positions writing.  Then there's a\ncorresponding set of concrete classes (the \"codec chain\") that write\ntoday's index format.  There is no change to the index format.\n\nHere are the details:\n\n  * This only applies to postings (not stored fields, term vectors,\n    norms, field infos)\n\n  * Both SegmentMerger & FreqProxTermsWriter now use the same codec\n    API to write postings.  I think this is a big step forward: we now\n    have a single set of classes that ever write the postings.\n\n  * You can't yet customize this codec chain; we can add that at some\n    point.  It's all package private.\n\n  * I don't yet allow the codec to override SegmentInfo.files(); at\n    some point (when I first try to make a codec that uses different\n    files) I will add this.\n\nI ran a quick performance test, indexing wikipedia, and found\nnegligible performance cost of this.\n\nThe next step, which is trickier, is to modularize/genericize the\nclasses the read from the index, and then refactor\nSegmentTerm{Enum,Docs,Positions} to use that codec API.\n\nThen, finally, I want to make a codec that uses PFOR to encode\npostings.",
            "date": "2008-10-20T12:41:00.972+0000",
            "id": 0
        },
        {
            "author": "Paul Elschot",
            "body": "bq. We inline payloads with positions which would also mess up the int blocks.\n\nWhich begs the question whether we should also allow compression of these payloads.\nI think we should do that because normally only one or two bytes will be used as payload per position.\nThinking about this: position+payload actually looks a lot like docId+freq, could that\nbe used to simplify future index formats for inverted terms?\nBtw. allowing a payload to accompany the field norms would allow to store a kind of\ndictionary for the position payloads. This could help to keep the position payloads small\nso they would compress nicely.\n\nbq. Both SegmentMerger & FreqProxTermsWriter now use the same codec API to write postings.\n\nThat is indeed a big step.\n\nbq. It's all package private.\n\nGood for now, making it public might actually reduce flexibility for new index formats.\n\n",
            "date": "2008-10-20T18:58:53.353+0000",
            "id": 1
        },
        {
            "author": "Paul Elschot",
            "body": "bq. Skipping offsets and TermInfo offsets hardwire the file pointers of  frq & prox files yet I need to change these to block + offset, etc.\n\nDoes the offset imply that there is also a need for random access into each block?\nFor such blocks PFOR patching might better be avoided.\nEven with patching random access is possible, but it is not available yet at LUCENE-1410.\n",
            "date": "2008-10-20T19:07:41.714+0000",
            "id": 2
        },
        {
            "author": "Eks Dev",
            "body": "Just a few random thoughts on this topic\n\n- I am sure I read somewhere in these pdfs that were floating around that it would make sense to use VInts for very short postings and PFOR for the rest. I just do not remember rationale behind it.   \n\n- During omitTf() discussion, we came up with cool idea to actually inline very short postings into term dict instead of storing offset. This way we spare one seek per term in many cases, as well as some space for storing offset. I do not know if this is a problem, but sounds reasonable. With standard Zipfian distribution, a lot of postings should get inlined. Use cases where we have query expansion on many terms (think spell checker, synonyms ...) should benefit from that heavily. These postings are small but there is a lot of them, so it adds up... seek is deadly :)\n\nI am sorry to miss the party here with PFOR, but let us hope this credit crunch gets over soon so I that I could dedicate some time to fun things like this :)\n\ncheers, eks \n\n\n  ",
            "date": "2008-10-20T19:27:48.165+0000",
            "id": 3
        },
        {
            "author": "Doug Cutting",
            "body": "+1 This sounds like a great way to approach flexible indexing: incrementally.",
            "date": "2008-10-20T19:51:11.032+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "bq. During omitTf() discussion, we came up with cool idea to actually inline very short postings into term dict instead of storing offset.\n\nYes, there's this issue:\n\n  https://issues.apache.org/jira/browse/LUCENE-1278\n\nAnd you had found this one:\n\n  http://www.siam.org/proceedings/alenex/2008/alx08_01transierf.pdf\n\nAnd then Doug referenced this:\n\n  http://citeseer.ist.psu.edu/cutting90optimizations.html\n\nI think the idea makes tons of sense (saving a seek) and one of my\ngoals in phase 2 (genericizing the reading of an index) is to make\npulsing a drop-in codec as an example & litmus test.  Terms iteration\nmay suffer, though, unless we put this in a separate file.\n\nI also think, at the opposite end of the spectrum, it would make sense\nfor very common terms to use simple n-bit packing (PFOR minus the\nexceptions).  For massive terms we need the fastest search we can\nget, since that gates when you have to start sharding.\n\nbq. I am sorry to miss the party here with PFOR, but let us hope this credit crunch gets over soon so I that I could dedicate some time to fun things like this\n\nWell the stock market seems to think the credit crunch is improving,\ntoday... of course who knows what'll happen tomorrow!  Good luck :)\n\nAlso, I'd like to explore improving the terms dict indexing -- I don't\nthink we need to load a TermInfo instance for every indexed term, into\nRAM.  I think we just need the term & seek data (into the tis file),\nthen you seek there and skip to the TermInfo you need.  This should\nsave a good amount of RAM for large indices with odd terms, sicne each\nTermInfo instance requires a pointer to it (4 or 8 bytes), an object\nheader (8 bytes at least) then 20 bytes for the members.\n\nAll these explorations should become simple drop-in codecs, once I can\nfinish phase 2.\n",
            "date": "2008-10-20T19:57:33.137+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nDoes the offset imply that there is also a need for random access into each block?\nFor such blocks PFOR patching might better be avoided.\nEven with patching random access is possible, but it is not available yet at LUCENE-1410.\n{quote}\n\nYeah this is one of the reasons why I'm thinking for frequent terms we\nmay want to fallback to pure nbit packing (which would make random\naccess simple).\n\nBut, for starters would could simply implement random access as \"load\n& decode the entire block, then look at the part you want\" and then\nassess the cost.  While it will clearly increase the cost of queries\nthat do alot of skipping (eg AND query of N terms), it may not matter\nso much since these queries should be fairly fast now.  It's the OR of\nfrequent term queries that we need to improve since that limits how\nbig an index you can put on one box.\n",
            "date": "2008-10-20T20:07:15.539+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. Which begs the question whether we should also allow compression of these payloads.\n\nI think that's interesting, but would probably be rather application dependent.\n\n{quote}\nBtw. allowing a payload to accompany the field norms would allow to store a kind of\ndictionary for the position payloads. This could help to keep the position payloads small\nso they would compress nicely.\n{quote}\n\nCouldn't stored fields, once they are faster (with column-stride\nfields, LUCENE-1231) solve this?\n",
            "date": "2008-10-20T20:11:18.739+0000",
            "id": 7
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\n+1 This sounds like a great way to approach flexible indexing: incrementally. \n{quote}\n\nCouldn't agree more. This is great!\n\n{quote}\nThe next step, which is trickier, is to modularize/genericize the\nclasses the read from the index, and then refactor\nSegmentTerm(Enum,Docs,Positions) to use that codec API.\n{quote}\n\nYes this is definitely the tricky part. I've been thinking a bit about this and was wondering if for the read APIs we could do something similar as with the new Token API (LUCENE-1422)? TermDocs could have a list of Attributes that the posting list offers. If for example no payloads are stored in the posting list, then TermDocs should not offer that corresponding Attribute.\nThis approach should be just as fast as the current API. When the application opens a TermDocs, it could check for the offered Attributes before it starts iterating the postinglist, and keep references to the Attribute. (in fact that's exactly the same approach as the TokenStream/Token/Consumer approach in LUCENE-1422).\n\nThoughts?",
            "date": "2008-10-21T20:26:36.449+0000",
            "id": 8
        },
        {
            "author": "Paul Elschot",
            "body": "bq. ... it would make sense to use VInts for very short postings and PFOR for the rest. I just do not remember rationale behind it.\nbq. ... cool idea to actually inline very short postings into term dict instead of storing offset.\n\nIirc the rationale was that PFOR has most performance benefits on integer arrays of more than 100 elements.\nShorter lists of numbers might also benefit from using (P)FOR instead of VInt, I don't know how big the break even size is.\n\nbq. for starters (we) could simply implement random access as \"load & decode the entire block, then look at the part you want\" and then assess the cost.\n\nI've just started some performance tests on PFOR patching (i.e. filling in the exceptions), and I'm not happy with what I'm seeing. More on this later at 1410.\n\n\nOn allowing a payload to accompany the field norms:\nbq. Couldn't stored fields, once they are faster (with column-stride fields, LUCENE-1231) solve this?\n\nYes.\n",
            "date": "2008-10-21T21:17:57.169+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "bq. TermDocs could have a list of Attributes that the posting list offers.\n\nI like this approach.\n\nThough unlike LUCENE-1422, where Token remains separate from\nTokenStream (and I'm still not sure it should be...?), I think for\nTermDocs there would not be the analog of a separate Token.\nIe, it would look something like this:\n\n  myPerDocAttr = termDocs.getAttribute(MyPerDoc.class);\n\n  while(termDocs.next()) {\n    x = myPerDocAttr.getValue(...);\n  }\n\nHowever, this form of flexibility is actually beyond what I'm aiming\nfor, for the first step of reader flexibility (there are so many\nfacets of \"flexible indexing\"!).\n\nFor starters I'd like to allow flexibility on how you encode the\nexisting postings (doc/freq/positions/payloads).  Whereas this\nflexibility is in extending what stuff is actually stored into & read\nfrom the index.  I think we should do both, but my focus now is on the\nfirst one, specifically to be able to drop in a codec that uses\npulsing, a less RAM-intestive terms dict indexing, and/or PFOR, etc.\n",
            "date": "2008-10-22T09:18:45.467+0000",
            "id": 10
        }
    ],
    "component": "core/index",
    "description": "In working on LUCENE-1410 (PFOR compression) I tried to prototype\nswitching the postings files to use PFOR instead of vInts for\nencoding.\n\nBut it quickly became difficult.  EG we currently mux the skip data\ninto the .frq file, which messes up the int blocks.  We inline\npayloads with positions which would also mess up the int blocks.\nSkipping offsets and TermInfo offsets hardwire the file pointers of\nfrq & prox files yet I need to change these to block + offset, etc.\n\nSeparately this thread also started up, on how to customize how Lucene\nstores positional information in the index:\n\n  http://www.gossamer-threads.com/lists/lucene/java-user/66264\n\nSo I decided to make a bit more progress towards \"flexible indexing\"\nby first modularizing/isolating the classes that actually write the\nindex format.  The idea is to capture the logic of each (terms, freq,\npositions/payloads) into separate interfaces and switch the flushing\nof a new segment as well as writing the segment during merging to use\nthe same APIs.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1426",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Next steps towards flexible indexing",
    "systemSpecification": true,
    "version": ""
}