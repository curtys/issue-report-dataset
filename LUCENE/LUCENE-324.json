{
    "comments": [
        {
            "author": "Ray Tsang",
            "body": "Created an attachment (id=13749)\nPatch for ChineseTokenizer to correctly count offsets\n",
            "date": "2004-12-14T18:10:05.000+0000",
            "id": 0
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Ray: is there a simple way you can show that this is indeed a needed fix?  Maybe\na short class that shows that offsets are wrong.\n\n\nLucene developers: can anyone confirm whether this is really needed it?  I don't\nuse ChineseTokenizer enough to know for sure if this is a good fix, or something\nthat will break the code.",
            "date": "2004-12-15T08:13:20.000+0000",
            "id": 1
        },
        {
            "author": "Ray Tsang",
            "body": "Created an attachment (id=13758)\nTestcase that tests ChineseTokenizer and OTHER_LETTER offsets\n\nThe problem arises when OTHER_LETTER characters and the rest of the characters\nare mixed together.  When given a string \"a&#22825;b\", tokens and corresponding\noffsets should be the following:\na : (0, 1)\n&#22825; : (1, 2)\nb : (2, 3)",
            "date": "2004-12-15T12:01:06.000+0000",
            "id": 2
        },
        {
            "author": "Ray Tsang",
            "body": "I haven't done a formal trace of the code yet, but I think it would make sense\nthat the offset should only be incremented if the character is pushed into the\nbuffer.  Current code, howerver, increments offset by default, regardless\nwhether the character is pushed into the buffer.\n\nIf that's the case, then there are more places that needs to be fixed.",
            "date": "2004-12-15T12:16:19.000+0000",
            "id": 3
        },
        {
            "author": "Erik Hatcher",
            "body": "Ray - ???  (let's see if JIRA can handle Chinese :)  Sorry for the delay in applying this patch.",
            "date": "2005-12-05T08:09:01.000+0000",
            "id": 4
        }
    ],
    "component": "modules/analysis",
    "description": "Apparently, in ChineseTokenizer, offset should be decremented like bufferIndex\nwhen Character is OTHER_LETTER.  This directly affects startOffset and endOffset\nvalues.\n\nThis is critical to have Highlighter working correctly because Highlighter marks\nmatching text based on these offset values.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-324",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Trivial",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "org.apache.lucene.analysis.cn.ChineseTokenizer missing offset decrement",
    "systemSpecification": false,
    "version": ""
}