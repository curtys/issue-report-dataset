{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "Initial patch.... it has tons of nocommits but I think it's basically\nworking correctly.\n\nThe packing is fairly simplistic now, but we can improve it with time\n(I know Dawid has done all sorts of cool things!): it chooses the top\nN nodes (sorted by incoming arc count) and saves them dereferenced so\nthat nodes w/ high in-count get a \"low\" address.  It also saves the\npointer as delta vs current position, if that would take fewer bytes.\nThe bytes are then in \"forward\" order.\n\nThe size savings varies by FST... eg, for the all-Wikipedia-terms FSA\n(no outputs) it reduces byte size by 21%.  If I map to ords (FST) then\nit's only 13% (I don't do anything to pack the outputs now, so the\nbytes required for them are unchanged).\n\nWhile the resulting FST is smaller, there is some hit to lookup (~8%\nfor the Wikipedia ord FST), because we have to deref some nodes.\n\nI only turned packing on for one thing: the Kuromoji FST (shrank by\n14%, 272 KB).\n",
            "date": "2012-01-26T17:40:20.723+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "New patch with all nocommits removed.  I also optimized Util.get and\nFST.findTargetArc a bit.  I turned off packing for Kuromoji, since the\nJAR actually got larger (despite FST getting smaller)... strange.\n\nI think it's ready!\n",
            "date": "2012-01-27T17:34:50.570+0000",
            "id": 1
        },
        {
            "author": "Uwe Schindler",
            "body": "Mike: This his somehow opposite to the last comment:\n\nbq. I only turned packing on for one thing: the Kuromoji FST (shrank by 14%, 272 KB).\n\nvs.:\n\nbq. I turned off packing for Kuromoji, since the JAR actually got larger (despite FST getting smaller)... strange.\n\nWhat's going on? Does this mean the FST and the TokenInfoDict$fst.dat file was smaller, but the resulting JAR file bigger - can be a -gzip- _inflate_ compression problem? I can only repeat Simon: https://www.destroyallsoftware.com/talks/wat ;-)\n\nAs you wanted to enable this because of Kumoroji and now don't use it at all, is there any use-case in Lucene using the compaction?\n\nI am just confused!\n\nP.S.: Does this change FST file format again?",
            "date": "2012-01-27T21:21:12.482+0000",
            "id": 2
        },
        {
            "author": "Dawid Weiss",
            "body": "A smaller automaton may result in a larger JAR. A smaller automaton will usually have a denser representation and larger entropy. I've observed a similar phenomenon when I was working on automata compression for morfologik -- the ZIP file didn't grow there but it remained pretty much constant as the automaton was (progressively) made smaller and smaller.",
            "date": "2012-01-27T21:42:00.555+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "That's exactly right -- the FST got smaller, but harder for zip to compress, so the JAR got bigger.\n\nI haven't turned it on anywhere in Lucene yet... I think it need to be an explicit choice since it requires extra RAM after building.",
            "date": "2012-01-27T22:13:59.502+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "bq. P.S.: Does this change FST file format again?\n\nOh sorry forgot to answer: yes!",
            "date": "2012-01-27T22:16:43.131+0000",
            "id": 5
        },
        {
            "author": "Uwe Schindler",
            "body": "Thanks @ Dawid and Mike. I expected something like that, but when I read the issue, this was just funny! Thanks for clarification, it seems that there was background information (maybe only discussed in IRC) missing.\n\nAbout the Kumoroji: In my opinion, the JAR size is not the major thing. The reason why Robert and me was compacting structures was to a) reduce SVN checkout size (this should improve by packing) b) reduce memory footprint of the Analyzer (and that is also improvement). As the FST is only build once during rebuilding the binary FST from the dict and not at runtime, should we not package for memory efficiency?",
            "date": "2012-01-27T22:29:14.715+0000",
            "id": 6
        },
        {
            "author": "Robert Muir",
            "body": "Well it would be good to test kuromoji performance too before cutting over.\n\nif the performance is not impacted, I tend to agree with Uwe that decreasing memory usage is a good thing, \neven if the jar is slightly larger (how much?), especially in this case as it costs no code complexity to the analyzer.\n",
            "date": "2012-01-27T22:35:48.125+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "OK, I agree... I'll run perf tests of Kuromoji and post back...\n\nI'll re-check the JAR size impact; I think it was only a bit larger.",
            "date": "2012-01-28T00:16:59.657+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "OK I turned packing back on for Kuromoji's TokenInfoDict... this\nreduces size from 1954846 to 1498215 bytes (23% smaller = 445.9 KB).\n\nAnd.... now JAR is a bit smaller: 4533833 vs 4570053 bytes (~35\nKB).  What changed was... I tweaked the params to pack (there are 2\nint params) and got better packing than before.\n\nI wrote a simple perf test (Perf.java attached)... and as far as I can\ntell the perf change is within hotspot noise... with trunk I get this:\n{noformat}\n1366 msec; 688.1405563689605 tokens/msec\n1006 msec; 934.3936381709741 tokens/msec\n1020 msec; 921.5686274509804 tokens/msec\n938 msec; 1002.1321961620469 tokens/msec\n937 msec; 1003.2017075773746 tokens/msec\n942 msec; 997.8768577494692 tokens/msec\n938 msec; 1002.1321961620469 tokens/msec\n940 msec; 1000.0 tokens/msec\n939 msec; 1001.0649627263045 tokens/msec\n939 msec; 1001.0649627263045 tokens/msec\n{noformat}\n\nAnd with the packed FST I get this:\n\n{noformat}\n1366 msec; 688.1405563689605 tokens/msec\n1003 msec; 937.1884346959123 tokens/msec\n1014 msec; 927.0216962524655 tokens/msec\n934 msec; 1006.423982869379 tokens/msec\n935 msec; 1005.3475935828877 tokens/msec\n936 msec; 1004.2735042735043 tokens/msec\n935 msec; 1005.3475935828877 tokens/msec\n938 msec; 1002.1321961620469 tokens/msec\n936 msec; 1004.2735042735043 tokens/msec\n935 msec; 1005.3475935828877 tokens/msec\n{noformat}\n\nBut (annoyingly, as usual!) the results can differ quite a bit\ndepending on how hotspot flips coins on startup...\n",
            "date": "2012-01-28T11:21:41.831+0000",
            "id": 9
        },
        {
            "author": "Uwe Schindler",
            "body": "+1 to enable packing!",
            "date": "2012-01-28T11:46:44.465+0000",
            "id": 10
        },
        {
            "author": "Uwe Schindler",
            "body": "About that one:\n\n{code:java}\n// TODO: could require caller to pass in the\n// FSTReader... since a tokenStream is thread private\n// anyway...\nreturn fst.findTargetArc(ch, follow, arc, fst.getBytesReader(0));\n{code}\n\nThe problem is that the TokenInfoFST is a singleton, right? I am not sure that the caller should pass that in, it makes it too complicated. The change a few lines before is fine (getting the reader before loop execution), but a class thats used by multiple threads should not reuse across method calls.",
            "date": "2012-01-28T12:03:12.038+0000",
            "id": 11
        },
        {
            "author": "Michael McCandless",
            "body": "Right, but eg Viterbi is thread private instance right?  So, eg, Viterbi could hold an FST.BytesReader that it passes into TokenInfoFST.findTargetArc... really TokenInfoFST is just a \"thin wrapper\" around an FST.",
            "date": "2012-01-28T13:20:10.526+0000",
            "id": 12
        },
        {
            "author": "Uwe Schindler",
            "body": "Ok, should be easy to implement. And its an internal class, so its not risky. We should just not make public APIs that way.",
            "date": "2012-01-28T13:40:48.604+0000",
            "id": 13
        },
        {
            "author": "Michael McCandless",
            "body": "OK I added the FST.BytesReader to TokenInfoFST.findTargetArc (matching the FST API itself)... and discovered Viterbi isn't thread private (the instance is shared across threads)... so I just pull and reuse the reader in Viterbi.build.",
            "date": "2012-01-28T18:11:12.287+0000",
            "id": 14
        },
        {
            "author": "Uwe Schindler",
            "body": "Yes, I had the same problem at another place in Viterbi, too ;-) It's not easy to understand. In my case I wanted to reuse a scratch IntsRef for the targetMap retrieval, but that failed horrible :-)",
            "date": "2012-01-28T19:04:08.664+0000",
            "id": 15
        },
        {
            "author": "Robert Muir",
            "body": "Just some numbers with another (CJK) fst I have been playing with, this one uses BYTE2 + \"SingleByteOutput\"\nBefore:\n  Finished: 326915 words, 77222 nodes, 358677 arcs, 2617255 bytes... \n  Zipped: 1812629 bytes\nPacked:\n  Finished: 326915 words, 77222 nodes, 358677 arcs, 2027763 bytes...\n  Zipped: 1735486 bytes",
            "date": "2012-01-30T02:22:38.119+0000",
            "id": 16
        },
        {
            "author": "Dawid Weiss",
            "body": "I had the time to look at the patch, finally. Yes, this is pretty much the top-n nodes reordering that I did, albeit without any optimization of how many n to take (the hardcoded magic constants should probably be justified somehow? Or replaced by a default in FST somewhere?). Deciding how many nodes to reorder is I think hard -- I failed to provide any sensible fast heuristic for that and I simply do a simulated annealing to find a local optimum.\n\nOne thing I was wondering is why you decided to integrate the packer with the fst -- wouldn't it be cleaner to separate packing from construction? Granted, this requires a double pass over the fst nodes and more intermediate memory but it wouldn't add any more complexity to the builder which is already, ehm, a bit complex ;). You can compare this design in Morfologik:\n\nBuilder:\nhttp://morfologik.svn.sourceforge.net/viewvc/morfologik/morfologik-stemming/trunk/morfologik-fsa/src/main/java/morfologik/fsa/FSABuilder.java?revision=343&view=markup\n\nSerialization (optimized or not, takes ant FSA on input) (method #linearize):\nhttp://morfologik.svn.sourceforge.net/viewvc/morfologik/morfologik-stemming/trunk/morfologik-fsa/src/main/java/morfologik/fsa/CFSA2Serializer.java?revision=343&view=markup\n\nI any way, the patch looks good to me.",
            "date": "2012-01-30T08:30:28.460+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Dawid.\n\nbq. Yes, this is pretty much the top-n nodes reordering that I did, albeit without any optimization of how many n to take (the hardcoded magic constants should probably be justified somehow? Or replaced by a default in FST somewhere?).\n\nbq. Deciding how many nodes to reorder is I think hard \u2013 I failed to provide any sensible fast heuristic for that and I simply do a simulated annealing to find a local optimum.\n\nYeah the algo is simplistic now... and it does force caller to pick the params (though minInCountDeref=3 and maxDerefNodes=Inf are probably pretty good)... we can and should make it more sophisticated over time.  We have at least one spare bit to still use in the arc flags :)\n\nbq. One thing I was wondering is why you decided to integrate the packer with the fst \u2013 wouldn't it be cleaner to separate packing from construction? Granted, this requires a double pass over the fst nodes and more intermediate memory but it wouldn't add any more complexity to the builder which is already, ehm, a bit complex . You can compare this design in Morfologik:\n\nWell... it's tricky.  First, I decided to change node targets to ords so that pack could use an array to (relatively!) efficiently hold node data, eg inCount, newAddress, etc.  That required making the first pass FST \"modal\" (deref'd or not).  If we didn't do this then the packer would have to use even less RAM efficient data structures (eg Map<Int,X>) I think?\n\nSecond, the format written by the packer is tightly coupled with the FST reading, ie there are sizable differences when reading packed vs unpacked FST.\n\nBut I agree it'd be cleaner if we could move packing out (eg Util.pack), and more strongly decouple packing from FST format...\n\nOne thing I'd really like to somehow do is create different classes for FST writing vs reading, and different classes for each format.  We now have write-ord, write-non-ord, read-packed, read-unpacked (and even the two readers have 3 different modes depending on INPUT_TYPE).\n\n",
            "date": "2012-01-30T10:44:31.589+0000",
            "id": 18
        },
        {
            "author": "Dawid Weiss",
            "body": "bq. If we didn't do this then the packer would have to use even less RAM efficient data structures (eg Map<Int,X>) I think?\n\nYes, this is exactly what I used (although I used a primitive-backed hash maps from HPPC), but the overhead will be there, sure.\n\nbq. Second, the format written by the packer is tightly coupled with the FST reading, ie there are sizable differences when reading packed vs unpacked FST. \n\nRight. I have a different design in which the FSA is an abstract superclass and the implementation provides methods to walk the edges/ nodes. The writers simply walk that structure when serializing. Reading is delegated to a reader that can understand a particular format (and then provide a traversal implementation over raw bytes).\n\nI do have major simplifications over Lucene's version so this wouldn't be easy to do in Lucene's case without sacrificing performance.\n\n\n\n",
            "date": "2012-01-30T11:20:19.212+0000",
            "id": 19
        }
    ],
    "component": "core/FSTs",
    "description": "The FSTs produced by Builder can be further shrunk if you are willing\nto spend highish transient RAM to do so... our Builder today tries\nhard not to use much RAM (and has options to tweak down the RAM usage,\nin exchange for somewhat lager FST), even when building immense FSTs.\n\nBut for apps that can afford highish transient RAM to get a smaller\nnet FST, I think we should offer packing.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-3725",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Add optional packing to FST building",
    "systemSpecification": true,
    "version": ""
}