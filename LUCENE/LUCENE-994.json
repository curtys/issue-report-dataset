{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "\nAttached patch.\n\nI changed these defaults:\n\n  * Use ConcurrentMergeScheduler so merges are run in the background.\n\n  * Flush by RAM usage by default.  I set buffer size to 16 MB.\n\n  * Merge segments according to byte size, not doc count.\n\nMost unit tests just passed, but a handful had to be switched back to\nLogDocMergePolicy because they were checking specific doc-count based\ndetails on how merges are selected.  All tests pass now.\n\nI added an entry in CHANGES.txt under \"Changes in runtime behavior\"\nincluding a NOTE for ParallelReader users.  I think when we release\n2.3 we should also put a caveat into the release announcement calling\nattention to this specifically for users of ParallelReader.\n\nI also fixed up contrib/benchmark to accept double-typed params and to\npull the defaults for its Open/CreateIndexTask from IndexWriter's\ndefaults.\n\nFinally I had to make a few small changes to gdata-server.",
            "date": "2007-09-19T21:42:35.698+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "I just committed this!\n\nThis is a non-backwards-compatible change (and affects at least users\nof ParallelReader).  I put a comment in the top section of CHANGES.txt\nexplaining this.",
            "date": "2007-09-25T20:04:26.501+0000",
            "id": 1
        },
        {
            "author": "Mark Miller",
            "body": "Perhaps this is expected, but my experience:\n\nI load all my docs and then optimize the index. I load with a mergefactor of 3 because I have found it takes just as much time to merge as you go as it does to optimize everything at the end (I have not tested this with recent improvements).\n\nAfter changing to the new default merge policy my app (which does a lot more loading a doc than just index it with Lucene) lost 46% of its performance (processing the data, fully loading lucene and my database, and then optimizing an index).\nSwitching back to LogDocMergePolicy() returns my performance.\n\nI am using flushbyram(42) and the concurrent merger.\n\nTriple checked this time <G>\n\nIs this expected?\n\n- Mark",
            "date": "2007-09-25T22:52:33.109+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "\nThis is certainly not expected!\n\nSo you are flushing by RAM usage, and then find that merging according\nto doc count gives substantially better performance than merging\naccording to byte size of the segments?\n\nI'll run a test with contrib/benchmark on wikipedia content.  I'll set\nmergeFactor to 3 and ramBufferMB = 42 and I'll optimize in the end.\nIs there anything else interesting in how you're using Lucene?\n\nDo you have any sense of where this sizable slowdown shows up?  EG, is\nthe optimize in the end substantially slower, or something?\n\nIs there any way to tease out the time actually spent in Lucene vs the\nrest of your application?",
            "date": "2007-09-26T09:44:46.186+0000",
            "id": 3
        },
        {
            "author": "Mark Miller",
            "body": "Okay, I ran some tests loading about 4000 docs:\n\nautocommit=false, non compound format, mergefactor=3, flushbyram=42, build: latest from trunk (yesterday)\n\nnew merge policy i load about 30 docs per second:\n\ntime for load: 142828ms\ntime for optimize: 2422ms\n\nLogDocMergePolicy() I load about 50 docs per second:\n\ntime for load: 86781ms\noptimize: 4891ms\n\nSo it looks like optimize is quicker, but I pay for it during the load?\n\nI am not doing anything else special with Lucene that I can think of, and I got duplicate results for a much larger load.\n\nNot too easy to pull out the non Lucene parts without just writing a test from scratch for Lucene.\n\n- Mark\n\n",
            "date": "2007-09-26T10:47:30.199+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "Hmmm ... it seems like your index is fairly small because optimize\nruns pretty quickly in both cases.  But that would mean (I think)\nyou're not actually flushing very many segments since you have a high\nRAM buffer size (42 MB).  So then I'm baffled why merge policy would\nbe changing your numbers so much because your 4000 doc test should not\n(I think?) actually be doing that much merging.\n\nAre you creating the index from scratch in each test?  How large is\nthe resulting index?  Are you using FSDirectory?\n\nI ran my own test on Wikipedia content.  I ran this alg:\n\n  analyzer=org.apache.lucene.analysis.SimpleAnalyzer\n  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n  directory=FSDirectory\n  docs.file=/lucene/wikifull.txt\n\n  ram.flush.mb=42\n  max.field.length=2147483647\n  merge.factor=3\n  compound=false\n  autocommit=false\n\n  doc.maker.forever=false\n  doc.add.log.step=5000\n\n  ResetSystemErase\n  CreateIndex\n  {AddDoc >: *\n  Optimize\n  CloseIndex\n\n  RepSumByName\n\nto index all of wikipedia with the same params you're using (flush @\n42 MB, compound false, merge factor 3).\n\nLogByteSizeMergePolicy (the current default) gives this output (times\nare best of 2 runs):\n\n  indexing 1198 sec\n  optimize  282 sec\n\nLogDocMergePolicy took this long\n\n  indexing 1216 sec\n  optimize  270 sec\n\nI think those numbers are \"within the noise\" of each other, ie pretty\nmuch the same.  This is what I would expect.  So we need to figure out\nwhy I'm seeing different results than you.\n\nCan you call writer.setInfoStream(System.out) and attach the resulting\noutput from each of your 4000 doc runs?  Thanks!\n",
            "date": "2007-09-26T14:49:28.065+0000",
            "id": 5
        },
        {
            "author": "Mark Miller",
            "body": "Sorry about the small test...just started using 4000 because I was having same results with 20000.\n\nA sample run with 20000:\n\nold merge:\nload: 1320453 ms\noptimize: 8891 ms\n\nnew merge:\nload: 393625\noptimize: 17937 ms\n\nIts a fresh index each time of docs about 5-10k. I am using StandardAnalyzer.\n\nAnd I forgot to mention a big quirk: I am writing to two indexes, but only analyzing to a tokenstream once (cachingtoken filter) to have a stemmed and unstemmed index. So obviously a slowdown in writing an index would be a bit exaggerated.\n\nStill, its a major difference for my app.\n\nI will get you some debug output from the writers.",
            "date": "2007-09-26T17:39:35.335+0000",
            "id": 6
        },
        {
            "author": "Yonik Seeley",
            "body": "So based on your 20000 run, the \"new merge\" completed 3 times as fast?\n\nSome of the differences are going to be luck as to when big segment merges are done.\n\nIf scheme \"A\" just did a big merge right before the optimize,  much of that is wasted effort (the entire index will be rewritten anyway).  If scheme \"B\" was just about to do a big merge, but then optimize was called, it wins.\n\nFor a particular test run, tweaking the parameters can result in huge differences, but they may be \"false\".\n\nThe only way I can think of minimizing this effect is to do very large runs and cap the maximum size of segments to get rid of the possibility of random huge segment merges.\n",
            "date": "2007-09-26T18:59:03.473+0000",
            "id": 7
        },
        {
            "author": "Mark Miller",
            "body": "It was 3 times as fast, but to be fair, its more often closer to 2 times as fast. I just gave the result of the latest run. After running the test many many times, the new merge is much closer to 1/2 as fast as the old, and it has never been faster than that...rarely its slower....1/3 is worst I say actually.\n\n",
            "date": "2007-09-26T19:11:33.010+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "Mark, are you working on the debug output?  I'm hoping that gives a clue as to why you see such performance loss when merging by net byte size of each segment rather than by doc count... thanks.",
            "date": "2007-09-28T16:13:38.570+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "Reopening until we get to the bottom of the performance loss...",
            "date": "2007-09-28T16:13:55.637+0000",
            "id": 10
        },
        {
            "author": "Mark Miller",
            "body": "Sorry for the delay. Here is the debug output. As I said, I am actually writing to two indexes per doc, but I am also doing other processing and storing, so the slow down must be significant anyway (well over 25%?). The first Writer has an analyzer that stems and caches the unstemmed form , the second writer reads from the cached unstemmed tokens.\n\nIf the debug does not lead anywhere I can try to isolate the slowdown out of my app code (if it exists without writing with two writers, though the writing is sequential). Also, I will try with some other merge factors etc.\n\nI think that the performance gap grows with the number of documents.\n\nAttached: 4 files, one for each writer with the old policy and the new. Run details: 4000 some docs, 30 doc/s new, 50 doc/s old",
            "date": "2007-09-29T17:08:29.582+0000",
            "id": 11
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Mark!  OK, I noticed a few things from the logs:\n\n  * It looks like you are actually flushing every 10 docs, not 42 MB.\n\n  * You seem to have a mergeFactor of 10 through all the indexing\n    except at some point near the end, before optimize is called, you\n    switch to mergeFactor 3?\n\nThat said, the LogByteSizeMergePolicy is definitely not acting right.\nOH, I see the problem!\n\nOK, the bug happens when autoCommit is false and your docs have stored\nfields / term vectors and you're using LogByteSizeMergePolicy.  In\nthis case I am incorrectly calculating the byte size of the segment:\nI'm counting the shared doc stores against all segments.  This then\ncauses merge policy to think all segments are about the same size\n(since the doc stores grow very large).\n\nI'll open a new issue & fix it.  Thanks for testing Mark :)",
            "date": "2007-09-29T19:27:11.424+0000",
            "id": 12
        },
        {
            "author": "Mark Miller",
            "body": "Anytime Michael. Thanks for pointing out the mergefactor issue to me. I recently retrofitted my indexer with google guice, and it seems that something is not working as expected. Glad this little debug session worked out for all <g> \n\nCan't thank you enough for all of your Lucene patches. Keep em coming!",
            "date": "2007-09-29T19:51:59.408+0000",
            "id": 13
        },
        {
            "author": "Michael McCandless",
            "body": "Marking this as fixed again; I opened LUCENE-1009 for the slowdown in merging.",
            "date": "2007-09-29T19:58:26.645+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "> Anytime Michael. Thanks for pointing out the mergefactor issue to\n> me. I recently retrofitted my indexer with google guice, and it\n> seems that something is not working as expected. Glad this little\n> debug session worked out for all <g>\n\nSure!  Make sure you also fix your flushing to actually flush at 42 MB\nRAM buffer (things should go MUCH faster with that :).\n\n> Can't thank you enough for all of your Lucene patches. Keep em\n> coming!\n\nYou're welcome!  I enjoy it :)\n",
            "date": "2007-09-29T20:07:27.939+0000",
            "id": 15
        },
        {
            "author": "Yonik Seeley",
            "body": "While trying Solr with the latest Lucene, I ran into this back-incompatibility:\nCaused by: java.lang.IllegalArgumentException: this method can only be called when the merge policy is LogDocMergePolicy\n\tat org.apache.lucene.index.IndexWriter.getLogDocMergePolicy(IndexWriter.java:316)\n\tat org.apache.lucene.index.IndexWriter.setMaxMergeDocs(IndexWriter.java:768)\n\n\nIt's not an issue at all for Solr - we'll fix things up when we officially upgrade Lucene versions, but it does seem like it might affect a number of apps that try and just drop in a new lucene jar.  Thoughts?\n",
            "date": "2007-09-30T22:02:12.970+0000",
            "id": 16
        },
        {
            "author": "Mark Miller",
            "body": "It was my impression that this Lucene release would be unusual in that you shouldn't just drop the jar without first making sure you are in compliance with the new changes? Since some apps are going to break no matter what (few they may be) perhaps you just make a big fuss about possible incompatible changes?",
            "date": "2007-09-30T22:35:52.690+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "\n> While trying Solr with the latest Lucene, I ran into this back-incompatibility:\n> Caused by: java.lang.IllegalArgumentException: this method can only be called when the merge policy is LogDocMergePolicy\n> at org.apache.lucene.index.IndexWriter.getLogDocMergePolicy(IndexWriter.java:316)\n> at org.apache.lucene.index.IndexWriter.setMaxMergeDocs(IndexWriter.java:768)\n>\n> It's not an issue at all for Solr - we'll fix things up when we\n> officially upgrade Lucene versions, but it does seem like it might\n> affect a number of apps that try and just drop in a new lucene\n> jar. Thoughts?\n\nHmm, good catch.\n\nThis should only happen when \"setMaxMergeDocs\" is called (this is the\nonly method that requires a LogDocMergePolicy).  I think we have\nvarious options:\n\n  1. Leave things as is and put up-front comment in the release saying\n     you could either switch to LogDocMergePolicy, or, use\n     \"setMaxMergeMB\" on the default LogByteSizeMergePolicy, instead.\n     Also put details in the javadocs for this method explaining these\n     options.\n\n  2. Switch back to LogDocMergePolicy by default \"out of the box\".\n\n  3. If setMaxMergeDocs() is called, switch back to LogDocMergePolicy\n     \"on-demand\".\n\n  4. Modify LogByteSizeMergePolicy to in fact accept both\n     \"maxMergeDocs\" or \"maxMergeMB\", allowing either one or both just\n     like \"flush by RAM\" and/or \"flush by doc count\" is being done in\n     LUCENE-1007.\n\nI think I like option 4 the best.  3 seems to magical (violates\n\"principle of least surprise\").  2 I think is bad because it's best to\nmatch the merge policy with how we are flushing (by RAM by default).\n1 is clearly disruptive to people who want to drop Lucene JAR in and\ntest.\n\nI'll open a new issue.  Thanks Yonik!",
            "date": "2007-10-01T07:57:20.560+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "\n> It was my impression that this Lucene release would be unusual in\n> that you shouldn't just drop the jar without first making sure you\n> are in compliance with the new changes? Since some apps are going to\n> break no matter what (few they may be) perhaps you just make a big\n> fuss about possible incompatible changes?\n\nI *think* this release should in fact \"drop in\" for most apps.  The\nonly known case where there is non-backwards compatibility (besides\nthis setMaxMergeDocs issue) is users of ParallelReader, I think?  I\nthink Lucene 3.0 is when we are \"allowed\" to remove deprecated APIs,\nswitch to Java 1.5, etc.\n",
            "date": "2007-10-01T08:01:10.398+0000",
            "id": 19
        },
        {
            "author": "Hoss Man",
            "body": "lucene's \"commitment\" to backward compatibility requires that 2.X be API compatible with all previous 2.Y releases...\n\nhttp://wiki.apache.org/jakarta-lucene/BackwardsCompatibility\n\n...the performance characteristics and file formats (and merge behavior) doesn't need to be exactly the same (so switching the Merge Policy should be fine) as long as a note is made about this in the \"run time behavior\" section of CHANGES.txt (we already have one) ... but any code that could compile and run against 2.2 should still run against 2.3 ... it might be really slow without some minor tweaks, and it might violate \"principle of least surprise\" but it's important that it still run so that people don't have to fear a minor version upgrade.\n\nif we can't provide that -- then the release should be called 3.0, but i haven't seen anything that least  me to think it's not possible.  (it just might not be pretty)",
            "date": "2007-10-01T20:31:23.814+0000",
            "id": 20
        },
        {
            "author": "Mark Miller",
            "body": "Quick question due to a new issue I am seeing in my application...could the concurrent merge possibly break apps that add a doc and then expect to be able to find it *immediately* after? I suspect this is not the case, just wondering based on some odd new behavior I am seeing. For example, if you call adddoc and it triggers a background merge, the doc is still immediately  visible to the next call from the same thread right? No possibility for a race?",
            "date": "2007-10-25T17:48:49.988+0000",
            "id": 21
        },
        {
            "author": "Yonik Seeley",
            "body": "{quote}\nFor example, if you call adddoc and it triggers a background merge, the doc is still immediately visible\nto the next call from the same thread right?\n{quote}\n\n\"visible\" by opening a new reader you mean?\nI don't think so... this was never a guarantee (although it might have been normally true in the past), and concurrent merge breaks this.\nSo does autocommit=false.\n",
            "date": "2007-10-25T17:56:14.105+0000",
            "id": 22
        },
        {
            "author": "Mark Miller",
            "body": "Well that would explain a lot then. I use both the concurrent merge and autocommit=false, where in the past I did not. Before that I certainly did seem to be able to count on it, as long as it was sequential in the same thread. I never *wanted* to count on it, but when other developers start using your libraries in ways you never intended or okay'd...so much for ever warming searchers.\n\nIsn't this a problem for certain unit tests? After adding a bunch of docs and then searching to see if they are there must you pause for a bit to make sure enough ms have passed?",
            "date": "2007-10-25T18:06:50.059+0000",
            "id": 23
        },
        {
            "author": "Yonik Seeley",
            "body": "Yes, I guess one could consider it a minor breakage.\nmaxBufferedDocs previously ensured that changes were flushed every \"n\" docs.  They were flushed in a visible way, and I don't recall anything saying that one shouldn't open a reader before the writer was closed.",
            "date": "2007-10-25T18:16:12.339+0000",
            "id": 24
        },
        {
            "author": "Mark Miller",
            "body": "Sorry Yonik...I was not being explicit enough -- I *am* closing the Writer before opening the Reader. Which is why I assumed I could count on this behavior. Semi randomly it is failing in my app now though. I am not positive it is due to Lucene, I just thought that maybe the concurrent merge was somehow not adding the document before triggering the merge in a background thread? Perhaps you dont see the doc till the background threads are done merging? Just looking for someone to tell me, no, even with concurrent merge, as long as you close the writer and then open a new reader, you are guaranteed to find the doc just added (if all from the same thread). I really do assume this is the case, I just have not changed anything else other than updating Lucene, so I am grasping at some straws...",
            "date": "2007-10-25T18:25:57.692+0000",
            "id": 25
        },
        {
            "author": "Michael McCandless",
            "body": "When you close the writer & open a reader, the reader should see all\ndocs added, regardless of whether the writer was using concurrent\nmerge scheduler or serial merge scheduler, autoCommit true or false,\nflushing by ram or by doc count, etc.\n\nIndexWriter.close() first flushes any buffered docs to a new segment,\nand then allows merges to run if they are necessary.  It will also\nwait for all merges to finish (in the case of concurrent merge\nscheduler) unless you call close(false) which will abort all running\nmerges.\n\nIf you're not seeing this behavior then something is seriously wrong!\nCan you post some more details about how you see this intermittant\nfailure?\n",
            "date": "2007-10-25T18:32:06.172+0000",
            "id": 26
        },
        {
            "author": "Doug Cutting",
            "body": "> After adding a bunch of docs and then searching to see if they are there must you pause for a bit to make sure enough ms have passed?\n\nNo.  You could previously never rely on a newly added being visible to search until you called IndexWriter#close().  Added documents have always been buffered and all buffers were only flushed by IndexWriter#close().  It used to be the case that the buffer was memory-only and a fixed number of documents.  So the last up to MaxBufferedDocs added would not yet be visible.\n\nNow there is an IndexWriter#flush() method that flushes buffers without closing the IndexWriter.  And with the \"autocommit=false\" feature, nothing is visible to searchers until either #close() or #flush() is called.  The primary change of concurrent merging is that calls to addDocument() generally return faster, with merging work done in the background, but concurrent merging and \"autocommit=false\" don't fundamentally change the need to call #close() or #flush() in order to guarantee that all changes are visible to searchers.\n\nAt least that's my understanding...\n",
            "date": "2007-10-25T18:32:32.060+0000",
            "id": 27
        },
        {
            "author": "Mark Miller",
            "body": "Based on the responses I am going to assume the problem is not with Lucene or concurrent merge. I have to figure it out though, so if I can determine otherwise, you'll be the first to know. Gotto assume its me first though.",
            "date": "2007-10-25T18:42:23.130+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nBased on the responses I am going to assume the problem is not with Lucene or concurrent merge. I have to figure it out though, so if I can determine otherwise, you'll be the first to know. Gotto assume its me first though.\n{quote}\n\nI sure hope you're right!  Keep us posted...\n\n{quote}\nNo.  You could previously never rely on a newly added being visible to search until you called IndexWriter#close().  Added documents have always been buffered and all buffers were only flushed by IndexWriter#close().  It used to be the case that the buffer was memory-only and a fixed number of documents.  So the last up to MaxBufferedDocs added would not yet be visible.\n\nNow there is an IndexWriter#flush() method that flushes buffers without closing the IndexWriter.  And with the \"autocommit=false\" feature, nothing is visible to searchers until either #close() or #flush() is called.  The primary change of concurrent merging is that calls to addDocument() generally return faster, with merging work done in the background, but concurrent merging and \"autocommit=false\" don't fundamentally change the need to call #close() or #flush() in order to guarantee that all changes are visible to searchers.\n\nAt least that's my understanding...\n{quote}\n\nThis is my understanding too, except calling flush() with\nautoCommit=false does not actually make the changes visible to readers\n(though it does flush buffered added/deleted docs to the Directory).\nOnly close() will make all changes visible to readers when\nautoCommit=false.\n",
            "date": "2007-10-25T19:06:57.133+0000",
            "id": 29
        }
    ],
    "component": "core/index",
    "description": "This is follow-through from LUCENE-845, LUCENE-847 and LUCENE-870;\nI'll commit this once those three are committed.\n\nOut of the box performance of IndexWriter is maximized when flushing\nby RAM instead of a fixed document count (the default today) because\ndocuments can vary greatly in size.\n\nLikewise, merging performance should be faster when merging by net\nsegment size since, to minimize the net IO cost of merging segments\nover time, you want to merge segments of equal byte size.\n\nFinally, ConcurrentMergeScheduler improves indexing speed\nsubstantially (25% in a simple initial test in LUCENE-870) because it\nruns the merges in the backround and doesn't block\nadd/update/deleteDocument calls.  Most machines have concurrency\nbetween CPU and IO and so it makes sense to default to this\nMergeScheduler.\n\nNote that these changes will break users of ParallelReader because the\nparallel indices will no longer have matching docIDs.  Such users need\nto switch IndexWriter back to flushing by doc count, and switch the\nMergePolicy back to LogDocMergePolicy.  It's likely also necessary to\nswitch the MergeScheduler back to SerialMergeScheduler to ensure\ndeterministic docID assignment.\n\nI think the combination of these three default changes, plus other\nperformance improvements for indexing (LUCENE-966, LUCENE-843,\nLUCENE-963, LUCENE-969, LUCENE-871, etc.) should make for some sizable\nperformance gains Lucene 2.3!",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-994",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Change defaults in IndexWriter to maximize \"out of the box\" performance",
    "systemSpecification": true,
    "version": "2.3"
}