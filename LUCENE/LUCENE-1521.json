{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "Ugh, right.  Plus another one (* 16) in TermVectorsTermsWriter.java.  I'll fix.",
            "date": "2009-01-16T15:59:49.872+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "Committed revision 735043.  Thanks Shon!",
            "date": "2009-01-16T16:19:00.888+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "Reopening for backport to 2.4.1.",
            "date": "2009-02-19T01:37:22.841+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "Committed revision 745803 on 2.4 branch.",
            "date": "2009-02-19T10:10:25.960+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "Note that this issue only hits an index with many (> ~268 million) docs.",
            "date": "2009-05-21T09:38:22.443+0000",
            "id": 4
        },
        {
            "author": "Elliot Metsger",
            "body": "I received this on 2.4.1, not sure if it is this bug or not:\nException in thread \"main\" java.lang.RuntimeException: after flush: fdx size mismatch: 10 docs vs 0 length in bytes of _sl3.fdx\n        at org.apache.lucene.index.StoredFieldsWriter.closeDocStore(StoredFieldsWriter.java:94)\n        at org.apache.lucene.index.DocFieldConsumers.closeDocStore(DocFieldConsumers.java:83)\n        at org.apache.lucene.index.DocFieldProcessor.closeDocStore(DocFieldProcessor.java:47)\n        at org.apache.lucene.index.DocumentsWriter.closeDocStore(DocumentsWriter.java:367)\n        at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:567)\n        at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3540)\n        at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3450)\n        at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:3363)\n        at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3408)\n        at edu.jhu.library.ivoa.VOImageAccessUrlDownload.go(VOImageAccessUrlDownload.java:357)\n        at edu.jhu.library.ivoa.VOImageAccessUrlDownload.main(VOImageAccessUrlDownload.java:103)\n\nI'm working with over 500,000 docs in this particular index.",
            "date": "2009-08-27T20:32:45.896+0000",
            "id": 5
        },
        {
            "author": "Elliot Metsger",
            "body": "Nevermind, it doesn't look like this is an occurrence of this bug.  Not sure what happened... underlying storage is a ZFS file system.  Anyway, this thread http://www.mail-archive.com/solr-user@lucene.apache.org/msg22264.html was helpful, explaining what may be happening.",
            "date": "2009-08-27T20:44:00.712+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "Is there any thing in your env that might be removing index files out from under the IndexWriter?  Are you changing your Directory's default locking impl, or disabling locking?\n\nZFS should be fine -- I use it in my daily development.  What a fabulous file system :)  Snapshots & clones are very addictive...",
            "date": "2009-08-28T08:52:23.687+0000",
            "id": 7
        }
    ],
    "component": "core/index",
    "description": "When closing index that contains 500,000,000 randomly generated documents, an exception is thrown:\n\njava.lang.RuntimeException: after flush: fdx size mismatch: 500000000 docs vs 4000000004 length in bytes of _0.fdx\n\tat org.apache.lucene.index.StoredFieldsWriter.closeDocStore(StoredFieldsWriter.java:94)\n\tat org.apache.lucene.index.DocFieldConsumers.closeDocStore(DocFieldConsumers.java:83)\n\tat org.apache.lucene.index.DocFieldProcessor.closeDocStore(DocFieldProcessor.java:47)\n\tat org.apache.lucene.index.DocumentsWriter.closeDocStore(DocumentsWriter.java:367)\n\tat org.apache.lucene.index.IndexWriter.flushDocStores(IndexWriter.java:1688)\n\tat org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3518)\n\tat org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3442)\n\tat org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1623)\n\tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1588)\n\tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1562)\n        ...\n\nThis appears to be a bug at StoredFieldsWriter.java:93:\n\n      if (4+state.numDocsInStore*8 != state.directory.fileLength(state.docStoreSegmentName + \".\" + IndexFileNames.FIELDS_INDEX_EXTENSION))\n\nwhere the multiplication by 8 is causing integer overflow. The fix would be to cast state.numDocsInStore to long before multiplying.\n\nIt appears that this is another instance of the mistake that caused bug LUCENE-1519. I did a cursory seach for \\*8 against the code to see if there might be yet more instances of the same mistake, but found none. \n\n",
    "hasPatch": false,
    "hasScreenshot": false,
    "id": "LUCENE-1521",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Critical",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "\"fdx size mismatch\" exception in StoredFieldsWriter.closeDocStore() when closing index with 500M documents",
    "systemSpecification": true,
    "version": "2.4"
}