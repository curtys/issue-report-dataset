{
    "comments": [
        {
            "author": "Yonik Seeley",
            "body": "Patch for FSIndexInput to use a positional read call that doesn't use explicit synchronization.  Note that the implementation of that read call may still involve some synchronization depending on the JVM and OS (notably Windows which lacks a native pread AFAIK).",
            "date": "2006-12-19T18:37:43.000+0000",
            "id": 0
        },
        {
            "author": "Yonik Seeley",
            "body": "This change should be faster on heavily loaded multi-threaded servers using the non-compound index format.\nPerformance tests are needed to see if there is any negative impact on single-threaded performance.\n\nCompound index format (CSIndexInput) still does synchronization because the base IndexInput is not cloned (and hence shared by all CSIndexInput clones).  It's unclear if getting rid of the synchronization is worth the cloning overhead in this case.",
            "date": "2006-12-19T18:45:15.000+0000",
            "id": 1
        },
        {
            "author": "Doug Cutting",
            "body": "This patch continues to use BufferedIndexInput and allocates a new ByteBuffer for each call to read().  I wonder if it might be more efficient to instead directly extend IndexInput and always represent the buffer as a ByteBuffer?",
            "date": "2006-12-19T19:17:28.000+0000",
            "id": 2
        },
        {
            "author": "Yonik Seeley",
            "body": "CSIndexInput synchronization could also be elimitated if there was a pread added to IndexInput\n\n  public abstract void readBytes(byte[] b, int offset, int len, long fileposition)\n\nUnfortunately, that would break any custom Directory based implementations out there, and we can't provide a suitable default with seek & read because we don't know what object to synchronize on.\nWorth it or not???",
            "date": "2006-12-20T01:39:13.000+0000",
            "id": 3
        },
        {
            "author": "Yonik Seeley",
            "body": "Here is a patch that directly extends IndexInput to make things a little easier.\nI started with the code for BufferedIndexInput to avoid any bugs in read().\nThey share enough code that a common subclass could be factored out if desired (or changes made in BufferedIndexInput to enable easier sharing).\n\nByteBuffer does have offset, length, etc, but I did not use them because BufferedIndexInput currently allocates the byte[] on demand, and thus would add additional checks to readByte().  Also, the NIO Buffer.get() isn't as efficient as our own array access.",
            "date": "2006-12-20T06:01:58.000+0000",
            "id": 4
        },
        {
            "author": "Bogdan Ghidireac",
            "body": "You can find a NIO variation of IndexInput attached to this issue: http://issues.apache.org/jira/browse/LUCENE-519\n\nI had good results on multiprocessor machines under heavy load.\n\nRegards,\nBogdan",
            "date": "2006-12-20T09:25:18.000+0000",
            "id": 5
        },
        {
            "author": "Yonik Seeley",
            "body": "Thanks for the pointer Bogdan, it's interesting you use transferTo instead of read... is there any advantage to this?  You still need to create a new object every read(), but at least it looks like a smaller object.\n\nIt's also been pointed out to me that http://issues.apache.org/jira/browse/LUCENE-414 has some more NIO code.",
            "date": "2006-12-20T15:25:32.000+0000",
            "id": 6
        },
        {
            "author": "Bogdan Ghidireac",
            "body": "The Javadoc says that transferTo can be more efficient because the OS can transfer bytes directly from the filesystem cache to the target channel without actually copying them. ",
            "date": "2006-12-20T15:42:18.000+0000",
            "id": 7
        },
        {
            "author": "Yonik Seeley",
            "body": "> The Javadoc says that transferTo can be more efficient because the OS can transfer bytes\n> directly from the filesystem cache to the target channel without actually copying them.\n\nUnfortunately, only for DirectByteBuffers and other FileChannels, not for HeapByteBuffers.\nSounds like we just need to do some benchmarking, but I have a bad feeling that all the checking overhead Sun added to NIO will cause it to be slower in the single threaded case.\n",
            "date": "2006-12-21T15:33:44.000+0000",
            "id": 8
        },
        {
            "author": "Yonik Seeley",
            "body": "Attaching test that reads a file in different ways, either random access or serially, from a number of threads.\n",
            "date": "2006-12-21T22:45:36.000+0000",
            "id": 9
        },
        {
            "author": "Yonik Seeley",
            "body": "Single-threaded random access performance of a fully cached 64MB file on my home PC (WinXP) , Java6:\n\nconfig: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=6518936\nanswer=81332126, ms=7781, MB/sec=167.5603649916463\n\nconfig: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=6518936\nanswer=81332126, ms=9203, MB/sec=141.66980332500273\n\nconfig: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=6518936\nanswer=81332126, ms=11672, MB/sec=111.70212474297463\n\nconfig: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=1024 filelen=6518936\nanswer=81332126, ms=17328, MB/sec=75.2416435826408",
            "date": "2006-12-21T22:50:36.000+0000",
            "id": 10
        },
        {
            "author": "Brian Pinkerton",
            "body": "Most of my workloads would benefit by removing the synchronization in FSIndexInput, so I took a closer look at this issue.  I found exactly the opposite results that Yonik did on two platforms that I use frequently in production (Solaris and Linux), and by a significant margin.  I even get the same behavior on the Mac, though I'm not running Java6 there.\n\n# uname -a\nLinux xxx 2.6.9-22.0.1.ELsmp #1 SMP Tue Oct 18 18:39:27 EDT 2005 i686 i686 i386 GNU/Linux\n# java -version\njava version \"1.6.0_02\"\nJava(TM) SE Runtime Environment (build 1.6.0_02-b05)\nJava HotSpot(TM) Client VM (build 1.6.0_02-b05, mixed mode, sharing)\n\nconfig: impl=ChannelPread serial=false nThreads=200 iterations=10 bufsize=1024 filelen=10485760\nanswer=0, ms=88543, MB/sec=236.85124741650947\nconfig: impl=ClassicFile serial=false nThreads=200 iterations=10 bufsize=1024 filelen=10485760\nanswer=0, ms=150560, MB/sec=139.29011689691816\n\n\n\n# uname -a\nSunOS xxx 5.10 Generic_118844-26 i86pc i386 i86pc\n# java -version\njava version \"1.6.0\"\nJava(TM) SE Runtime Environment (build 1.6.0-b105)\nJava HotSpot(TM) Server VM (build 1.6.0-b105, mixed mode)\n\nconfig: impl=ChannelPread serial=false nThreads=200 iterations=10 bufsize=1024 filelen=10485760\nanswer=0, ms=39621, MB/sec=529.3031473208652\n\nconfig: impl=ClassicFile serial=false nThreads=200 iterations=10 bufsize=1024 filelen=10485760\nanswer=0, ms=119057, MB/sec=176.14688762525515\n\n",
            "date": "2007-12-10T00:23:29.518+0000",
            "id": 11
        },
        {
            "author": "Yonik Seeley",
            "body": "Brad, one possible difference is the number of threads we tested with.\nI tested single-threaded (nThreads=1) to see what kind of slowdown a single query might see.\n\nA normal production  system shouldn't see 200 concurrent running search threads unless it's just about to fall over, or unless it's one of those massive multi-core systems.  After you pass a certain amount of parallelism, NIO can help.",
            "date": "2007-12-10T13:56:46.928+0000",
            "id": 12
        },
        {
            "author": "Brian Pinkerton",
            "body": "Whoops; I should have paid more attention to the args.  The results in the single-threaded case still favor pread, but by a slimmer margin:\n\nLinux:\n\nconfig: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=10485760\nanswer=0, ms=9983, MB/sec=210.0723229490133\n\nconfig: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=10485760\nanswer=0, ms=9247, MB/sec=226.7926895209257\n\n\nSolaris 10:\n\nconfig: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=10485760\nanswer=0, ms=7381, MB/sec=284.12843788104595\n\nconfig: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=10485760\nanswer=0, ms=6245, MB/sec=335.81297037630105\n\n\nMac OS X:\n\nconfig: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=10485760\nanswer=-914995, ms=19945, MB/sec=105.14675357232389\n\nconfig: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=10485760\nanswer=-914995, ms=26378, MB/sec=79.50382894836606\n\n",
            "date": "2007-12-10T17:30:55.681+0000",
            "id": 13
        },
        {
            "author": "Doug Cutting",
            "body": "> Brad, [...]\n\nThat's Brian.  And right, the difference in your tests is the number of threads.\n\nPerhaps this is a case where one size will not fit all.  MmapDirectory is fastest on 64-bit platforms with lots of threads, while good-old-FSDirectory has always been fastest for single-threaded access.  Perhaps a PreadDirectory would be the Directory of choice for multi-threaded access of large indexes on 32-bit hardware?  It would be useful to benchmark this patch against MmapDirectory, since they both remove synchronization.",
            "date": "2007-12-10T17:31:56.781+0000",
            "id": 14
        },
        {
            "author": "Doug Cutting",
            "body": "My prior remarks were posted before I saw Brian's latest benchmarks.\n\nWhile it would still be good to throw mmap into the mix, pread now looks like a strong contender for the one that might beat all.  It works well on 32-bit hardware, it's unsynchronized, and it's fast.  What's not to like?\n",
            "date": "2007-12-10T18:00:49.377+0000",
            "id": 15
        },
        {
            "author": "Yonik Seeley",
            "body": "Weird... I'm still getting slower results from pread on WinXP.\nCan someone else verify on a windows box?\n\n{code}\nYonik@spidey ~\n$ c:/opt/jdk16/bin/java -server FileReadTest testfile ClassicFile false 1 200\nconfig: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=9616000\nanswer=160759732, ms=14984, MB/sec=128.35024025627337\n\n$ c:/opt/jdk16/bin/java -server FileReadTest testfile ClassicFile false 1 200\nconfig: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=9616000\nanswer=160759732, ms=14640, MB/sec=131.36612021857923\n\n\n$ c:/opt/jdk16/bin/java -server FileReadTest testfile ChannelPread false 1 200\nconfig: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=9616000\nanswer=160759732, ms=21766, MB/sec=88.35798952494717\n\n$ c:/opt/jdk16/bin/java -server FileReadTest testfile ChannelPread false 1 200\nconfig: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=9616000\nanswer=160759732, ms=21718, MB/sec=88.55327378211622\n\n\n$ c:/opt/jdk16/bin/java -version\njava version \"1.6.0_02\"\nJava(TM) SE Runtime Environment (build 1.6.0_02-b06)\nJava HotSpot(TM) Client VM (build 1.6.0_02-b06, mixed mode)\n{code}",
            "date": "2007-12-10T19:55:14.186+0000",
            "id": 16
        },
        {
            "author": "robert engels",
            "body": "I sent this via email, but probably need to add to the thread...\n\nI posted a bug on this to Sun a long while back.  This is a KNOWN BUG on Windows.\n\nNIO preads actually sync behind the scenes on some platforms.  Using multiple file descriptors is much faster.\n\nSee bug http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734\n\n\n",
            "date": "2007-12-10T20:07:14.786+0000",
            "id": 17
        },
        {
            "author": "Doug Cutting",
            "body": "So it looks like pread is ~50% slower on Windows, and ~5-25% faster on other platforms.  Is that enough of a difference that it might be worth having FSDirectory use different implementations of FSIndexInput based on the value of Constants.WINDOWS (and perhaps JAVA_VERSION)?",
            "date": "2007-12-10T20:38:52.613+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nIs that enough of a difference that it might be worth having FSDirectory use different implementations of FSIndexInput based on the value of Constants.WINDOWS (and perhaps JAVA_VERSION)?\n{quote}\n\n+1\n\nI think having good out-of-the-box defaults is extremely important (most users won't tune), and given the substantial cross platform differences here I think we should conditionalize the defaults according to the platform.",
            "date": "2007-12-10T20:56:10.862+0000",
            "id": 19
        },
        {
            "author": "robert engels",
            "body": "As an aside, if the Lucene people voted on the Java bug (and or sent emails via the proper channels), hopefully the underlying bug can be fixed in the JVM.\n\nIn my opinion it is a serious one - ruins any performance gains of using NIO on files.",
            "date": "2007-12-10T21:07:37.937+0000",
            "id": 20
        },
        {
            "author": "Yonik Seeley",
            "body": "Updated test that fixes some thread synchronization issues to ensure that the \"answer\" is the same for all methods.\n\nBrian, in some of your tests the answer is \"0\"... is this because your test file consists of zeros (created via /dev/zero or equiv)?  UNIX systems treat blocks of zeros differently than normal files (they are stored as holes).  It shouldn't make too much of a difference in this case, but just to be sure, could you try with a real file?\n",
            "date": "2007-12-10T22:34:07.378+0000",
            "id": 21
        },
        {
            "author": "Brian Pinkerton",
            "body": "Yeah, the file was full of zeroes.  But I created the files w/o holes and was using filesystems that don't compress file contents.  Just to be sure, though, I repeated the tests with a file with random contents; the results above still hold.\n",
            "date": "2007-12-11T08:14:51.856+0000",
            "id": 22
        },
        {
            "author": "Brian Pinkerton",
            "body": "BTW, I think the performance win with Yonik's patch for some workloads could be far greater than what the simple benchmark illustrates.  Sure, pread might be marginally faster.   But the real win is avoiding synchronized access to the file.\n\nI did some IO tracing a while back on one particular workload that is characterized by:\n* a small number of large compound indexes\n* short average execution time, particularly compared to disk response time\n* a 99+% FS cache hit rate\n* cache misses that tend to cluster on rare queries\n\nIn this workload where each query hits each compound index, the locking in FSIndexInput means that a single rare query clobbers the response time for all queries.  The requests to read cached data are serialized (fairly, even) with those that hit the disk.  While we can't get rid of the rare queries, we can allow the common ones to proceed against cached data right away.\n\n",
            "date": "2007-12-11T09:47:45.640+0000",
            "id": 23
        },
        {
            "author": "Michael McCandless",
            "body": "I ran Yonik's most recent FileReadTest.java on the platforms below,\ntesting single-threaded random access for fully cached 64 MB file.\n\nI tested two Windows XP Pro machines and got opposite results from\nYonik.  Yonik is your machine XP Home?\n\nI'm showing ChannelTransfer to be much faster on all platforms except\nWindows Server 2003 R2 Enterprise x64 where it's about the same as\nChannelPread and ChannelFile.\n\nThe ChannelTransfer test is giving the wrong checksum, but I think\njust a bug in how checksum is computed (it's using \"len\" which with\nChannelTransfer is just the chunk size written on each call to\nwrite).  So I think the MB/sec is still correct.\n\nMac OS X 10.4 (Sun java 1.5)\n  config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=32565, MB/sec=412.15331797942576\n\n  config: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=19512, MB/sec=687.8727347273473\n\n  config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=19492, MB/sec=688.5785347835009\n\n  config: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=147783, ms=16009, MB/sec=838.3892060715847\n\nLinux 2.6.22.1 (Sun java 1.5)\n  config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=37879, MB/sec=354.33281765622115\n\n  config: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=21845, MB/sec=614.4093751430535\n\n  config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=21902, MB/sec=612.8103734818737\n\n  config: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=147783, ms=15978, MB/sec=840.015821754913\n\nWindows Server 2003 R2 Enterprise x64 (Sun java 1.6)\n\n  config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=32703, MB/sec=410.4141149130049\n\n  config: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=23344, MB/sec=574.9559972583961\n\n  config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=23329, MB/sec=575.3256804835183\n\n  config: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=147783, ms=23422, MB/sec=573.0412774314747\n\nWindows XP Pro SP2, laptop (Sun Java 1.5)\n\n  config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=71253, MB/sec=188.36782731955148\n\n  config: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=57463, MB/sec=233.57243443607192\n\n  config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=58043, MB/sec=231.23844046655068\n\n  config: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=147783, ms=20039, MB/sec=669.7825640001995\n\nWindows XP Pro SP2, older desktop (Sun Java 1.6)\n\n  config: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=53047, MB/sec=253.01662299470283\n\n  config: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=34047, MB/sec=394.2130819161747\n\n  config: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-44611, ms=34078, MB/sec=393.8544750278772\n\n  config: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=6518936 filelen=67108864\n  answer=147783, ms=18781, MB/sec=714.6463340610192\n",
            "date": "2007-12-11T18:54:28.161+0000",
            "id": 24
        },
        {
            "author": "Michael McCandless",
            "body": "I also just ran a test with 4 threads, random access, on Linux 2.6.22.1:\n\n  config: impl=ClassicFile serial=false nThreads=4 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-195110, ms=120856, MB/sec=444.22363142913883\n\n  config: impl=ChannelFile serial=false nThreads=4 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-195110, ms=88272, MB/sec=608.2006887801341\n\n  config: impl=ChannelPread serial=false nThreads=4 iterations=200 bufsize=6518936 filelen=67108864\n  answer=-195110, ms=77672, MB/sec=691.2026367288084\n\n  config: impl=ChannelTransfer serial=false nThreads=4 iterations=200 bufsize=6518936 filelen=67108864\n  answer=594875, ms=38390, MB/sec=1398.465517061735\n\nChannelTransfer got even faster (scales up with added threads better).\n",
            "date": "2007-12-11T18:58:32.502+0000",
            "id": 25
        },
        {
            "author": "Yonik Seeley",
            "body": "Mike, it looks like you are running with a bufsize of 6.5MB!\nApologies for my hard-to-use benchmark program :-(",
            "date": "2007-12-11T19:20:04.560+0000",
            "id": 26
        },
        {
            "author": "Yonik Seeley",
            "body": "I'll try fixing the transferTo test before anyone re-runs any tests.",
            "date": "2007-12-11T19:29:55.109+0000",
            "id": 27
        },
        {
            "author": "Michael McCandless",
            "body": "Doh!!  Woops :)  I will rerun...",
            "date": "2007-12-11T19:31:06.089+0000",
            "id": 28
        },
        {
            "author": "Yonik Seeley",
            "body": "OK, uploading latest version of the test that should fix ChannelTransfer (it's also slightly optimized to not create a new object per call).\n\nWell, at least we've learned that printing out all the input params for benchmarking programs is  good practice :-)",
            "date": "2007-12-11T20:10:36.698+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks!  I'll re-run.\n\n{quote}\nWell, at least we've learned that printing out all the input params for benchmarking programs is good practice :)\n{quote}\n\nYes indeed :)",
            "date": "2007-12-11T20:16:37.887+0000",
            "id": 30
        },
        {
            "author": "Michael McCandless",
            "body": "OK my results on Win XP now agree with Yonik's.\n\nOn UNIX & OS X, ChannelPread is a bit (2-14%) better, but on windows\nit's quite a bit (31-34%) slower.\n\nWin Server 2003 R2 Enterprise x64 (Sun Java 1.6):\n{code}\nconfig: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=68094, MB/sec=197.10654095808735\n\nconfig: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=72594, MB/sec=184.88818359644048\n\nconfig: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=98328, MB/sec=136.5000081360345\n\nconfig: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=201563, MB/sec=66.58847506734867\n{code}\n\nWin XP Pro SP2, laptop (Sun Java 1.5):\n{code}\nconfig: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=47449, MB/sec=282.8673481000653\n\nconfig: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=54899, MB/sec=244.4811890926975\n\nconfig: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=71683, MB/sec=187.237877878995\n\nconfig: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=149475, MB/sec=89.79275999330991\n{code}\n\nLinux 2.6.22.1 (Sun Java 1.5):\n{code}\nconfig: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=41162, MB/sec=326.0719304212623\n\nconfig: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=53114, MB/sec=252.69745829724744\n\nconfig: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=40226, MB/sec=333.65914582608264\n\nconfig: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=59163, MB/sec=226.86092321214272\n{code}\n\nMac OS X 10.4 (Sun Java 1.5):\n{code}\nconfig: impl=ClassicFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=85894, MB/sec=156.25972477705076\n\nconfig: impl=ChannelFile serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=109939, MB/sec=122.08381738964336\n\nconfig: impl=ChannelPread serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=75517, MB/sec=177.73180608339845\n\nconfig: impl=ChannelTransfer serial=false nThreads=1 iterations=200 bufsize=1024 filelen=67108864\nanswer=110480725, ms=130156, MB/sec=103.12066136021389\n{code}\n",
            "date": "2007-12-11T20:39:44.068+0000",
            "id": 31
        },
        {
            "author": "Testo Nakada",
            "body": "I think bufsize has way much bigger impact than the implementation. I found that 64KB buffer size is at least 5-6 times faster than 1KB. Should we tune this parameter instead for maximum performance.",
            "date": "2008-01-13T17:05:11.353+0000",
            "id": 32
        },
        {
            "author": "Jason Rutherglen",
            "body": "lucene-753.patch\n\nMade NIOFSDirectory that inherits from FSDirectory and includes the patch.  ",
            "date": "2008-06-29T18:22:22.252+0000",
            "id": 33
        },
        {
            "author": "Michael McCandless",
            "body": "\nCarrying forward from this thread:\n\n  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200806.mbox/%3C85d3c3b60806240501y96d3637r72b2181fa829fa00@mail.gmail.com%3E\n\nJason Rutherglen <jason.rutherglen@gmail.com> wrote:\n\n{quote}\nAfter thinking more about the pool of RandomAccessFiles I think\nLUCENE-753 is the best solution.  I am not sure how much work nor if\npool of RandomAccessFiles creates more synchronization problems and if\nit is only to benefit windows, does not seem worthwhile.\n{quote}\n\nIt wasn't clear to me that pread would in fact perform better than\nletting each thread uses its own private RandomAccessFile.\n\nSo I modified (attached) FileReadTest.java to add a new SeparateFile\nimplementation, which opens a private RandomAccessFile per-thread and\nthen just does \"classic\" seeks & reads on that file.  Then I ran the\ntest on 3 platforms (results below), using 4 threads.\n\nThe results are very interesting -- using SeparateFile is always\nfaster, especially so on WinXP Pro (115% faster than the next fastest,\nClassicFile) but also surprisingly so on Linux (44% faster than the\nnext fastest, ChannelPread).  On Mac OS X it was 5% faster than\nChannelPread.  So on all platforms it's faster, when using multiple\nthreads, to use separate files.\n\nI don't have a Windows server class machine readily accessible so if\nsomeone could run on such a machine, and run on other machines\n(Solaris) to see if these results are reproducible, that'd be great.\n\nThis is a strong argument for some sort of pooling of\nRandomAccessFiles under FSDirectory, though the counter balance is\nclearly added complexity.  I think if we combined the two approaches\n(use separate RandomAccessFile objects per thread as managed by a\npool, and then use the best mode (classic on Windows & channel pread\non all others)) we'd likely get the best performance yet.\n\nMac OS X 10.5.3, single WD Velociraptor hard drive, Sun JRE 1.6.0_05\n\n{code}\n\nconfig: impl=ClassicFile serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=151884, MB/sec=176.73715203708093\n\nconfig: impl=SeparateFile serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=97820, MB/sec=274.4177632386015\n\nconfig: impl=ChannelPread serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=103059, MB/sec=260.4677476008888\n\nconfig: impl=ChannelFile serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=176250, MB/sec=152.30380482269504\n\nconfig: impl=ChannelTransfer serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=365904, MB/sec=73.36226332589969\n\n{code}\n\n\nLinux 2.6.22.1, 6-drive RAID 5 array, Sun JRE 1.6.0_06\n\n{code}\n\nconfig: impl=ClassicFile serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=75592, MB/sec=355.1109323737962\n\nconfig: impl=SeparateFile serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=35505, MB/sec=756.0497282072947\n\nconfig: impl=ChannelPread serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=51075, MB/sec=525.5711326480665\n\nconfig: impl=ChannelFile serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=95640, MB/sec=280.6727896277708\n\nconfig: impl=ChannelTransfer serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=93711, MB/sec=286.45031639828835\n\n{code}\n\n\n\nWIN XP PRO, laptop, Sun JRE 1.4.2_15:\n\n{code}\n\nconfig: impl=ClassicFile serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=135349, MB/sec=198.32836297275932\n\nconfig: impl=SeparateFile serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=62970, MB/sec=426.2910211211688\n\nconfig: impl=ChannelPread serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=174606, MB/sec=153.73781886074937\n\nconfig: impl=ChannelFile serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=152171, MB/sec=176.4038193873997\n\nconfig: impl=ChannelTransfer serial=true nThreads=4 iterations=100 bufsize=1024 filelen=67108864\nanswer=-23909200, ms=275603, MB/sec=97.39932293915524\n\n{code}\n",
            "date": "2008-06-30T12:25:38.454+0000",
            "id": 34
        },
        {
            "author": "Jason Rutherglen",
            "body": "Interesting results.  The question would be, what would the algorithm for allocating RandomAccessFiles to which file look like?  When would a new file open, when would a file be closed?  If it is based on usage would it be based on the rate of calls to readInternal?  This seems like an OS filesystem topic that maybe there is some standard algorithm for.   How would the pool avoid the same synchronization issue given the default small buffer size of 1024?  If there are 30 threads executing searches, there will not be 30 RandomAccessFiles per file so there is still contention over the limited number of RandomAccessFiles allocated.   ",
            "date": "2008-06-30T12:56:09.074+0000",
            "id": 35
        },
        {
            "author": "Yonik Seeley",
            "body": "Added a PooledPread impl to FileReadTest, but at least on Windows it always seems slower than non-pooled.  I suppose it might be because of the extra synchronization.",
            "date": "2008-06-30T17:47:47.734+0000",
            "id": 36
        },
        {
            "author": "Michael McCandless",
            "body": "I think you have a small bug -- minCount is initialized to 0 but should be something effectively infinite instead?",
            "date": "2008-06-30T18:36:49.692+0000",
            "id": 37
        },
        {
            "author": "Yonik Seeley",
            "body": "Thanks Mike, after the bug is fixed, PooledPread is now faster on Windows when more than 1 thread is used.\n",
            "date": "2008-06-30T19:26:33.628+0000",
            "id": 38
        },
        {
            "author": "Michael McCandless",
            "body": "OK I re-ran only PooledPread, SeparateFile and ChannelPread since they\nare the \"leading contenders\" on all platforms.\n\nAlso, I changed to serial=false.\n\nNow the results are very close on all but windows, but on windows I'm\nseeing the opposite of what Yonik saw: PooledPread is slowest, and\nSeparateFile is fastest.  But this is a laptop (Win XP Pro), and it is\nJRE 1.4.  Also I ran with pool size == number of threads == 4.\n\n\nMac OS X 10.5.3, single WD Velociraptor hard drive, Sun JRE 1.6.0_05\n\n{code}\nconfig: impl=PooledPread serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864\nanswer=-23830370, ms=120807, MB/sec=222.20190551871994\n\nconfig: impl=SeparateFile serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864\nanswer=-23830326, ms=119641, MB/sec=224.36744594244448\n\nconfig: impl=ChannelPread serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864\nanswer=-23830370, ms=119217, MB/sec=225.1654176837196\n{code}\n\n\nLinux 2.6.22.1, 6-drive RAID 5 array, Sun JRE 1.6.0_06\n\n{code}\nconfig: impl=PooledPread serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864\nanswer=-23830370, ms=52613, MB/sec=510.2074696367818\n\nconfig: impl=SeparateFile serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864\nanswer=-23830370, ms=52715, MB/sec=509.22025230010433\n\nconfig: impl=ChannelPread serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864\nanswer=-23830370, ms=53792, MB/sec=499.0248661511005\n{code}\n\n\nWIN XP PRO, laptop, Sun JRE 1.4.2_15:\n\n{code}\nconfig: impl=PooledPread serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864\nanswer=-23830370, ms=209956, MB/sec=127.85319590771401\n\nconfig: impl=SeparateFile serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864\nanswer=-23830370, ms=89101, MB/sec=301.27098012367986\n\nconfig: impl=ChannelPread serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=67108864\nanswer=-23830370, ms=184087, MB/sec=145.81988733587923\n{code}",
            "date": "2008-06-30T22:13:47.697+0000",
            "id": 39
        },
        {
            "author": "Brian Pinkerton",
            "body": "I was curious about the discrepancy between the ChannelPread implementation and the SeparateFile implementation that Yonik saw.  At least on Mac OS X, the kernel implementation of read is virtually the same as pread, so there shouldn't be any appreciable performance difference unless the VM is doing something funny.  Sure enough, the implementations of read() under RandomAccessFile and read() under FileChannel are totally different.  The former relies on a buffer allocated either on the stack or by malloc, while the latter allocates a native buffer and copies the results to the original array.\n\nSwitching to a native buffer in the benchmark yields identical results for ChannelPread and SeparateFile on 1.5 and 1.6 on OS X.  I'm attaching an implementation of ChannelPreadDirect that uses a native buffer.\n\nThis may be a moot point because any implementation inside Lucene needs to consume a byte[] and not a ByteBuffer, but at least it's informative.\n\n",
            "date": "2008-06-30T23:29:47.035+0000",
            "id": 40
        },
        {
            "author": "Yonik Seeley",
            "body": "Here are some of my results with 4 threads and a pool size of 4 fds per file.  Win XP on a Pentium4 w/ Java5_0_11 -server\n\n{code}\nconfig: impl=PooledPread serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=9616000\nanswer=322211190, ms=51891, MB/sec=74.12460735002217\n\nconfig: impl=ChannelPread serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=9616000\nanswer=322211190, ms=71175, MB/sec=54.04144713733755\n\nconfig: impl=ClassicFile serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=9616000\nanswer=322211190, ms=62699, MB/sec=61.34707092617107\n\nconfig: impl=SeparateFile serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=4 filelen=9616000\nanswer=322211410, ms=21324, MB/sec=180.37891577565185\n{code}\n",
            "date": "2008-07-01T03:53:37.710+0000",
            "id": 41
        },
        {
            "author": "Michael McCandless",
            "body": "\nOK it's looking like SeparateFile is the best overall choice...  it\nmatches the best performance on Unix platforms and is very much the\nlead on Windows.\n\nIt's somewhat surprising to me that after all this time, with these\nnew IO APIs, the most naive approach (using a separate\nRandomAccessFile per thread) still yields the best performance.  In\nfact, opening multiple IndexReaders to gain concurrency is doing just\nthis.\n\nOf course this is a synthetic benchmark.  Actual IO with Lucene is\nsomewhat different.  EG it's a mix of serial (when iterating through a\nterm's docs with no skipping) and somewhat random access (when\nretrieving term vectors or stored fields), and presumably a mix of\nhits & misses to the OS's IO cache.  So until we try this out with a\nreal index and real queries we won't know for sure.\n\n{quote}\nThe question would be, what would the algorithm for allocating\nRandomAccessFiles to which file look like?\n{quote}\n\nIdeally it would be based roughly on contention.  EG a massive CFS\nfile in your index should have a separate file per-thread, if there\nare not too many threads, whereas tiny CFS files in the index likely\ncould share / synchronize on a single file\n\nI think it would have thread affinity (if the same thread wants the\nsame file we give back the same RandomAccessFile that thread last\nused, if it's available).\n\n{quote}\nWhen would a new file open, when would a file be closed?\n{quote}\n\nI think this should be reference counting.  The first time Lucene\ncalls FSDirectory.openInput on a given name, we must for-real open the\nfile (Lucene relies on OS protecting open files).  Further opens on\nthat file incRef it.  Closes decRef it and when the reference count\ngets to 0 we close it for real.\n\n{quote}\nIf it is based on usage would it be based on the rate of calls to\nreadInternal?\n{quote}\n\nFortunately, Lucene tends to call IndexInput.clone() when it wants to\nactively make use of a file.\n\nSo I think the pool could work something like this: FSIndexInput.clone\nwould \"check out\" a file from the pool.  The pool decides at that\npoint to either return a SharedFile (which has locking per-read, like\nwe do now), or a PrivateFile (which has no locking because you are the\nonly thread currently using that file), based on some measure of\ncontention plus some configuration of the limit of allowed open files.\n\nOne problem with this approach is I'm not sure clones are always\nclosed, since they are currently very lightweight and can rely on GC\nto reclaim them.\n\nAn alternative approach would be to sync() on every block (1024 bytes\ndefault now) read, find a file to use, and use it, but I fear that\nwill have poor performance.\n\nIn fact, if we build this pool, we can again try all these alternative\nIO APIs, maybe even leaving that choice to the Lucene user as\n\"advanced tuning\".",
            "date": "2008-07-01T10:05:33.716+0000",
            "id": 42
        },
        {
            "author": "robert engels",
            "body": "As I stated quit a while ago, this has been a long accepted bug in the JDK.\n\nSee http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734\n\nIt was filed and accepted over 3 years ago.\n\nThe problem is that the pread performs an unnecessary lock on the file descriptor, instead of using Windows \"overlapped\" reads.",
            "date": "2008-07-01T11:58:57.561+0000",
            "id": 43
        },
        {
            "author": "robert engels",
            "body": "The point being - please vote for this issue so it can be fixed properly. It is really a trivial fix, but it needs to be done by SUN.",
            "date": "2008-07-01T12:06:54.541+0000",
            "id": 44
        },
        {
            "author": "Yonik Seeley",
            "body": ".bq OK it's looking like SeparateFile is the best overall choice... it matches the best performance on Unix platforms and is very much the\nlead on Windows.\n\nThe other implementations are fully-featured though (they could be used in lucene w/ extra synchronization, etc).  SeparateFile (opening a new file descriptor per reader) is not a real implementation that could be used... it's more of a theoretical maximum IMO.  Also remember that you can't open a new fd on demand since the file might already be deleted.  We would need a real PooledClassicFile implementation (like PooledPread).\n\nOn non-windows it looks like ChannelPread is probably the right choice.. near max performance and min fd usage\n\n",
            "date": "2008-07-01T14:28:31.704+0000",
            "id": 45
        },
        {
            "author": "Jason Rutherglen",
            "body": "Core2Duo Windows XP JDK1.5.15.  PooledPread for 4 threads and pool size 2 the performance does not compare well to SeparateFile.  PooledPread for 30 threads does not improve appreciably over ClassicFile.  If there were 30 threads, how many RandomAccessFiles would there need to be to make a noticeable impact?  The problem I see with the pooled implementation is setting the global file descriptor limit properly, will the user set this?  There would almost need to be a native check to see if what the user is trying to do is possible.  \n\nThe results indicate there is significant contention in the pool code.  The previous tests used a pool size the same as the number of threads which is probably not how most production systems look, at least the SOLR installations I've worked on.  In SOLR the web request thread is the thread that executes the search, so the number of threads is determined by the J2EE server which can be quite high.  Unless the assumption is the system is set for an unusually high number of file descriptors.  \n\nThere should probably be a MMapDirectory test as well.  \n\n{noformat}\nconfig: impl=PooledPread serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=2 filelen=18110448\nanswer=53797223, ms=32715, MB/sec=221.4329573590096\n\nconfig: impl=SeparateFile serial=false nThreads=4 iterations=100 bufsize=1024 poolsize=2 filelen=18110448\nanswer=53797223, ms=18687, MB/sec=387.6587574249478\n\nconfig: impl=SeparateFile serial=false nThreads=30 iterations=100 bufsize=1024 poolsize=2 filelen=18110448\nanswer=403087371, ms=137871, MB/sec=394.0737646060448\n\nconfig: impl=PooledPread serial=false nThreads=30 iterations=100 bufsize=1024 poolsize=2 filelen=18110448\nanswer=403087487, ms=526504, MB/sec=103.19265190767781\n\nconfig: impl=ChannelPread serial=false nThreads=30 iterations=100 bufsize=1024 poolsize=2 filelen=18110448\nanswer=403087487, ms=624291, MB/sec=87.02887595688549\n\nconfig: impl=ClassicFile serial=false nThreads=30 iterations=100 bufsize=1024 poolsize=2 filelen=18110448\nanswer=403087487, ms=587430, MB/sec=92.48990347786119\n\nconfig: impl=PooledPread serial=false nThreads=30 iterations=100 bufsize=1024 poolsize=4 filelen=18110448\nanswer=403087487, ms=552971, MB/sec=98.25351419875544\n{noformat}",
            "date": "2008-07-01T16:15:36.604+0000",
            "id": 46
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nSeparateFile (opening a new file descriptor per reader) is not a real implementation that could be used... it's more of a theoretical maximum IMO. Also remember that you can't open a new fd on demand since the file might already be deleted. We would need a real PooledClassicFile implementation (like PooledPread).\n{quote}\n\nTrue, we'd have to make a real pool, but I'd think we want the sync() to be on cloning and not on every read.  I think Lucene's usage of the open files (clones are made & used up quickly and closed) would work well with that approach.  I think at this point we should build out an underlying pool and then test all of these different approaches under the pool.\n\nAnd yes we cannot just open a new fd on demand if the file has been deleted.  But I'm thinking that may not matter in practice.  Ie if the pool wants to open a new fd, it can attempt to do so, and if the file was deleted it must then return a shared access wrapper to the fd it already has open.  Large segments are where the contention will be and large segments are not often deleted.  Plus people tend to open new readers if such a large change has taken place to the index.\n\n{quote}\nOn non-windows it looks like ChannelPread is probably the right choice.. near max performance and min fd usage\n{quote}\n\nBut on Linux I saw 44% speedup for serial=true case with 4 threads using SeparateFile vs ChannelPread, which I was very surprised by.  But then again it's synthetic so it may not matter in real Lucene searches.",
            "date": "2008-07-02T08:34:59.940+0000",
            "id": 47
        },
        {
            "author": "Jason Rutherglen",
            "body": "lucene-753.patch\n\nAdded javadoc and removed unnecessary NIOFSIndexOutput class.",
            "date": "2008-07-02T12:55:44.553+0000",
            "id": 48
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. (clones are made & used up quickly and closed) \n\nIIRC, clones are often not closed at all.\nAnd for term expanding queries, you can get a *lot* of them all at once.\n\nbq. And yes we cannot just open a new fd on demand if the file has been deleted. But I'm thinking that may not matter in practice. Ie if the pool wants to open a new fd, it can attempt to do so, and if the file was deleted it must then return a shared access wrapper to the fd it already has open.\n\nAt first blush, sounds a bit too complex for the benefits.\n- one would have to reserve the last fd for synchronized access... can't really hand it out for unsynchronized exclusive access and then go and share it later.\n- the shared access should use pread... not seek+read\n\nbq. But on Linux I saw 44% speedup for serial=true case with 4 threads using SeparateFile vs ChannelPread, which I was very surprised by.\n\nIn the serial case, there are half the system calls (no seek).  When both implementations have a single single system call, all the extra code and complexity that Sun threw into FileChannel comes into play.  Compare that with RandomAccessFile.read() which drops down to a native call and presumably just the read with little overhead.  I wish Sun would just add a RandomAccessFile.read with a file position.\n\nIf access will be truly serial sometimes, larger buffers would help with that larger read() setup cost. \n",
            "date": "2008-07-02T13:20:20.716+0000",
            "id": 49
        },
        {
            "author": "Michael McCandless",
            "body": "bq. And for term expanding queries, you can get a lot of them all at once.\n\nRight but that'd all be under one thread right?  The pool would always give the same RandomAccessFile (private or shared) for the same filename X thread.\n\nbq. one would have to reserve the last fd for synchronized access... can't really hand it out for unsynchronized exclusive access and then go and share it later.\n\nWell, I think you'd hand it out first, as a shared file (so you reserve the right to hand it out again, later).  If other threads come along you would open a new one (if you are under the budget) and loan it to them privately (so no syncing during read).  I think sync'ing with no contention (the first shared file we hand out) should be OK performance in modern JVMs.\n\nbq. the shared access should use pread... not seek+read\n\nBut not on Windows...\n\nbq. At first blush, sounds a bit too complex for the benefits.\n\nYeah I'm on the fence too ... but this lack of concurrency hurts our search performance.  It's ashame users have to resort to multiple IndexReaders.  Though it still remains to be seen how much the pool or pread approaches really improve end to end search performance (vs other bottlenecks like IndexReader.isDeleted).\n\nWindows is an important platform and doing the pool approach, vs leaving Windows with classic if we do pread approach, lets us have good concurrency on Windows too.",
            "date": "2008-07-02T18:01:06.164+0000",
            "id": 50
        },
        {
            "author": "Brian Gardner",
            "body": "This probably doesn't help much, but I implemented a pool and submitted a patch very similar to the SeparateFile approach.  Before being directed to this thread: \nhttps://issues.apache.org/jira/browse/LUCENE-1337\n\nIn our implementation the synchronization/lack of concurrency has been a big issue for us.  On several occasions we've had to remove new features that perform searches from frequently hit pages, because threads build up waiting for synchronized access to the underlying files.  It is possible that I would still have issue even with my patch, considering from my tests that I'm only increasing throughput by 300%,  but it would be easier for me to tune and scale my application since resource utilization and contention would be visible from the OS level. \n\n\n> At first blush, sounds a bit too complex for the benefits.\n\nMy vote is that the benefits outway the complexity, especially considering it's an out-of-the box solutions that works well for all platforms and single threaded as well as multi-threaded envirnments.  If it's helpful, I can spend the time to implement some of the missing feature(s) of the pool that will be needed for it to be an acceptable solution (i.e, shared access once a file has been deleted, and perhaps a time-based closing mechanism).",
            "date": "2008-07-16T19:32:44.609+0000",
            "id": 51
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nIn our implementation the synchronization/lack of concurrency has been a big issue for us. On several occasions we've had to remove new features that perform searches from frequently hit pages, because threads build up waiting for synchronized access to the underlying files.  It is possible that I would still have issue even with my patch, considering from my tests that I'm only increasing throughput by 300%, but it would be easier for me to tune and scale my application since resource utilization and contention would be visible from the OS level. \n{quote}\n\nCan you describe your test -- OS, JRE version, size/type of your index, number of cores, amount of RAM, type of IO system, etc?  It's awesome that you see 300% gain in search throughput.  Is your index largely cached in the OS's IO cache, or not?\n\n{quote}\nMy vote is that the benefits outway the complexity, especially considering it's an out-of-the box solutions that works well for all platforms and single threaded as well as multi-threaded envirnments. If it's helpful, I can spend the time to implement some of the missing feature(s) of the pool that will be needed for it to be an acceptable solution (i.e, shared access once a file has been deleted, and perhaps a time-based closing mechanism).\n{quote}\n\nIf we can see sizable concurreny gains, reliably & across platforms, I agree we should pursue this approach.  One particular frustration is: if you optimize your index, thinking this gains you better search performance, you're actually making things far worse as far as concurrency is concerned because now you are down to a single immense file.  I think we do need to fix this situation.\n\nOn your patch, I think in addition to shared-access on a now-deleted file, we should add a global control on the \"budget\" of number of open files (right now I think your patch has a fixed cap per-filename).  Probably the budget should be expressed as a multiplier off the minimum number of open files, rather than a fixed cap, so that an index with many segments is allowed to use more.  Ideally over time the pool works out such that for small files in the index (small segments) since there is very little contention they only hold 1 descriptor open, but for large files many descriptors are opened.\n\nI created a separate test (will post a patch & details to this issue) to explore using SeparateFile inside FSDirectory, but unfortunately I see mixed results on both the cached & uncached cases.  I'll post details separately.\n\nOne issue with your patch is it's using Java 5 only classes (Lucene is still on 1.4); once you downgrade to 1.4 I wonder if the added synchronization will become costly.\n\nI like how your approach is to pull a RandomAccessFile from the pool only when a read is taking place -- this automatically takes care of creating new descriptors when there truly is contention.  But one concern I have is that this defeats the OS's IO system's read-ahead optimization since from the OS's perspective the file descriptors are getting shuffled.  I'm not sure if this really matters much in Lucene, because many things (reading stored fields & term vectors) are likely not helped much by read-ahead, but for example a simple TermQuery on a large term should in theory benefit from read-ahead.  You could gain this back with a simple thread affinity, such that the same thread gets the same file descriptor it got last time, if it's available.  But that added complexity may offset any gains.\n",
            "date": "2008-07-17T10:21:42.907+0000",
            "id": 52
        },
        {
            "author": "Michael McCandless",
            "body": "\nI attached FSDirectoryPool.patch, which adds\noal.store.FSDirectoryPool, a Directory that will open a new file for\nevery unique thread.\n\nThis is intended only as a test (to see if shows consistent\nimprovement in concurrency) -- eg it does not close all these files,\nnor make any effort to budget itself if there are too many threads,\nit's not really a pool, etc.  But it should give us an upper bound on\nthe gains we could hope for.\n\nI also added a \"pool=true|false\" config option to contrib/benchmark so\nyou can run tests with and without separate files.\n\nI ran some quick initial tests but didn't see obvious gains.  I'll go\nback & re-run more carefully to confirm, and post back.\n",
            "date": "2008-07-17T18:18:30.275+0000",
            "id": 53
        },
        {
            "author": "Michael McCandless",
            "body": "I created a large index (indexed Wikipedia 4X times over, with stored\nfields & tv offsets/positions = 72 GB).  I then randomly sampled 50\nterms > 1 million freq, plus 200 terms > 100,000 freq plus 100 terms >\n10,000 freq plus 100 terms > 1000 freq.  Then I warmed the OS so these\nqueries are fully cached in the IO cache.\n\nIt's a highly synthetic test.  I'd really love to test on real\nqueries, instead of single term queries.\n\nThen I ran this alg:\n\n{code}\nanalyzer=org.apache.lucene.analysis.standard.StandardAnalyzer\n\nquery.maker = org.apache.lucene.benchmark.byTask.feeds.FileBasedQueryMaker\nfile.query.maker.file = /lucene/wikiQueries.txt\n\ndirectory=FSDirectory\npool=true\n\nwork.dir=/lucene/bigwork\n\nOpenReader\n\n{ \"Warmup\" SearchTrav(20) > : 5\n\n{ \"Rounds\"\n  [{ \"Search\" Search > : 500]: 16\n  NewRound\n}: 2\n\nCloseReader \n\nRepSumByPrefRound Search\n{code}\n\nI ran with 2, 4, 8 and 16 threads, on a Intel quad Mac Pro (2 cpus,\neach dual core) OS X 10.5.4, with 6 GB RAM, Sun JRE 1.6.0_05 and a\nsingle WD Velociraptor hard drive.  To keep the number of searches\nconstant I changed the 500 count above to match (ie with 8 threads I\nchanged 500 -> 1000, 4 threads I changed it to 2000, etc.).\n\nHere're the results -- each run is best of 2, and all searches are\nfully cached in OS's IO cache:\n\n||Number of Threads||Patch rec/s||Trunk rec/s||Pctg gain||\n|2|78.7|74.9|5.1%|\n|4|74.1|68.2|8.7%|\n|8|37.7|32.7|15.3%|\n|16|19.2|16.3|17.8%|\n\nI also ran the same alg, replacing Search task with SearchTravRet(10)\n(retrieves the first 10 docs (hits) of each search), first warming so\nit's all fully cached:\n\n||Number of Threads||Patch rec/s||Trunk rec/s||Pctg gain||\n|2|1589.6|1519.8|4.6%|\n|4|1460.9|1395.3|4.7%|\n|8|748.9|676.0|10.8%|\n|16|382.7|338.4|13.1%|\n\nSo there are smallish gains, but rememember these are upper bounds on\nthe gains because no pooling is happening.  I'll test uncached next.\n",
            "date": "2008-07-19T10:29:49.019+0000",
            "id": 54
        },
        {
            "author": "Michael McCandless",
            "body": "\nOK I ran the uncached test, using the Search task.  JRE & hardware are\nthe same as above.\n\nI generated a larger (6150) set of queries to make sure the threads\nnever wrap around and do the same queries again.  I also run only 1\nround for the same reason.  Between tests I evict the OS's IO cache.\n\n||Number of Threads||Patch rec/s||Trunk rec/s||Pctg gain||\n|2|32.2|23.8|35.3%|\n|4|16.4|12.7|29.1%|\n|8|8.5|3.5|142.9%|\n|16|3.8|2.7|40.7%|\n\nThe gains are better.  The 8 thread case I don't get; I re-ran it and\nit still came out much better (135.7%).  It could be 8 threads is the\nsweet spot for concurrency on this hardware.",
            "date": "2008-07-22T18:44:05.185+0000",
            "id": 55
        },
        {
            "author": "Matthew Mastracci",
            "body": "I just tried out the latest NIOFSDirectory patch and I'm seeing a bug.  If I go back to the regular FSDirectory, everything works fine.\n\nI can't reproduce it on a smaller testcase.  It only happens with the live index.\n\nAny ideas on where to debug?\n\n{noformat} \nCaused by: java.lang.IndexOutOfBoundsException: Index: 24444, Size: 4\n\tat java.util.ArrayList.RangeCheck(ArrayList.java:547)\n\tat java.util.ArrayList.get(ArrayList.java:322)\n\tat org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:260)\n\tat org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:249)\n\tat org.apache.lucene.index.TermBuffer.read(TermBuffer.java:68)\n\tat org.apache.lucene.index.SegmentTermEnum.next(SegmentTermEnum.java:123)\n\tat org.apache.lucene.index.SegmentTermEnum.scanTo(SegmentTermEnum.java:154)\n\tat org.apache.lucene.index.TermInfosReader.scanEnum(TermInfosReader.java:223)\n\tat org.apache.lucene.index.TermInfosReader.get(TermInfosReader.java:217)\n\tat org.apache.lucene.index.SegmentReader.docFreq(SegmentReader.java:678)\n\tat org.apache.lucene.index.MultiSegmentReader.docFreq(MultiSegmentReader.java:373)\n\tat org.apache.lucene.search.IndexSearcher.docFreq(IndexSearcher.java:87)\n\tat org.apache.lucene.search.Similarity.idf(Similarity.java:457)\n\tat org.apache.lucene.search.TermQuery$TermWeight.<init>(TermQuery.java:44)\n\tat org.apache.lucene.search.TermQuery.createWeight(TermQuery.java:146)\n\tat org.apache.lucene.search.BooleanQuery$BooleanWeight.<init>(BooleanQuery.java:187)\n\tat org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:362)\n\tat org.apache.lucene.search.Query.weight(Query.java:95)\n\tat org.apache.lucene.search.Searcher.createWeight(Searcher.java:171)\n\tat org.apache.lucene.search.Searcher.search(Searcher.java:132)\n{noformat} \n\nThe index is not using the compound file format:\n\n{noformat} \n7731499698 Jul 28 03:46 _6zk.fdt\n 232014520 Jul 28 03:50 _6zk.fdx\n        32 Jul 28 03:50 _6zk.fnm\n3775713450 Jul 28 04:06 _6zk.frq\n  58003634 Jul 28 04:07 _6zk.nrm\n2944298834 Jul 28 04:18 _6zk.prx\n    432418 Jul 28 04:18 _6zk.tii\n  30784106 Jul 28 04:19 _6zk.tis\n 217354711 Jul 28 08:18 _76i.fdt\n   6509864 Jul 28 08:18 _76i.fdx\n        32 Jul 28 08:18 _76i.fnm\n 144348761 Jul 28 08:18 _76i.frq\n   1627470 Jul 28 08:18 _76i.nrm\n 295528445 Jul 28 08:19 _76i.prx\n     52622 Jul 28 08:19 _76i.tii\n   3858378 Jul 28 08:19 _76i.tis\n 199621206 Jul 29 13:29 _7cm.fdt\n   5994720 Jul 29 13:29 _7cm.fdx\n        32 Jul 29 13:29 _7cm.fnm\n 136445620 Jul 29 13:29 _7cm.frq\n   1498684 Jul 29 13:29 _7cm.nrm\n 284805312 Jul 29 13:30 _7cm.prx\n     48346 Jul 29 13:30 _7cm.tii\n   3522117 Jul 29 13:30 _7cm.tis\n   3914068 Jul 29 13:30 _7cn.fdt\n    119184 Jul 29 13:30 _7cn.fdx\n        32 Jul 29 13:30 _7cn.fnm\n   2993343 Jul 29 13:30 _7cn.frq\n     29800 Jul 29 13:30 _7cn.nrm\n   7380878 Jul 29 13:30 _7cn.prx\n      5277 Jul 29 13:30 _7cn.tii\n    378816 Jul 29 13:30 _7cn.tis\n    383147 Jul 29 13:30 _7cq.fdt\n     11240 Jul 29 13:30 _7cq.fdx\n        32 Jul 29 13:30 _7cq.fnm\n    290398 Jul 29 13:30 _7cq.frq\n      2814 Jul 29 13:30 _7cq.nrm\n    763135 Jul 29 13:30 _7cq.prx\n      1581 Jul 29 13:30 _7cq.tii\n    115971 Jul 29 13:30 _7cq.tis\n        19 Jul 29 13:30 date\n        20 Jul 21 01:53 segments.gen\n       155 Jul 29 13:30 segments_d61\n{noformat} ",
            "date": "2008-07-29T17:57:01.629+0000",
            "id": 56
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I just tried out the latest NIOFSDirectory patch and I'm seeing a bug. If I go back to the regular FSDirectory, everything works fine. \n\nIs the index itself corrupt, ie, NIOFSDirectory did something bad when writing the index?  Or, is it only in reading the index with NIOFSDirectory that you see this?  IE, can you swap in FSDirectory on your existing index and the problem goes away?",
            "date": "2008-07-31T09:39:43.248+0000",
            "id": 57
        },
        {
            "author": "Matthew Mastracci",
            "body": "bq. Is the index itself corrupt, ie, NIOFSDirectory did something bad when writing the index? Or, is it only in reading the index with NIOFSDirectory that you see this? IE, can you swap in FSDirectory on your existing index and the problem goes away?\n\nI haven't seen any issues with writing the index under NIOFSDirectory.  The failures seem to happen only when reading.  When I switch to FSDirectory (or MMapDirectory), the same index that fails under NIOFSDirectory works flawlessly (indicating that the index is not corrupt).\n\nThe error with NIOFSDirectory is determinate and repeatable (same error every time, same location, same query during warmup).\n\nI couldn't reproduce this on a smaller index, unfortunately.\n",
            "date": "2008-08-08T23:05:16.856+0000",
            "id": 58
        },
        {
            "author": "Michael McCandless",
            "body": "bq. The error with NIOFSDirectory is determinate and repeatable (same error every time, same location, same query during warmup).\n\nDid you see a prior exception, before hitting the AIOOBE?  If so, I think this is just LUCENE-1262 all over again.  That issue was fixed in BufferedIndexInput, but the NIOFSIndexInput has copied a bunch of code from BufferedIndexInput (something I think we must fix before committing it -- I think it should inherit from BufferedIndexInput instead) and so it still has that bug.  I'll post a patch with the bug re-fixed so you can at least test it to see if it resolves your exception.",
            "date": "2008-08-22T10:19:43.613+0000",
            "id": 59
        },
        {
            "author": "Jason Rutherglen",
            "body": "I can possibly work on this, just go through and reedit the BufferedIndexInput portions of the code.  Inheriting is difficult because of the ByteBuffer code.  Needs to be done line by line.",
            "date": "2008-08-22T10:38:18.258+0000",
            "id": 60
        },
        {
            "author": "Michael McCandless",
            "body": "Attached new rev of NIOFSDirectory.\n\nBesides re-fixing LUCENE-1262, I also found & fixed a bug in the NIOFSIndexInput.clone() method.\n\nMatthew, could you give this one a shot to see if it fixes your case?  Thanks.",
            "date": "2008-08-22T11:57:18.536+0000",
            "id": 61
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I can possibly work on this, just go through and reedit the BufferedIndexInput portions of the code. Inheriting is difficult because of the ByteBuffer code. Needs to be done line by line.\n\nThat would be awesome, Jason.  I think we should then commit NIOFSDirectory to core as at least a way around this bottleneck on all platforms but Windows.  Maybe we can do this in time for 2.4?",
            "date": "2008-08-22T12:04:53.632+0000",
            "id": 62
        },
        {
            "author": "Matthew Mastracci",
            "body": "bq. Matthew, could you give this one a shot to see if it fixes your case? Thanks.\n\nMichael,\n\nI ran this new patch against our big index and it works very well.  If I have time, I'll run some benchmarks to see what our real-life performance improvements are like.  \n\nNote that I'm only running it for our read-only snapshot of the index, however, so this hasn't been tested for writing to a large index.\n",
            "date": "2008-08-22T17:06:58.108+0000",
            "id": 63
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I ran this new patch against our big index and it works very well. If I have time, I'll run some benchmarks to see what our real-life performance improvements are like.\n\nSuper, thanks!  This was the same index that would reliably hit the exception above?",
            "date": "2008-08-22T17:44:53.190+0000",
            "id": 64
        },
        {
            "author": "Matthew Mastracci",
            "body": "bq.  Super, thanks! This was the same index that would reliably hit the exception above?\n\nCorrect - it would hit the exception every time at startup.\n\nI've been running NIOFSDirectory for the last couple of hours with zero exceptions (except for running out of file descriptors after starting it the first time :)).  The previous incarnation was running MMapDirectory.\n\nThanks for all the work on this patch.\n\n\n",
            "date": "2008-08-22T18:01:00.204+0000",
            "id": 65
        },
        {
            "author": "Matthew Mastracci",
            "body": "This exception popped up out of the blue a few hours in.  No exceptions before it.  I'll see if I can figure out whether it was caused by our index snapshotting or if it's a bug elsewhere in NIOFSDirectory.\n\nI haven't seen any exceptions like this with MMapDirectory, but it's possible there's something that we're doing that isn't correct.\n\n{noformat} \nCaused by: java.nio.channels.ClosedChannelException\n\tat sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:91)\n\tat sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:616)\n\tat com.dotspots.analyzer.index.NIOFSDirectory$NIOFSIndexInput.read(NIOFSDirectory.java:186)\n\tat com.dotspots.analyzer.index.NIOFSDirectory$NIOFSIndexInput.refill(NIOFSDirectory.java:218)\n\tat com.dotspots.analyzer.index.NIOFSDirectory$NIOFSIndexInput.readByte(NIOFSDirectory.java:232)\n\tat org.apache.lucene.store.IndexInput.readVInt(IndexInput.java:76)\n\tat org.apache.lucene.index.TermBuffer.read(TermBuffer.java:63)\n\tat org.apache.lucene.index.SegmentTermEnum.next(SegmentTermEnum.java:123)\n\tat org.apache.lucene.index.SegmentTermEnum.scanTo(SegmentTermEnum.java:154)\n\tat org.apache.lucene.index.TermInfosReader.scanEnum(TermInfosReader.java:223)\n\tat org.apache.lucene.index.TermInfosReader.get(TermInfosReader.java:217)\n\tat org.apache.lucene.index.SegmentReader.docFreq(SegmentReader.java:678)\n\tat org.apache.lucene.index.MultiSegmentReader.docFreq(MultiSegmentReader.java:373)\n\tat org.apache.lucene.index.MultiReader.docFreq(MultiReader.java:310)\n\tat org.apache.lucene.search.IndexSearcher.docFreq(IndexSearcher.java:87)\n\tat org.apache.lucene.search.Searcher.docFreqs(Searcher.java:178)\n{noformat} ",
            "date": "2008-08-22T22:01:31.793+0000",
            "id": 66
        },
        {
            "author": "Michael McCandless",
            "body": "Interesting...\n\nAre you really sure you're not accidentally closing the searcher before calling Searcher.docFreqs?  Are you calling docFreqs directly from your app?\n\nIt looks like MMapIndexInput.close() is a noop so it would not have detected calling Searcher.docFreqs after close, whereas NIOFSdirectory (and the normal FSDirectory) will.\n\nIf you try the normal FSDirectory do you also an exception like this?\n\nIncidentally, what sort of performance differences are you noticing between these three different ways of accessing an index in the file system?",
            "date": "2008-08-23T12:54:49.371+0000",
            "id": 67
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. Maybe we can do this in time for 2.4?\n+1\n\nLatest patch is looking good to me!\nIs there a reason we don't do lazy allocation in clone() like FSIndexInput?\n\nAlso, our finalizers aren't technically thread safe which could lead to a double close in the finalizer (although I doubt if this particular case would ever happen).   If we need to keep them, we could change Descriptor.isOpen to volatile and there should be pretty much no cost since it's only checked in close().\n",
            "date": "2008-08-23T12:56:09.394+0000",
            "id": 68
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Is there a reason we don't do lazy allocation in clone() like FSIndexInput?\n\nYonik, do you mean BufferedIndexInput.clone (not FSIndexInput)?\n\nI think once we fix NIOFSIndexInput to subclass from BufferedIndexInput, then cloning should be lazy again.  Jason are you working on this (subclassing from BufferedIndexInput)?  If not I can take it.\n\nbq. Also, our finalizers aren't technically thread safe which could lead to a double close in the finalizer\n\nHmmm... I'll update both FSDirectory and NIOFSDiretory's isOpen's to be volatile.",
            "date": "2008-08-23T13:08:37.109+0000",
            "id": 69
        },
        {
            "author": "Jason Rutherglen",
            "body": "Mike I have have not started on the subclassing from BufferedIndexInput yet.  I can work on it monday though.  ",
            "date": "2008-08-23T13:21:41.311+0000",
            "id": 70
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Mike I have have not started on the subclassing from BufferedIndexInput yet. I can work on it monday though. \n\nOK, thanks!",
            "date": "2008-08-23T13:23:15.661+0000",
            "id": 71
        },
        {
            "author": "Michael McCandless",
            "body": "Updated patch with Yonik's volatile suggestion -- thanks Yonik!\n\nAlso, I removed NIOFSDirectory.createOutput since it was doing the same thing as super().",
            "date": "2008-08-23T13:24:49.205+0000",
            "id": 72
        },
        {
            "author": "Matthew Mastracci",
            "body": "Michael,\n\nbq. Are you really sure you're not accidentally closing the searcher before calling Searcher.docFreqs? Are you calling docFreqs directly from your app?\n\nOur IndexReaders are actually managed in a shared pool (currently 8 IndexReaders, shared round-robin style as requests come in).  We have some custom reference counting logic that's supposed to keep the readers alive as long as somebody has them open.  As new index snapshots come in, the IndexReaders are re-opened and reference counts ensure that any old index readers in use are kept alive until the searchers are done with them.  I'm guessing we have an error in our reference counting logic that just doesn't show up under MMapDirectory (as you mentioned, close() is a no-op).\n\nWe're calling docFreqs directly from our app.  I'm guessing that it just happens to be the most likely item to be called after we roll to a new index snapshot.\n\nI don't have hard performance numbers right now, but we were having a hard time saturating I/O or CPU with FSDirectory.  The locking was basically killing us.  When we switched to MMapDirectory and turned on compound files, our performance jumped at least 2x.  The preliminary results I'm seeing with NIOFSDirectory seem to indicate that it's slightly faster than MMapDirectory.\n\nI'll try setting our app back to using the old FSDirectory and see if the exceptions still occur.  I'll also try to fiddle with our unit tests to make sure we're correctly ref-counting all of our index readers.\n\nBTW, I ran a quick FSDirectory/MMapDirectory/NIOFSDirectory shootout.  It uses a parallel benchmark that roughly models what our real-life benchmark is like.  I ran the benchmark once through to warm the disk cache, then got the following.  The numbers are fairly stable across various runs once the disk caches are warm:\n\nFS: 33644ms\nMMap: 28616ms\nNIOFS: 33189ms\n\nI'm a bit surprised at the results myself, but I've spent a bit of time tuning the indexes to maximize concurrency.  I'll double-check that the benchmark is correctly running all of the tests.\n\nThe benchmark effectively runs 10-20 queries in parallel at a time, then waits for all queries to complete.  It does this end-to-end for a number of different query batches, then totals up the time to complete each batch.\n",
            "date": "2008-08-23T22:30:41.875+0000",
            "id": 73
        },
        {
            "author": "Jason Rutherglen",
            "body": "LUCENE-753.patch\n\nNIOFSIndexInput now extends BufferedIndexInput.  I was unable to test however and wanted to just get this up.  ",
            "date": "2008-08-25T14:46:11.960+0000",
            "id": 74
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nFS: 33644ms\nMMap: 28616ms\nNIOFS: 33189ms\n\nI'm a bit surprised at the results myself, but I've spent a bit of time tuning the indexes to maximize concurrency. I'll double-check that the benchmark is correctly running all of the tests.\n{quote}\nThis is surprising -- your benchmark is very concurrent, yet FSDir and NIOFSDir are close to the same net throughput, while MMapDir is quite a bit faster.  Is this on a non-Windows OS?",
            "date": "2008-08-26T09:31:26.467+0000",
            "id": 75
        },
        {
            "author": "Michael McCandless",
            "body": "\nNew patch attached.  Matthew if you could try this version out on your\nindex, that'd be awesome.\n\nI didn't like how we were still copying the hairy readBytes & refill\nmethods from BufferedIndexInput, so I made some small additional mods\nto BufferedIndexInput to notify subclass when a byte[] buffer gets\nallocated, which then allowed us to fully inherit these methods.\n\nBut, then I realized we were duplicating alot of code from\nFSIndexInput, so I switched to subclassing that instead and that made\nthings even simpler.\n\nSome other things also fixed:\n\n  * We were ignoring bufferSize (eg setBufferSize).\n\n  * We weren't closing the FileChannel\n\n  * clone() now lazily clones the buffer again\n\nTo test this, I made NIOFSDirectory the default IMPL in\nFSDirectory.getDirectory and ran all tests.  One test failed at first\n(because we were ignoring setBufferSize calls); with the new patch,\nall tests pass.\n\nI also built first 150K docs of wikipedia and ran various searches\nusing NIOFSDirectory and all seems good.\n\nThe class is quite a bit simpler now, however there's one thing I\ndon't like: when you use CFS, the NIOFSIndexInput.readInternal method\nwill wrap the CSIndexInput's byte[] (from it's parent\nBufferedIndexInput class) for every call (every 1024 bytes read from\nthe file).  I'd really like to find a clean way to reuse a single\nByteBuffer.  Not yet sure how to do that though...\n\n",
            "date": "2008-08-26T14:44:53.010+0000",
            "id": 76
        },
        {
            "author": "Michael McCandless",
            "body": "New version attached.  This one re-uses a wrapped byte buffer even when it's CSIndexInput that's calling it.\n\nI plan to commit in a day or two.",
            "date": "2008-08-28T17:53:03.556+0000",
            "id": 77
        },
        {
            "author": "Michael McCandless",
            "body": "I just committed revision 690539, adding NIOFSDirectory.  I will leave this open, but move off of 2.4, until we can get similar performance gains on Windows...",
            "date": "2008-08-30T17:33:54.338+0000",
            "id": 78
        },
        {
            "author": "robert engels",
            "body": "SUN is accepting outside bug fixes to the Open JDK, and merging them to the commercial JDK (in most cases).\n\nIf the underlying bug is fixed in the Windows JDK - not too hard - then you fix this properly in Lucene.\n\nIf you don't fix it in the JDK you are always going to have the 'running out of file handles' synchronization, vs, the \"locked position\" synchronization - there is no way to fix this in user code...",
            "date": "2008-08-31T15:21:49.790+0000",
            "id": 79
        },
        {
            "author": "Yonik Seeley",
            "body": "Attaching new FileReadTest.java that fixes a concurrency bug in SeparateFile - each reader needed it's own file position.",
            "date": "2009-09-15T20:32:05.985+0000",
            "id": 80
        },
        {
            "author": "Uwe Schindler",
            "body": "This issue was resolved a long time ago, but left open for the stupid Windows Sun JRE bug which was never resolved. With Lucene 3.x and trunk we have better defaults (use e.g. MMapDirectory on Windows-64).\n\nUsers should default to FSDirectory.open() and use the returned directory for best performance.",
            "date": "2011-01-25T15:43:12.398+0000",
            "id": 81
        }
    ],
    "component": "core/store",
    "description": "As suggested by Doug, we could use NIO pread to avoid synchronization on the underlying file.\nThis could mitigate any MT performance drop caused by reducing the number of files in the index format.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-753",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Use NIO positional read to avoid synchronization in FSIndexInput",
    "systemSpecification": true,
    "version": ""
}