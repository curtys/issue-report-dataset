{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "I fixed only StandardAnalyzer to skip terms longer than 255 chars by\ndefault (it turns out SimpleAnalyzer, WhitespaceAnalyzer, StopAnalyzer\nalready prune tokens at 255 chars).\n\nYou can change the max allowed token length by calling\nStandardAnalyzer.setMaxTokenLength.\n\nI didn't use LengthFilter, because 1) I wanted to avoid copying the\nmassive term only to then filter it (makes performance faster) and, 2)\nI wanted to increment position increment for the next valid token\nafter a series of too-long tokens.\n\nAll tests pass.  I plan to commit in a day or two.\n",
            "date": "2008-01-04T19:08:13.783+0000",
            "id": 0
        }
    ],
    "component": "",
    "description": "Discussion that led to this:\n\n  http://www.gossamer-threads.com/lists/lucene/java-dev/56103\n\nI believe nearly any time a token > 100 characters in length is\nproduced, it's a bug in the analysis that the user is not aware of.\n\nThese long tokens cause all sorts of problems, downstream, so it's\nbest to catch them early at the source.\n\nWe can accomplish this by tacking on a LengthFilter onto the chains\nfor StandardAnalyzer, SimpleAnalyzer, WhitespaceAnalyzer, etc.\n\nShould we do this in 2.3?  I realize this is technically a break in\nbackwards compatibility, however, I think it must be incredibly rare\nthat this break would in fact break something real in the application?",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1118",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "core analyzers should not produce tokens > N (100?) characters in length",
    "systemSpecification": true,
    "version": ""
}