{
    "comments": [
        {
            "author": "Tom Burton-West",
            "body": "Patch against recent trunk.   Can someone please suggest an appropriate existing unit test to use as a model for creating a unit test for this?   Would it be appropriate to include a small index file for testing or is it better to programatically create the index file?",
            "date": "2010-04-14T17:20:40.955+0000",
            "id": 0
        },
        {
            "author": "Tom Burton-West",
            "body": "For an example of how this utility can be used please see: http://www.hathitrust.org/blogs/large-scale-search/slow-queries-and-common-words-part-1",
            "date": "2010-04-14T17:23:12.110+0000",
            "id": 1
        },
        {
            "author": "Otis Gospodnetic",
            "body": "I think creating a small index with a couple of docs would be the way to go.",
            "date": "2010-04-14T21:38:19.595+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "Programmatically indexing those docs is fine -- most tests make a MockRAMDir, index a few docs into it, and test against that.\n\nThis tool looks useful, thanks Tom!\n\nNote that with flex scoring (LUCENE-2392) we are planning on storing this statistic (sum of tf for the term across all docs) in the terms dict, for fields that enable statistics.  So when that lands, this tool can pull from that, or regenerate it if the field didn't store stats.",
            "date": "2010-04-14T22:19:09.019+0000",
            "id": 3
        },
        {
            "author": "Mark Miller",
            "body": "Perhaps this should be combined with high freq terms tool ... could make a ton of this little guys, so prob best to consolidate them.",
            "date": "2010-04-15T00:06:48.963+0000",
            "id": 4
        },
        {
            "author": "Tom Burton-West",
            "body": "New patch includes a (pre-flex ) version of HighFreqTerms that finds the top N terms with the highest docFreq and looks up the total term frequency and outputs the list of terms sorted by highest term frequency (which approximates the largest entries in the *prx files).    I'm not sure how to combine the GetTermInfo program, with either version of HighFreqTerms  in a way that leads to sane command line arguments and argument processing.   I suppose that HighFreqTerms could have a flag that turns on or off the inclusion of total term frequency.",
            "date": "2010-04-15T18:11:55.566+0000",
            "id": 5
        },
        {
            "author": "Tom Burton-West",
            "body": "Updated the HighFreqTermsWithTF to use flex API. \n\n I don't understand the flex API well enough yet to determine if I should have used DocsEnum.read/DocsEnum.getBulkResult()  to do a bulk read instead of DocsEnum.nextDoc() and DocsEnum.freq()..",
            "date": "2010-04-16T19:13:59.571+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good Tom -- thanks for cutting over to flex.  You could in fact use the bulk read API here; it'd be faster.  But performance isn't a big deal here :)\n\nMaybe you should require a field instead of defaulting to \"ocr\"?\n\nWhy does GetTermInfo.getTermInfo take a String[] fields (it's not used I think)?\n\nProbably we should cutover to BytesRef here too, eg TermInfoWithTotalTF?\n\nMaybe you could share the code between HighFreqTermsWithTF.getTermFreqOrdered & GetTermInfo.getTermInfo?  (They both loop, summing up the .freq() of each doc to get the total term freq).\n\nSmall typo in javadoc thier -> their.\n\n",
            "date": "2010-04-17T15:58:43.314+0000",
            "id": 7
        },
        {
            "author": "Tom Burton-West",
            "body": "Revised patch updated everything  to flex.  Replaces all references to Term with BytesRef and field.  \nGetTermInfo now requires a field instead of default= ocr\nremoved unused String[] fields argument\nGetTermInfo now uses shared code HighFreqTermsWithTF.getTotalTF(); to get total tf.\nRemoved GetTermInfo dependency on TermInfoWithTotalTF[] and inlined it into HighFreqTermsWithTF.\n\nStill don't understand the bulk read API, but given that I have indexes with *frq files of 60GB I'd like to use it.  Is there some documentation, code, or a test case I might look at ?\n\n",
            "date": "2010-04-20T18:19:52.775+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Still don't understand the bulk read API, but given that I have indexes with *frq files of 60GB I'd like to use it. Is there some documentation, code, or a test case I might look at ?\n\nI just committed some small improvements to the javdadocs for this -- can you look & see if it's understandable now?\n\nAlso, have a look at oal.search.TermScorer -- it consumes the bulk API.",
            "date": "2010-04-22T13:18:04.208+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks for the updated patch Tom... feedback:\n\n  * Maybe do away with the \"hack to allow tokens with whitespace\"?\n    One should use quotes with their shell for this?  (And eg the hack\n    doens't work with tokens that have 2 spaces).\n\n  * Can you rename things like total_tf --> totalTF (consistent w/\n    Lucene's standard code style)\n\n  * Maybe rename TermInfoWithTotalTF -> TermStats?  (It also has\n    .docFreq)\n\n  * Maybe rename TermInfoWithTotalTF.termFreq -> .totalTermFreq?\n\n  * Maybe rename .getTermFreqOrdered -> .sortByTotalTermFreq?\n\n  * You don't really need a priority queue to for the\n    getTermFreqOrdered case?  Ie, instead, just fill in the\n    .totalTermFreq and then do a normal sort (make a\n    Comparator<TermStats> that sorts by the .totalTermFreq)\n",
            "date": "2010-04-22T13:40:53.902+0000",
            "id": 10
        },
        {
            "author": "Tom Burton-West",
            "body": "Added unit tests. Made changes outlined by Mike.  Still working on BulkRead.",
            "date": "2010-04-27T20:49:13.351+0000",
            "id": 11
        },
        {
            "author": "Tom Burton-West",
            "body": "Patch that includes unit tests and changes outlined in Mike's comment",
            "date": "2010-04-27T21:05:13.819+0000",
            "id": 12
        },
        {
            "author": "Tom Burton-West",
            "body": "Updated to use BulkResult api.  ",
            "date": "2010-04-30T22:18:02.184+0000",
            "id": 13
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good Tom!\n\nI cleaned things up a bit -- eg, you don't need to use the class members when interacting w/ the bulk DocsEnum API.\n\nI think it's ready to go in!",
            "date": "2010-05-04T17:12:11.800+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "I think we should just replace the current HighFreqTerms with the HighFreqTermsWithTF?",
            "date": "2010-05-10T18:42:02.073+0000",
            "id": 15
        },
        {
            "author": "Tom Burton-West",
            "body": "Hi Mike,\n\nThanks for all your help. \n\nIf we replace the current HighFreqTerms with the HighFreqTermsWithTF should there be a command line switch so that you could ask for the default behavior of the current HighFreqTerms?  Or perhaps the default should be the current behavior and the switch should turn on the additional step of gathering and reporting on the totalTF for the terms.   \n\nI haven't bench-marked it but I'm wondering if  getting the totalTF  could  take a significant  additional amount of time for large indexes.  When I ask for the top 10,000 terms using HighFreqTermsWithTF for our 500,000 document indexes it takes about 40 minutes to an hour.  I'm guessing that most of that time is taken in the first step of getting the top 10,000 terms by docFreq, but still it seems that reading the data and calculating the totalTF for 10,000 terms might be a significant enough fraction of the total time that the option to skip that step might be useful.\n\n\nTom\n\n\n",
            "date": "2010-05-10T19:52:32.414+0000",
            "id": 16
        },
        {
            "author": "Michael McCandless",
            "body": "Tom, I agree, we should make it optional to compute the totalTF, and probably default it to off?  Can you tweak the latest patch to do this?",
            "date": "2010-05-10T20:07:40.532+0000",
            "id": 17
        },
        {
            "author": "Tom Burton-West",
            "body": "I tweaked the latest patch to mimic the current HighFreqTerms unless you give it a -t argument.  However, while testing the argument parsing I found a bug I suspect I inserted into the patch a few versions ago.   Am in the process of writing a unit test to exercise the bug and then will fix bug and post both tests and code.",
            "date": "2010-05-12T16:08:22.081+0000",
            "id": 18
        },
        {
            "author": "Tom Burton-West",
            "body": "Rewrote argument processing so the default behavior is that of HighFreqTerms.  The field and number of terms are now both optional with the default being all fields and 100 terms (same default as currrent HighFreqTerms).  If a -t flag is used the totalTermFreq stats will be read,calculated, and displayed. \n\nThe bug surfaced when not specifying a field.  Added test data with multiple fields and tests to check that correct results are returned with and without a field being specified.  Fixed bug and new tests pass.\n\nWith the increasing number of options, I started thinking about more robust command line argument processing.  I'm used to languages where there is a commonly used Getopt(s)  library.  There appear to be several for Java with different features, different levels of active development and different licenses. Is it worth the overhead of using one, and if so which one would be the best to use?\n\nTom\n",
            "date": "2010-05-12T22:16:01.615+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good Tom!  I'll re-merge my small changes from the prior patch, add a CHANGES, and commit.\n\nI don't think we need to upgrade to CL processing lib...",
            "date": "2010-05-13T16:12:21.399+0000",
            "id": 20
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Tom!",
            "date": "2010-05-13T17:04:37.532+0000",
            "id": 21
        },
        {
            "author": "Tom Burton-West",
            "body": "Since many people will want to use branch 3.x instead of trunk, I back-ported the flex version to 3x ( patched against http://svn.apache.org/repos/asf/lucene/dev/branches/branch_3x/lucene : 955141)\nMike, can this be committed to branch_3x?\n\nTom",
            "date": "2010-06-17T17:35:33.602+0000",
            "id": 22
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Tom!\n\nReopening for backport to 3x....",
            "date": "2010-06-18T09:30:14.199+0000",
            "id": 23
        },
        {
            "author": "Michael McCandless",
            "body": "New patch, just cleans up a few minor things...",
            "date": "2010-06-18T09:31:21.554+0000",
            "id": 24
        },
        {
            "author": "Grant Ingersoll",
            "body": "Bulk close for 3.1",
            "date": "2011-03-30T15:49:56.712+0000",
            "id": 25
        }
    ],
    "component": "modules/other",
    "description": "This is a pair of command line utilities that provide information on the total number of occurrences of a term in a Lucene index.  The first  takes a field name, term, and index directory and outputs the document frequency for the term and the total number of occurrences of the term in the index (i.e. the sum of the tf of the term for each document).   The second reads the index to determine the top N most frequent terms (by document frequency) and then outputs a list of those terms along with  the document frequency and the total number of occurrences of the term. Both utilities are useful for estimating the size of the term's entry in the *prx files and consequent Disk I/O demands. ",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2393",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Trivial",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Utility to output total term frequency and df from a lucene index",
    "systemSpecification": true,
    "version": ""
}