{
    "comments": [
        {
            "author": "Eks Dev",
            "body": "This is indeed a problem. Recently we moved to solr on tomcat and we hit it, slightly different form. \n\nThe nature of the problem is in high thread churn on tomcat, and when combined with expensive analyzers it wracks gc() havoc (*even without stale ClosableThreadLocals from this issue*). We are attacking this problem currently by reducing maxThreads and increasing minSpareThreads (also reducing time to forced thread renew). The goal is to increase life-time of threads, and to contain them to reasonable limits. I would appreciate any tips into this direction.\n\nThe problem with this strategy is if some cheep requests, not really related to your search saturate smallish thread pool... I am looking for a way to define separate thread pools for search/update requests and one for the rest as it does not make sense to have 100 search threads searching lucene on dual core box. Not really experienced with tomcat... \n\nOf course, keeping Analyzer creation cheep helps(e.g. make expensive, background structures thread-safe that can be shared and only thin analyzer using them). But this is not always easy.\n\nJust sharing experience here, maybe someone finds it helpful. Hints always welcome :)\n\n    \n ",
            "date": "2012-03-03T16:31:10.280+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "I think it should be safe to use a WeakHashMap for the hardRefs instead of HashMap?\n\nThis way, if a thread has finished and its Thread object is otherwise GCable, the entries in hardRefs should be cleared... though, it's not clear to me precisely when they will be cleared.  If it's only on future access to the WeakHashMap (get or set), which seems likely because I think WeakHashMap uses a WeakReference for the keys and therefore won't really remove an entry util it's later \"touched\", then again only on set will the object be cleared and we haven't really improved the situation.\n\nMatthew, did you try that change, and, did it improve the scenario above?\n\nFailing that, I think we have to purge it get... maybe we can amortize it (every Nth get, where N is a factor of how many entries are in the map...).\n\nAlso: I don't think PagedBytes should use CloseableThreadLocal... I think it should just new byte[].\n\nSeparately: maybe SnowballAnalyzer is too heavy...?  Does it have some static data that ought to be loaded once and shared across analyzers... but isn't today?",
            "date": "2012-03-11T14:00:37.326+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nSeparately: maybe SnowballAnalyzer is too heavy...? Does it have some static data that ought to be loaded once and shared across analyzers... but isn't today?\n{quote}\n\nI think the analyzers are going to be heavy.\n\nIf we start going down the path of trying to speed up their instantiation time, then I vote to remove reusable tokenstreams completely.\n\nThat is: i don't think we should suffer the 'worst of both worlds'. either we go thru all the effort to make things reusable, or we dont\nand instead worry about instantiation time, etc.\n",
            "date": "2012-03-11T14:24:12.432+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "Patch.\n\nI cutover to WeakHashMap for the hard refs, and now purge periodically\nfrom both set and get.  The purge frequency is 20X the number of\nthreads in the map, so that the amortized cost remains linear.\n\nI also stopped using CTL in PagedBytes, and added some additional\ntests for it.\n\n",
            "date": "2012-03-11T17:17:27.441+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nI think the analyzers are going to be heavy.\n\nIf we start going down the path of trying to speed up their instantiation time, then I vote to remove reusable tokenstreams completely.\n\nThat is: i don't think we should suffer the 'worst of both worlds'. either we go thru all the effort to make things reusable, or we dont\n and instead worry about instantiation time, etc.\n{quote}\n\nSorry, you're right: it's fine for the Analyzer to be heavy/slow to\ninit/etc., as long as the TokenStreams then share static state.\n\nSo, what I meant is: are the TokenStreams returned from\nSnowballAnalyzer somehow not sharing static stuff...?  Or, why would\nthey be so heavy...?\n\n",
            "date": "2012-03-11T17:21:55.802+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Matthew!",
            "date": "2012-03-14T15:43:40.034+0000",
            "id": 5
        }
    ],
    "component": "core/other",
    "description": "We tracked down a large memory leak (effectively a leak anyway) caused\nby how Analyzer users CloseableThreadLocal.\nCloseableThreadLocal.hardRefs holds references to Thread objects as\nkeys.  The problem is that it only frees these references in the set()\nmethod, and SnowballAnalyzer will only call set() when it is used by a\nNEW thread.\n\nThe problem scenario is as follows:\n\nThe server experiences a spike in usage (say by robots or whatever)\nand many threads are created and referenced by\nCloseableThreadLocal.hardRefs.  The server quiesces and lets many of\nthese threads expire normally.  Now we have a smaller, but adequate\nthread pool.  So CloseableThreadLocal.set() may not be called by\nSnowBallAnalyzer (via Analyzer) for a _long_ time.  The purge code is\nnever called, and these threads along with their thread local storage\n(lucene related or not) is never cleaned up.\n\nI think calling the purge code in both get() and set() would have\navoided this problem, but is potentially expensive.  Perhaps using \nWeakHashMap instead of HashMap may also have helped.  WeakHashMap \npurges on get() and set().  So this might be an efficient way to\nclean up threads in get(), while set() might do the more expensive\nMap.keySet() iteration.\n\nOur current work around is to not share SnowBallAnalyzer instances\namong HTTP searcher threads.  We open and close one on every request.\n\nThanks,\nMatt",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-3841",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "CloseableThreadLocal does not work well with Tomcat thread pooling",
    "systemSpecification": true,
    "version": "3.5"
}