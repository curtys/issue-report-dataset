{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "Patch; tests pass.\n\nI had to fix up Edge/NGramTokenizers to work w/ spoon feeding, but otherwise no analyzers seem to be failing, at least on one run...\n\nI had to do some sneaky things with MockTokenizer to work around its state machine...",
            "date": "2012-03-20T21:48:39.541+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "Fixed a few things...",
            "date": "2012-03-20T22:02:47.610+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "+1 Mike, here's an updated patch... the random test for icutokenizer now passes (spoonfeeding caught a bug).\n\nBut, now testHugeDoc fails... (not a random test).",
            "date": "2012-03-20T22:28:51.622+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "I think that new read method needs to use the incoming offset (ie, pass location + offset, not location, as 2nd arg to input.read)?  Does testHugeDoc then pass?",
            "date": "2012-03-20T22:33:08.093+0000",
            "id": 3
        },
        {
            "author": "Robert Muir",
            "body": "Thats it! But this 'new read method' is not really new, its from commons-io! we should open a bug over there...",
            "date": "2012-03-20T22:39:17.492+0000",
            "id": 4
        },
        {
            "author": "Robert Muir",
            "body": "I opened IO-311 for the missing offset bug.",
            "date": "2012-03-20T23:19:59.898+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Rob!",
            "date": "2012-03-20T23:34:30.225+0000",
            "id": 6
        },
        {
            "author": "Robert Muir",
            "body": "I think we have bugs in some tokenizers. Its currently very hard to reproduce and we get no random seed :(\n\nI think the issue is the maxWordLength=20. This is not long enough to catch bugs in tokenizers I think,\nwe should exceed whatever buffersize they use for example.\n\nSo I think we need to refactor this logic so that the multithreaded tests take maxWordLength, and ensure\nthis parameter is always respected.\n\nThis way, tests for things like tokenizers can bump this up to things like CharTokenizer.IO_BUFFER_SIZE*2\nor whatever makes sense to them, to ensure we really test them well.\n\nI don't like the fact that only my stupid trivial test (testHugeDoc) found the IO-311 bug, what if we\ndidn't have that silly test? \n\nI'll add a patch.",
            "date": "2012-03-21T00:51:48.782+0000",
            "id": 7
        },
        {
            "author": "Robert Muir",
            "body": "patch for the maxWordLength issue.\n\nThis also makes the single-threaded version that the multi-threaded versions call private, so that its not accidentally used (losing test coverage).\n\nNow we can beef up tokenizer tests to test longer strings, for stemmers and filters i think 20 is probably fine though.",
            "date": "2012-03-21T00:54:48.778+0000",
            "id": 8
        }
    ],
    "component": "",
    "description": "Throw an exception from the Reader while tokenizing, stop after not consuming all tokens, sometimes spoon-feed chars from the reader...",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-3894",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Make BaseTokenStreamTestCase a bit more evil",
    "systemSpecification": true,
    "version": ""
}