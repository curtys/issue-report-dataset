{
    "comments": [
        {
            "author": "Koji Sekiguchi",
            "body": "a patch attached.",
            "date": "2008-11-23T16:48:21.940+0000",
            "id": 0
        },
        {
            "author": "Koji Sekiguchi",
            "body": "renamed correctPosition() to correct() because it is for correcting token offset, not for token position.",
            "date": "2009-03-19T11:34:30.963+0000",
            "id": 1
        },
        {
            "author": "Mark Miller",
            "body": "Anyone want to step up for this one or should we push it off to 3.0?",
            "date": "2009-06-11T02:20:38.823+0000",
            "id": 2
        },
        {
            "author": "Robert Muir",
            "body": "just as an alternative, i have a different mechanism as part of lucene-1488 patch I am working on. But maybe its good to have options, as it depends on the ICU library.\n\nbelow is excerpt from javadoc.\n\nA TokenFilter that transforms text with ICU.\n\nICU provides text-transformation functionality via its Transliteration API.\nAlthough script conversion is its most common use, a transliterator can actually perform a more general class of tasks. \n...\nSome useful transformations for search are built-in:\n* Conversion from Traditional to Simplified Chinese characters\n* Conversion from Hiragana to Katakana\n* Conversion from Fullwidth to Halfwidth forms.\n...\nExample usage:\n * stream = new ICUTransformFilter(stream, Transliterator.getInstance(\"Traditional-Simplified\"));\n",
            "date": "2009-06-11T03:49:06.795+0000",
            "id": 3
        },
        {
            "author": "Koji Sekiguchi",
            "body": "If I can vote for it, I want it to be part of 2.9. I know several (at least five) companies use this feature in production. We use it via Solr (SOLR-822), but we hope it to be part of Lucene core.",
            "date": "2009-06-11T16:19:25.325+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "I'd like to get this in for 2.9.",
            "date": "2009-06-11T18:32:38.480+0000",
            "id": 5
        },
        {
            "author": "Koji Sekiguchi",
            "body": "updated patch attached.\n- sync trunk (smart chinese analyzer(LUCENE-1629), etc.)\n- added a useful idiom to get CharStream and make private CharReader constructor",
            "date": "2009-06-19T00:43:04.303+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks for the update Koji!\n\nThe patch looks good.  Some questions:\n\n  * Can you add a CHANGES entry describing this new feature, as well\n    as the change in type of Tokenizer.input?\n\n  * Can we rename NormalizeMap -> NormalizeCharMap?\n\n  * Could you add some javadocs to NormalizeCharMap,\n    MappingCharFilter, BaseCharFilter?\n\n  * The BaseCharFilter correct method looks spookily costly (has a for\n    loop, going backwards for all added mappings).  It seems like in\n    practice it should not be costly, because typically one corrects\n    the offset only for the \"current\" token?  And, one could always\n    build their own CharFilter (eg using arrays of ints or something)\n    if they needed a more efficient mapping.\n\n  * MappingCharFilter's match method is recursive.  But I think the\n    depth of that recursion equals the length of character sequence\n    that's being mapped, right?  So risk of stack overlflow should be\n    basically zero, unless someone does some insanely long character\n    string mappings?\n\n\nI have some back-compat concerns. First, I see these 2 failures in\n\"ant test-tag\":\n\n{code}\n[junit] Testcase: testExclusiveLowerNull(org.apache.lucene.search.TestRangeQuery):\tCaused an ERROR\n[junit] input\n[junit] java.lang.NoSuchFieldError: input\n[junit] \tat org.apache.lucene.search.TestRangeQuery$SingleCharAnalyzer$SingleCharTokenizer.incrementToken(TestRangeQuery.java:247)\n[junit] \tat org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:160)\n[junit] \tat org.apache.lucene.index.DocFieldConsumersPerField.processFields(DocFieldConsumersPerField.java:36)\n[junit] \tat org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:234)\n[junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:773)\n[junit] \tat org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:751)\n[junit] \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2354)\n[junit] \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2328)\n[junit] \tat org.apache.lucene.search.TestRangeQuery.insertDoc(TestRangeQuery.java:306)\n[junit] \tat org.apache.lucene.search.TestRangeQuery.initializeIndex(TestRangeQuery.java:289)\n[junit] \tat org.apache.lucene.search.TestRangeQuery.testExclusiveLowerNull(TestRangeQuery.java:317)\n[junit] \n[junit] \n[junit] Testcase: testInclusiveLowerNull(org.apache.lucene.search.TestRangeQuery):\tCaused an ERROR\n[junit] input\n[junit] java.lang.NoSuchFieldError: input\n[junit] \tat org.apache.lucene.search.TestRangeQuery$SingleCharAnalyzer$SingleCharTokenizer.incrementToken(TestRangeQuery.java:247)\n[junit] \tat org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:160)\n[junit] \tat org.apache.lucene.index.DocFieldConsumersPerField.processFields(DocFieldConsumersPerField.java:36)\n[junit] \tat org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:234)\n[junit] \tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:773)\n[junit] \tat org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:751)\n[junit] \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2354)\n[junit] \tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:2328)\n[junit] \tat org.apache.lucene.search.TestRangeQuery.insertDoc(TestRangeQuery.java:306)\n[junit] \tat org.apache.lucene.search.TestRangeQuery.initializeIndex(TestRangeQuery.java:289)\n[junit] \tat org.apache.lucene.search.TestRangeQuery.testInclusiveLowerNull(TestRangeQuery.java:351)\n{code}\n\nThese are JAR drop-inability failures, because the type of\nTokenizer.input changed from Reader to CharStream.  Since CharStream\nsubclasses Reader, references to Tokenizer.input would be fixed w/ a\nsimple recompile.\n\nHowever, assignments to \"input\" by external subclasses of Tokenizer\nwill result in compilation error.  You have to replace such\nassignments with {{this.input = CharReader.get(input)}}.  Since input\nis protected, any subclass can up and assign to it.  The good news is\nthis'd be a catastrophic compilation error (vs something silent at\nruntime); the bad news is that's [unfortunately] against our\nback-compat policies.\n\nAny ideas how we can fix this to \"migrate\" to CharStream without\nbreaking back compat?\n",
            "date": "2009-06-19T12:25:17.800+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "I think we should make an exception to back-compat here, and simply\nchange TokenStream.input from Reader to CharStream (subclasses\nReader).  Properly respecting back-compat will be alot of work, and,\nif external subclasses are directly assigning to input, they really\nought to be using reaset(Reader) instead.\n\nI updated the patch with the above issues, fixed some whitespace\nissues, added Tokenizer.reset(CharStream) and patched back-compat.\n",
            "date": "2009-06-21T00:15:03.349+0000",
            "id": 8
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Oops. Thanks for the updated patch, Mike!\n{quote}\n    *  Can you add a CHANGES entry describing this new feature, as well\n      as the change in type of Tokenizer.input?\n    * Can we rename NormalizeMap -> NormalizeCharMap?\n    * Could you add some javadocs to NormalizeCharMap,\n      MappingCharFilter, BaseCharFilter?\n{quote}\nYour patch looks good!\n{quote}\n    * The BaseCharFilter correct method looks spookily costly (has a for\n      loop, going backwards for all added mappings). It seems like in\n      practice it should not be costly, because typically one corrects\n      the offset only for the \"current\" token? And, one could always\n      build their own CharFilter (eg using arrays of ints or something)\n      if they needed a more efficient mapping.\n{quote}\nYes, users can create their own CharFilter if they needed a more efficient mapping.\n{quote}\n    * MappingCharFilter's match method is recursive. But I think the\n      depth of that recursion equals the length of character sequence\n      that's being mapped, right? So risk of stack overlflow should be\n      basically zero, unless someone does some insanely long character\n      string mappings?\n{quote}\nYou are correct.\n\n{quote}\nI think we should make an exception to back-compat here, and simply\nchange TokenStream.input from Reader to CharStream (subclasses\nReader). Properly respecting back-compat will be alot of work, and,\nif external subclasses are directly assigning to input, they really\nought to be using reaset(Reader) instead. \n{quote}\nI agree with you, Mike.",
            "date": "2009-06-21T00:42:24.911+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "OK thanks Koji.  I'll add a bit more to the javadocs of BaseCharFilter about the performance caveats.\n\nI plan to commit in a day or too.\n\nThanks for persisting Koji!\n\nSolr has already committed CharFilter, and had to workaround it not being in Lucene with classes like CharStreamAwareTokenizer, etc.  Koji, are you planning to work out a patch for Solr to switch to Lucene's impl?",
            "date": "2009-06-21T09:51:41.756+0000",
            "id": 10
        },
        {
            "author": "Koji Sekiguchi",
            "body": "bq. Solr has already committed CharFilter, and had to workaround it not being in Lucene with classes like CharStreamAwareTokenizer, etc. Koji, are you planning to work out a patch for Solr to switch to Lucene's impl?\n\nYeah, why not! :)",
            "date": "2009-06-21T22:48:49.036+0000",
            "id": 11
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Added TestMappingCharFilter test case (copied from Solr).",
            "date": "2009-06-22T15:12:20.271+0000",
            "id": 12
        },
        {
            "author": "Michael McCandless",
            "body": "OK I just committed this.  Thanks Koji!  Can you open a Solr issue & work out a patch so Solr can cutover to this?  Thanks.",
            "date": "2009-06-23T19:16:41.040+0000",
            "id": 13
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Thank you Mike for committing this! I'll open a ticket for Solr soon. BTW, I cannot see TestMappingCharFilter that is in the latest patch I attached. Is there a problem in the test or just slipped over?",
            "date": "2009-06-23T22:40:30.571+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "Woops, sorry, I did indeed see your new patch and applied it but then failed to svn add.  OK I just committed them.  Thanks!",
            "date": "2009-06-24T00:12:40.586+0000",
            "id": 15
        },
        {
            "author": "Koji Sekiguchi",
            "body": "an additional test for CharFilter that I forgot to move from Solr... Mike, can you commit this? Thank you. :)",
            "date": "2009-06-24T02:06:39.820+0000",
            "id": 16
        },
        {
            "author": "Michael McCandless",
            "body": "bq. an additional test for CharFilter that I forgot to move from Solr... Mike, can you commit this? Thank you.\n\nDone... thanks!",
            "date": "2009-06-24T08:55:09.747+0000",
            "id": 17
        }
    ],
    "component": "modules/analysis",
    "description": "This proposes to import CharFilter that has been introduced in Solr 1.4.\n\nPlease see for the details:\n- SOLR-822\n- http://www.nabble.com/Proposal-for-introducing-CharFilter-to20327007.html",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1466",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "CharFilter - normalize characters before tokenizer",
    "systemSpecification": true,
    "version": "2.4"
}