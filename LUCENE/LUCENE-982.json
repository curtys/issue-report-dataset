{
    "comments": [
        {
            "author": "Yonik Seeley",
            "body": "+1\n sounds like a great idea.",
            "date": "2007-08-16T15:57:15.536+0000",
            "id": 0
        },
        {
            "author": "Mike Klaas",
            "body": "One heuristic that has been quite useful for us is to skip optimizing segments that occupy some fixed fraction of the index.  The remainder of the segments are optimized as usual (the heuristic can be applied recursively).  70% is a decent number.",
            "date": "2007-08-21T21:04:05.486+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "Attached patch to implement new optimize(int maxNumSegments).\n\nI fixed LogMergePolicy to respect the maxNumSegments arg.  When\noptimizing, we first concurrently do every mergeFactor section of\nsegment merges (going back from the tail).\n\nThen, for the final partial (< mergeFactor segments) merge, we pick\ncontiguous segments that are the smallest net size, as long as the\nresulting merged segment is not > 2X larger than the segment to its\nleft (to prevent creating a lopsided index over time).\n",
            "date": "2007-11-19T11:45:28.913+0000",
            "id": 2
        },
        {
            "author": "Michael Busch",
            "body": "Mike,\n\nI looked at your patch and I'm wondering if it wouldn't make more\nsense to limit the overall size of the segments (MB and/or num docs)\ninvolved in a merge rather than the number of segments?",
            "date": "2007-11-21T18:55:58.952+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nI looked at your patch and I'm wondering if it wouldn't make more\nsense to limit the overall size of the segments (MB and/or num docs)\ninvolved in a merge rather than the number of segments?\n{quote}\n\nThanks for reviewing :)\n\nI think that's a good idea!\n\nBut I'm torn on which is \"better\" as a first step.\n\nIf we limit by size, then the benefit is, even as your index grows\nvery large, the cost of optimizing is constant once you hit the max\nsegment size.  You keep your optimize cost down.\n\nBut, then, your searches will get slower and slower as your index\ngrows since these large segments never get merged (actually you'd have\nto set maxMergeDocs as well so normal merging wouldn't merge them).\n\nBut limiting by segment count, I think you keep you search costs\nlower, at the expense of higher and higher optimize costs as your\nindex gets larger.\n\nI think people optimize because they want to pay a high cost, once,\nnow, in order to have fast[er] searches.  So by limiting segment count\nduring optimizing, we still leave the increasing cost (as your index\ngrows) on the optimize() call.\n\nI think we should eventually do both?\n\nThe good news is with the ability to customize MergePolicy, anyone can\ncustomize what it means to \"optimize\" an index just by implementing\ntheir own MergePolicy.\n",
            "date": "2007-11-21T20:47:59.490+0000",
            "id": 4
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI think people optimize because they want to pay a high cost, once,\nnow, in order to have fast[er] searches. So by limiting segment count\nduring optimizing, we still leave the increasing cost (as your index\ngrows) on the optimize() call.\n{quote}\nYeah good point. I understand the usecase for maxNumSegments.\n\n{quote}\nI think we should eventually do both?\n{quote}\n+1",
            "date": "2007-11-22T07:56:52.866+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "OK I plan to commit this in a day or two.",
            "date": "2007-11-26T20:44:04.466+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "I just committed this.",
            "date": "2007-11-30T10:10:02.664+0000",
            "id": 7
        }
    ],
    "component": "",
    "description": "Spinning this out from the discussion in LUCENE-847.\n\nI think having a way to \"slightly optimize\" your index would be useful\nfor many applications.\n\nThe current optimize() call is very expensive for large indices\nbecause it always optimizes fully down to 1 segment.  If we add a new\nmethod which instead is allowed to stop optimizing once it has <=\nmaxNumSegments segments in the index, this would allow applications to\neg optimize down to say <= 10 segments after doing a bunch of updates.\nThis should be a nice compromise of gaining good speedups of searching\nwhile not spending the full (and typically very high) cost of\noptimizing down to a single segment.\n\nSince LUCENE-847 is now formalizing an API for decoupling merge policy\nfrom IndexWriter, if we want to add this new optimize method we need\nto take it into account in LUCENE-847.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-982",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Create new method optimize(int maxNumSegments) in IndexWriter",
    "systemSpecification": true,
    "version": "2.3"
}