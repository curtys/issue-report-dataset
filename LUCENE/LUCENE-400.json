{
    "comments": [
        {
            "author": "Sebastian Kirsch",
            "body": "Created an attachment (id=15504)\nNGramFilter\n",
            "date": "2005-06-22T06:09:52.000+0000",
            "id": 0
        },
        {
            "author": "Sebastian Kirsch",
            "body": "Created an attachment (id=15505)\nNGramAnalyzerWrapper (wraps an NGramFilter around an analyzer.)\n",
            "date": "2005-06-22T06:10:29.000+0000",
            "id": 1
        },
        {
            "author": "Sebastian Kirsch",
            "body": "Created an attachment (id=15506)\nJUnit TestCase for NGramFilter\n",
            "date": "2005-06-22T06:12:35.000+0000",
            "id": 2
        },
        {
            "author": "Robert Newson",
            "body": " * <p>For example, the sentence \"please divide this sentence into ngrams\" would be\n * tokenized into the tokens \"please divide\", \"this sentence\", \"sentence into\", and\n *  \"into ngrams\".\n\nThe comment should read;\n\n * <p>For example, the sentence \"please divide this sentence into ngrams\" would be\n * tokenized into the tokens \"please divide\", \"divide this\", \"this sentence\",\n\"sentence into\", and\n *  \"into ngrams\".",
            "date": "2005-06-29T10:26:46.000+0000",
            "id": 3
        },
        {
            "author": "Sebastian Kirsch",
            "body": "Created an attachment (id=15818)\nJUnit test class for NGramAnalyzerWrapper\n\nThe tests in this class are concerned with the interaction between QueryParser\nand an NGramAnalyzer, and whether searching works as expected on an index\nconstructed with an NGramAnalyzer.\n\nOne of the test cases throws an exception that I haven't investigated yet. So\nproceed with caution if you use the QueryParser with NGramAnalyzer. \n\n..E....\nTime: 1.771\nThere was 1 error:\n1)\ntestNGramAnalyzerWrapperPhraseQueryParsingFails(org.apache.lucene.analysis.NGramAnalyzerWrapperTest)java.lang.NullPointerException\n\n\tat\norg.apache.lucene.index.MultipleTermPositions.skipTo(MultipleTermPositions.java:178)\n\n\tat\norg.apache.lucene.search.PhrasePositions.skipTo(PhrasePositions.java:47)\n\tat org.apache.lucene.search.PhraseScorer.doNext(PhraseScorer.java:73)\n\tat org.apache.lucene.search.PhraseScorer.next(PhraseScorer.java:66)\n\tat org.apache.lucene.search.Scorer.score(Scorer.java:47)\n\tat\norg.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:102)\n\tat org.apache.lucene.search.Hits.getMoreDocs(Hits.java:65)\n\tat org.apache.lucene.search.Hits.<init>(Hits.java:44)\n\tat org.apache.lucene.search.Searcher.search(Searcher.java:40)\n\tat org.apache.lucene.search.Searcher.search(Searcher.java:32)\n\tat\norg.apache.lucene.analysis.NGramAnalyzerWrapperTest.queryParsingTest(NGramAnalyzerWrapperTest.java:75)\n\n\tat\norg.apache.lucene.analysis.NGramAnalyzerWrapperTest.testNGramAnalyzerWrapperPhraseQueryParsingFails(NGramAnalyzerWrapperTest.java:100)\n\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\n\tat\norg.apache.lucene.analysis.NGramAnalyzerWrapperTest.main(NGramAnalyzerWrapperTest.java:36)\n\n\nFAILURES!!!\nTests run: 6,  Failures: 0,  Errors: 1\n",
            "date": "2005-07-29T21:56:59.000+0000",
            "id": 4
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Sebastian, ever figured out the problem?  Also, is there a way to get rid of the Commons Collections?  Lucene has no run-time dependencies on other libraries.",
            "date": "2006-07-09T21:51:29.000+0000",
            "id": 5
        },
        {
            "author": "Sebastian Kirsch",
            "body": "Hi Otis,\n\nI did not figure out the problem. Getting rid of Commons Collection should be no problem; I am just using them as FIFOs. However, I do not have the time at the moment to implement this.\n\nKind regards, Sebastian",
            "date": "2006-08-07T21:14:35.000+0000",
            "id": 6
        },
        {
            "author": "Grant Ingersoll",
            "body": "Lucene has NGram support",
            "date": "2008-01-12T22:56:43.893+0000",
            "id": 7
        },
        {
            "author": "Steve Rowe",
            "body": "Lucene has *character* NGram support, but not *word* NGram support, which this filter supplies:\n\nbq. This filter constructs n-grams (token combinations up to a fixed size, sometimes called \"shingles\") from a token stream.",
            "date": "2008-01-13T05:36:33.958+0000",
            "id": 8
        },
        {
            "author": "Grant Ingersoll",
            "body": "Good catch, Steve.  I will reopen, as a word based ngram filter is useful.",
            "date": "2008-01-13T13:33:59.698+0000",
            "id": 9
        },
        {
            "author": "Steve Rowe",
            "body": "Repackaged these four files as a patch, with the following modifications to the code:\n\n* Renamed files and variables to refer to \"n-grams\" as \"shingles\", to avoid confusion with the character-level n-gram code already in Lucene's sandbox\n* Placed code in the o.a.l.analysis.shingle package\n* Converted commons-collections FIFO usages to LinkedLists\n* Removed @author from javadocs\n* Changed deprecated Lucene API usages to alternate forms; addressed all compilation warnings\n* Changed code style to conform to Lucene conventions\n* Changed field setters to return null instead of a reference to the class instance, then changed instantiations to use individual setter calls instead of the chained calling style\n* Added ASF license to each file\n\nAll tests pass.\n\nAlthough I left in the ShingleAnalyzerWrapper and its test in the patch, no other Lucene filter (AFAICT) has such a filter wrapping facility.  My vote is to remove these two files.",
            "date": "2008-01-14T04:15:11.735+0000",
            "id": 10
        },
        {
            "author": "Grant Ingersoll",
            "body": "Thanks, Steve.  I will mark this as 2.4",
            "date": "2008-01-14T12:29:02.923+0000",
            "id": 11
        },
        {
            "author": "Steve Rowe",
            "body": "Removed the duplicate link (to LUCENE-759), since that issue is about character-level n-grams, and this issue is about word-level n-grams.",
            "date": "2008-01-14T18:40:02.135+0000",
            "id": 12
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Thanks for bringing this up to date.  I'll commit it after 2.3 is out.\n",
            "date": "2008-01-14T18:55:08.782+0000",
            "id": 13
        },
        {
            "author": "Grant Ingersoll",
            "body": "ping, Otis, do you still plan to commit?",
            "date": "2008-03-03T13:28:15.156+0000",
            "id": 14
        },
        {
            "author": "Steve Rowe",
            "body": "re-ping, Otis, do you still plan to commit?",
            "date": "2008-03-18T17:20:42.525+0000",
            "id": 15
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Sorry for hogging.  Got some local compilation issues with the query builder in contrib, so assigning to Grant to get this in.",
            "date": "2008-03-25T22:39:26.551+0000",
            "id": 16
        },
        {
            "author": "Grant Ingersoll",
            "body": "Committed revision 642612.\n\nThanks Sebastian and Steve",
            "date": "2008-03-29T21:09:54.264+0000",
            "id": 17
        }
    ],
    "component": "modules/analysis",
    "description": "This filter constructs n-grams (token combinations up to a fixed size, sometimes\ncalled \"shingles\") from a token stream.\n\nThe filter sets start offsets, end offsets and position increments, so\nhighlighting and phrase queries should work.\n\nPosition increments > 1 in the input stream are replaced by filler tokens\n(tokens with termText \"_\" and endOffset - startOffset = 0) in the output\nn-grams. (Position increments > 1 in the input stream are usually caused by\nremoving some tokens, eg. stopwords, from a stream.)\n\nThe filter uses CircularFifoBuffer and UnboundedFifoBuffer from Apache\nCommons-Collections.\n\nFilter, test case and an analyzer are attached.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-400",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "NGramFilter -- construct n-grams from a TokenStream",
    "systemSpecification": false,
    "version": ""
}