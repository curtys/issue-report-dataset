{
    "comments": [
        {
            "author": "Koji Sekiguchi",
            "body": "to apply this patch, LUCENE-1448 also need to be applied.\n{code}\n$ svn co -r713975 http://svn.apache.org/repos/asf/lucene/java/trunk\n$ cd trunk\n$ patch -p0 < LUCENE-1448.patch\n$ patch -p0 < LUCENE-1522.patch\n{code}\n",
            "date": "2009-01-19T14:50:43.232+0000",
            "id": 0
        },
        {
            "author": "Koji Sekiguchi",
            "body": "The attached patch has \"colored tag highlighting\" feature. :)\n\nI provided the following colored tags:\n\n{code:title=BaseFragmentsBuilder.java}\npublic static final String[] COLORED_PRE_TAGS = {\n    \"<b style=\\\"background:yellow\\\">\", \"<b style=\\\"background:lawngreen\\\">\", \"<b style=\\\"background:aquamarine\\\">\",\n    \"<b style=\\\"background:magenta\\\">\", \"<b style=\\\"background:palegreen\\\">\", \"<b style=\\\"background:coral\\\">\",\n    \"<b style=\\\"background:wheat\\\">\", \"<b style=\\\"background:khaki\\\">\", \"<b style=\\\"background:lime\\\">\",\n    \"<b style=\\\"background:deepskyblue\\\">\"\n};\n{code}\n\nA sample picture will be attached shortly.",
            "date": "2009-02-23T17:05:04.474+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "\nThis highlighter looks very interesting!  I love the colored tags, and\nthe fast performance on large docs, and the extensive unit tests.\n\nWhen I applied the patch to current trunk, I see some tests failing,\neg:\n\n{code}\n    [junit] Testcase: test1PhraseLongMVB(org.apache.lucene.search.highlight2.FieldPhraseListTest):\tFAILED\n    [junit] expected:<sppeeeed(1.0)((8[8,93]))> but was:<sppeeeed(1.0)((8[7,92]))>\n    [junit] junit.framework.ComparisonFailure: expected:<sppeeeed(1.0)((8[8,93]))> but was:<sppeeeed(1.0)((8[7,92]))>\n    [junit] \tat org.apache.lucene.search.highlight2.FieldPhraseListTest.test1PhraseLongMVB(FieldPhraseListTest.java:175)\n{code}\n\nIs this approach guaranteed to only highlight term occurrences that\nactually contribute to the document match?  Can it handle all /\narbitrary Query subclasses?  How does it score fragments?\n\nI also like that you first generate hits in the document, and from\nthose hits you generate fragments (if I'm reading the code correctly);\nthis is a nicely scalable approach.\n",
            "date": "2009-03-12T10:28:31.864+0000",
            "id": 2
        },
        {
            "author": "Koji Sekiguchi",
            "body": "{quote}\nThis highlighter looks very interesting! I love the colored tags, and\nthe fast performance on large docs, and the extensive unit tests.\n{quote}\n\nThank you for paying attention on this issue, Mike!\n\nbq. When I applied the patch to current trunk, I see some tests failing,\n\nNote that this issue depends on LUCENE-1448, so you apply LUCENE-1448.patch first, then apply LUCENE-1522.patch.\n\n{noformat}\n# To apply LUCENE-1448.patch, check out revision 713975!!!\n$ svn co -r713975 http://svn.apache.org/repos/asf/lucene/java/trunk\n$ cd trunk\n$ patch -p0 < LUCENE-1448.patch\n$ patch -p0 < LUCENE-1522.patch\n{noformat}\n\nI'll post comment later for the rest of your questions. :)",
            "date": "2009-03-12T11:15:00.043+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Note that this issue depends on LUCENE-1448\n\nWoops, right I had skipped that step.",
            "date": "2009-03-12T12:15:06.888+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "Does this highlighter have a \"max tokens to analyze\" setting?  Or does it always visit all terms in each document?",
            "date": "2009-03-12T22:03:34.373+0000",
            "id": 5
        },
        {
            "author": "Mark Harwood",
            "body": "I'm guessing that's not an issue given it uses stored TermVectors rather than re-analyzing?\n\nAt some stage I hope to take a closer look at this contribution.  I'd be interested to see if all the Highlighter1  Junit tests could be adapted to work with Highlighter2 and get some comparative benchmarks.",
            "date": "2009-03-12T22:45:25.333+0000",
            "id": 6
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Mike, I'm sorry for late reply.\n\nbq. Is this approach guaranteed to only highlight term occurrences that actually contribute to the document match?\n\nI'm not sure if I understand what you are asking, but if you talk about \"hl.requireFieldMatch feature in Solr\", YES. highlighter2 has the feature:\n\n{code:java}\n/**\n * a constructor. A FragListBuilder and a FragmentsBuilder can be specified (plugins).\n * \n * @param phraseHighlight true of false for phrase highlighting\n * @param fieldMatch true of false for field matching\n * @param fragListBuilder an instance of FragListBuilder\n * @param fragmentsBuilder an instance of FragmentsBuilder\n */\npublic Highlighter( boolean phraseHighlight, boolean fieldMatch, FragListBuilder fragListBuilder, FragmentsBuilder fragmentsBuilder ){\n  this.phraseHighlight = phraseHighlight;\n  this.fieldMatch = fieldMatch;\n  this.fragListBuilder = fragListBuilder;\n  this.fragmentsBuilder = fragmentsBuilder;\n}\n{code}\n\nbq. Can it handle all / arbitrary Query subclasses?\n\nCurrently, no. Highlighter2 calls flatten() method to try to flat the sourceQuery in the beginning. In flatten() method, it recognizes TermQuery and PhraseQuery, and BooleanQuery that contains TermQuery and PhraseQuery:\n\n{code:title=FieldQuery.java}\nvoid flatten( Query sourceQuery, Collection<Query> flatQueries ){\n  if( sourceQuery instanceof BooleanQuery ){\n    BooleanQuery bq = (BooleanQuery)sourceQuery;\n    for( BooleanClause clause : bq.getClauses() ){\n      if( !clause.isProhibited() )\n        flatten( clause.getQuery(), flatQueries );\n    }\n  }\n  else if( sourceQuery instanceof TermQuery ){\n    if( !flatQueries.contains( sourceQuery ) )\n      flatQueries.add( sourceQuery );\n  }\n  else if( sourceQuery instanceof PhraseQuery ){\n    if( !flatQueries.contains( sourceQuery ) ){\n      PhraseQuery pq = (PhraseQuery)sourceQuery;\n      if( pq.getTerms().length > 1 )\n        flatQueries.add( pq );\n      else if( pq.getTerms().length == 1 ){\n        flatQueries.add( new TermQuery( pq.getTerms()[0] ) );\n      }\n    }\n  }\n  // else discard queries\n}\n{code}\n\nBut I'm always positive to support all / arbitrary Query subclasses in H2. :)\n\nbq. How does it score fragments?\n\nCurrently, H2 takes into account query time boost and tf in fragment. For example, if we have q=\"a OR b^3\" and two fragment candidates f1=\"a a a\" and f2=\"a b\", f1 gets 3 and f2 gets 4, getBestFragments() will return f2 first, then f1 when ScoreOrderFragmentsBuilder (default) is used.\n",
            "date": "2009-03-15T00:58:32.940+0000",
            "id": 7
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Mark,\n\nbq. I'm guessing that's not an issue given it uses stored TermVectors rather than re-analyzing?\n\nCorrect.\n\nbq. At some stage I hope to take a closer look at this contribution.\n\nVery nice!\n\nbq. I'd be interested to see if all the Highlighter1 Junit tests could be adapted to work with Highlighter2 and get some comparative benchmarks.\n\nI'm not sure all H1 test cases could be adapted to work with H2 because boundary of fragments will be different between H1 and H2, but benchmarks of performance is on my todo list.",
            "date": "2009-03-15T01:05:31.161+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I'm not sure if I understand what you are asking, but if you talk about \"hl.requireFieldMatch feature in Solr\", YES. highlighter2 has the feature:\n\nActually I was asking whether every fragment that's returned is\nguaranteed to show a match to my original query.\n\nEG if my query is a PhraseQuery, is it guaranteed that all fragments\npresented are valid matches?  If I search for \"Alan Greenspan's\nmortgage\", is it ever possible to see a fragment that contains only\n\"Alan Greenspan\"?\n\nbq. Currently, no. Highlighter2 calls flatten() method to try to flat the sourceQuery in the beginning. In flatten() method, it recognizes TermQuery and PhraseQuery, and BooleanQuery that contains TermQuery and PhraseQuery:\n\nOK so eg *SpanQuery won't work?  It seems like both highlighters take\nthis \"flatten\" approach, which can lose the constraints for\ninteresting queries (like Span, or a custom query).\n\nI think a nice [eventual] model would be if we could simply re-run the\nscorer on the single document (using InstantiatedIndex maybe, or\nsimply some sort of wrapper on the term vectors which are already a\nmini-inverted-index for a single doc), but extend the scorer API to\ntell us the exact term occurrences that participated in a match (which\nI don't think is exposed today).\n\nEG ExactPhraseScorere.phraseFreq has the logic to check term positions\nand find all positions where the phrase matches.  Right now that\nmethod throws away the specific position where each match occurred,\nbut if instead we had it call a normally no-op method\n(recordDocMatchPosition(int position, float score) or some such), we\ncould then make use of it for highlighting.\n",
            "date": "2009-03-16T10:35:57.248+0000",
            "id": 9
        },
        {
            "author": "Koji Sekiguchi",
            "body": "{quote}\nActually I was asking whether every fragment that's returned is\nguaranteed to show a match to my original query.\n\nEG if my query is a PhraseQuery, is it guaranteed that all fragments\npresented are valid matches? If I search for \"Alan Greenspan's\nmortgage\", is it ever possible to see a fragment that contains only\n\"Alan Greenspan\"?\n{quote}\n\nI see. Yes, H2 guarantees it.\n\n{quote}\nOK so eg *SpanQuery won't work? It seems like both highlighters take\nthis \"flatten\" approach, which can lose the constraints for\ninteresting queries (like Span, or a custom query).\n{quote}\n\nH2 doesn't support SpanQuery right now. I'll look at SpanScorer and LUCENE-1425 to see whether I can support \"interesting queries\" in H2, before going to \"eventual model\" (looks great) you mentioned above.\n",
            "date": "2009-03-16T16:22:18.489+0000",
            "id": 10
        },
        {
            "author": "Mark Miller",
            "body": "I don't think its easy to get a speedy highlighter that works with positions for all of the Lucene queries. In the long term, I'd love to see a fast highlighter that works with positions for all of Lucene's queries . I'd also like it to work if you don't have termvectors stored (though be faster if they are perhaps, as it is now).\n\nEssentially we have each of these pieces separately now - the difficulty is doing it with one highlighter. \n\nWe have the standard Highlighter with two modes: one that doesn't handle positions, and one that handles position sensitive highlighting for Spans and almost all of the queries. This framework is great - its customizable, it handles a lot of corner cases, it works without termvectors, it gets faster with termvectors. Unfortunately, it runs through the source stream one token at a time, and doesn't scale well. Getting hit positions for position sensitive clauses requires converting the query to a span query and calling getSpans on a memory index\n\nWe also have the termvector highlighters that can work from offsets in the query and avoid running through a token at a time. You need termvectors for this approach, and its difficult to handle positions, but it scales.\n\nThe difficulty and goal is in merging the qualities of both. ",
            "date": "2009-03-16T17:51:45.729+0000",
            "id": 11
        },
        {
            "author": "Mark Miller",
            "body": "{quote}I think a nice [eventual] model would be if we could simply re-run the\nscorer on the single document (using InstantiatedIndex maybe, or\nsimply some sort of wrapper on the term vectors which are already a\nmini-inverted-index for a single doc), but extend the scorer API to\ntell us the exact term occurrences that participated in a match (which\nI don't think is exposed today).{quote}\n\nVariations on this have been tossed around before, but this sounds like a slightly more interesting approach than whats been mentioned. Its sort of how the current highlighter handles positions, but avoids the messy step of trying to convert any query to a spanquery.\n\nNot sure it solves being able to gets offsets from the query terms and still mask for positions though - if that step can be completed, we can start by using the current SpanScorer logic with this patch, until we get the pieces into core Lucene. ",
            "date": "2009-03-16T17:59:08.168+0000",
            "id": 12
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI think a nice [eventual] model would be if we could simply re-run the\nscorer on the single document (using InstantiatedIndex maybe, or\nsimply some sort of wrapper on the term vectors which are already a\nmini-inverted-index for a single doc), but extend the scorer API to\ntell us the exact term occurrences that participated in a match (which\nI don't think is exposed today).\n{quote}\n\nBut, if you have for example a document 'a b c a b c' and the query\n'a AND b', then this approach would only highlight the first two terms,\nno?",
            "date": "2009-03-16T19:32:47.291+0000",
            "id": 13
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nI'd also like it to work if you don't have termvectors stored (though\nbe faster if they are perhaps, as it is now).\n{quote}\n\nI agree.\n\n{quote}\nGetting hit positions for position sensitive clauses requires\nconverting the query to a span query and calling getSpans on a memory\nindex \n{quote}\n\nIs the reason why H1 creates the full token stream (even when\nTermVectors is the source) in order to build the MemoryIndex?\n\nIf term vectors (w/ positions, offsets) were stored, wouldn't it be\npossible to make a simple index (or at least TermDocs, TermPositions)\nwrapped on those TermVectors? \n",
            "date": "2009-03-16T22:37:00.642+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nBut, if you have for example a document 'a b c a b c' and the query\n'a AND b', then this approach would only highlight the first two terms,\nno?\n{quote}\n\nAhh right -- in fact, nothing would be highlighted because the scorer\nfor AND queries doesn't visit positions at all (it doesn't need to).\n\nI guess we'd have to ask such scorers to forcefully visit positions &\nenumerate all matches within one doc, when running in \"highlight\"\nmode.  Hmm, feeling like a big change...\n\nBut maybe it could work.  It'd be sort of like a positional-aware\n\"explain\", ie \"show me the term occurrences that allowed the full\nquery to accept this document\".\n\nImagine query \"(a AND b) OR (c AND d)\".  When looking at the fragments\nfor each doc, I would want to see both a AND b, or both c AND d, but\nnever (for example) just a and d.\n\nBut, flattening could produce just a and d (I think?); and I think H1\ncould do the same even with SpanScorer (Mark is that true?  I don't\nfully understand the Query -> SpanQuery conversion).\n\nWhereas if we could ask for positions of the \"real\" matches I think it\nwould work correctly?\n",
            "date": "2009-03-16T22:38:11.491+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Not sure it solves being able to gets offsets from the query terms and still mask for positions though\n\nCan you explain that more?\n",
            "date": "2009-03-16T22:45:11.666+0000",
            "id": 16
        },
        {
            "author": "Mark Miller",
            "body": "{quote}Is the reason why H1 creates the full token stream (even when\nTermVectors is the source) in order to build the MemoryIndex?\n\nIf term vectors (w/ positions, offsets) were stored, wouldn't it be\npossible to make a simple index (or at least TermDocs, TermPositions)\nwrapped on those TermVectors? {quote}\n\nIt creates the full tokenstream because it was designed to work without termvectors, and so without offset info for the query terms, it rebuilds the stream and processes a token at a time - the api gives you hooks to highlight at any of these tokens - thats essentially the bottleneck I think - taking everything a token at a time, but the whole API is based on that fact. With the SpanScorer version, we can get almost any info from the MemoryIndex, but it was convenient to fit into the current highlighter API to start. I had it in my mind to break from the API and make a largedoc highlighter that didn't need termvectors, but I found the memory index and getspans to still be too slow in my initial testing. I'd hoped to work more on it, but havn't had a chance. So essentially, while more can be done with termvectors, the improvements break the current API at a pretty deep level - no one has done the work to solve that I guess - which is why we have the alternate highlighters.\n\n*edit*\n\nI suppose one of the main problems with my briefly tested large doc approach I tried is that it still requires that you rebuild the tokenstream (and I was attempting to not use termvectors either).  Avoiding the need for that would probably make it much more competitive.",
            "date": "2009-03-16T22:57:03.926+0000",
            "id": 17
        },
        {
            "author": "Marvin Humphrey",
            "body": "> It'd be sort of like a positional-aware \"explain\", ie \"show me the term\n> occurrences that allowed the full query to accept this document\".\n\nFWIW, this is more or less how the KinoSearch highlighter now works in svn\ntrunk.  It doesn't use a Scorer, though, but instead the KS analogue to\nLucene's \"Weight\" class.\n\nThe (Weight) is fed what is essentially a single doc index, using stored term\nvectors.  Weight.highlightSpans() returns an array of \"span\" objects, each of \nwhich has a start offset, a length, and a score.  The Highlighter then \nprocesses these span objects to create a \"heat map\" and choose its excerpt \npoints.\n\nThe idea is that by delegating responsibility for creating the scoring spans, we\nmake it easier to support arbitrary Query implementations with a single\nHighlighter class.",
            "date": "2009-03-16T23:25:02.385+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\n> It'd be sort of like a positional-aware \"explain\", ie \"show me the term\n> occurrences that allowed the full query to accept this document\".\n\nFWIW, this is more or less how the KinoSearch highlighter now works in svn\ntrunk. It doesn't use a Scorer, though, but instead the KS analogue to\nLucene's \"Weight\" class.\n\nThe (Weight) is fed what is essentially a single doc index, using stored term\nvectors. Weight.highlightSpans() returns an array of \"span\" objects, each of \nwhich has a start offset, a length, and a score. The Highlighter then \nprocesses these span objects to create a \"heat map\" and choose its excerpt \npoints.\n\nThe idea is that by delegating responsibility for creating the scoring spans, we\nmake it easier to support arbitrary Query implementations with a single\nHighlighter class.\n{quote}\n\nAwesome!\n\nDo you require term vectors to be stored, for highlighting (cannot\nre-analyze the text)?\n\nFor queries that normally do not use positions at all (simple AND/OR\nof terms), how does your highlightSpans() work?\n\nFor BooleanQuery, is coord factor used to favor fragment sets that\ninclude more unique terms?\n\nAre you guaranteed to always present a net set of fragments that\n\"matches\" the query? (eg the example query above).\n\nI think the base litmus test for a hightlighter is: if one were to\ntake all fragments presented for a document (call this a \"fragdoc\")\nand make a new document from it, would that document match the\noriginal query?\n\nIn fact, I think the perfect highlighter would \"logically\" work as\nfollows: take a single document and enumerate every single possible\nfragdoc.  Each fragdoc is allowed to have maxNumFragments fragments,\nwhere each fragment has a min/max number of characters.  The set of\nfragdocs is of course ridiculously immense.\n\nTake this massive collection of fragdocs and build a new temporary\nindex, then run your Query against that index.  Many of the fragdocs\nwould not match the Query, so they are eliminated right off (this is\nthe litmus test).  Then, of the ones that do, you want the highest\nscoring fragdocs.\n\nObviously you can't actually implement a highlighter like that, but I\nthink \"logically\" that is the optimal highlighter that we are trying\nto emulate with more efficient implementations.\n\nI think having the Query/Weight/Scorer class be the single-source for\nhits, explanation & highlight spans is the right approach.  Having a\nwhole separate package trying to reverse-engineer where matches had\ntaken place between Query and Document is hard to get right.  EG\nBooleanScorer2's coord factor would naturally/correctly influence the\nselection.\n\nI also think building a [reduced, just Postings] IndexReader API on top of\nTermVectors ought to be a simple way to get great performance here.\n",
            "date": "2009-03-17T09:29:43.816+0000",
            "id": 19
        },
        {
            "author": "Marvin Humphrey",
            "body": "> Do you require term vectors to be stored, for highlighting (cannot\n> re-analyze the text)?\n\nYes, but that's not fundamental to the design.  You just have to hand the\nWeight some sort of single-doc index that includes sufficient data to\ndetermine what parts of the text contributed to the hit and how much they\ncontributed.  The Weight needn't care whether that single-doc index was\ncreated on the fly or stored at index time.\n\n> For queries that normally do not use positions at all (simple AND/OR\n> of terms), how does your highlightSpans() work?\n\nANDQuery, ORQuery, and RequiredOptionalQuery just return the union of the\nspans produced by their children.\n\n> For BooleanQuery, is coord factor used to favor fragment sets that\n> include more unique terms?\n\nNo; I don't think that would be fine grained enough to help.\n\nThere's a HeatMap class that performs additional weighting.  Spans that\ncluster together tightly (i.e. that could fit together within the excerpt) are\nboosted.\n\n> Are you guaranteed to always present a net set of fragments that\n> \"matches\" the query? (eg the example query above).\n\nNo.  The KS version supplies a single fragment.  It naturally prefers\nfragments with rarer terms, because the span scores are multiplied by the\nWeight's weighting factor (which includes IDF).  \n\nOnce that fragment is selected, the KS highlighter worries a lot about\ntrimming to sensible sentence boundaries.\n\nIn my own subjective judgment, supplying a single maximally coherent fragment\nwhich prefers clusters of rare terms results in an excerpt which \"scans\" as\nquickly as possible, conveying the gist of the content with minimal \"visual\neffort\".  I used Google's excerpting as a model.\n\n> I think the base litmus test for a hightlighter is: if one were to\n> take all fragments presented for a document (call this a \"fragdoc\")\n> and make a new document from it, would that document match the\n> original query?\n\nWith out the aid of formal studies to guide us, this is a subjective call.\nFWIW, I disagree.  In my view, visual scanning speed and coherence\nare more important than completeness.  \n\nI'm not a big fan of the multi-fragment approach, because I think it takes too\nmuch effort to grok each individual entry.  Furthermore, the fact that the\nfragments don't start on sentence boundaries (whenever feasible) adds to the\nvisual effort needed to orient yourself.\n\nSearch results contain a lot of junk.  The user needs to be able to parse the\nresults page as quickly as possible and refine their search query as needed.\nNoisy excerpts, with lots of elipses and few sentences that can be \"swallowed\nwhole\" impede that.  Trees vs. Forest.\n\nAgain, that's my own aesthetic judgment, but I'll wager that there are studies\nout there showing that fragments which start at the top of a sentence are\neasier to consume, and I think that's important.\n\n> In fact, I think the perfect highlighter would \"logically\" work as\n> follows: take a single document and enumerate every single possible\n> fragdoc. \n\nKS uses a sliding window rather than chunking up the text into fragdocs of\nfixed length.\n\n> Having a whole separate package trying to reverse-engineer where matches had\n> taken place between Query and Document is hard to get right.\n\nExactly.\n\nPS: Obviously, refinements of the highlighting algo will help Lucy, too. I\ndon't suppose you want to continue this on the Lucy dev list so that Lucy\nbanks some community credit for this discussion.  :\\",
            "date": "2009-03-17T13:31:06.511+0000",
            "id": 20
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nPS: Obviously, refinements of the highlighting algo will help Lucy, too. I\ndon't suppose you want to continue this on the Lucy dev list so that Lucy\nbanks some community credit for this discussion. :\\\n{quote}\n\nWell... remember that more discussions between you and I and Nathan on\nLucy-dev (as much as I love having them) don't really \"count\" as a\n\"bigger\" community.  In other words, like the scoring of a\nBooleanQuery, there is a very strong coord factor at play when\nmeasuring \"community\".  If you and I and nathan have fewer\nconversations on Lucy-dev, but then two other new people join in, that\nis a much stronger community.\n\nSo, maybe send a note to lucy-dev, referencing this as a relevant\ndiscussion to Lucy's approach to highlighting... and leave a\ntantalizing invite here for others to make the jump to lucy-dev.\nGrowing a community is not easy!\n",
            "date": "2009-03-17T15:00:25.754+0000",
            "id": 21
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\n> Do you require term vectors to be stored, for highlighting (cannot\n> re-analyze the text)?\n\nYes, but that's not fundamental to the design. You just have to hand the\nWeight some sort of single-doc index that includes sufficient data to\ndetermine what parts of the text contributed to the hit and how much they\ncontributed. The Weight needn't care whether that single-doc index was\ncreated on the fly or stored at index time.\n{quote}\n\nOK.\n\n{quote}\n> For queries that normally do not use positions at all (simple AND/OR\n> of terms), how does your highlightSpans() work?\n\nANDQuery, ORQuery, and RequiredOptionalQuery just return the union of the\nspans produced by their children.\n{quote}\n\nHmm -- it seems like that loses information.  Ie, for ANDQuery, you\nlose the fact that you should try to include a match from each of the\nsub-clauses' spans.\n\n{quote}\n> For BooleanQuery, is coord factor used to favor fragment sets that\n> include more unique terms?\n\nNo; I don't think that would be fine grained enough to help.\n{quote}\n\nWhat I meant was: all other things being equal, do you more strongly\nfavor a fragment that has all N of the terms in a query vs another\nfragment that has fewer than N but say higher net number of\noccurrences.\n\n{quote}\nThere's a HeatMap class that performs additional weighting. Spans that\ncluster together tightly (i.e. that could fit together within the excerpt) are\nboosted.\n{quote}\n\nThat sounds great.\n\n{quote}\n> Are you guaranteed to always present a net set of fragments that\n> \"matches\" the query? (eg the example query above).\n\nNo. The KS version supplies a single fragment. It naturally prefers\nfragments with rarer terms, because the span scores are multiplied by the\nWeight's weighting factor (which includes IDF).\n{quote}\n\nHmm OK.\n\n{quote}\nOnce that fragment is selected, the KS highlighter worries a lot about\ntrimming to sensible sentence boundaries.\n{quote}\n\nI totally agree: easy/fast consumability is very important, so\nchoosing entire sentences, or at least anchoring the start or maybe\nend on a sentence boundary, is important.  Lucene's H1 doesn't do this\nootb today I think (though you could plug in your own fragmenter).\n\n{quote}\nIn my own subjective judgment, supplying a single maximally coherent fragment\nwhich prefers clusters of rare terms results in an excerpt which \"scans\" as\nquickly as possible, conveying the gist of the content with minimal \"visual\neffort\". I used Google's excerpting as a model.\n{quote}\n\nGoogle picks more than one fragment; it seems like it picks one or two\nfragments.\n\nI'm torn on whether IDF should really come into play though...\n\n{quote}\n> I think the base litmus test for a hightlighter is: if one were to\n> take all fragments presented for a document (call this a \"fragdoc\")\n> and make a new document from it, would that document match the\n> original query?\n\nWith out the aid of formal studies to guide us, this is a subjective call.\nFWIW, I disagree. In my view, visual scanning speed and coherence\nare more important than completeness.\n\nI'm not a big fan of the multi-fragment approach, because I think it takes too\nmuch effort to grok each individual entry. Furthermore, the fact that the\nfragments don't start on sentence boundaries (whenever feasible) adds to the\nvisual effort needed to orient yourself.\n\nSearch results contain a lot of junk. The user needs to be able to parse the\nresults page as quickly as possible and refine their search query as needed.\nNoisy excerpts, with lots of elipses and few sentences that can be \"swallowed\nwhole\" impede that. Trees vs. Forest.\n\nAgain, that's my own aesthetic judgment, but I'll wager that there are studies\nout there showing that fragments which start at the top of a sentence are\neasier to consume, and I think that's important.\n{quote}\n\nI agree, it's not cut and dry here; this is all quite subjective.\n\nI think one case that's tricky is two terms that do not tend do\nco-occur in proximity.  Eg search for python greenspan on Google, and\nmost of the fragdocs consist of two fragments, one for each term.  Ie\ngoogle is trying to include all the terms in the fragdoc (my \"coord\nfactor\" question above).\n\n{quote}\n> In fact, I think the perfect highlighter would \"logically\" work as\n> follows: take a single document and enumerate every single possible\n> fragdoc.\n\nKS uses a sliding window rather than chunking up the text into fragdocs of\nfixed length.\n{quote}\n\nOr, the allowed length of each fragment could span a specified min/max\nrange.\n\nAnd I like the sliding window approach instead of the pre-fragment\napproach.\n\n(Note: a fragdoc is one or more fragments stuck together, ie, the\nentire excerpt.)\n",
            "date": "2009-03-17T15:03:53.596+0000",
            "id": 22
        },
        {
            "author": "Mark Miller",
            "body": "{quote}But, flattening could produce just a and d (I think?); and I think H1\ncould do the same even with SpanScorer (Mark is that true? I don't\nfully understand the Query -> SpanQuery conversion). {quote}\n\nRight - SpanScorer won't follow boolean logic - it will just break down each clause and not highlight  a NOT - similar to standard H1. If a particular clause is position sensitive, it will only be 'lit if its found in a valid position, but thats as deep as it goes.\n\n",
            "date": "2009-03-17T15:18:41.355+0000",
            "id": 23
        },
        {
            "author": "Michael Busch",
            "body": "I wrote the highlighter for the OmniFind Yahoo Edition a few years ago\nand I totally agree that all this stuff is very subjective.\n\nThe OYE highlighter is of course based on Lucene and uses a sliding\nwindow too. It also uses information about sentence boundaries and\nprefers fragments that start at the beginning of a sentence.\n\nSo it goes through the document and generates fragment candidates on\nthe fly. It calculates a score for each fragment and puts it into a\npriority queue. The score is calculated using different heuristics:\n- fragments are boosted that start at the beginning of a sentence\n- the more highlighted terms a fragment contains, the higher is it\nscored\n- more different highlighted terms scores higher than a lot of \n- occurrences of the same term\n- no tf-idf is used\n- if a fragment does not start at the beginning of a sentence, then it\nis scored higher if the highlighted term(s) occur(s) more in the middle\nof the fragment: e.g. 'a b c d e' scores lower than 'b c a d e' if 'a'\nis the highlighted term; this is being done to show as much context as \npossible around a highlighted term\n- only a single long fragment is created if it contains all query terms\n(like google)\n- The queue tries to gather fragments, so that the union of the fragments\ncontain as many different query terms as possible. So it might toss a\nfragment in favor of one with a lower score, if it increases the\ntotal number of different highlighted terms.\n- For performance reasons there is an early termination if the\nfragments in the queue contain all query terms.\n\nInitially this highlighter also imitated Lucene's behavior to find the\nhighlighted positions. Last year I changed it to use SpanQueries. With\nour flexible query parser (which I introduced on java-dev recently) we\nhave two different QueryBuilders. One creates the \"normal\" query, that\nis executed to find the matching docs. Then a different QueryBuilder\ncreates SpanQueries from the same query for the highlighter.\n\nThe output of the highlighter is not formatted html, but rather an\nobject containing the unformatted text, together with offset\ninformation for both fragments and highlights. These offset spans can\ncarry additional information, which can be used for multi-color\nhighlighting too. We then use an HTMLFormatter class to generate the\nformatted text, also an XMLFormatter that keeps the offset information\nseparate from the actual text is possible (we're currently working on\nsuch a XMLFormatter). This is useful for frontends written in e.g. Flex. \n\nThe performance of our highlighter is good and so far we have been\npretty happy with the quality of the excerpts, but there is still much\nroom for improvements.\n\nI'd be happy to help working on a new highlighter. I think this is a\nvery important component, and Lucene's core should have a very good\nand flexible one.",
            "date": "2009-03-17T19:08:40.811+0000",
            "id": 24
        },
        {
            "author": "Marvin Humphrey",
            "body": ">> ANDQuery, ORQuery, and RequiredOptionalQuery just return the union of the\n>> spans produced by their children.\n> \n> Hmm - it seems like that loses information.  Ie, for ANDQuery, you lose the \n> fact that you should try to include a match from each of the sub-clauses' spans.\n\nA good idea.  ANDQuery's highlightSpans() method could probably be improved by\npost-processing the child spans to take this into account.  That way we\nwouldn't have to gum up the main Highlighter code with a bunch of conditionals\nwhich afford special treatment to certain query types.\n\n> What I meant was: all other things being equal, do you more strongly\n> favor a fragment that has all N of the terms in a query vs another\n> fragment that has fewer than N but say higher net number of occurrences.\n\nNo, the diversity of the terms in a fragment isn't factored in.  The span \nobjects only tell the Highlighter that a particular range of characters \nwas important; they don't say why.\n\nHowever, note that IDF would prevent a bunch of hits on \"the\" from causing too\nhot a hotspot in the heat map.  So you're likely to see fragments with high\ndiscriminatory value.\n\n> Google picks more than one fragment; it seems like it picks one or two\n> fragments.\n\nI probably overstated my opposition to supplying an excerpt containing more\nthan one fragment.  It seems OK to me to select more than one, so long as they\nall scan easily, and so long as the excerpts don't get long enough to force\nexcessive scrolling and slow down the time it takes the user to scan the whole\nresults page.  \n\nWhat bothers me is that the excerpts don't scan easily right now.  I consider\nthat a much more important defect than the fact that the fragdoc doesn't hit \nevery term (which isn't even possible for large queries), and it seemed to me \nthat pursuing exhaustive term matching was likely to yield even more highly \nfragmented, visually chaotic fragdocs.  ",
            "date": "2009-03-17T20:04:45.995+0000",
            "id": 25
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\n>> ANDQuery, ORQuery, and RequiredOptionalQuery just return the union of the\n>> spans produced by their children.\n> \n> Hmm - it seems like that loses information. Ie, for ANDQuery, you lose the \n> fact that you should try to include a match from each of the sub-clauses' spans.\n\nA good idea. ANDQuery's highlightSpans() method could probably be improved by\npost-processing the child spans to take this into account. That way we\nwouldn't have to gum up the main Highlighter code with a bunch of conditionals\nwhich afford special treatment to certain query types.\n{quote}\n\nI think we may need a tree-structured result returned by the\nWeight/Scorer, compactly representing the \"space\" of valid fragdocs\nfor this one doc.  And then somehow we walk that tree,\nenumerating/scoring individual \"valid\" fragdocs that are created from\nthat tree.\n\n{quote}\n> What I meant was: all other things being equal, do you more strongly\n> favor a fragment that has all N of the terms in a query vs another\n> fragment that has fewer than N but say higher net number of occurrences.\n\nNo, the diversity of the terms in a fragment isn't factored in. The span \nobjects only tell the Highlighter that a particular range of characters \nwas important; they don't say why.\n\nHowever, note that IDF would prevent a bunch of hits on \"the\" from causing too\nhot a hotspot in the heat map. So you're likely to see fragments with high\ndiscriminatory value.\n{quote}\n\nThis still seems subjectively wrong to me.  If I search for \"president\nbush\", probably bush is the rarer term and so you would favor showing\nme a single fragment that had bush occur twice, over a fragment that\nhad a single occurrence of president and bush?\n\n{quote}\n> Google picks more than one fragment; it seems like it picks one or two\n> fragments.\n\nI probably overstated my opposition to supplying an excerpt containing more\nthan one fragment. It seems OK to me to select more than one, so long as they\nall scan easily, and so long as the excerpts don't get long enough to force\nexcessive scrolling and slow down the time it takes the user to scan the whole\nresults page.\n\nWhat bothers me is that the excerpts don't scan easily right now. I consider\nthat a much more important defect than the fact that the fragdoc doesn't hit \nevery term (which isn't even possible for large queries), and it seemed to me \nthat pursuing exhaustive term matching was likely to yield even more highly \nfragmented, visually chaotic fragdocs.\n{quote}\n\nWhich excerpts don't scan easily right now?  Google's, KS's, Lucene's\nH1 or H2?\n\nI think with a tree structure representing the search space for all\nfragdocs, we could then efficiently enumerate fradocs with an\nappropriate scoring model (favoring sentence starts or surrounding\ncontext, breadth of terms, etc.).  This way we can do a real search\n(on all fragdocs) subject to the preference for\nconsumability/breadth.\n",
            "date": "2009-03-18T10:55:29.736+0000",
            "id": 26
        },
        {
            "author": "Michael McCandless",
            "body": "\nOK to sum up here with observations / wish list / ideas /\ncontroversies / etc. for Lucene's future merged highlighter:\n\n  * Fragmenter should aim for fast \"eye + brain scanning\n    consumability\" (eg, try hard to start on sentence boundaries,\n    include context)\n\n  * Let's try for single source -- each Query/Weight/Scorer should be\n    able to enumerate the set of term positions/spans that caused it\n    to match a specific doc (like explain(), but provides\n    positions/spans detailing the match).  Trying to \"reverse\n    engineer\" the matching is brittle\n\n  * Sliding window is better than static \"top down\" fragmentation\n\n  * To scale, we should make a simple IndexReader impl on top of term\n    vectors, but still allow the \"re-index single doc on the fly\"\n    option\n\n  * Favoring breadth (more unique terms instead of many occurences of\n    certain terms) seems important, except for too-many-term queries\n    where this gets unwieldy\n\n  * Prefer a single fragment if it scores well enough, but fall back\n    to several, if necessary, to show \"breadth\"\n\n  * Produce structured output so non-HTML front ends (eg Flex) can\n    render\n\n  * Try to include \"context around the hits\", when possible (eg the\n    \"favor middle of hte sentence\" that Michael described)\n\n  * Maybe or maybe don't let IDF affect fragment scoring\n\n  * Performance is important -- use TermVectors if present, add early\n    termination if you've already found a good enough fragdoc, etc.\n\n  * Maybe a tree-based fragdoc enumeration / searching model; I think\n    this'd be even more efficient than sliding window, especially for\n    large docs\n\n  * Multi-color, HeatMap default ootb HTML UIs are nice\n\n  * It's all very subjective and quite a good challenge!!\n\nIn the meantime, it seems like we should commit this H2 and give users\nthe choice?  We can then iterate over time on our wish list....\n",
            "date": "2009-03-18T10:59:26.692+0000",
            "id": 27
        },
        {
            "author": "Marvin Humphrey",
            "body": "> I think we may need a tree-structured result returned by the\n> Weight/Scorer, compactly representing the \"space\" of valid fragdocs\n> for this one doc. And then somehow we walk that tree,\n> enumerating/scoring individual \"valid\" fragdocs that are created from\n> that tree.\n\nSomething like that.  An array of span scores is too limited; a full fledged\nclass would do better.  Designing that class requires striking a balance\nbetween what information we think is useful and what information Highlighter\ncan sanely reduce.  By proposing the tree structure, you're suggesting that \nHighlighter will reverse engineer boolean matching; that sounds like a lot of \nwork to me.  \n\n>> However, note that IDF would prevent a bunch of hits on \"the\" from causing too\n>> hot a hotspot in the heat map. So you're likely to see fragments with high\n>> discriminatory value.\n> \n> This still seems subjectively wrong to me. If I search for \"president\n> bush\", probably bush is the rarer term and so you would favor showing\n> me a single fragment that had bush occur twice, over a fragment that\n> had a single occurrence of president and bush?\n\nWe've ended up in a false dichotomy.  Favoring high IDF terms -- or more\naccurately, high scoring character position spans -- and favoring fragments \nwith high term diversity are not mutually exclusive.  \n\nStill, the KS highlighter probably wouldn't do what you describe.  The proximity\nboosting accelerates as the spans approach each other, and maxes out if \nthey're adjacent.  So \"bush bush\" might be prefered over \"president bush\", \nbut \"bush or bush\" proabably wouldn't.\n\nI don't think that there's anything wrong with preferring high term diversity;\nthe KS highlighter doesn't happen to support favoring fragments with high term\ndiversity now, but would be improved by adding that capability.  I just don't\nthink term diversity is so important that it qualifies as a \"base litmus\ntest\".\n\nThere are other ways of choosing good fragments, and IDF is one of them.  If\nyou want to show why a doc matched a query, it makes sense to show the section\nof the document that contributed most to the score, surrounded by a little\ncontext.  \n\n> Which excerpts don't scan easily right now? Google's, KS's, Lucene's\n> H1 or H2?\n\nLucene H1.  Too many elipses, and fragments don't prefer to start on sentence\nboundaries.  \n\nI have to qualify the assertion that the fragments don't scan well with the caveat \nthat I'm basing this on a personal impression.  However, I'm pretty confident \nabout that impression.  I would be stunned if there were not studies out there\ndemonstrating that sentence fragments which begin at the top are easier to\nconsume than sentence fragments which begin in the middle.",
            "date": "2009-03-18T14:26:08.091+0000",
            "id": 28
        },
        {
            "author": "Mark Miller",
            "body": "bq. Lucene H1. Too many elipses, and fragments don't prefer to start on sentence boundaries.\n\nThats not necessarily a property of the Highlighter, just the basic implementations we currently supply for the pluggable classes. You can supply a custom fragmenter and you can control the number of fragments.",
            "date": "2009-03-18T14:33:49.586+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nSomething like that. An array of span scores is too limited; a full fledged\nclass would do better. Designing that class requires striking a balance\nbetween what information we think is useful and what information Highlighter\ncan sanely reduce.\n{quote}\n\nAgreed, and I'm not sure about the tree structure (just floating\nideas...).  It could very well be overkill.\n\n{quote}\nBy proposing the tree structure, you're suggesting that \nHighlighter will reverse engineer boolean matching; that sounds like a lot of \nwork to me.\n{quote}\n\nIt wouldn't be reverse engineered: BooleanQuery/Weight/Scorer2 itself\nwill have returned that.  Ie we would add a method to\n\"getSpanTree()\".\n\n{quote}\nStill, the KS highlighter probably wouldn't do what you describe.  The proximity\nboosting accelerates as the spans approach each other, and maxes out if \nthey're adjacent.  So \"bush bush\" might be prefered over \"president bush\", \nbut \"bush or bush\" proabably wouldn't.\n{quote}\n\nOK, it sounds like one can simply use different models to score\nfragdocs and it's still an open debate how much each of these criteria\n(IDF, showing surround context, being on sentence boundary, diversity\nof terms) should impact the score.  I agree, the \"basic litmus test\" I\nproposed is too strong.\n\n{quote}\nbq. Lucene H1. Too many elipses, and fragments don't prefer to start on sentence boundaries.\n\nThats not necessarily a property of the Highlighter, just the basic\nimplementations we currently supply for the pluggable classes. You can\nsupply a custom fragmenter and you can control the number of\nfragments.\n{quote}\n\nI agree: H1 is very pluggable and one could plug in a better\nfragmenter, but we don't offer such an impl in H1, and this is a case\nwhere \"out-of-the-box defaults\" are very important.\n",
            "date": "2009-03-18T15:50:01.448+0000",
            "id": 30
        },
        {
            "author": "Marvin Humphrey",
            "body": "> OK, it sounds like one can simply use different models to score\n> fragdocs and it's still an open debate how much each of these criteria\n> (IDF, showing surround context, being on sentence boundary, diversity\n> of terms) should impact the score. \n\nWith Michael Busch's priority queue approach, the algorithm for choosing the\nfragments can be abstracted into the class of object we put in the queue and\nits lessThan() method.  The output from the queue just has to be something the\nHighlighter can chew.",
            "date": "2009-03-18T16:37:58.155+0000",
            "id": 31
        },
        {
            "author": "David Kaelbling",
            "body": "Hi,\n\nOur application wants to find and highlight all the hits in a document,\nnot just the best one(s).  If future highlighters still allowed this,\neven if only by judicious use of subclasses, I would be happy :-)\n\n\tThanks,\n\tDavid\n\n-- \nDavid Kaelbling\nSenior Software Engineer\nBlack Duck Software, Inc.\n\ndkaelbling@blackducksoftware.com\nT +1.781.810.2041\nF +1.781.891.5145\n\nhttp://www.blackducksoftware.com\n\n\n",
            "date": "2009-03-18T17:03:53.645+0000",
            "id": 32
        },
        {
            "author": "Michael McCandless",
            "body": "Randomly searching in Google I came across this:\n\n    http://stackoverflow.com/questions/82151/is-there-a-fast-accurate-highlighter-for-lucene\n\n...which emphasizes how important it is that the highlighter only highlight \"matching\" fragdocs when possible.\n\n(Meaning, if you were to copy & paste the full excerpt you are looking at, index it as a document, would your current search match it).",
            "date": "2009-03-23T20:46:39.035+0000",
            "id": 33
        },
        {
            "author": "Mark Miller",
            "body": "I think you are reading more into that than I see - that guy is just frustrated that PhraseQueries don't highlight correctly. That was/is a common occurrence and you can find tons of examples. There are one or two JIRA highlighters that address it, and the their is the Span highlighter (more interestingly, there is a link to the birth of the Span highlighter idea on that page - thanks M. Harwood).\n\nWhen users see the PhraseQuery look right, I havn't seen any other repeated complaints really. While it would be nice to match boolean logic fully, I almost don't think its worth the effort. You likely have an interest in those terms anyway - its not a given that the terms that caused the match (non positional) matter. I have not seen a complaint on that one - mostly just positional type stuff. And I think we have positional solved fairly well with the current API - its just too darn slow. Not that I am against things being sweet and perfect, and getting exact matches, but there has been lots of talk in the past about integrating the highlighter into core and making things really fast and efficient - and generally it comes down to what work actually gets done (and all this stuff ends up at the hard end of the pool).\n\nWhen I wrote the SpanScorer, many times it was discussed how things should *really* be done. Most methods involved working with core - but what has been there for a couple years now is the SpanScorer that plugs into the current highlighter API and nothing else has made any progress. Not really an argument, just kind of thinking out loud at this point...\n\nI'm all for improving the speed and accuracy of the highlighter at the end of the day, but its a tall order considering how much attention the Highlighter has managed to receive in the past. Its large on ideas and low on sweat.\n\n*edit*\nA lot of the sweat that is given has been fragmented by the 3 or 4 alternate highlighters.",
            "date": "2009-03-23T21:09:46.018+0000",
            "id": 34
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I think you are reading more into that than I see - that guy is just frustrated that PhraseQueries don't highlight correctly\n\nBut that's really quite a serious problem; it's the kind that\nimmediately erodes user's trust.  Though if this user had used\nSpanScorer it would have been fixed (right?).\n\nIs there any reason not to use SpanScorer (vs QueryScorer)?\n\nThe \"final inch\" (search UI) is exceptionally important!\n\nbq. When users see the PhraseQuery look right, I havn't seen any other repeated complaints really.\n\nOK.\n\nbq. And I think we have positional solved fairly well with the current API - its just too darn slow.\n\nWell... I'd still like to explore some way to better integrate w/ core\n(just don't have enough time, but maybe if I keep talking about it\nhere, someone else will get the itch + time ;).\n\nI think an IndexReader impl around loaded TermVectors can get us OK\nperformance (no re-analysis nor linear scan of resynthesized\nTokenStream).\n\nbq. Not that I am against things being sweet and perfect, and getting exact matches, but there has been lots of talk in the past about integrating the highlighter into core and making things really fast and efficient - and generally it comes down to what work actually gets done (and all this stuff ends up at the hard end of the pool).\n\nWell this is open source after all.  Things get \"naturally\nprioritized\".\n\nbq. A lot of the sweat that is given has been fragmented by the 3 or 4 alternate highlighters.\n\nYeah also another common theme in open-source development, though it's\nin good company: evolution and capitalism share the same \"flaw\".\n",
            "date": "2009-03-23T22:07:14.708+0000",
            "id": 35
        },
        {
            "author": "Mark Miller",
            "body": "{quote}But that's really quite a serious problem; it's the kind that\nimmediately erodes user's trust. Though if this user had used\nSpanScorer it would have been fixed (right?).{quote}\n\nRight - my point was more that it was a common complaint and has been solved in one way or another for a long time. Even back when that post occured, there was a JIRA highlighter that worked with phrase queries I think. There have been at least one or two besides the SpanScorer.\n\n{quote}Is there any reason not to use SpanScorer (vs QueryScorer)?{quote}\n\nIt is slower when working with position sensitive clauses - because it actually does some work. For non position sensitive terms, its the same speed as the standard. Makes sense to me to always use it, but if you don't care and want every term highlighted, why pay the price I guess...\n\n{quote}\nWell... I'd still like to explore some way to better integrate w/ core\n(just don't have enough time, but maybe if I keep talking about it\nhere, someone else will get the itch + time .\n{quote}\n\nRight - don't get me wrong - I was just getting thoughts in my head down. These types of brain dumps you higher level guys do def leads to work getting done - the SpanScorer came directly from these types of discussions, and quite a bit later - the original discussion happened before my time.\n\n{quote}\nWell this is open source after all. Things get \"naturally\nprioritized\".\n\n    A lot of the sweat that is given has been fragmented by the 3 or 4 alternate highlighters.\n\nYeah also another common theme in open-source development, though it's\nin good company: evolution and capitalism share the same \"flaw\".\n{quote}\n\nRight. I suppose I was just suggesting that something more practical might make more sense (more musing than suggesting). And practical in terms of how much activity we have seen in the highlighter area (fairly low, and not usually to the extent needed to get something committed and in use).\n\nAnd the split work on the highlighters is fine - but if we had the right highlighter base, more work could have been concentrated on the highlighter thats most used. Not really a complaint, but idea for the future. If we can get something better going, perhaps we can get to the point were people work with the current implementation rather than creating a new one every time.",
            "date": "2009-03-23T22:43:15.550+0000",
            "id": 36
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\n(Meaning, if you were to copy & paste the full excerpt you are looking at, index it as a document, would your current search match it).\n{quote}\n\nI think this is an unrealistic requirement in some cases (e.g. AND queries). I agree it makes sense for phrases to show them entirely in a fragment (even if that means not to show the beginning of a sentence). But often you have only one or two lines of text to display an extract. Then it might be a better choice to show two decently sized fragments with some context around the highlighted terms, rather than showing e.g. 4 short fragments just to show all 4 highlighted query terms (e.g. for query '+a +b +c +d')\n\n",
            "date": "2009-03-23T22:52:20.810+0000",
            "id": 37
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I think this is an unrealistic requirement in some cases (e.g. AND queries).\n\nI agree.",
            "date": "2009-03-23T23:09:36.101+0000",
            "id": 38
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Added more comment and index time synonym support and its test cases.",
            "date": "2009-06-28T11:18:50.123+0000",
            "id": 39
        },
        {
            "author": "Koji Sekiguchi",
            "body": "I added package.html to explain the algorithm of H2. :)",
            "date": "2009-06-30T17:08:43.849+0000",
            "id": 40
        },
        {
            "author": "Michael McCandless",
            "body": "Is it possible to decouple this issue from LUCENE-1448 (whose latest patch is quite old and no longer applies and may not make it into 2.9)?  Ie, is it only the unit tests that rely on this fix to pass (because they test multi-valued fields)?",
            "date": "2009-07-06T19:44:44.599+0000",
            "id": 41
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Thank you for your advice, Michael.\n\nbq. because they test multi-valued fields\n\nRight.\n\nI commentted out the part of LUCENE-1448 dependencies, ie, test cases for multi valued:\n\n{code}\n/*\n * ----------------------------------\n *  THIS TEST DEPENDS ON LUCENE-1448\n *  UNCOMMENT WHEN IT IS COMMITTED.\n * ----------------------------------\n  public void testXXXXXXXMVB() throws Exception {\n      :\n  }\n*/\n{code}\n\nAll tests pass without LUCENE-1448.",
            "date": "2009-07-07T01:41:46.444+0000",
            "id": 42
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good, thanks Koji!  All tests now pass for me.\n\nI attached a new patch, that updates the website w/ references to highlighter2.  I plan to commit in a day or two.",
            "date": "2009-07-07T18:47:03.344+0000",
            "id": 43
        },
        {
            "author": "Uwe Schindler",
            "body": "Mike: As I have done these type of site updates many times because of TrieRange:\n\nYou should add highligther2 also to the source dirs and the package prefixes for the combined all-javadocs in the main build.xml. And as far as I know, also the main site docs also reference the contribs in developer-resources.xml.\n\nEverything else looks good!",
            "date": "2009-07-07T19:14:40.360+0000",
            "id": 44
        },
        {
            "author": "Michael McCandless",
            "body": "bq. You should add highligther2 also to the source dirs and the package prefixes for the combined all-javadocs in the main build.xml.\n\nAhh right.  I'll fix that!\n\nbq.  the main site docs also reference the contribs in developer-resources.xml.\n\nWhoa OK I'll update that as well!  Thanks for the pointers ;)",
            "date": "2009-07-07T19:32:26.444+0000",
            "id": 45
        },
        {
            "author": "Mark Miller",
            "body": "Hmmm... is it still Highlighter2?\n\nCan we come up with a better name that kind of gives a feel for the difference between this and the other highlighter?\n\nFastVectorHighlighter?\n\nWell - maybe that will triger a better idea from someone else anyway ... :)",
            "date": "2009-07-07T20:35:16.369+0000",
            "id": 46
        },
        {
            "author": "Koji Sekiguchi",
            "body": "bq. FastVectorHighlighter?\n\n+1 from me. I'll attach a renamed version of the patch tomorrow, since I have to be out soon and back late tonight.",
            "date": "2009-07-08T01:07:30.107+0000",
            "id": 47
        },
        {
            "author": "Koji Sekiguchi",
            "body": "renamed Highlighter2 to FastVectorHighlighter :)",
            "date": "2009-07-09T12:01:33.014+0000",
            "id": 48
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good Koji!  I will commit shortly.  Thanks for persisting here ;)",
            "date": "2009-07-09T13:05:51.955+0000",
            "id": 49
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Koji!",
            "date": "2009-07-09T13:07:10.006+0000",
            "id": 50
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Now LUCENE-1448 and LUCENE-1759 has been committed, these tests should be uncommentted. All tests pass.",
            "date": "2009-08-02T04:14:38.761+0000",
            "id": 51
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Koji, I'll commit this (turning on the tests).  Always nice to add more tests!",
            "date": "2009-08-02T18:00:33.729+0000",
            "id": 52
        },
        {
            "author": "Koji Sekiguchi",
            "body": "There is a bug in BaseFragmentsBuilder. When the highlighting field is not stored, StringIndexOutOfBoundException will be thrown. I'd like to reopen this issue so the fix can be included in 2.9. I'll post the fix soon.",
            "date": "2009-08-16T03:02:48.231+0000",
            "id": 53
        },
        {
            "author": "Koji Sekiguchi",
            "body": "The patch includes the fix and a test case.",
            "date": "2009-08-16T03:04:34.788+0000",
            "id": 54
        },
        {
            "author": "Mark Miller",
            "body": "Thanks Koji!\n\nr804786",
            "date": "2009-08-16T21:57:46.874+0000",
            "id": 55
        }
    ],
    "component": "modules/highlighter",
    "description": "I've written this highlighter for my project to support bi-gram token stream (general token stream (e.g. WhitespaceTokenizer) also supported. see test code in patch). The idea was inherited from my previous project with my colleague and LUCENE-644. This approach needs highlight fields to be TermVector.WITH_POSITIONS_OFFSETS, but is fast and can support N-grams. This depends on LUCENE-1448 to get refined term offsets.\n\nusage:\n{code:java}\nTopDocs docs = searcher.search( query, 10 );\nHighlighter h = new Highlighter();\nFieldQuery fq = h.getFieldQuery( query );\nfor( ScoreDoc scoreDoc : docs.scoreDocs ){\n  // fieldName=\"content\", fragCharSize=100, numFragments=3\n  String[] fragments = h.getBestFragments( fq, reader, scoreDoc.doc, \"content\", 100, 3 );\n  if( fragments != null ){\n    for( String fragment : fragments )\n      System.out.println( fragment );\n  }\n}\n{code}\n\nfeatures:\n- fast for large docs\n- supports not only whitespace-based token stream, but also \"fixed size\" N-gram (e.g. (2,2), not (1,3)) (can solve LUCENE-1489)\n- supports PhraseQuery, phrase-unit highlighting with slops\n{noformat}\nq=\"w1 w2\"\n<b>w1 w2</b>\n---------------\nq=\"w1 w2\"~1\n<b>w1</b> w3 <b>w2</b> w3 <b>w1 w2</b>\n{noformat}\n- highlight fields need to be TermVector.WITH_POSITIONS_OFFSETS\n- easy to apply patch due to independent package (contrib/highlighter2)\n- uses Java 1.5\n- looks query boost to score fragments (currently doesn't see idf, but it should be possible)\n- pluggable FragListBuilder\n- pluggable FragmentsBuilder\n\nto do:\n- term positions can be unnecessary when phraseHighlight==false\n- collects performance numbers\n",
    "hasPatch": true,
    "hasScreenshot": true,
    "id": "LUCENE-1522",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "another highlighter",
    "systemSpecification": true,
    "version": ""
}