{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "This would be great!\n\nBut, note that term vectors today do not store the term char[] again -- they piggyback on the term char[] already stored for the postings.  Though, I believe they store \"int textStart\" (increments by term length per unique term), which is less compact than the termID would be (increments +1 per unique term), so if eg we someday use packed ints we'd be more RAM efficient by storing termIDs...",
            "date": "2010-03-18T09:21:21.938+0000",
            "id": 0
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "Slightly off-topic ... Having a facility to obtain termID-s per segment (or better yet per index) would greatly benefit Solr's UnInverted field creation, which currently needs to assign term ids by linear scanning.",
            "date": "2010-03-18T14:39:31.479+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "This issue is just about how IndexWriter's RAM buffer stores its terms...\n\nBut, the flex API adds long ord() and seek(long ord) to the TermsEnum API.\n",
            "date": "2010-03-18T15:01:24.472+0000",
            "id": 2
        },
        {
            "author": "Michael Busch",
            "body": "bq. This issue is just about how IndexWriter's RAM buffer stores its terms... \n\nActually, when I talked about the TermVectors I meant we should explore to store the termIDs on *disk*, rather than the strings.  It would help things like similarity search and facet counting.\n\n{quote}\nBut, note that term vectors today do not store the term char[] again - they piggyback on the term char[] already stored for the postings.\n{quote}\n\nYeah I think I'm familiar with that part (secondary entry point in TermsHashPerField, hashes based on termStart).  Haven't looked much into how the \"rest\" of the TermVector in-memory data structures are working.  \n\n{quote}\nThough, I believe they store \"int textStart\" (increments by term length per unique term), which is less compact than the termID would be (increments +1 per unique term)\n{quote}\n\nActually we wouldn't need a second hashtable for the secondary TermsHash anymore, right?  It would just have like the primary TermsHash a parallel array with the things that the TermVectorsTermsWriter.Postinglist class currently contains (freq, lastOffset, lastPosition)?  And the index into that array would be the termID of course.\n\nThis would be a nice simplification, because no hash collisions, no hash table resizing based on load factor, etc. would be necessary for non-primary TermsHashes?\n\nbq.  so if eg we someday use packed ints we'd be more RAM efficient by storing termIDs...\n\nHow does the read performance of packed ints compare to \"normal\" int[] arrays?  I think nowadays RAM is less of an issue?  And with a searchable RAM buffer we might want to sacrifice a bit more RAM for higher search performance?  Oh man, will we need flexible indexing for the in-memory index too? :) ",
            "date": "2010-03-18T17:51:36.421+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Actually, when I talked about the TermVectors I meant we should explore to store the termIDs on disk, rather than the strings. It would help things like similarity search and facet counting.\n\nAhhhh that would be great!\n\nbq. Actually we wouldn't need a second hashtable for the secondary TermsHash anymore, right? It would just have like the primary TermsHash a parallel array with the things that the TermVectorsTermsWriter.Postinglist class currently contains (freq, lastOffset, lastPosition)? And the index into that array would be the termID of course.\n\nHmm the challenge is that the tracking done for term vectors is just within a single doc.  Ie the hash used for term vectors only holds the terms for that one doc (so it's much smaller), vs the primary hash that holds terms for all docs in the current RAM buffer.  So we'd be burning up much more RAM if we also key into the term vector's parallel arrays using the primary term id?\n\nBut I do think we should cutover to parallel arrays for TVTW, too.\n\nbq. How does the read performance of packed ints compare to \"normal\" int[] arrays? I think nowadays RAM is less of an issue? And with a searchable RAM buffer we might want to sacrifice a bit more RAM for higher search performance?\n\nIt's definitely slower to read/write to/from packed ints, and I agree, indexing and searching speed trumps RAM efficiency.\n\nbq. Oh man, will we need flexible indexing for the in-memory index too?\n\nEG custom attrs appearing in the TokenStream?  Yes we will need to... but hopefully once we get serialization working cleanly for the attrs this'll be easy?  With ByteSliceWriter/Reader you just .writeBytes and .readBytes...\n\nI don't think we should allow Codecs to be used in the RAM buffer anytime soon though... ;)\n\n",
            "date": "2010-03-18T18:36:36.500+0000",
            "id": 4
        },
        {
            "author": "Michael Busch",
            "body": "bq. Hmm the challenge is that the tracking done for term vectors is just within a single doc.\n\nDuh! Of course you're right.\n",
            "date": "2010-03-18T18:57:28.368+0000",
            "id": 5
        },
        {
            "author": "Michael Busch",
            "body": "Changes the indexer to use parallel arrays instead of PostingList objects (for both FreqProx* and TermVectors*).\n\nAll core & contrib & bw tests pass.  I haven't done performance tests yet.  \n\nI'm wondering how to manage the size of the parallel array?  I started with an initial size for the parallel array equal to the size of the postingsHash array.  When it's full then I allocate a new one with 1.5x size.  When shrinkHash() is called I also shrink the parallel array to the same size as postingsHash.  How does that sound?",
            "date": "2010-03-22T08:05:17.490+0000",
            "id": 6
        },
        {
            "author": "Michael Busch",
            "body": "One change I should make to the patch is how to track the memory consumption.  When the parallel array is allocated or grown then bytesAllocated() should be called?  And when a new termID is added, should only then bytesUsed() be called?",
            "date": "2010-03-22T08:28:39.454+0000",
            "id": 7
        },
        {
            "author": "Michael Busch",
            "body": "Made the memory tracking changes as described in my previous comment.\n\nAll tests still pass.",
            "date": "2010-03-22T08:56:11.858+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "Looks great Michael!\n\nI think *ParallelPostingsArray.reset do not need to zero-fill the arrays -- these are overwritten when that termID is first used, right?\n",
            "date": "2010-03-22T11:19:23.158+0000",
            "id": 9
        },
        {
            "author": "Michael Busch",
            "body": "bq. I think *ParallelPostingsArray.reset do not need to zero-fill the arrays - these are overwritten when that termID is first used, right?\n\nGood point!  I'll remove the reset() methods.",
            "date": "2010-03-22T15:21:34.620+0000",
            "id": 10
        },
        {
            "author": "Michael Busch",
            "body": "Removed reset().  All tests still pass.",
            "date": "2010-03-22T16:04:17.584+0000",
            "id": 11
        },
        {
            "author": "Michael Busch",
            "body": "I did some performance experiments:\n\nI indexed 1M wikipedia documents using the cheap WhiteSpaceAnalyzer, no cfs files, disabled any merging,  RAM buffer size = 200MB, single writer thread, TermVectors enabled.  \n\nTest machine: MacBook Pro, 2.53 GHz Intel Core 2 Duo, 4 GB 1067 MHz DDR3, MacOS X 10.5.8.\n\nh4. Results with -Xmx2000m:\n\n|| || Write performance || Gain || Number of segments ||\n| trunk | 833 docs/sec |  |  41 |\n| trunk + parallel arrays | 869 docs/sec | {color:green} + 4.3% {color} | 32|\n\n\nh4. Results with -Xmx256m:\n\n|| || Write performance || Gain || Number of segments ||\n| trunk | 467 docs/sec |  | 41 |  \n| trunk + parallel arrays | 871 docs/sec | {color:green} +86.5% {color} | 32|\n\nSo I think these results are interesting and roughly as expected.  4.3% is a nice small performance gain.\nBut running the tests with a low heap shows how much cheaper the garbage collection becomes.  Setting IW's RAM buffer to 200MB and the overall heap to 256MB forces the gc to run frequently.  The mark times are much more costly if we have all long-living PostingList objects in memory compared to parallel arrays.\n\nSo this is probably not a huge deal for \"normal\" indexing.  But once we can search on the RAM buffer it becomes much more attractive to fill up the RAM as much as you can.  And exactly in that case we safe a lot with this improvement. \n\nAlso note that the number of segments decreased by 22% (from 41 to 32).  This shows that the parallel-array approach needs less RAM, thus flushes less often and will cause less segment merges in the long run.  So a longer test with actual segment merges would show even bigger gains (with both big and small heaps).\n\nSo overall, I'm very happy with these results!\n",
            "date": "2010-03-23T00:48:19.643+0000",
            "id": 12
        },
        {
            "author": "Michael McCandless",
            "body": "Sweet, this looks great Michael!  Less RAM used and faster indexing (much less GC load) -- win/win.\n\nIt's a little surprising that the segment count dropped from 41 -> 32?  Ie, how much less RAM do the parallel arrays take?  They save the object header per-unique-term, and 4 bytes on 64bit JREs since the \"pointer\" is now an int and not a real pointer?  But, other things use RAM (the docIDs in the postings themselves, norms, etc.) so it's surprising the savings was so much that you get 22% fewer segments... are you sure there isn't a bug in the RAM usage accounting?",
            "date": "2010-03-23T09:07:04.879+0000",
            "id": 13
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nso it's surprising the savings was so much that you get 22% fewer segments... are you sure there isn't a bug in the RAM usage accounting?\n{quote}\n\nYeah it seems a bit suspicious.  I'll investigate.  But, keep in mind that TermVectors were enabled too.  And the number of \"unique terms\" in the 2nd TermsHash is higher, i.e. if you summed up numPostings from the 2nd TermsHash in each round that sum should be higher than numPostings from the first TermsHash. ",
            "date": "2010-03-23T15:16:41.308+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "bq. But, keep in mind that TermVectors were enabled too.\n\nOK, but, RAM used by TermVectors* shouldn't participate in the accounting... ie it only holds RAM for the one doc, at a time.\n\nbq. And the number of \"unique terms\" in the 2nd TermsHash is higher, i.e. if you summed up numPostings from the 2nd TermsHash in each round that sum should be higher than numPostings from the first TermsHash.\n\n1st TermsHash = current trunk and 2nd TermsHash = this patch?  Ie, it has more unique terms at flush time (because it's more RAM efficient)?  If so, then yes, I agree :)  But 22% fewer still seems too good to be true...",
            "date": "2010-03-23T15:54:47.748+0000",
            "id": 15
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nOK, but, RAM used by TermVectors* shouldn't participate in the accounting... ie it only holds RAM for the one doc, at a time.\n{quote}\n\nMan, my brain is lacking the TermVector synapses...",
            "date": "2010-03-23T16:19:28.760+0000",
            "id": 16
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nThey save the object header per-unique-term, and 4 bytes on 64bit JREs since the \"pointer\" is now an int and not a real pointer?\n{quote}\n\nWe actually save on 64bit JVMs (which I used for my tests) 28 bytes per unique-term:\n\nh4. Trunk:\n{code}\n    // Why + 4*POINTER_NUM_BYTE below?\n    //   +1: Posting is referenced by postingsFreeList array\n    //   +3: Posting is referenced by hash, which\n    //       targets 25-50% fill factor; approximate this\n    //       as 3X # pointers\n    bytesPerPosting = consumer.bytesPerPosting() + 4*DocumentsWriter.POINTER_NUM_BYTE;\n\n...\n\n  @Override\n  int bytesPerPosting() {\n    return RawPostingList.BYTES_SIZE + 4 * DocumentsWriter.INT_NUM_BYTE;\n  }\n\n...\nabstract class RawPostingList {\n  final static int BYTES_SIZE = DocumentsWriter.OBJECT_HEADER_BYTES + 3*DocumentsWriter.INT_NUM_BYTE;\n\n...\n\n  // Coarse estimates used to measure RAM usage of buffered deletes\n  final static int OBJECT_HEADER_BYTES = 8;\n  final static int POINTER_NUM_BYTE = Constants.JRE_IS_64BIT ? 8 : 4;\n{code}\n\nThis needs 8 bytes + 3 * 4 bytes + 4 * 4 bytes + 4 * 8 bytes = 68 bytes. \n\nh4. 2329:\n{code}\n    //   +3: Posting is referenced by hash, which\n    //       targets 25-50% fill factor; approximate this\n    //       as 3X # pointers\n    bytesPerPosting = consumer.bytesPerPosting() + 3*DocumentsWriter.INT_NUM_BYTE;\n\n...\n\n  @Override\n  int bytesPerPosting() {\n    return ParallelPostingsArray.BYTES_PER_POSTING + 4 * DocumentsWriter.INT_NUM_BYTE;\n  }\n\n...\n\nfinal static int BYTES_PER_POSTING = 3 * DocumentsWriter.INT_NUM_BYTE;\n{code}\n\nThis needs 3 * 4 bytes + 4 * 4 bytes + 3 * 4 bytes = 40 bytes.\n\n\nI checked how many bytes were allocated for postings when the first segment was flushed.  Trunk flushed after 6400 docs and had 103MB allocated for PostingList objects.  2329 flushed after 8279 docs and had 94MB allocated for the parallel arrays, and 74MB out of the 94MB were actually used.\n\nThe first docs in the wikipedia dataset seem pretty large with many unique terms.\n\nI think this sounds reasonable?",
            "date": "2010-03-23T18:04:10.545+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "OK indeed it does sounds reasonable!  Sweet :)  I think you should commit it!  Make sure you \"svn switch\" your checkout first :)  And pass Solr's tests!\n",
            "date": "2010-03-23T18:20:47.646+0000",
            "id": 18
        },
        {
            "author": "Michael Busch",
            "body": "Cool, will do!  Thanks for the review and good questions... and the whole idea! :)",
            "date": "2010-03-23T19:02:56.452+0000",
            "id": 19
        },
        {
            "author": "Michael Busch",
            "body": "Committed revision 926791.",
            "date": "2010-03-23T21:25:52.846+0000",
            "id": 20
        },
        {
            "author": "Michael McCandless",
            "body": "I think we need to fix how RAM is managed for this... right now, if\nyou turn on IW's infoStream you'll see a zillion prints where IW tries\nto balance RAM (it \"runs hot\"), but, nothing can be freed.  We do this\nper-doc, after the parallel arrays resize themselves to net/net over\nour allowed RAM buffer.\n\nA few ideas on how we can fix:\n\n  * I think we have to change when we flush.  It's now based on RAM\n    used (not alloc'd), but I think we should switch it to use RAM\n    alloc'd after we've freed all we can.  Ie if we free things up and\n    we've still alloc'd over the limit, we flush.  This'll fix the\n    running hot we now see...\n\n  * TermsHash.freeRAM is now a no-op right?  We have to fix this to\n    actually free something when it can because you can imagine\n    indexing docs that are postings heavy but then switching to docs\n    that are byte[] block heavy.  On that switch you have to balance\n    the allocations (ie, shrink your postings).  I think we should\n    walk the threads/fields and use ArrayUtil.shrink to shrink down,\n    but, don't shrink by much at a time (to avoid running hot) -- IW\n    will invoke this method again if more shrinkage is needed.\n\n  * Also, shouldn't we use ArrayUtil.grow to increase?  Instead of\n    always a 50% growth?  Because with such a large growth you can\n    easily have horrible RAM efficiency... ie that 50% growth can\n    suddenly put you over the limit and then you flush, having\n    effectively used only half of the allowed RAM buffer in the worst\n    case.\n",
            "date": "2010-03-29T10:58:00.389+0000",
            "id": 21
        },
        {
            "author": "Michael McCandless",
            "body": "Reopening to fix the RAM balancing problems...",
            "date": "2010-03-29T11:00:03.368+0000",
            "id": 22
        },
        {
            "author": "Michael Busch",
            "body": "Good catch!\n\nThanks for the thorough explanation and suggestions.  I think it all makes sense.  Will work on a patch.",
            "date": "2010-03-29T16:34:03.193+0000",
            "id": 23
        },
        {
            "author": "Michael Busch",
            "body": "This patch:\n * Changes DocumentsWriter to trigger the flush using bytesAllocated instead of bytesUsed to improve the \"running hot\" issue Mike's seeing\n * Improves the ParallelPostingsArray to grow using ArrayUtil.oversize()\n\nIn IRC we discussed changing TermsHashPerField to shrink the parallel arrays in freeRAM(), but that involves tricky thread-safety changes, because one thread could call DocumentsWriter.balanceRAM(), which triggers freeRAM() across *all* thread states, while other threads keep indexing.\n\nWe decided to leave it the way it currently works: we discard the whole parallel array during flush and don't reuse it.  This is not as optimal as it could be, but once LUCENE-2324 is done this won't be an issue anymore anyway.\n\nNote that this new patch is against the flex branch: I thought we'd switch it over soon anyway?  I can also create a patch for trunk if that's preferred.",
            "date": "2010-04-01T00:32:37.375+0000",
            "id": 24
        },
        {
            "author": "Michael McCandless",
            "body": "Fixed a couple of issues on the last patch:\n\n  * We weren't notifying DW that we freed up RAM when setting postingsArray to null\n\n  * We can shrink postingsArray separately from the termsHash, and, instead of nulling it, we can simply downsize it\n\nTests pass, and indexing perf on first 10M 1 KB sized wikipedia docs is a bit faster though probably in the noise (1296 sec on current flex head vs 1238 sec with this patch).",
            "date": "2010-04-01T17:58:54.941+0000",
            "id": 25
        },
        {
            "author": "Michael McCandless",
            "body": "New patch attached:\n\n  * Does away with separate tracking of used vs alloc, in IndexWriter.\n    This distinction added much complexity and only saved a small\n    number of free/alloc's per flush cycle, especially now that\n    postings realloc only in big chunks (parallel arrays).\n\n  * Fixed some over-counting of bytes used.\n\nThe indexing throughput is basically unchanged after this (on first\n10M 1KB Wikipedia docs), so I think this is a good simplification.\n",
            "date": "2010-04-01T23:49:19.092+0000",
            "id": 26
        },
        {
            "author": "Michael Busch",
            "body": "Looks great!  I like the removal of bytesAlloc - nice simplification.",
            "date": "2010-04-02T00:54:59.051+0000",
            "id": 27
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Michael... I'll commit shortly.  It's a good simplification.\n",
            "date": "2010-04-02T09:14:11.631+0000",
            "id": 28
        },
        {
            "author": "Michael Busch",
            "body": "Thanks!  I think we can resolve this now?",
            "date": "2010-04-02T16:23:08.701+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "Woops, right!",
            "date": "2010-04-02T16:43:32.521+0000",
            "id": 30
        },
        {
            "author": "Michael McCandless",
            "body": "Reopening -- this fixed causes an intermittent deadlock in\nTestStressIndexing2.\n\nIt's actually a pre-existing issue, whereby if a flush happens only\nbecause of deletions (ie no indexed docs), and you're using multiple\nthreads, it's possible some idled threads would fail to be notified\nto wake up and continue indexing once the flush completes.\n\nThe fix here increased the chance of hitting that bug because the RAM\naccounting has a bug whereby it overly-aggressively flushes because of\ndeletions, ie, rather than free up RAM allocated but not used for\nindexing, it flushes.\n\nI first fixed the deadlock case (need to clear DW's flushPending when\nwe only flush deletes).\n\nThen I fixed the shared deletes/indexing RAM by:\n\n  * Not reusing the RAM for postings arrays -- we now null this out\n    for every field after flushing\n\n  * Calling balanceRAM when deletes have filled up RAM before deciding\n    to flush, because this can free RAM up, making more space for\n    deletes.\n\nI also further simplified things -- no more separate call to\ndoBalanceRAM, and added a fun unit test that randomly alternates\nbetween pure indexing and pure deleting, asserting that the flushing\ndoesn't \"run hot\" on any of those transitions.\n",
            "date": "2010-04-05T19:21:08.612+0000",
            "id": 31
        },
        {
            "author": "Michael Busch",
            "body": "We could move the if (postingsArray == null) check to start(), then we don't have to check for every new term?\n\n",
            "date": "2010-04-05T19:49:34.675+0000",
            "id": 32
        },
        {
            "author": "Michael McCandless",
            "body": "bq. We could move the if (postingsArray == null) check to start(), then we don't have to check for every new term?\n\nExcellent, I'll do that!",
            "date": "2010-04-06T09:11:23.085+0000",
            "id": 33
        },
        {
            "author": "Michael McCandless",
            "body": "New patch, init'ing the postings arrays in THPF.start instead of per term.",
            "date": "2010-04-06T09:29:25.360+0000",
            "id": 34
        },
        {
            "author": "Michael McCandless",
            "body": "Third time's a charm?",
            "date": "2010-04-06T19:57:16.814+0000",
            "id": 35
        },
        {
            "author": "Uwe Schindler",
            "body": "This is broken now in stable branch. We should fix it, hudsons clover tests are hung in benchmark.",
            "date": "2010-05-04T15:54:09.532+0000",
            "id": 36
        },
        {
            "author": "Michael Busch",
            "body": "I probably missed something here.  What exactly is broken and how is it related to this patch?",
            "date": "2010-05-04T16:16:42.562+0000",
            "id": 37
        },
        {
            "author": "Michael McCandless",
            "body": "Michael I'll take care of it -- we just need to merge all commits under this issue, to stable.  Only the 1st commit made it... I'm working on it now.",
            "date": "2010-05-04T16:47:04.767+0000",
            "id": 38
        },
        {
            "author": "Michael McCandless",
            "body": "OK I merged all commits to 3x.",
            "date": "2010-05-04T17:18:06.492+0000",
            "id": 39
        },
        {
            "author": "Michael Busch",
            "body": "Ah got it.  Thanks for taking care of it!",
            "date": "2010-05-04T17:49:49.739+0000",
            "id": 40
        },
        {
            "author": "Grant Ingersoll",
            "body": "Bulk close for 3.1",
            "date": "2011-03-30T15:49:50.506+0000",
            "id": 41
        }
    ],
    "component": "core/index",
    "description": "This is Mike's idea that was discussed in LUCENE-2293 and LUCENE-2324.\n\nIn order to avoid having very many long-living PostingList objects in TermsHashPerField we want to switch to parallel arrays.  The termsHash will simply be a int[] which maps each term to dense termIDs.\n\nAll data that the PostingList classes currently hold will then we placed in parallel arrays, where the termID is the index into the arrays.  This will avoid the need for object pooling, will remove the overhead of object initialization and garbage collection.  Especially garbage collection should benefit significantly when the JVM runs out of memory, because in such a situation the gc mark times can get very long if there is a big number of long-living objects in memory.\n\nAnother benefit could be to build more efficient TermVectors.  We could avoid the need of having to store the term string per document in the TermVector.  Instead we could just store the segment-wide termIDs.  This would reduce the size and also make it easier to implement efficient algorithms that use TermVectors, because no term mapping across documents in a segment would be necessary.  Though this improvement we can make with a separate jira issue.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2329",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Use parallel arrays instead of PostingList objects",
    "systemSpecification": true,
    "version": ""
}