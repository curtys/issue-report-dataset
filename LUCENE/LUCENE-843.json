{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "I'm attaching a patch with my current state.  NOTE: this is very rough\nand very much a work in progress and nowhere near ready to commit!  I\nwanted to get it out there sooner rather than later to get feedback,\nmaybe entice some daring early adopters, iterate, etc.\n\nIt passes all unit tests except the disk-full tests.\n\nThere are some big issues yet to resolve:\n\n  * Merge policy has problems when you \"flush by RAM\" (this is true\n    even before my patch).  Not sure how to fix yet.\n\n  * Thread safety and thread concurrency aren't there yet.\n\n  * Norms are not flushed (just use up RAM until you close the\n    writer).\n\n  * Many other things on my TODO list :)\n\n",
            "date": "2007-03-22T17:06:45.135+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "New rev of the patch:\n\n  * Fixed at least one data corruption case\n\n  * Added more asserts (run with \"java -ea\" so asserts run)\n\n  * Some more small optimizations\n\n  * Updated to current trunk so patch applies cleanly\n\n",
            "date": "2007-03-25T14:30:36.581+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "\nAnother rev of the patch:\n\n  * Got thread concurrency working: removed \"synchronized\" from entire\n    call to MultiDocWriter.addDocument and instead synchronize two\n    quick steps (init/finish) addDocument leaving the real work\n    (processDocument) unsynchronized.\n\n  * Fixed bug that was failing to delete temp files from index\n\n  * Reduced memory usage of Posting by inlining positions, start\n    offset, end offset into a single int array.\n\n  * Enabled IndexLineFiles.java (tool I use for local benchmarking) to\n    run multiple threads\n\n  * Other small optimizations\n\nBTW, one of the nice side effects of this patch is it cleans up the\nmergeSegments method of IndexWriter by separating out \"flush\" of added\ndocs & deletions because it's no longer a merge, from the \"true\"\nmergeSegments whose purpose is then to merge disk segments.\nPreviously mergeSegments was getting rather confusing with the\ndifferent cases/combinations of added docs or not, deleted docs or\nnot, any merges or not.\n\n",
            "date": "2007-03-28T12:49:18.734+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "Another rev of the patch.  All tests pass except disk full tests.  The\ncode is still rather \"dirty\" and not well commented.\n\nI think I'm close to finishing optimizing and now I will focus on\nerror handling (eg disk full), adding some deeper unit tests, more\ntesting on corner cases like massive docs or docs with massive terms,\netc., flushing pending norms to disk, cleaning up / commenting the\ncode and various other smaller items.\n\nHere are the changes in this rev:\n\n  * A proposed backwards compatible change to the Token API to also\n    allow the term text to be delivered as a slice (offset & length)\n    into a char[] array instead of String.  With an analyzer/tokenizer\n    that takes advantage of this, this was a decent performance gain\n    in my local testing.  I've created a SimpleSpaceAnalyzer that only\n    splits words at the space character to test this.\n\n  * Added more asserts (run java -ea to enable asserts).  The asserts\n    are quite useful and now often catch a bug I've introduced before\n    the unit tests do.\n\n  * Changed to custom int[] block buffering for postings to store\n    freq, prox's and offsets.  With this buffering we no longer have\n    to double the size of int[] arrays while adding positions, nor do\n    we have to copy ints whenever we needs more space for these\n    arrays.  Instead I allocate larger slices out of the shared int[]\n    arrays.  This reduces memory and improves performance.\n\n  * Changed to custom char[] block buffering for postings to store\n    term text.  This also reduces memory and improves performance.\n\n  * Changed to single file for RAM & flushed partial segments (was 3\n    separate files before)\n\n  * Changed how I merge flushed partial segments to match what's\n    described in LUCENE-854\n\n  * Reduced memory usage when indexing large docs (25 MB plain text\n    each).  I'm still consuming more RAM in this case than the\n    baseline (trunk) so I'm still working on this one ...\n\n  * Fixed a slow memory leak when building large (20+ GB) indices\n\n",
            "date": "2007-04-02T14:43:21.323+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "Some details on how I measure RAM usage: both the baseline (current\nlucene trunk) and my patch have two general classes of RAM usage.\n\nThe first class, \"document processing RAM\", is RAM used while\nprocessing a single doc. This RAM is re-used for each document (in the\ntrunk, it's GC'd and new RAM is allocated; in my patch, I explicitly\nre-use these objects) and how large it gets is driven by how big each\ndocument is.\n\nThe second class, \"indexed documents RAM\", is the RAM used up by\npreviously indexed documents.  This RAM grows with each added\ndocument and how large it gets is driven by the number and size of\ndocs indexed since the last flush.\n\nSo when I say the writer is allowed to use 32 MB of RAM, I'm only\nmeasuring the \"indexed documents RAM\".  With trunk I do this by\ncalling ramSizeInBytes(), and with my patch I do the analagous thing\nby measuring how many RAM buffers are held up storing previously\nindexed documents.\n\nI then define \"RAM efficiency\" (docs/MB) as how many docs we can hold\nin \"indexed documents RAM\" per MB RAM, at the point that we flush to\ndisk.  I think this is an important metric because it drives how large\nyour initial (level 0) segments are.  The larger these segments are\nthen generally the less merging you need to do, for a given # docs in\nthe index.\n\nI also measure overall RAM used in the JVM (using\nMemoryMXBean.getHeapMemoryUsage().getUsed()) just prior to each flush\nexcept the last, to also capture the \"document processing RAM\", object\noverhead, etc.\n",
            "date": "2007-04-03T10:20:01.923+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "To do the benchmarking I created a simple standalone tool\n(demo/IndexLineFiles, in the last patch) that indexes one line at a\ntime from a large previously created file, optionally using multiple\nthreads.  I do it this way to minimize IO cost of pulling the document\nsource because I want to measure just indexing time as much as possible.\n\nEach line is read and a doc is created with field \"contents\" that is\nnot stored, is tokenized, and optionally has term vectors with\nposition+offsets.  I also optionally add two small only-stored fields\n(\"path\" and \"modified\").  I think these are fairly trivial documents\ncompared to typical usage of Lucene.\n\nFor the corpus, I took Europarl's \"en\" content, stripped tags, and\nprocessed into 3 files: one with 100 tokens per line (= ~550 bytes),\none with 1000 tokens per line (= ~5,500 bytes) and with 10000 tokens\nper line (= ~55,000 bytes) plain text per line.\n\nAll settings (mergeFactor, compound file, etc.) are left at defaults.\nI don't optimize the index in the end.  I'm using my new\nSimpleSpaceAnalyzer (just splits token on the space character and\ncreates token text as slice into a char[] array instead of new\nString(...)) to minimize the cost of tokenization.\n\nI ran the tests with Java 1.5 on a Mac Pro quad (2 Intel CPUs, each\ndual core) OS X box with 2 GB RAM.  I give java 1 GB heap (-Xmx1024m).\n",
            "date": "2007-04-03T10:20:22.454+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "A couple more details on the testing: I run java -server to get all\noptimizations in the JVM, and the IO system is a local OS X RAID 0 of\n4 SATA drives.\n\nUsing the above tool I ran an initial set of benchmarks comparing old\n(= Lucene trunk) vs new (= this patch), varying document size (~550\nbytes to ~5,500 bytes to ~55,000 bytes of plain text from Europarl\n\"en\").\n\nFor each document size I run 4 combinations of whether term vectors\nand stored fields are on or off and whether autoCommit is true or\nfalse.  I measure net docs/sec (= total # docs indexed divided by\ntotal time taken), RAM efficiency (= avg # docs flushed with each\nflush divided by RAM buffer size), and avg HEAP RAM usage before each\nflush.\n\nHere are the results for the 10K tokens (= ~55,000 bytes plain text)\nper document:\n\n  20000 DOCS @ ~55,000 bytes plain text\n  RAM = 32 MB\n  NUM THREADS = 1\n  MERGE FACTOR = 10\n\n\n    No term vectors nor stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          20000 docs in 200.3 secs\n          index size = 358M\n\n        new\n          20000 docs in 126.0 secs\n          index size = 356M\n\n        Total Docs/sec:             old    99.8; new   158.7 [   59.0% faster]\n        Docs/MB @ flush:            old    24.2; new    49.1 [  102.5% more]\n        Avg RAM used (MB) @ flush:  old    74.5; new    36.2 [   51.4% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          20000 docs in 202.7 secs\n          index size = 358M\n\n        new\n          20000 docs in 120.0 secs\n          index size = 354M\n\n        Total Docs/sec:             old    98.7; new   166.7 [   69.0% faster]\n        Docs/MB @ flush:            old    24.2; new    48.9 [  101.7% more]\n        Avg RAM used (MB) @ flush:  old    74.3; new    37.0 [   50.2% less]\n\n\n\n    With term vectors (positions + offsets) and 2 small stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          20000 docs in 374.7 secs\n          index size = 1.4G\n\n        new\n          20000 docs in 236.1 secs\n          index size = 1.4G\n\n        Total Docs/sec:             old    53.4; new    84.7 [   58.7% faster]\n        Docs/MB @ flush:            old    10.2; new    49.1 [  382.8% more]\n        Avg RAM used (MB) @ flush:  old   129.3; new    36.6 [   71.7% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          20000 docs in 385.7 secs\n          index size = 1.4G\n\n        new\n          20000 docs in 182.8 secs\n          index size = 1.4G\n\n        Total Docs/sec:             old    51.9; new   109.4 [  111.0% faster]\n        Docs/MB @ flush:            old    10.2; new    48.9 [  380.9% more]\n        Avg RAM used (MB) @ flush:  old    76.0; new    37.3 [   50.9% less]\n\n",
            "date": "2007-04-03T12:11:07.171+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "Here are the results for \"normal\" sized docs (1K tokens = ~5,500 bytes plain text each):\n\n  200000 DOCS @ ~5,500 bytes plain text\n  RAM = 32 MB\n  NUM THREADS = 1\n  MERGE FACTOR = 10\n\n\n    No term vectors nor stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          200000 docs in 397.6 secs\n          index size = 415M\n\n        new\n          200000 docs in 167.5 secs\n          index size = 411M\n\n        Total Docs/sec:             old   503.1; new  1194.1 [  137.3% faster]\n        Docs/MB @ flush:            old    81.6; new   406.2 [  397.6% more]\n        Avg RAM used (MB) @ flush:  old    87.3; new    35.2 [   59.7% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          200000 docs in 394.6 secs\n          index size = 415M\n\n        new\n          200000 docs in 168.4 secs\n          index size = 408M\n\n        Total Docs/sec:             old   506.9; new  1187.7 [  134.3% faster]\n        Docs/MB @ flush:            old    81.6; new   432.2 [  429.4% more]\n        Avg RAM used (MB) @ flush:  old   126.6; new    36.9 [   70.8% less]\n\n\n\n    With term vectors (positions + offsets) and 2 small stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          200000 docs in 754.2 secs\n          index size = 1.7G\n\n        new\n          200000 docs in 304.9 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   265.2; new   656.0 [  147.4% faster]\n        Docs/MB @ flush:            old    46.7; new   406.2 [  769.6% more]\n        Avg RAM used (MB) @ flush:  old    92.9; new    35.2 [   62.1% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          200000 docs in 743.9 secs\n          index size = 1.7G\n\n        new\n          200000 docs in 244.3 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   268.9; new   818.7 [  204.5% faster]\n        Docs/MB @ flush:            old    46.7; new   432.2 [  825.2% more]\n        Avg RAM used (MB) @ flush:  old    93.0; new    36.6 [   60.6% less]\n\n\n\n",
            "date": "2007-04-03T12:15:31.110+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "\nLast is the results for small docs (100 tokens = ~550 bytes plain text each):\n\n  2000000 DOCS @ ~550 bytes plain text\n  RAM = 32 MB\n  NUM THREADS = 1\n  MERGE FACTOR = 10\n\n\n    No term vectors nor stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          2000000 docs in 886.7 secs\n          index size = 438M\n\n        new\n          2000000 docs in 230.5 secs\n          index size = 435M\n\n        Total Docs/sec:             old  2255.6; new  8676.4 [  284.7% faster]\n        Docs/MB @ flush:            old   128.0; new  4194.6 [ 3176.2% more]\n        Avg RAM used (MB) @ flush:  old   107.3; new    37.7 [   64.9% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          2000000 docs in 888.7 secs\n          index size = 438M\n\n        new\n          2000000 docs in 239.6 secs\n          index size = 432M\n\n        Total Docs/sec:             old  2250.5; new  8348.7 [  271.0% faster]\n        Docs/MB @ flush:            old   128.0; new  4146.8 [ 3138.9% more]\n        Avg RAM used (MB) @ flush:  old   108.1; new    38.9 [   64.0% less]\n\n\n\n    With term vectors (positions + offsets) and 2 small stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          2000000 docs in 1480.1 secs\n          index size = 2.1G\n\n        new\n          2000000 docs in 462.0 secs\n          index size = 2.1G\n\n        Total Docs/sec:             old  1351.2; new  4329.3 [  220.4% faster]\n        Docs/MB @ flush:            old    93.1; new  4194.6 [ 4405.7% more]\n        Avg RAM used (MB) @ flush:  old   296.4; new    38.3 [   87.1% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          2000000 docs in 1489.4 secs\n          index size = 2.1G\n\n        new\n          2000000 docs in 347.9 secs\n          index size = 2.1G\n\n        Total Docs/sec:             old  1342.8; new  5749.4 [  328.2% faster]\n        Docs/MB @ flush:            old    93.1; new  4146.8 [ 4354.5% more]\n        Avg RAM used (MB) @ flush:  old   297.1; new    38.6 [   87.0% less]\n\n\n\n  200000 DOCS @ ~5,500 bytes plain text\n\n\n    No term vectors nor stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          200000 docs in 397.6 secs\n          index size = 415M\n\n        new\n          200000 docs in 167.5 secs\n          index size = 411M\n\n        Total Docs/sec:             old   503.1; new  1194.1 [  137.3% faster]\n        Docs/MB @ flush:            old    81.6; new   406.2 [  397.6% more]\n        Avg RAM used (MB) @ flush:  old    87.3; new    35.2 [   59.7% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          200000 docs in 394.6 secs\n          index size = 415M\n\n        new\n          200000 docs in 168.4 secs\n          index size = 408M\n\n        Total Docs/sec:             old   506.9; new  1187.7 [  134.3% faster]\n        Docs/MB @ flush:            old    81.6; new   432.2 [  429.4% more]\n        Avg RAM used (MB) @ flush:  old   126.6; new    36.9 [   70.8% less]\n\n\n\n    With term vectors (positions + offsets) and 2 small stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          200000 docs in 754.2 secs\n          index size = 1.7G\n\n        new\n          200000 docs in 304.9 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   265.2; new   656.0 [  147.4% faster]\n        Docs/MB @ flush:            old    46.7; new   406.2 [  769.6% more]\n        Avg RAM used (MB) @ flush:  old    92.9; new    35.2 [   62.1% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          200000 docs in 743.9 secs\n          index size = 1.7G\n\n        new\n          200000 docs in 244.3 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   268.9; new   818.7 [  204.5% faster]\n        Docs/MB @ flush:            old    46.7; new   432.2 [  825.2% more]\n        Avg RAM used (MB) @ flush:  old    93.0; new    36.6 [   60.6% less]\n\n\n\n",
            "date": "2007-04-03T12:16:20.792+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "A few notes from these results:\n\n  * A real Lucene app won't see these gains because frequently the\n    retrieval of docs from the content source, and the tokenization,\n    take substantial amounts of time whereas for this test I've\n    intentionally minimized the cost of those steps but they are very\n    low for this test because I'm 1) pulling one line at a time from a\n    big text file, and 2) using my simplistic SimpleSpaceAnalyzer\n    which just breaks tokens at the space character.\n\n  * Best speedup is ~4.3X faster, for tiny docs (~550 bytes) with term\n    vectors and stored fields enabled and using autoCommit=false.\n\n  * Least speedup is still ~1.6X faster, for large docs (~55,000\n    bytes) with autoCommit=true.\n\n  * The autoCommit=false cases are a little unfair to the new patch\n    because with the new patch, you get a single-segment (optimized)\n    index in the end, but with existing Lucene trunk, you don't.\n\n  * With term vectors and/or stored fields, autoCommit=false is quite\n    a bit faster with the patch, because we never pay the price to\n    merge them since they are written once.\n\n  * With term vectors and/or stored fields, the new patch has\n    substantially better RAM efficiency.\n\n  * The patch is especially faster and has better RAM efficiency with\n    smaller documents.\n\n  * The actual HEAP RAM usage is quite a bit more stable with the\n    patch, especially with term vectors & stored fields enabled.  I\n    think this is because the patch creates far less garbage for GC to\n    periodically reclaim.  I think this also means you could push your\n    RAM buffer size even higher to get better performance.\n",
            "date": "2007-04-03T12:21:06.549+0000",
            "id": 9
        },
        {
            "author": "Marvin Humphrey",
            "body": "> The actual HEAP RAM usage is quite a bit more \n> stable with the  patch, especially with term vectors \n> & stored fields enabled. I think this is because the \n> patch creates far less garbage for GC to periodically \n> reclaim. I think this also means you could push your \n> RAM buffer size even higher to get better performance. \n\nFor KinoSearch, the sweet spot seems to be a buffer of around 16 MB when benchmarking with the Reuters corpus on my G4 laptop. Larger than that and things actually slow down, unless the buffer is large enough that it never needs flushing. My hypothesis is that RAM fragmentation is slowing down malloc/free.  I'll be interested as to whether you see the same effect.",
            "date": "2007-04-03T14:23:33.562+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "\n>> The actual HEAP RAM usage is quite a bit more\n>> stable with the patch, especially with term vectors\n>> & stored fields enabled. I think this is because the\n>> patch creates far less garbage for GC to periodically\n>> reclaim. I think this also means you could push your\n>> RAM buffer size even higher to get better performance.\n>\n> For KinoSearch, the sweet spot seems to be a buffer of around 16 MB\n> when benchmarking with the Reuters corpus on my G4 laptop. Larger\n> than that and things actually slow down, unless the buffer is large\n> enough that it never needs flushing. My hypothesis is that RAM\n> fragmentation is slowing down malloc/free. I'll be interested as to\n> whether you see the same effect.\n\nInteresting.  OK I will run the benchmark across increasing RAM sizes\nto see where the sweet spot seems to be!\n",
            "date": "2007-04-03T15:33:58.364+0000",
            "id": 11
        },
        {
            "author": "Michael McCandless",
            "body": "\nOK I ran old (trunk) vs new (this patch) with increasing RAM buffer\nsizes up to 96 MB.\n\nI used the \"normal\" sized docs (~5,500 bytes plain text), left stored\nfields and term vectors (positions + offsets) on, and\nautoCommit=false.\n\nHere're the results:\n\nNUM THREADS = 1\nMERGE FACTOR = 10\nWith term vectors (positions + offsets) and 2 small stored fields\nAUTOCOMMIT = false (commit only once at the end)\n\n\n1 MB\n\n  old\n    200000 docs in 862.2 secs\n    index size = 1.7G\n\n  new\n    200000 docs in 297.1 secs\n    index size = 1.7G\n\n  Total Docs/sec:             old   232.0; new   673.2 [  190.2% faster]\n  Docs/MB @ flush:            old    47.2; new   278.4 [  489.6% more]\n  Avg RAM used (MB) @ flush:  old    34.5; new     3.4 [   90.1% less]\n\n\n\n2 MB\n\n  old\n    200000 docs in 828.7 secs\n    index size = 1.7G\n\n  new\n    200000 docs in 279.0 secs\n    index size = 1.7G\n\n  Total Docs/sec:             old   241.3; new   716.8 [  197.0% faster]\n  Docs/MB @ flush:            old    47.0; new   322.4 [  586.7% more]\n  Avg RAM used (MB) @ flush:  old    37.9; new     4.5 [   88.0% less]\n\n\n\n4 MB\n\n  old\n    200000 docs in 840.5 secs\n    index size = 1.7G\n\n  new\n    200000 docs in 260.8 secs\n    index size = 1.7G\n\n  Total Docs/sec:             old   237.9; new   767.0 [  222.3% faster]\n  Docs/MB @ flush:            old    46.8; new   363.1 [  675.4% more]\n  Avg RAM used (MB) @ flush:  old    33.9; new     6.5 [   80.9% less]\n\n\n\n8 MB\n\n  old\n    200000 docs in 678.8 secs\n    index size = 1.7G\n\n  new\n    200000 docs in 248.8 secs\n    index size = 1.7G\n\n  Total Docs/sec:             old   294.6; new   803.7 [  172.8% faster]\n  Docs/MB @ flush:            old    46.8; new   392.4 [  739.1% more]\n  Avg RAM used (MB) @ flush:  old    60.3; new    10.7 [   82.2% less]\n\n\n\n16 MB\n\n  old\n    200000 docs in 660.6 secs\n    index size = 1.7G\n\n  new\n    200000 docs in 247.3 secs\n    index size = 1.7G\n\n  Total Docs/sec:             old   302.8; new   808.7 [  167.1% faster]\n  Docs/MB @ flush:            old    46.7; new   415.4 [  788.8% more]\n  Avg RAM used (MB) @ flush:  old    47.1; new    19.2 [   59.3% less]\n\n\n\n24 MB\n\n  old\n    200000 docs in 658.1 secs\n    index size = 1.7G\n\n  new\n    200000 docs in 243.0 secs\n    index size = 1.7G\n\n  Total Docs/sec:             old   303.9; new   823.0 [  170.8% faster]\n  Docs/MB @ flush:            old    46.7; new   430.9 [  822.2% more]\n  Avg RAM used (MB) @ flush:  old    70.0; new    27.5 [   60.8% less]\n\n\n\n32 MB\n\n  old\n    200000 docs in 714.2 secs\n    index size = 1.7G\n\n  new\n    200000 docs in 239.2 secs\n    index size = 1.7G\n\n  Total Docs/sec:             old   280.0; new   836.0 [  198.5% faster]\n  Docs/MB @ flush:            old    46.7; new   432.2 [  825.2% more]\n  Avg RAM used (MB) @ flush:  old    92.5; new    36.7 [   60.3% less]\n\n\n\n48 MB\n\n  old\n    200000 docs in 640.3 secs\n    index size = 1.7G\n\n  new\n    200000 docs in 236.0 secs\n    index size = 1.7G\n\n  Total Docs/sec:             old   312.4; new   847.5 [  171.3% faster]\n  Docs/MB @ flush:            old    46.7; new   438.5 [  838.8% more]\n  Avg RAM used (MB) @ flush:  old   138.9; new    52.8 [   62.0% less]\n\n\n\n64 MB\n\n  old\n    200000 docs in 649.3 secs\n    index size = 1.7G\n\n  new\n    200000 docs in 238.3 secs\n    index size = 1.7G\n\n  Total Docs/sec:             old   308.0; new   839.3 [  172.5% faster]\n  Docs/MB @ flush:            old    46.7; new   441.3 [  844.7% more]\n  Avg RAM used (MB) @ flush:  old   302.6; new    72.7 [   76.0% less]\n\n\n\n80 MB\n\n  old\n    200000 docs in 670.2 secs\n    index size = 1.7G\n\n  new\n    200000 docs in 227.2 secs\n    index size = 1.7G\n\n  Total Docs/sec:             old   298.4; new   880.5 [  195.0% faster]\n  Docs/MB @ flush:            old    46.7; new   446.2 [  855.2% more]\n  Avg RAM used (MB) @ flush:  old   231.7; new    94.3 [   59.3% less]\n\n\n\n96 MB\n\n  old\n    200000 docs in 683.4 secs\n    index size = 1.7G\n\n  new\n    200000 docs in 226.8 secs\n    index size = 1.7G\n\n  Total Docs/sec:             old   292.7; new   882.0 [  201.4% faster]\n  Docs/MB @ flush:            old    46.7; new   448.0 [  859.1% more]\n  Avg RAM used (MB) @ flush:  old   274.5; new   112.7 [   59.0% less]\n\n\nSome observations:\n\n  * Remember the test is already biased against \"new\" because with the\n    patch you get an optimized index in the end but with \"old\" you\n    don't.\n\n  * Sweet spot for old (trunk) seems to be 48 MB: that is the peak\n    docs/sec @ 312.4.\n\n  * New (with patch) seems to just get faster the more memory you give\n    it, though gradually.  The peak was 96 MB (the largest I ran).  So\n    no sweet spot (or maybe I need to give more memory, but, above 96\n    MB the trunk was starting to swap on my test env).\n\n  * New gets better and better RAM efficiency, the more RAM you give.\n    This makes sense: it's better able to compress the terms dict, the\n    more docs are merged in RAM before having to flush to disk.  I\n    would also expect this curve to be somewhat content dependent.\n",
            "date": "2007-04-05T13:21:55.288+0000",
            "id": 12
        },
        {
            "author": "Michael McCandless",
            "body": "I attached a new iteration of the patch.  It's quite different from\nthe last patch.\n\nAfter discussion on java-dev last time, I decided to retry the\n\"persistent hash\" approach, where the Postings hash lasts across many\ndocs and then a single flush produces a partial segment containing all\nof those docs.  This is in contrast to the previous approach where\neach doc makes its own segment and then they are merged.\n\nIt turns out this is even faster than my previous approach, especially\nfor smaller docs and especially when term vectors are off (because no\nquicksort() is needed until the segment is flushed).  I will attach\nnew benchmark results.\n\nOther changes:\n\n  * Changed my benchmarking tool / testing (IndexLineFiles):\n\n    - I turned off compound file (to reduce time NOT spent on\n      indexing).\n\n    - I noticed I was not downcasing the terms, so I fixed that\n\n    - I now do my own line processing to reduce GC cost of\n      \"BufferedReader.readLine\" (to reduct time NOT spent on\n      indexing).\n\n  * Norms now properly flush to disk in the autoCommit=false case\n\n  * All unit tests pass except disk full\n\n  * I turned on asserts for unit tests (jvm arg -ea added to junit ant\n    task).  I think we should use asserts when running tests.  I have\n    quite a few asserts now.\n\nWith this new approach, as I process each term in the document I\nimmediately write the prox/freq in their compact (vints) format into\nshared byte[] buffers, rather than accumulating int[] arrays that then\nneed to be re-processed into the vint encoding.  This speeds things up\nbecause we don't double-process the postings.  It also uses less\nper-document RAM overhead because intermediate postings are stored as\nvints not as ints.\n\nWhen enough RAM is used by the Posting entries plus the byte[]\nbuffers, I flush them to a partial RAM segment.  When enough of these\nRAM segments have accumulated I flush to a real Lucene segment\n(autoCommit=true) or to on-disk partial segments (autoCommit=false)\nwhich are then merged in the end to create a real Lucene segment.\n",
            "date": "2007-04-30T10:39:19.367+0000",
            "id": 13
        },
        {
            "author": "Marvin Humphrey",
            "body": "How are you writing the frq data in compressed format?  The  works fine for\nprx data, because the deltas are all within a single doc -- but for  the freq\ndata, the deltas are tied up in doc num deltas, so you have to decompress it\nwhen performing merges.  \n\nTo continue our discussion from java-dev... \n\n * I haven't been able to come up with a file format tweak that \n   gets around this doc-num-delta-decompression problem to enhance the speed\n   of frq data merging. I toyed with splitting off the freq from the\n   doc_delta, at the price of increasing the file size in the common case of\n   freq == 1, but went back to the old design.  It's not worth the size\n   increase for what's at best a minor indexing speedup.\n * I've added a custom MemoryPool class to KS which grabs memory in 1 meg\n   chunks, allows resizing (downwards) of only the last allocation, and can\n   only release everything at once.  From one of these pools, I'm allocating\n   RawPosting objects, each of which is a doc_num, a freq, the term_text, and\n   the pre-packed prx data (which varies based on which Posting subclass\n   created the RawPosting object).  I haven't got things 100% stable yet, but\n   preliminary results seem to indicate that this technique, which is a riff\n   on your persistent arrays, improves indexing speed by about 15%.",
            "date": "2007-04-30T11:44:28.278+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "\n> How are you writing the frq data in compressed format? The works fine for\n> prx data, because the deltas are all within a single doc -- but for the freq\n> data, the deltas are tied up in doc num deltas, so you have to decompress it\n> when performing merges.\n\nFor each Posting I keep track of the last docID that its term occurred\nin; when this differs from the current docID I record the \"delta code\"\nthat needs to be written and then I later write it with the final freq\nfor this document.\n\n> * I haven't been able to come up with a file format tweak that\n>   gets around this doc-num-delta-decompression problem to enhance the speed\n>   of frq data merging. I toyed with splitting off the freq from the\n>   doc_delta, at the price of increasing the file size in the common case of\n>   freq == 1, but went back to the old design. It's not worth the size\n>   increase for what's at best a minor indexing speedup.\n\nI'm just doing the \"stitching\" approach here: it's only the very first\ndocCode (& freq when freq==1) that must be re-encoded on merging.  The\none catch is you must store the last docID of the previous segment so\nyou can compute the new delta at the boundary.  Then I do a raw\n\"copyBytes\" for the remainder of the freq postings.\n\nNote that I'm only doing this for the \"internal\" merges (of partial\nRAM segments and flushed partial segments) I do before creating a real\nLucene segment.  I haven't changed how the \"normal\" Lucene segment\nmerging works (though I think we should look into it -- I opened a\nseparate issue): it still re-interprets and then re-encodes all\ndocID/freq's.\n\n> * I've added a custom MemoryPool class to KS which grabs memory in 1 meg\n>   chunks, allows resizing (downwards) of only the last allocation, and can\n>   only release everything at once. From one of these pools, I'm allocating\n>    RawPosting objects, each of which is a doc_num, a freq, the term_text, and\n>   the pre-packed prx data (which varies based on which Posting subclass\n>   created the RawPosting object). I haven't got things 100% stable yet, but\n>   preliminary results seem to indicate that this technique, which is a riff\n>   on your persistent arrays, improves indexing speed by about 15%.\n\nFabulous!!\n\nI think it's the custom memory management I'm doing with slices into\nshared byte[] arrays for the postings that made the persistent hash\napproach work well, this time around (when I had previously tried this\nit was slower).\n",
            "date": "2007-04-30T12:14:37.980+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "Results with the above patch:\n\nRAM = 32 MB\nNUM THREADS = 1\nMERGE FACTOR = 10\n\n\n  2000000 DOCS @ ~550 bytes plain text\n\n\n    No term vectors nor stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          2000000 docs in 782.8 secs\n          index size = 436M\n\n        new\n          2000000 docs in 93.4 secs\n          index size = 430M\n\n        Total Docs/sec:             old  2554.8; new 21421.1 [  738.5% faster]\n        Docs/MB @ flush:            old   128.0; new  4058.0 [ 3069.6% more]\n        Avg RAM used (MB) @ flush:  old   140.2; new    38.0 [   72.9% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          2000000 docs in 780.2 secs\n          index size = 436M\n\n        new\n          2000000 docs in 90.6 secs\n          index size = 427M\n\n        Total Docs/sec:             old  2563.3; new 22086.8 [  761.7% faster]\n        Docs/MB @ flush:            old   128.0; new  4118.4 [ 3116.7% more]\n        Avg RAM used (MB) @ flush:  old   144.6; new    36.4 [   74.8% less]\n\n\n\n    With term vectors (positions + offsets) and 2 small stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          2000000 docs in 1227.6 secs\n          index size = 2.1G\n\n        new\n          2000000 docs in 559.8 secs\n          index size = 2.1G\n\n        Total Docs/sec:             old  1629.2; new  3572.5 [  119.3% faster]\n        Docs/MB @ flush:            old    93.1; new  4058.0 [ 4259.1% more]\n        Avg RAM used (MB) @ flush:  old   193.4; new    38.5 [   80.1% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          2000000 docs in 1229.2 secs\n          index size = 2.1G\n\n        new\n          2000000 docs in 241.0 secs\n          index size = 2.1G\n\n        Total Docs/sec:             old  1627.0; new  8300.0 [  410.1% faster]\n        Docs/MB @ flush:            old    93.1; new  4118.4 [ 4323.9% more]\n        Avg RAM used (MB) @ flush:  old   150.5; new    38.4 [   74.5% less]\n\n\n\n  200000 DOCS @ ~5,500 bytes plain text\n\n\n    No term vectors nor stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          200000 docs in 352.2 secs\n          index size = 406M\n\n        new\n          200000 docs in 86.4 secs\n          index size = 403M\n\n        Total Docs/sec:             old   567.9; new  2313.7 [  307.4% faster]\n        Docs/MB @ flush:            old    83.5; new   420.0 [  402.7% more]\n        Avg RAM used (MB) @ flush:  old    76.8; new    38.1 [   50.4% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          200000 docs in 399.2 secs\n          index size = 406M\n\n        new\n          200000 docs in 89.6 secs\n          index size = 400M\n\n        Total Docs/sec:             old   501.0; new  2231.0 [  345.3% faster]\n        Docs/MB @ flush:            old    83.5; new   422.6 [  405.8% more]\n        Avg RAM used (MB) @ flush:  old    76.7; new    36.2 [   52.7% less]\n\n\n\n    With term vectors (positions + offsets) and 2 small stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          200000 docs in 594.2 secs\n          index size = 1.7G\n\n        new\n          200000 docs in 229.0 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   336.6; new   873.3 [  159.5% faster]\n        Docs/MB @ flush:            old    47.9; new   420.0 [  776.9% more]\n        Avg RAM used (MB) @ flush:  old   157.8; new    38.0 [   75.9% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          200000 docs in 605.1 secs\n          index size = 1.7G\n\n        new\n          200000 docs in 181.3 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   330.5; new  1103.1 [  233.7% faster]\n        Docs/MB @ flush:            old    47.9; new   422.6 [  782.2% more]\n        Avg RAM used (MB) @ flush:  old   132.0; new    37.1 [   71.9% less]\n\n\n\n  20000 DOCS @ ~55,000 bytes plain text\n\n\n    No term vectors nor stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          20000 docs in 180.8 secs\n          index size = 350M\n\n        new\n          20000 docs in 79.1 secs\n          index size = 349M\n\n        Total Docs/sec:             old   110.6; new   252.8 [  128.5% faster]\n        Docs/MB @ flush:            old    25.0; new    46.8 [   87.0% more]\n        Avg RAM used (MB) @ flush:  old   112.2; new    44.3 [   60.5% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          20000 docs in 180.1 secs\n          index size = 350M\n\n        new\n          20000 docs in 75.9 secs\n          index size = 347M\n\n        Total Docs/sec:             old   111.0; new   263.5 [  137.3% faster]\n        Docs/MB @ flush:            old    25.0; new    47.5 [   89.7% more]\n        Avg RAM used (MB) @ flush:  old   111.1; new    42.5 [   61.7% less]\n\n\n\n    With term vectors (positions + offsets) and 2 small stored fields\n\n      AUTOCOMMIT = true (commit whenever RAM is full)\n\n        old\n          20000 docs in 323.1 secs\n          index size = 1.4G\n\n        new\n          20000 docs in 183.9 secs\n          index size = 1.4G\n\n        Total Docs/sec:             old    61.9; new   108.7 [   75.7% faster]\n        Docs/MB @ flush:            old    10.4; new    46.8 [  348.3% more]\n        Avg RAM used (MB) @ flush:  old    74.2; new    44.9 [   39.5% less]\n\n\n      AUTOCOMMIT = false (commit only once at the end)\n\n        old\n          20000 docs in 323.5 secs\n          index size = 1.4G\n\n        new\n          20000 docs in 135.6 secs\n          index size = 1.4G\n\n        Total Docs/sec:             old    61.8; new   147.5 [  138.5% faster]\n        Docs/MB @ flush:            old    10.4; new    47.5 [  354.8% more]\n        Avg RAM used (MB) @ flush:  old    74.3; new    42.9 [   42.2% less]\n\n",
            "date": "2007-04-30T13:49:00.272+0000",
            "id": 16
        },
        {
            "author": "Yonik Seeley",
            "body": "How does this work with pending deletes?\nI assume that if autocommit is false, then you need to wait until the end when you get a real lucene segment to delete the pending terms?\n\nAlso, how has the merge policy (or index invariants) of lucene segments changed?\nIf autocommit is off, then you wait until the end to create a big lucene segment.  This new segment may be much larger than segments to it's \"left\".  I suppose the idea of merging rightmost segments should just be dropped in favor of merging the smallest adjacent segments?  Sorry if this has already been covered... as I said, I'm trying to follow along at a high level.\n",
            "date": "2007-04-30T14:10:35.223+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "> How does this work with pending deletes?\n> I assume that if autocommit is false, then you need to wait until the end when you get a real lucene segment to delete the pending terms?\n\nYes, all of this sits \"below\" the pending deletes layer since this\nchange writes a single segment either when RAM is full\n(autoCommit=true) or when writer is closed (autoCommit=false).  Then\nthe deletes get applied like normal (I haven't changed that part).\n\n> Also, how has the merge policy (or index invariants) of lucene segments changed?\n> If autocommit is off, then you wait until the end to create a big lucene segment.  This new segment may be much larger than segments to it's \"left\".  I suppose the idea of merging rightmost segments should just be dropped in favor of merging the smallest adjacent segments?  Sorry if this has already been covered... as I said, I'm trying to follow along at a high level.\n\nHas not been covered, and as usual these are excellent questions\nYonik!\n\nI haven't yet changed anything about merge policy, but you're right\nthe current invariants won't hold anymore.  In fact they already don't\nhold if you \"flush by RAM\" now (APIs are exposed in 2.1 to let you do\nthis).  So we need to do something.\n\nI like your idea to relax merge policy (& invariants) to allow\n\"merging of any adjacent segments\" (not just rightmost ones) and then\nmake the policy merge the smallest ones / most similarly sized ones,\nmeasuring size by net # bytes in the segment.  This would preserve the\n\"docID monotonicity invariance\".\n\nIf we take that approach then it would automatically resolve\nLUCENE-845 as well (which would otherwise block this issue).\n",
            "date": "2007-04-30T18:54:27.421+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "Attached latest patch.\n\nI'm now working towards simplify & cleaning up the code & design:\neliminated dead code leftover from the previous iterations, use\nexisting RAMFile instead of my own new class, refactored\nduplicate/confusing code, added comments, etc. It's getting closer to\na committable state but still has a ways to go.\n\nI also renamed the class from MultiDocumentWriter to DocumentsWriter.\n\nTo summarize the current design:\n\n  1. Write stored fields & term vectors to files in the Directory\n     immediately (don't buffer these in RAM).\n\n  2. Write freq & prox postings to RAM directly as a byte stream\n     instead of first pass as int[] and then second pass as a byte\n     stream.  This single-pass instead of double-pass is a big\n     savings.  I use slices into shared byte[] arrays to efficiently\n     allocate bytes to the postings the need them.\n\n  3. Build Postings hash that holds the Postings for many documents at\n     once instead of a single doc, keyed by unique term.  Not tearing\n     down & rebuilding the Postings hash w/ every doc saves alot of\n     time.  Also when term vectors are off this saves quicksort for\n     every doc and this gives very good performance gain.\n\n     When the Postings hash is full (used up the allowed RAM usage) I\n     then create a real Lucene segment when autoCommit=true, else a\n     \"partial segment\".\n\n  4. Use my own \"partial segment\" format that differs from Lucene's\n     normal segments in that it is optimized for merging (and unusable\n     for searching).  This format, and the merger I created to work\n     with this format, performs merging mostly by copying blocks of\n     bytes instead of reinterpreting every vInt in each Postings list.\n     These partial segments are are only created when IndexWriter has\n     autoCommit=false, and then on commit they are merged into the\n     real Lucene segment format.\n\n  5. Reuse the Posting, PostingVector, char[] and byte[] objects that\n     are used by the Postings hash.\n\nI plan to keep simplifying the design & implementation.  Specifically,\nI'm going to test removing #4 above entirely (using my own \"partial\nsegment\" format that's optimized for merging not searching).\n\nWhile doing this may give back some of the performance gains, that\ncode is the source of much added complexity in the patch, and, it\nduplicates the current SegmentMerger code.  It was more necessary\nbefore (when we would merge thousands of single-doc segments in\nmemory) but now that each segment contains many docs I think we are no\nlonger gaining as much performance from it.\n\nI plan instead to write all segments in the \"real\" Lucene segment\nformat and use the current SegmentMerger, possibly w/ some small\nchanges, to do the merges even when autoCommit=false.  Since we have\nanother issue (LUCENE-856) to optimize segment merging I can carry\nover any optimizations that we may want to keep into that issue.  If\nthis doesn't lose much performance it will make the approach here even\nsimpler.\n",
            "date": "2007-05-21T18:14:40.872+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "Latest working patch attached.\n\nI've cutover to using Lucene's normal segment merging for all merging\n(ie, I no longer use a different merge-efficient format for segments\nwhen autoCommit=false); this has substantially simplified the code.\n\nAll unit tests pass except disk-full test and certain contrib tests\n(gdata-server, lucli, similarity, wordnet) that I think I'm not\ncausing.\n\nOther changes:\n\n  * Consolidated flushing of a new segment back into IndexWriter\n    (previously DocumentsWriter would do its own flushing when\n    autoCommit=false).\n\n    I would also like to consolidate merging entirely into\n    IndexWriter; right now DocumentsWriter does its own merging of the\n    flushed segments when autoCommit=false (this is because those\n    segments are \"partial\" meaning they do not have their own stored\n    fields or term vectors).  I'm trying to find a clean way to do\n    this...\n\n  * Thread concurrency now works: each thread writes into a separate\n    Postings hash (up until a limit (currently 5) at which point the\n    threads share the Postings hashes) and then when flushing the\n    segment I merge the docIDs together. I flush when the total RAM\n    used across threads is over the limit.  I ran a test comparing\n    thread concurrency on current trunk vs this patch, which I'll post\n    next.\n\n  * Reduced bytes used per-unique-term to be lower than current\n    Lucene.  This means the worst-case document (many terms, all of\n    which are unique) should use less RAM overall than Lucene trunk\n    does.\n\n  * Added some new unit test cases; added missing \"writer.close()\" to\n    one of the contrib tests.\n\n  * Cleanup, comments, etc.  I think the code is getting more\n    \"approachable\" now.\n",
            "date": "2007-06-08T13:31:03.015+0000",
            "id": 20
        },
        {
            "author": "Michael McCandless",
            "body": "I ran a benchmark using more than 1 thread to do indexing, in order to\ntest & compare concurrency of trunk and the patch.  The test is the\nsame as above, and runs on a 4 core Mac Pro (OS X) box with 4 drive\nRAID 0 IO system.\n\nHere are the raw results:\n\nDOCS = ~5,500 bytes plain text\nRAM = 32 MB\nMERGE FACTOR = 10\nWith term vectors (positions + offsets) and 2 small stored fields\nAUTOCOMMIT = false (commit only once at the end)\n\nNUM THREADS = 1\n\n        new\n          200000 docs in 172.3 secs\n          index size = 1.7G\n\n        old\n          200000 docs in 539.5 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   370.7; new  1161.0 [  213.2% faster]\n        Docs/MB @ flush:            old    47.9; new   334.6 [  598.7% more]\n        Avg RAM used (MB) @ flush:  old   131.9; new    33.1 [   74.9% less]\n\n\nNUM THREADS = 2\n\n        new\n          200001 docs in 130.8 secs\n          index size = 1.7G\n\n        old\n          200001 docs in 452.8 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   441.7; new  1529.3 [  246.2% faster]\n        Docs/MB @ flush:            old    47.9; new   301.5 [  529.7% more]\n        Avg RAM used (MB) @ flush:  old   226.1; new    35.2 [   84.4% less]\n\n\nNUM THREADS = 3\n\n        new\n          200002 docs in 105.4 secs\n          index size = 1.7G\n\n        old\n          200002 docs in 428.4 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   466.8; new  1897.9 [  306.6% faster]\n        Docs/MB @ flush:            old    47.9; new   277.8 [  480.2% more]\n        Avg RAM used (MB) @ flush:  old   289.8; new    37.0 [   87.2% less]\n\n\nNUM THREADS = 4\n\n        new\n          200003 docs in 104.8 secs\n          index size = 1.7G\n\n        old\n          200003 docs in 440.4 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   454.1; new  1908.5 [  320.3% faster]\n        Docs/MB @ flush:            old    47.9; new   259.9 [  442.9% more]\n        Avg RAM used (MB) @ flush:  old   293.7; new    37.1 [   87.3% less]\n\n\nNUM THREADS = 5\n\n        new\n          200004 docs in 99.5 secs\n          index size = 1.7G\n\n        old\n          200004 docs in 425.0 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   470.6; new  2010.5 [  327.2% faster]\n        Docs/MB @ flush:            old    47.9; new   245.3 [  412.6% more]\n        Avg RAM used (MB) @ flush:  old   390.9; new    38.3 [   90.2% less]\n\n\nNUM THREADS = 6\n\n        new\n          200005 docs in 106.3 secs\n          index size = 1.7G\n\n        old\n          200005 docs in 427.1 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   468.2; new  1882.3 [  302.0% faster]\n        Docs/MB @ flush:            old    47.8; new   248.5 [  419.3% more]\n        Avg RAM used (MB) @ flush:  old   340.9; new    38.7 [   88.6% less]\n\n\nNUM THREADS = 7\n\n        new\n          200006 docs in 106.1 secs\n          index size = 1.7G\n\n        old\n          200006 docs in 435.2 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   459.6; new  1885.3 [  310.2% faster]\n        Docs/MB @ flush:            old    47.8; new   248.7 [  420.0% more]\n        Avg RAM used (MB) @ flush:  old   408.6; new    39.1 [   90.4% less]\n\n\nNUM THREADS = 8\n\n        new\n          200007 docs in 109.0 secs\n          index size = 1.7G\n\n        old\n          200007 docs in 469.2 secs\n          index size = 1.7G\n\n        Total Docs/sec:             old   426.3; new  1835.2 [  330.5% faster]\n        Docs/MB @ flush:            old    47.8; new   251.3 [  425.5% more]\n        Avg RAM used (MB) @ flush:  old   448.9; new    39.0 [   91.3% less]\n\n\n\nSome quick comments:\n\n  * Both trunk & the patch show speedups if you use more than 1 thread\n    to do indexing.  This is expected since the machine has concurrency. \n\n  * The biggest speedup is from 1->2 threads but still good gains from\n    2->5 threads.\n\n  * Best seems to be 5 threads.\n\n  * The patch allows better concurrency: relatively speaking it speeds\n    up faster than the trunk (the % faster increases as we add\n    threads) as you increase # threads.  I think this makes sense\n    because we flush less often with the patch, and, flushing is time\n    consuming and single threaded.\n",
            "date": "2007-06-08T13:33:30.778+0000",
            "id": 21
        },
        {
            "author": "Michael McCandless",
            "body": "Attached latest patch.\n\nI think this patch is ready to commit.  I will let it sit for a while\nso people can review it.\n\nWe still need to do LUCENE-845 before it can be committed as is.\n\nHowever one option instead would be to commit this patch, but leave\nIndexWriter flushing by doc count by default and then later switch it\nto flush by net RAM usage once LUCENE-845 is done.  I like this option\nbest.\n\nAll tests pass (I've re-enabled the disk full tests and fixed error\nhandling so they now pass) on Windows XP, Debian Linux and OS X.\n\nSummary of the changes in this rev:\n\n  * Finished cleaning up & commenting code\n\n  * Exception handling: if there is a disk full or any other exception\n    while adding a document or flushing then the index is rolled back\n    to the last commit point.\n\n  * Added more unit tests\n\n  * Removed my profiling tool from the patch (not intended to be\n    committed)\n\n  * Fixed a thread safety issue where if you flush by doc count you\n    would sometimes get more than the doc count at flush than you\n    requested.  I moved the thread synchronization for determining\n    flush time down into DocumentsWriter.\n\n  * Also fixed thread safety of calling flush with one thread while\n    other threads are still adding documents.\n\n  * The biggest change is: absorbed all merging logic back into\n    IndexWriter.\n\n    Previously in DocumentsWriter I was tracking my own\n    flushed/partial segments and merging them on my own (but using\n    SegmentMerger).  This makes DocumentsWriter much simpler: now its\n    sole purpose is to gather added docs and write a new segment.\n\n    This turns out to be a big win:\n\n      - Code is much simpler (no duplication of \"merging\"\n        policy/logic)\n\n      - 21-25% additional performance gain for autoCommit=false case\n        when stored fields & vectors are used\n\n      - IndexWriter.close() no longer takes an unexpected long time to\n        close in autoCommit=false case\n\n    However I had to make a change to the index format to do this.\n    The basic idea is to allow multiple segments to share access to\n    the \"doc store\" (stored fields, vectors) index files.\n\n    The change is quite simple: FieldsReader/VectorsReader are now\n    told the doc offset that they should start from when seeking in\n    the index stream (this info is stored in SegmentInfo).  When\n    merging segments we don't merge the \"doc store\" files when all\n    segments are sharing the same ones (big performance gain), else,\n    we make a private copy of the \"doc store\" files (ie as segments\n    normally are on the trunk today).\n\n    The change is fully backwards compatible (I added a test case to\n    the backwards compatibility unit test to be sure) and the change\n    is only used when autoCommit=false.\n\n    When autoCommit=false, the writer will append stored fields /\n    vectors to a single set of files even though it is flushing normal\n    segments whenever RAM is full.  These normal segments all refer to\n    the single shared set of \"doc store\" files.  Then when segments\n    are merged, the newly merged segment has its own \"private\" doc\n    stores again.  So the sharing only occurs for the \"level 0\"\n    segments.\n\n    I still need to update fileformats doc with this change.\n",
            "date": "2007-06-15T19:00:53.772+0000",
            "id": 22
        },
        {
            "author": "Yonik Seeley",
            "body": "> When merging segments we don't merge the \"doc store\" files when all segments are sharing the same ones (big performance gain), \n\nIs this only in the case where the segments have no deleted docs?\n",
            "date": "2007-06-15T21:26:33.486+0000",
            "id": 23
        },
        {
            "author": "Michael McCandless",
            "body": "> > When merging segments we don't merge the \"doc store\" files when all segments are sharing the same ones (big performance gain),\n> \n> Is this only in the case where the segments have no deleted docs? \n\nRight.  Also the segments must be contiguous which the current merge\npolicy ensures but future merge policies may not.\n",
            "date": "2007-06-16T01:00:45.903+0000",
            "id": 24
        },
        {
            "author": "Michael McCandless",
            "body": "OK, I attached a new version (take9) of the patch that reverts back to\nthe default of \"flush after every 10 documents added\" in IndexWriter.\nThis removes the dependency on LUCENE-845.\n\nHowever, I still think we should later (once LUCENE-845 is done)\ndefault IndexWriter to flush by RAM usage since this will generally\ngive the best \"out of the box\" performance.  I will open a separate\nissue to change the default after this issue is resolved.\n",
            "date": "2007-06-18T13:56:56.704+0000",
            "id": 25
        },
        {
            "author": "Steven Parkes",
            "body": "I've started looking at this, what it would take to merge with the merge policy stuff (LUCENE-847). Noticed that there are a couple of test failures?",
            "date": "2007-06-20T15:44:39.657+0000",
            "id": 26
        },
        {
            "author": "Michael McCandless",
            "body": "Oh, were the test failures only in the TestBackwardsCompatibility?\n\nBecause I changed the index file format, I added 2 more ZIP files to\nthat unit test, but, \"svn diff\" doesn't pick up the new zip files.  So\nI'm attaching them.  Can you pull off these zip files into your\nsrc/test/org/apache/lucene/index and test again?  Thanks.\n\n",
            "date": "2007-06-20T15:58:57.684+0000",
            "id": 27
        },
        {
            "author": "Steven Parkes",
            "body": "Yeah, that was it.\n\nI'll be delving more into the code as I try to figure out how it will dove tail with the merge policy factoring.",
            "date": "2007-06-20T17:37:19.381+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "> Yeah, that was it.\n\nPhew!\n\n> I'll be delving more into the code as I try to figure out how it will\n> dove tail with the merge policy factoring.\n\nOK, thanks.  I am very eager to get some other eyeballs looking for\nissues with this patch!\n\nI *think* this patch and the merge policy refactoring should be fairly\nseparate.\n\nWith this patch, \"flushing\" RAM -> Lucene segment is no longer a\n\"mergeSegments\" call which I think simplifies IndexWriter.  Previously\nmergeSegments had lots of extra logic to tell if it was merging RAM\nsegments (= a flush) vs merging \"real\" segments but now it's simpler\nbecause mergeSegments really only merges segments.\n",
            "date": "2007-06-20T23:25:49.492+0000",
            "id": 29
        },
        {
            "author": "Michael Busch",
            "body": "Hi Mike,\n\nmy first comment on this patch is: Impressive!\n\nIt's also quite overwhelming at the beginning, but I'm trying to dig into it. I'll probably have more questions, here's the first one:\n\nDoes DocumentsWriter also solve the problem DocumentWriter had before LUCENE-880? I believe the answer is yes. Even though you close the TokenStreams in the finally clause of invertField() like DocumentWriter did before 880 this is safe, because addPosition() serializes the term strings and payload bytes into the posting hash table right away. Is that right?",
            "date": "2007-06-21T03:05:36.528+0000",
            "id": 30
        },
        {
            "author": "Michael Busch",
            "body": "Mike,\n\nthe benchmarks you run focus on measuring the pure indexing performance. I think it would be interesting to know how big the speedup is in real-life scenarios, i. e. with StandardAnalyzer and maybe even HTML parsing? For sure the speedup will be less, but it should still be a significant improvement. Did you run those kinds of benchmarks already?",
            "date": "2007-06-21T06:51:13.488+0000",
            "id": 31
        },
        {
            "author": "Michael McCandless",
            "body": "> Does DocumentsWriter also solve the problem DocumentWriter had\n> before LUCENE-880? I believe the answer is yes. Even though you\n> close the TokenStreams in the finally clause of invertField() like\n> DocumentWriter did before 880 this is safe, because addPosition()\n> serializes the term strings and payload bytes into the posting hash\n> table right away. Is that right?\n\nThat's right.  When I merged in the fix for LUCENE-880, I realized\nwith this patch it's fine to close the token stream immediately after\nprocessing all of its tokens because everything about the token stream\nhas been \"absorbed\" into postings hash.\n\n> the benchmarks you run focus on measuring the pure indexing\n> performance. I think it would be interesting to know how big the\n> speedup is in real-life scenarios, i. e. with StandardAnalyzer and\n> maybe even HTML parsing? For sure the speedup will be less, but it\n> should still be a significant improvement. Did you run those kinds\n> of benchmarks already?\n\nGood question ... I haven't measured the performance cost of using\nStandardAnalyzer or HTML parsing but I will test & post back.",
            "date": "2007-06-21T09:35:44.319+0000",
            "id": 32
        },
        {
            "author": "Michael McCandless",
            "body": "OK I ran tests comparing analyzer performance.\n\nIt's the same test framework as above, using the ~5,500 byte Europarl\ndocs with autoCommit=true, 32 MB RAM buffer, no stored fields nor\nvectors, and CFS=false, indexing 200,000 documents.\n\nThe SimpleSpaceAnalyzer is my own whitespace analyzer that minimizes\nGC cost by not allocating a Term or String for every token in every\ndocument.\n\nEach run is best time of 2 runs:\n\n  ANALYZER            PATCH (sec) TRUNK (sec)  SPEEDUP\n  SimpleSpaceAnalyzer  79.0       326.5        4.1 X\n  StandardAnalyzer    449.0       674.1        1.5 X\n  WhitespaceAnalyzer  104.0       338.9        3.3 X\n  SimpleAnalyzer      104.7       328.0        3.1 X\n\nStandardAnalyzer is definiteely rather time consuming!\n",
            "date": "2007-06-21T14:00:19.389+0000",
            "id": 33
        },
        {
            "author": "Michael Busch",
            "body": "> OK I ran tests comparing analyzer performance.\n\nThanks for the numbers Mike. Yes the gain is less with StandardAnalyzer\nbut 1.5X faster is still very good!\n\n\nI have some question about the extensibility of your code. For flexible\nindexing we want to be able in the future to implement different posting\nformats and we might even want to allow our users to implement own \nposting formats.\n\nWhen I implemented multi-level skipping I tried to keep this in mind. \nTherefore I put most of the functionality in the two abstract classes\nMultiLevelSkipListReader/Writer. Subclasses implement the actual format\nof the skip data. I think with this design it should be quite easy to\nimplement different formats in the future while limiting the code\ncomplexity.\n\nWith the old DocumentWriter I think this is quite simple to do too by\nadding a class like PostingListWriter, where subclasses define the actual \nformat (because DocumentWriter is so simple).\n\nDo you think your code is easily extensible in this regard? I'm \nwondering because of all the optimizations you're doing like e. g.\nsharing byte arrays. But I'm certainly not familiar enough with your code \nyet, so I'm only guessing here.\n",
            "date": "2007-06-21T17:08:52.600+0000",
            "id": 34
        },
        {
            "author": "Michael McCandless",
            "body": "> Do you think your code is easily extensible in this regard? I'm \n> wondering because of all the optimizations you're doing like e. g.\n> sharing byte arrays. But I'm certainly not familiar enough with your code \n> yet, so I'm only guessing here.\n\nGood question!\n\nDocumentsWriter is definitely more complex than DocumentWriter, but it\ndoesn't prevent extensibility and I think will work very well when we\ndo flexible indexing.\n\nThe patch now has dedicated methods for writing into the freq/prox/etc\nstreams ('writeFreqByte', 'writeFreqVInt', 'writeProxByte',\n'writeProxVInt', etc.), but, this could easily be changed to instead\nuse true IndexOutput streams.  This would then hide all details of\nshared byte arrays from whoever is doing the writing.\n\nThe way I roughly see flexible indexing working in the future is\nDocumentsWriter will be responsible for keeping track of unique terms\nseen (in its hash table), holding the Posting instance (which could be\nsubclassed in the future) for each term, flushing a real segment when\nfull, handling shared byte arrays, etc.  Ie all the \"infrastructure\".\n\nBut then the specific logic of what bytes are written into which\nstreams (freq/prox/vectors/others) will be handled by a separate class\nor classes that we can plug/unplug according to some \"schema\".\nDocumentsWriter would call on these classes and provide the\nIndexOutput's for all streams for the Posting, per position, and these\nclasses write their own format into the IndexOutputs.\n\nI think a separation like that would work well: we could have good\nperformance and also extensibility.  Devil is in the details of\ncourse...\n\nI obviously haven't factored DocumentsWriter in this way (it has its\nown addPosition that writes the current Lucene index format) but I\nthink this is very doable in the future.\n",
            "date": "2007-06-21T17:59:00.729+0000",
            "id": 35
        },
        {
            "author": "Doron Cohen",
            "body": "Mike, I am considering testing the performance of this patch on a somewhat different use case, real one I think. After indexing 25M docs of TREC .gov2 (~500GB of docs) I pushed the index terms to create a spell correction index, by using the contrib spell checker. Docs here are *very* short - For each index term a document is created, containing some N-GRAMS. On the specific machine I used there are 2 CPUs but the SpellChecker indexing does not take advantage of that. Anyhow, 126,684,685 words==documents were indexed. \nFor the docs adding step I had:\n    mergeFactor = 100,000\n    maxBufferedDocs = 10,000\nSo no merging took place.\nThis step took 21 hours, and created 12,685 segments, total size 15 - 20 GB. \nThen I optimized the index with\n    mergeFactor = 400\n(Larger values were hard on the open files limits.)\n\nI thought it would be interesting to see how the new code performs in this scenario, what do you think?\n\nIf you too find this comparison interesting, I have two more questions:\n  - what settings do you recommend? \n  - is there any chance for speed-up in optimize()?  I didn't read your \n    new code yet, but at least from some comments here it seems that \n    on disk merging was not changed... is this (still) so? I would skip the \n    optimize part if this is not of interest for the comparison. (In fact I am \n    still waiting for my optimize() to complete, but if it is not of interest I \n    will just interrupt it...)\n\nThanks,\nDoron\n",
            "date": "2007-06-23T06:59:02.145+0000",
            "id": 36
        },
        {
            "author": "Michael McCandless",
            "body": "\n> I thought it would be interesting to see how the new code performs in this scenario, what do you think?\n\nYes I'd be very interested to see the results of this.  It's a\nsomewhat \"unusual\" indexing situation (such tiny docs) but it's a real\nworld test case.  Thanks!\n\n>  - what settings do you recommend?\n\nI think these are likely the important ones in this case:\n\n  * Flush by RAM instead of doc count\n    (writer.setRAMBufferSizeMB(...)).\n\n  * Give it as much RAM as you can.\n\n  * Use maybe 3 indexing threads (if you can).\n\n  * Turn off compound file.\n\n  * If you have stored fields/vectors (seems not in this case) use\n    autoCommit=false.\n\n  * Use a trivial analyzer that doesn't create new String/new Token\n    (re-use the same Token, and use the char[] based term text\n    storage instead of the String one).\n\n  * Re-use Document/Field instances.  The DocumentsWriter is fine with\n    this and it saves substantial time from GC especially because your\n    docs are so tiny (per-doc overhead is otherwise a killer).  In\n    IndexLineFiles I made a StringReader that lets me reset its String\n    value; this way I didn't have to change the Field instances stored\n    in the Document.\n\n>  - is there any chance for speed-up in optimize()?  I didn't read\n>    your new code yet, but at least from some comments here it seems\n>    that on disk merging was not changed... is this (still) so? I would\n\nCorrect: my patch doesn't touch merging and optimizing.  All it does\nnow is gather many docs in RAM and then flush a new segment when it's\ntime.  I've opened a separate issue (LUCENE-856) for optimizations\nin segment merging.",
            "date": "2007-06-23T09:43:11.430+0000",
            "id": 37
        },
        {
            "author": "Doron Cohen",
            "body": "Just to clarify your comment on reusing field and doc instances - to my understanding reusing a field instance is ok *only* after the containing doc was added to the index.\n\nFor a \"fair\" comparison I ended up not following most of your recommendations, including the reuse field/docs one and the non-compound one (apologies:-)), but I might use them later. \n\nFor the first 100,000,000 docs (==speller words) the speed-up is quite amazing:\n    Orig:    Speller: added 100000000 words in 10912 seconds = 3 hours 1 minutes 52 seconds\n    New:   Speller: added 100000000 words in 58490 seconds = 16 hours 14 minutes 50 seconds\nThis is 5.3 times faster !!!\n\nThis btw was with maxBufDocs=100,000 (I forgot to set the MEM param). \nI stopped the run now, I don't expect to learn anything new by letting it continue.\n\nWhen trying with  MEM=512MB, it at first seemed faster, but then there were now and then local slow-downs, and eventually it became a bit slower than the previous run. I know these are not merges, so they are either flushes (RAM directed), or GC activity. I will perhaps run with GC debug flags and perhaps add a print at flush so to tell the culprit for these local slow-downs.\n\nOther than that, I will perhaps try to index .GOV2 (25 Million HTML docs) with this patch. The way I indexed it before it took about 4 days - running in 4 threads, and creating 36 indexes. This is even more a real life scenario, it involves HTML parsing, standard analysis, and merging (to some extent). Since there are 4 threads each one will get, say, 250MB. Again, for a \"fair\" comparison, I will remain with compound.\n",
            "date": "2007-06-24T19:56:05.776+0000",
            "id": 38
        },
        {
            "author": "Michael McCandless",
            "body": "\n> Just to clarify your comment on reusing field and doc instances - to my\n> understanding reusing a field instance is ok *only* after the containing\n> doc was added to the index.\n\nRight, if your documents are very \"regular\" you should get a sizable\nspeedup (especially for tiny docs), with or without this patch, if you\nmake a single Document and add *separate* Field instances to it for\neach field, and then reuse the Document and Field instances for all\nthe docs you want to add.\n\nIt's not easy to reuse Field instances now (there's no\nsetStringValue()).  I made a ReusableStringReader to do this but you\ncould also make your own class that implements Fieldable.\n\n> For a \"fair\" comparison I ended up not following most of your\n> recommendations, including the reuse field/docs one and the non-compound\n> one (apologies:-)), but I might use them later.\n\nOK, when you say \"fair\" I think you mean because you already had a\nprevious run that used compound file, you had to use compound file in\nthe run with the LUCENE-843 patch (etc)?  The recommendations above\nshould speed up Lucene with or without my patch.\n\n> For the first 100,000,000 docs (==speller words) the speed-up is quite\n> amazing:\n>     Orig:    Speller: added 100000000 words in 10912 seconds = 3 hours 1\n>     minutes 52 seconds\n>     New:   Speller: added 100000000 words in 58490 seconds = 16 hours 14\n>     minutes 50 seconds\n> This is 5.3 times faster !!!\n\nWow!  I think the speedup might be even more if both of your runs followed\nthe suggestions above.\n\n> This btw was with maxBufDocs=100,000 (I forgot to set the MEM param).\n> I stopped the run now, I don't expect to learn anything new by letting it\n> continue.\n>\n> When trying with  MEM=512MB, it at first seemed faster, but then there\n> were now and then local slow-downs, and eventually it became a bit slower\n> than the previous run. I know these are not merges, so they are either\n> flushes (RAM directed), or GC activity. I will perhaps run with GC debug\n> flags and perhaps add a print at flush so to tell the culprit for these\n> local slow-downs.\n\nHurm, odd.  I haven't pushed RAM buffer up to 512 MB so it could be GC\ncost somehow makes things worse ... curious.\n\n> Other than that, I will perhaps try to index .GOV2 (25 Million HTML docs)\n> with this patch. The way I indexed it before it took about 4 days -\n> running in 4 threads, and creating 36 indexes. This is even more a real\n> life scenario, it involves HTML parsing, standard analysis, and merging\n> (to some extent). Since there are 4 threads each one will get, say,\n> 250MB. Again, for a \"fair\" comparison, I will remain with compound.\n\nOK, because you're doing StandardAnalyzer and HTML parsing and\npresumably loading one-doc-per-file, most of your time is spent\noutside of Lucene indexing so I'd expect less that 50% speedup in\nthis case.\n",
            "date": "2007-06-24T20:56:36.020+0000",
            "id": 39
        },
        {
            "author": "Michael McCandless",
            "body": "Re-opening this issue: I saw one failure of the contrib/benchmark\nTestPerfTasksLogic.testParallelDocMaker() tests due to an intermittant\nthread-safety issue.  It's hard to get the failure to happen (it's\nhappened only once in ~20 runs of contrib/benchmark) but I see where\nthe issue is.  Will commit a fix shortly.",
            "date": "2007-07-06T11:52:03.851+0000",
            "id": 40
        },
        {
            "author": "Steven Parkes",
            "body": "Did we lose the triggered merge stuff from 887, i.e.,, should it be\n\n        if (triggerMerge) {\n          /* new merge policy\n          if (0 == docWriter.getMaxBufferedDocs())\n            maybeMergeSegments(mergeFactor * numDocs / 2);\n          else\n            maybeMergeSegments(docWriter.getMaxBufferedDocs());\n          */\n          maybeMergeSegments(docWriter.getMaxBufferedDocs());\n        }\n ",
            "date": "2007-07-12T21:27:58.136+0000",
            "id": 41
        },
        {
            "author": "Michael McCandless",
            "body": "Woops ... you are right; thanks for catching it!  I will add a unit\ntest & fix it.  I will also make the flush(boolean triggerMerge,\nboolean flushDocStores) protected, not public, and move the javadoc\nback to the public flush().\n",
            "date": "2007-07-12T21:59:52.163+0000",
            "id": 42
        }
    ],
    "component": "core/index",
    "description": "I'm working on a new class (MultiDocumentWriter) that writes more than\none document directly into a single Lucene segment, more efficiently\nthan the current approach.\n\nThis only affects the creation of an initial segment from added\ndocuments.  I haven't changed anything after that, eg how segments are\nmerged.\n\nThe basic ideas are:\n\n  * Write stored fields and term vectors directly to disk (don't\n    use up RAM for these).\n\n  * Gather posting lists & term infos in RAM, but periodically do\n    in-RAM merges.  Once RAM is full, flush buffers to disk (and\n    merge them later when it's time to make a real segment).\n\n  * Recycle objects/buffers to reduce time/stress in GC.\n\n  * Other various optimizations.\n\nSome of these changes are similar to how KinoSearch builds a segment.\nBut, I haven't made any changes to Lucene's file format nor added\nrequirements for a global fields schema.\n\nSo far the only externally visible change is a new method\n\"setRAMBufferSize\" in IndexWriter (and setMaxBufferedDocs is\ndeprecated) so that it flushes according to RAM usage and not a fixed\nnumber documents added.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-843",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "improve how IndexWriter uses RAM to buffer added documents",
    "systemSpecification": true,
    "version": "2.2"
}