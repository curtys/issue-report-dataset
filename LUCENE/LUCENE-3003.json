{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "Attached patch.  I moved most of UnInvertedField into Lucene, as\noal.index.DocTermsOrds, but left the two faceting methods (getCounts,\ngetStats) in UnInvertedField.  UnInvertedField subclasses\nDocTermOrds.\n\nI added a simple \"OrdIterator\" API, for stepping through the ords for\na doc + field (but, Solr's UnInvertedField still just directly uses\nthe packed structures), and a Lucene test case that verifies this is\nworking right (though I still need a few more test cases).\n\nAll tests pass, but I have various small nocommits to work through.\n",
            "date": "2011-03-29T19:50:01.907+0000",
            "id": 0
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}Eventually we should fold this ability into docvalues, ie we'd\nwrite the byte[] image at indexing time, and then loading would be\nfast, instead of uninverting{quote}\n\nI'd guess that pulsing should be 'good enough' most of the time?  It seems like there'll be some overlap in terms of the gains from pulsing vis-\u00e0-vis DocValues?  ",
            "date": "2011-03-29T20:05:06.066+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I'd guess that pulsing should be 'good enough' most of the time? It seems like there'll be some overlap in terms of the gains from pulsing vis-\u00e0-vis DocValues?\n\nI think Pulsing codec probably doesn't help much here?\n\nIe Pulsing is good for terms that have only 1 or 2 docs.\n\nBut for this case (faceting), usually, you have relatively few terms\nand many docs per term?\n",
            "date": "2011-03-29T20:43:30.906+0000",
            "id": 2
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. Ie Pulsing is good for terms that have only 1 or 2 docs\n\nI thought the default is 16 docs?  If there are more then seek'ing to the postings should be negligible (in comparison to a larger aggregate index size when using CSF/DocValues, which'll consume more of the system IO cache)?",
            "date": "2011-03-29T20:48:25.600+0000",
            "id": 3
        },
        {
            "author": "Chris Male",
            "body": "+1 to committing this change then tackling the improvements separately.",
            "date": "2011-03-30T01:36:53.594+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "New patch, fixing all the nocommits, adding test case for non-null prefix passed to DTO.  I think it's ready to commit.",
            "date": "2011-03-30T23:20:21.613+0000",
            "id": 5
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. I'd guess that pulsing should be 'good enough' most of the time?\n\nThis already really pulses I think? If the bytes can fit in an int, they are \"inlined\" right in the pointer that would normally point out to the byte array.\n\nbq. But for this case (faceting), usually, you have relatively few terms and many docs per term?\n\nWe see everything.  But this structure was more optimized for a high number of unique terms, but relatively few per document.  This will perform well on a multi-valued author field, but relatively poorly on a large full-text field.\n\n\n",
            "date": "2011-03-30T23:36:22.201+0000",
            "id": 6
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. The first-pass that allocates lots of tiny byte[] looks like it could be inefficient. Maybe we could use the byte slices from the indexer for this...\n\nIt is inefficient - but I never saw a way around it since the lists are all being built in parallel (due to the fact that we are uninverting).\n\n\nAnother small & easy optimization I hadn't gotten around to yet was to lower the indexIntervalBits and make it configurable.  Another small optimization would be to store an array of offsets to length-prefixed byte arrays, rather than a BytesRef[].  At least the values are already in packed byte arrays via PagedBytes.\n\n\nI'd also love to hear others thoughts on this memory optimization for many small byte arrays:\n{code}\n              // We avoid a doubling strategy to lower memory usage.\n              // this faceting method isn't for docs with many terms.\n              // In hotspot, objects have 2 words of overhead, then fields, rounded up to a 64-bit boundary.\n              // TODO: figure out what array lengths we can round up to w/o actually using more memory\n              // (how much space does a byte[] take up?  Is data preceded by a 32 bit length only?\n              // It should be safe to round up to the nearest 32 bits in any case.\n              int newLen = (newend + 3) & 0xfffffffc;  // 4 byte alignment\n{code}",
            "date": "2011-03-30T23:57:45.284+0000",
            "id": 7
        },
        {
            "author": "Dawid Weiss",
            "body": "For what it's worth, the instrumentation interface allows one to get exact allocation sizes of objects. I put together a small spike at https://github.com/dweiss/poligon/tree/master/instrumenter that measures the actual allocation size of byte[]. On my hotspot, 64-bit, this yields:\n\n{noformat}\nbyte[0] takes 24 bytes.\nbyte[1] takes 32 bytes.\nbyte[2] takes 32 bytes.\nbyte[3] takes 32 bytes.\nbyte[4] takes 32 bytes.\nbyte[5] takes 32 bytes.\nbyte[6] takes 32 bytes.\nbyte[7] takes 32 bytes.\nbyte[8] takes 32 bytes.\nbyte[9] takes 40 bytes.\nbyte[10] takes 40 bytes.\nbyte[11] takes 40 bytes.\n...\n{noformat}\n\nIBM's VM yields the same (64-bit), but the version of jrockit that I have (which may be an old one, but is 64-bit!) yields:\n\n{noformat}\nbyte[0] takes 16 bytes.\nbyte[1] takes 24 bytes.\nbyte[2] takes 24 bytes.\nbyte[3] takes 24 bytes.\nbyte[4] takes 24 bytes.\nbyte[5] takes 24 bytes.\nbyte[6] takes 24 bytes.\nbyte[7] takes 24 bytes.\nbyte[8] takes 24 bytes.\nbyte[9] takes 32 bytes.\nbyte[10] takes 32 bytes.\nbyte[11] takes 32 bytes.\nbyte[12] takes 32 bytes.\nbyte[13] takes 32 bytes.\nbyte[14] takes 32 bytes.\nbyte[15] takes 32 bytes.\nbyte[16] takes 32 bytes.\nbyte[17] takes 40 bytes.\n{noformat}\n\nDon't have access to a 32-bit system right now, but if you're keen on checking, checkout that github repo and run:\n\n{noformat}\ncd instrumenter\nmvn package\njava -javaagent:target/instrumenter-0.1.0-SNAPSHOT.jar -version\n{noformat}",
            "date": "2011-03-31T07:18:32.429+0000",
            "id": 8
        },
        {
            "author": "Yonik Seeley",
            "body": "Thanks Dawid, this suggests that we could round up to the 8 byte boundary for free.\n",
            "date": "2011-03-31T14:12:13.365+0000",
            "id": 9
        },
        {
            "author": "Mark Miller",
            "body": "Attached: 32-bit results",
            "date": "2011-04-01T02:37:59.541+0000",
            "id": 10
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. Attached: 32-bit results\n\nAh, bummer.  It's every 8 bytes, but with a 4 byte offset!\nI guess we could make it based on if we detect 32 vs 64 bit jvm... but maybe first see if anyone has any ideas about how to use something like pagedbytes instead.",
            "date": "2011-04-01T15:45:00.910+0000",
            "id": 11
        },
        {
            "author": "Michael McCandless",
            "body": "bq. It is inefficient - but I never saw a way around it since the lists are all being built in parallel (due to the fact that we are uninverting).\n\nLucene's indexer (TermsHashPerField) has precisely this same problem\n-- every unique term must point to two (well, one if omitTFAP)\ngrowable byte arrays.  We use \"slices\" into a single big (paged)\nbyte[], where first slice is tiny and can only hold like 5 bytes, but\nthen points to the next slice which is a bit bigger, etc.\n\nWe could look @ refactoring that for this use too...\n\nThough this is \"just\" the one-time startup cost.\n\nbq. Another small & easy optimization I hadn't gotten around to yet was to lower the indexIntervalBits and make it configurable.\n\nI did make it configurable to the Lucene class (you can pass it in to\nctor), but for Solr I left it using every 128th term.\n\n{quote}\nAnother small optimization would be to store an array of offsets to length-prefixed byte arrays, rather than a BytesRef[]. At least the values are already in packed byte arrays via PagedBytes.\n{quote}\n\nBoth FieldCache and docvalues (branch) store an array-of-terms like\nthis (the array of offsets is packed ints).\n\nWe should also look at using an FST, which'd be the most compact but\nthe ord -> term lookup cost goes up.\n\nAnyway I think we can pursue these cool ideas on new [future]\nissues...\n",
            "date": "2011-04-01T16:32:18.951+0000",
            "id": 12
        },
        {
            "author": "Robert Muir",
            "body": "bulk move 3.2 -> 3.3",
            "date": "2011-06-03T16:40:46.661+0000",
            "id": 13
        },
        {
            "author": "Robert Muir",
            "body": "3.6 pruning: can we push this out to 4.0 (mark resolved?)",
            "date": "2012-03-06T02:08:33.348+0000",
            "id": 14
        }
    ],
    "component": "core/index",
    "description": "Solr's UnInvertedField lets you quickly lookup all terms ords for a\ngiven doc/field.\n\nLike, FieldCache, it inverts the index to produce this, and creates a\nRAM-resident data structure holding the bits; but, unlike FieldCache,\nit can handle multiple values per doc, and, it does not hold the term\nbytes in RAM.  Rather, it holds only term ords, and then uses\nTermsEnum to resolve ord -> term.\n\nThis is great eg for faceting, where you want to use int ords for all\nof your counting, and then only at the end you need to resolve the\n\"top N\" ords to their text.\n\nI think this is a useful core functionality, and we should move most\nof it into Lucene's core.  It's a good complement to FieldCache.  For\nthis first baby step, I just move it into core and refactor Solr's\nusage of it.\n\nAfter this, as separate issues, I think there are some things we could\nexplore/improve:\n\n  * The first-pass that allocates lots of tiny byte[] looks like it\n    could be inefficient.  Maybe we could use the byte slices from the\n    indexer for this...\n\n  * We can improve the RAM efficiency of the TermIndex: if the codec\n    supports ords, and we are operating on one segment, we should just\n    use it.  If not, we can use a more RAM-efficient data structure,\n    eg an FST mapping to the ord.\n\n  * We may be able to improve on the main byte[] representation by\n    using packed ints instead of delta-vInt?\n\n  * Eventually we should fold this ability into docvalues, ie we'd\n    write the byte[] image at indexing time, and then loading would be\n    fast, instead of uninverting\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-3003",
    "issuetypeClassified": "REFACTORING",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Move UnInvertedField into Lucene core",
    "systemSpecification": true,
    "version": ""
}