{
    "comments": [
        {
            "author": "Daniel Naber",
            "body": "Your old bug didn't just disappear, it has been closed because you didn't \nreply to our suggestions. See the history here: \nhttp://issues.apache.org/bugzilla/show_bug.cgi?id=30421 \nIf you can provide a test case, please attach it here. \n ",
            "date": "2005-01-21T06:54:05.000+0000",
            "id": 0
        },
        {
            "author": "dan",
            "body": "\nSuggestion #1\n\n>>>I fixed a bug that left a SegmentReader open in addIndexes(IndexReader[] \nreaders)\n>>>and it also left obsolete index files undeleted.\n>>>But this could have hardly caused your memory problems.\n\n\tI'm running the current version of Lucene.\n\nSuggestion #2\n\n>>>The call to writer.optimize() isn't necessary\n\n\tThe call was removed. Please read my comment about the failure \nhappening after the new index is written\n\nSuggestion #3\n\n>>>please try with StandardAnaylzer to make sure the problem isn't in your \nTermanalyzer\n\t\n\tWith the StandardAnalyzer, the results are the same.\n\tQuestion: Is it acceptable to merge using a different analyzer than was \nused to index?\n\nSuggestion #4\n\n>>>please provide a test case. If you can provide a test case, please attach it \nhere. \n\t\n\tOK. It's just these five lines of code plus alot of data.\n\tI've offered before to send the data before, but its won't post as an \nemail attachment.\n\n\t>>>>> code \n\n\tDirectory[] sources = new Directory[paths.length];\n\t...\n\n\tDirectory dest = FSDirectory.getDirectory( path, true);\n\tIndexWriter writer = new IndexWriter( dest, new StandardAnalyzer(), \ntrue);\n\n\twriter.addIndexes( sources);\n\twriter.close();\n\n",
            "date": "2005-01-21T10:48:49.000+0000",
            "id": 1
        },
        {
            "author": "Daniel Naber",
            "body": "You need to close the directory \"dest\" explicitly, not only the writer. Does \nthat make a difference? ",
            "date": "2005-01-22T03:32:16.000+0000",
            "id": 2
        },
        {
            "author": "dan",
            "body": "I just executed the following test case in a 700M VM and had the same outcome. \nSee details below.\n\nIn this test case, the sum of the input directories was ~42G and the amount \nwritten to\n'dest' directory is ~41G. This pattern is reliably repeatable. That is, it \nlooks like the merge works,\nbut the failure happens at the very end.\n\nIf I open 'dest' directory and call docCount I get 0.\n\n>>>>>>>>> revised code >>>>>>>>>>>\n\n\t...\n\tDirectory dest = FSDirectory.getDirectory( destination, true);\n\n\tIndexWriter writer = new IndexWriter( dest, new StandardAnalyzer(), \ntrue);\n\twriter.addIndexes( sources);\n\n\tlog(\"here\"); //never prints to screen\n\n\twriter.close();\n\tdest.close();\n\n>>>>>>>>> inputs >>>>>>>>>>>\n\n6.3G    ./index0/index\n5.9G    ./index1/index\n6.0G    ./index2/index\n5.2G    ./index3/index\n5.4G    ./index4/index\n3.8G    ./index5/index\n4.0G    ./index6/index\n5.3G    ./index7/index\n\n>>>>>>>>> output on screen >>>>>>>>>>>\n\n>>> merging: ./index0/index\n>>> merging: ./index1/index\n>>> merging: ./index2/index\n>>> merging: ./index3/index\n>>> merging: ./index4/index\n>>> merging: ./index5/index\n>>> merging: ./index6/index\n>>> merging: ./index7/index\nException in thread \"main\" java.lang.OutOfMemoryError\n\n>>>>>>>>> destination directory stats >>>>>>>>>>>\n\n41G     ./merged.0000/index\n\n\n\n",
            "date": "2005-01-23T09:28:22.000+0000",
            "id": 3
        },
        {
            "author": "Daniel Naber",
            "body": "There are two addIndexes() methods, one for IndexReaders and one for \ndirectories, does the problem occur with both? Does it also appear with \nsmaller indexes and a smaller JVM (I won't be able to reproduce problems if it \nrequires a 40 GB index)? Are you using Lucene 1.4.3? You will probably need to \nchange the Lucene code and add debug statements to see where the exception \noccurs. Also, are all indexes in the same format, i.e. compound or \nnon-compound? ",
            "date": "2005-01-24T02:23:01.000+0000",
            "id": 4
        },
        {
            "author": "dan",
            "body": "I'm running the tests now. It may take several days. I will report back.",
            "date": "2005-01-24T08:50:15.000+0000",
            "id": 5
        },
        {
            "author": "dan",
            "body": "Results of my next test:\n\nSame machine, same indexes, same size VM. If I reduce the sum of the inputs to \n< 30G the merge succeeds. If I add an additional index, bringing the total > \n30G the merge fails.\n\nI will continue with remaining tests tomorrow.\n\n>>>>>>>>> inputs >>>>>>>>>>>\n\n5.8G    ./index3\n5.9G    ./index4\n4.3G    ./index5\n4.4G    ./index6\n5.8G    ./index7\n\n>>>>>>>>> destination directory stats >>>>>>>>>>>\n\n>>> merging: /home/agense/raw/questions/index3/index\n>>> merging: /home/agense/raw/questions/index4/index\n>>> merging: /home/agense/raw/questions/index5/index\n>>> merging: /home/agense/raw/questions/index6/index\n>>> merging: /home/agense/raw/questions/index7/index\n\n>>>>>>>>> destination directory stats >>>>>>>>>>>\n\n26G     ./merged.0000\n\n",
            "date": "2005-01-24T13:33:54.000+0000",
            "id": 6
        },
        {
            "author": "dan",
            "body": "Results for the remainder of my testing:\n\nAll my indexes use the compound file format.\n\nSelecting random combinations of the test indexes, I am able to successfully \nmerge using both method signatures, as long as the sum of the inputs is less \nthan 30G.\n\nFor all attempts to merge all the indexes, using either method signature, I get \nthe out-of-memory condition.\n\nWhat is the next step?\n",
            "date": "2005-01-26T12:23:02.000+0000",
            "id": 7
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Have you tried running your app under an profiler?  I suggest you try that and\nsee where the memory is being allocated.  If there is a bug in Lucene, this may\nhelp us narrow down the area we need to look at.",
            "date": "2005-01-26T12:47:26.000+0000",
            "id": 8
        },
        {
            "author": "dan",
            "body": ">>>Have you tried running your app under an profiler? ...\n\nI started to spend time looking at various profilers, but concluded that all I \ndo is make a call to addIndexes inside a jar file. From there, it's all Lucene.\n\nTo profile, I'd have to find and build a profiler; rebuild Lucene with \nprofiling tags; understand the Lucene call stack and logic; make sense of the \noutput; etc. This is, in effect, debugging Lucene. My understanding is this is \nthe purview of the Lucene team.\n\n>>>If there is a bug in Lucene...\n\nWhat is it about my test results that would lead you to conclude something \nother than a Lucene bug at this point? If there is more evidence I can provide, \nplease let me know what tests I can run. Can you replicate my results? \n\nThanks.",
            "date": "2005-01-28T05:25:25.000+0000",
            "id": 9
        },
        {
            "author": "cutting@apache.org",
            "body": "How many fields and how many documents are in your index?\n\nCan you provide a stack trace from the OutOfMemoryException?  This would be very\nuseful.\n",
            "date": "2005-01-28T05:36:09.000+0000",
            "id": 10
        },
        {
            "author": "dan",
            "body": ">>>How many fields and how many documents are in your index?\n    \n7 million documents with 100 fields each\n\n>>>Can you provide a stack trace from the OutOfMemoryException?  \n\nThe code I've been testing is inside a try/catch block with print stack trace. \nThere is no trace. My experience has been that the stack doesn't print when out-\nof-memory exception is thrown (JVM 1.4.2)",
            "date": "2005-01-30T07:46:10.000+0000",
            "id": 11
        },
        {
            "author": "cutting@apache.org",
            "body": "Are the 100 fields all indexed, or are some only stored?  If indexed, 100 is a\nvery large number of indexed fields.  7M documents with 100 indexed fields could\nrequire 700MB when searching, since one byte per searched field per document of\nRAM is used to cache the norms for each field.  But that RAM is not required\nwhen indexing.  Merging an index, where you're having troubles, should not\nrequire much RAM.\n\nSince your problem only requires a 5-line program to demonstrate, and it only\nrequires the Lucene jar file, please create such a 5-line java program as a\nsingle file that depends only on the Lucene jar and demonstrate the problem with\nsomething like:\n\n  javac -classpath lucene.jar Test.java\n  java -classpath lucene.jar Test index1 index2 index3 ...\n\nTest.java should look something like:\n\nimport org.apache.lucene.index.IndexWriter;\npublic class Test {\n  public static void main(String[] paths) throws Exception {\n    Directory[] sources = new Directory[paths.length];\n    for (int i = 0; i < paths.length) {\n      sources[i] = FSDirectory.getDirectory(paths[i], false);\n    }\n    IndexWriter writer = \n      new IndexWriter(\"dest\", new StandardAnalyzer(), true);   \n    writer.addIndexes(sources);\n    writer.close();\n  }\n}\n\nOnce you have replicated the bug with such a program, please attach the program\nto this bug report.  This way we can be certain that there is nothing involved\nbut Lucene.\n\nNote that this code does not explicitly try to catch exceptions but rather lets\nthe JVM print a final stack trace if it exits in an exception.  That may work\nbetter.\n\nIf the problem still appears and we still don't get a stack trace then we can\ntry putting in log statements in SegmentMerger.java.\n\nThanks for your patience.",
            "date": "2005-02-01T02:59:54.000+0000",
            "id": 12
        },
        {
            "author": "dan",
            "body": ">>>>>>>>>>>> Code >>>>>>>>>>>>\n\nimport org.apache.lucene.index.IndexWriter;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.FSDirectory;\nimport org.apache.lucene.analysis.standard.StandardAnalyzer;\n\npublic class MergeTest\n{\n\tpublic static void main(String[] paths)\n\t\tthrows Exception\n\t{\n\t\tDirectory[] sources = new Directory[paths.length];\n\n\t\tfor (int i = 0; i < paths.length; i++)\n\t\t\tsources[i] = FSDirectory.getDirectory(paths[i], false);\n\n\t\tDirectory dest = FSDirectory.getDirectory( \"/home/dan/merged/\", \ntrue);\n\t\tIndexWriter writer = new IndexWriter(dest, new StandardAnalyzer\n(), true);\n\n\t\twriter.addIndexes(sources);\n\t\twriter.close();\n\t}\n}\n\n>>>>>>>>>>>> Inputs >>>>>>>>>>>>\n\n7.2G    index0\n7.0G    index1\n5.8G    index3\n5.8G    index4\n4.2G    index5\n5.3G    index6\n5.8G    index7\n6.0G    index8\n4.8G    index9\n52G     .\n\n>>>>>>>>>>>> Test Results >>>>>>>>>>>>\n\n#1 java -Xms700M -Xmx700M MergeTest index0 index1\n\tresult -> 13G success\n\n#2 java -Xms700M -Xmx700M MergeTest index0 index1 index3\n\tresult -> 18G success\n\n#3 java -Xms700M -Xmx700M MergeTest index0 index1 index3 index4\n\tresult -> 24G success\n\n#4 java -Xms700M -Xmx700M MergeTest index0 index1 index4 index5\n\tresult -> 22G success\n\n#5 java -Xms700M -Xmx700M MergeTest index0 index1 index3 index4 index5\n\tresult -> 27G Exception in thread \"main\" java.lang.OutOfMemoryError (no \nstack trace printed)\n\n#6 java -Xms700M -Xmx700M MergeTest index0 index1 index3 index4 index5\n\tresult -> 27G Exception in thread \"main\" java.lang.OutOfMemoryError (no \nstack trace printed)\n\n\n",
            "date": "2005-02-02T02:59:14.000+0000",
            "id": 13
        },
        {
            "author": "dan",
            "body": ">>>If indexed, 100 is a very large number of indexed fields.\n\nDoug, how are multi-value fields treated in calculating total fields? If I add \na field called \"link\" 18 times, is this considered 18 or 1?\n\nThanks.\n",
            "date": "2005-02-02T03:06:02.000+0000",
            "id": 14
        },
        {
            "author": "cutting@apache.org",
            "body": "Thanks.\n\nCan you please attach the output of 'ls -lt /home/dan/merged/' after it fails? \nThat may indicate where it is dying.\n\nWhat does 'ulimit -c' print?  If you're not getting a stack trace perhaps we can\nget a core dump.  One can get stack traces from java core dumps.\n\nAlso, why do you specify a minimum heap size with -Xms700M, rather than just let\nthe heap grow to its maximum?  I have had troubles before specifying -Xms and\nhave never found it advantageous.  Can you also please try once without that option?\n\nThanks again,\n\nDoug\n",
            "date": "2005-02-02T03:09:50.000+0000",
            "id": 15
        },
        {
            "author": "cutting@apache.org",
            "body": "The number of fields that I'm referring to is the number of unique field names\nthat are ever added as indexed.  So adding a field name multiple times to a\nsingle document will not change things.",
            "date": "2005-02-02T03:11:41.000+0000",
            "id": 16
        },
        {
            "author": "dan",
            "body": "Created an attachment (id=14155)\nls -lt on merged directory\n",
            "date": "2005-02-02T05:58:13.000+0000",
            "id": 17
        },
        {
            "author": "cutting@apache.org",
            "body": "Created an attachment (id=14156)\npatch to reduce memory requirements of segment merger\n\nOkay.  I see the problem.  You have over 159 indexed fields in over 5M\ndocuments, which, if norms are cached, requires over 700MB.\n\nI've attached a patch which fixes segment merging to not use cached access to\nthe norms.  Please try this and tell me how it works.\n\nYou will still have trouble searching this index in a 700MB JVM if you search\nall of the fields.\n\nDoug",
            "date": "2005-02-02T06:14:13.000+0000",
            "id": 18
        },
        {
            "author": "dan",
            "body": "I will try the patch and report back.\n\n>>>You will still have trouble searching this index in a 700MB...\n\nYes. I'm testing a redesigned index now.",
            "date": "2005-02-02T07:34:04.000+0000",
            "id": 19
        },
        {
            "author": "dan",
            "body": ">>>since one byte per searched field per document of RAM is used to cache the \nnorms for each field.\n\nHow does one programmatically flush the cache? I've been looking for such a \nmethod.\n",
            "date": "2005-02-02T11:22:22.000+0000",
            "id": 20
        },
        {
            "author": "dan",
            "body": "Created an attachment (id=14172)\nSegmentMerger patch test results\n\nThe SegmentMerger patch is a success. Thanks for looking into this.",
            "date": "2005-02-04T02:05:25.000+0000",
            "id": 21
        },
        {
            "author": "cutting@apache.org",
            "body": "Okay, the patch has been comitted.",
            "date": "2005-02-05T04:29:09.000+0000",
            "id": 22
        }
    ],
    "component": "core/index",
    "description": "I'm re-opening a bug I logged previously. My previous bug report has \ndisappeared. \n\nIssue: IndexWriter.addIndexes results in java.lang.OutOfMemoryError for large \nmerges.\n\nUntil this writing, I've been merging successfully only through repetition, \ni.e. I keep repeating merges until a success. As my index size has grown, my \nsuccess rate has steadily declined. I've reached the point where merges now \nfail 100% of the time. I can't merge.\n\nMy tests indicate the threshold is ~30GB on P4/800MB VM with 6 indexes. I have \nrepeated my tests on many different machines (not machine dependent). I have \nrepeated my test using local and attached storage devices (not storage \ndependent).\n\nFor what its worth, I believe the exception occurs entirely during the optimize \nprocess which is called implicitly after the merge. I say this because each \ntime it appears the correct amount of bytes are written to the new index. Is it \npossible to decouple the merge and optimize processes?\n\n\nThe code snippet follows. I can send you the class file and 120GB data set. Let \nme know how you want it.\n\n>>>>> code sample >>>>>\n\nDirectory[] sources = new Directory[paths.length];\n...\n\nDirectory dest = FSDirectory.getDirectory( path, true);\nIndexWriter writer = new IndexWriter( dest, new TermAnalyzer( \nStopWords.SEARCH_MAP), true);\n\nwriter.addIndexes( sources);\nwriter.close();",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-326",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "IndexWriter.addIndexes results in java.lang.OutOfMemoryError",
    "systemSpecification": true,
    "version": "1.4"
}