{
    "comments": [
        {
            "author": "Michael Busch",
            "body": "Patch that includes all mentioned improvements, but needs cleanup, documentation improvements and junits.",
            "date": "2009-06-16T09:52:03.749+0000",
            "id": 0
        },
        {
            "author": "Uwe Schindler",
            "body": "Why do you add a new class \"SmallToken\"? I think it should be the good old deprecated \"Token\".\n\nIn my opinion, I would not deprecate the old Token, instead the default factory should always create Token instead of all these default *Impl attributes.\n\nWhat was your concusion about my idea yesterday, to pass the Token around even with the old API and copy it on demand, if return value by next() is not the same? In this case, Token should be the only implementation of all standard attributes.",
            "date": "2009-06-16T10:04:35.277+0000",
            "id": 1
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nWhy do you add a new class \"SmallToken\"? I think it should be the good old deprecated \"Token\". \n{quote}\n\nI should have added a \"no commit\" comment to SmallToken. I just used it for performance tests, we don't have to commit it. It's an example of what an advanced user could do to speed up cloning. Token is still there and unchanged.\n\n{quote}\nIn my opinion, I would not deprecate the old Token, instead the default factory should always create Token instead of all these default *Impl attributes.\n{quote}\n\nYeah we shouldn't deprecate Token. Not sure about the default. It kind of depends on the use case. Cloning and memory is faster I think with only two or three Attributes compared to using the Token cloning is faster. But it'd be simpler to not have all the Impl classes... hmm not sure... \n\n{quote}\nWhat was your concusion about my idea yesterday, to pass the Token around even with the old API and copy it on demand\n{quote}\n\nI don't think the indexer should know at all about Token? It should only use the interfaces so that we can maintain the full flexibility.\nAlso I don't really like the fact very much that some user might get a performance hit. I had the idea to throw the exception in incrementToken() to automatically being able to fallback to the old API. I think this is nice and gets rid of the explicit useNewAPI methods. The only drawback is still the fact that we have to implement both old and new APIs in Lucene's tokenizers and filters until we remove the old API. Grant won't like this :) but I think this is better than the possible performance hit? Also we don't add new filters THAT often to Lucene and implementing both methods is often mostly copy&paste.\n\nDo you like the idea with the UnsupportedOperationException?\n",
            "date": "2009-06-16T10:28:48.226+0000",
            "id": 2
        },
        {
            "author": "Uwe Schindler",
            "body": "{quote}\nbq. What was your concusion about my idea yesterday, to pass the Token around even with the old API and copy it on demand\n\nI don't think the indexer should know at all about Token? It should only use the interfaces so that we can maintain the full flexibility.\nAlso I don't really like the fact very much that some user might get a performance hit. I had the idea to throw the exception in incrementToken() to automatically being able to fallback to the old API. I think this is nice and gets rid of the explicit useNewAPI methods. The only drawback is still the fact that we have to implement both old and new APIs in Lucene's tokenizers and filters until we remove the old API. Grant won't like this  but I think this is better than the possible performance hit? Also we don't add new filters THAT often to Lucene and implementing both methods is often mostly copy&paste.\n{quote}\n\nI did not mean to use Token in the indexer, I would like to remove the whole old API from the indexer (even the UOE). My idea would be the following:\n- Let the default interfaces implemented by Token, the default factory creates it for all requests to the default attributes\n- If somebody implements the new API, the indexer can use it without problems. If he doesn't, the default impl in TokenStream would call next(Token), using the token instance from AttributeSource. If the method returns with another Token instance (because it did not reuse, which is seldom I think), copy this returned token into the per instance AttributeSource Token.\n- The other way round, if one uses a TokenStream with the old API (own indexer, query parser,...), the TokenStream only implemented the new API, the deprectated old method would also have a default impl, ignoring the token supplied to next() and returning always the instance-token after calling incrementToken().\n\nBecause of this, the indexer would always use the new API, the old API is wrapped. Core analyzers only need to implement the new methods (or could even stay with the old).\n\nThere are two problems:\n- If somebody does not implement either method, the call to incrementToken() will loop endless, there should be some early break with UOE in this case. Do not know how to implement this without inspecting the stack trace in the default methods.\n- Filters are a bit tricky: They could pass in both methods per default to the delegate. The problem, if one only implements one of the methods, the other passes down to the delegate, and doing nothing. But this could be fixed by delegating in the deprecated old method always to the new one (or vice versa).\n\nHope this was clear, maybe I should create a patch.",
            "date": "2009-06-16T10:42:46.119+0000",
            "id": 3
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\n- If somebody implements the new API, the indexer can use it without problems. If he doesn't, the default impl in TokenStream would call next(Token), using the token instance from AttributeSource. If the method returns with another Token instance (because it did not reuse, which is seldom I think), copy this returned token into the per instance AttributeSource Token.\n{quote}\n\nWhat if you currently have a filter that caches the token instance passed into next(Token) and returns a different one. If you then copy the values from the returned token into the per instance Attribute Token, you alter the cached one, because it's the same instance.\n\nNot sure if this is common or even allowed, but these are the sideeffects I'm worried about.",
            "date": "2009-06-16T10:56:12.843+0000",
            "id": 4
        },
        {
            "author": "Uwe Schindler",
            "body": "{quote}\nWhat if you currently have a filter that caches the token instance passed into next(Token) and returns a different one. If you then copy the values from the returned token into the per instance Attribute Token, you alter the cached one, because it's the same instance.\n\nNot sure if this is common or even allowed, but these are the sideeffects I'm worried about.\n{quote}\n\nSee:\nhttp://lucene.apache.org/java/2_4_1/api/org/apache/lucene/analysis/TokenStream.html\n\nIt states in next(Token): Also, the producer must make no assumptions about a Token after it has been returned: the caller may arbitrarily change it. If the producer needs to hold onto the token for subsequent calls, it must clone() it before storing it. Note that a TokenFilter is considered a consumer.\n\nSo I see no problem with this.",
            "date": "2009-06-16T11:04:43.238+0000",
            "id": 5
        },
        {
            "author": "Shai Erera",
            "body": "I have a couple of TokenFilters that work that way - get a Token from the wrapped TokenStream, cache it (cloning) and return another Token instead. Especially as the Tokenizer fills the Token w/ characters, I think that cloning is the only option if you want to hold on to a Token that was returned by a wrapped TokenStream.",
            "date": "2009-06-16T11:34:12.676+0000",
            "id": 6
        },
        {
            "author": "Uwe Schindler",
            "body": "If you clone, you would not fall into the mentioned problem. The reuseable Token passed to next() is owned by the caller.",
            "date": "2009-06-16T11:51:08.049+0000",
            "id": 7
        },
        {
            "author": "Michael Busch",
            "body": "But, the additional copying would affect performance in Shai's case. The same with CachingTokenFilter and Tee/Sink filer/tokenizer. I don't think anymore it's such a corner case to not return the reusable token.\n\nFrom a user's perspective, the UnsupportedOperationException solution and your solution should not be distinguishable. The advantage of your solution is cleaner, less cluttered code, and it removes the burden from the developers to implement both APIs in the transition period.The disadvantage is a possible performance hit in the cases mentioned above.\nThe advantage of the UOE solution is no performance hit for users of the old API, the disadvantage the need to implement both APIs.\n\nEven though I think your solution is very elegant, Uwe, I'm in favor of the one that doesn't affect performance significantly. What do you or others think about the possible performance hit? Not a big deal? Or worth keeping the old code around for a while?",
            "date": "2009-06-16T15:41:55.347+0000",
            "id": 8
        },
        {
            "author": "Grant Ingersoll",
            "body": "Just linking some of these related issues together such that we can make sure we cover all of the bases with this new Token Stream stuff",
            "date": "2009-06-16T15:44:49.926+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "bq. What do you or others think about the possible performance hit?\n\nI haven't been able to keep up with this, but this one caught my eye!\n\nI think performance of analysis, especially the common case (all my tokenizers are new, I'm not cloning, etc.) is important.  It sounds like this performance hit was specifically on cloning... but still we should try not to lose performance if we can help it.",
            "date": "2009-06-16T16:28:09.075+0000",
            "id": 10
        },
        {
            "author": "Grant Ingersoll",
            "body": "I think cloning is more prevalent than people think.  For instance, the RemoveDuplicatesTokenFilter in Solr is in the default schema and is thus used, albeit naively, by a whole lot of people.  Furthermore, I've seen quite a few apps that need to buffer tokens before spitting them out, things like phrase detection, n-grams, entity extraction, sentence detection, part of speech detection, parsing, etc.",
            "date": "2009-06-16T16:36:54.317+0000",
            "id": 11
        },
        {
            "author": "Shai Erera",
            "body": "Perhaps I'm missing something, but I don't understand what is the performance hit that's been discussed. If you don't need to clone a Token in order to cache it, don't do it and I agree we shouldn't require it or anything.\n\nIn my scenario, I do need to cache a Token, and then on subsequent calls I use it to generate new Tokens. In next(Token) I populate the given Token with information from the cached Token. That goes until it's consumed, and then I call super.next(Token), and copy its content into the cached Token (I don't use clone() since that allocates a new Token which i don't need).\n\nI don't suffer in my scenario, I'm quite happy w/ how it works and expect to be able to do the exact same thing when I switch to the new API. If that will be the case, then it's fine with me.",
            "date": "2009-06-16T16:39:18.919+0000",
            "id": 12
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nPerhaps I'm missing something, but I don't understand what is the performance hit that's been discussed.\n{quote}\n\nWe're discussing here basically how we want to treat backwards-compatibility for the old API. In the patch attached here the default implementation of TokenStream.incrementToken() throws a UnsupportedOperationException. If the consumer, i.e. indexer, hits that exception it knows to fallback to the old API. This change has the same disadvantage as trunk currently has: for all streams/filters in the core and contrib we have to implement both the old next(Token) and the new incrementToken().\n\nTo avoid this disadvantage Uwe is proposing an elegant change: each TokenStreams owns a reusableToken, which is shared with the consumer (indexer). That's easily possible, because the patch here changes Token to implement all TokenAttribute interfaces. The default implementation of incrementToken() then calls next(Token) and compares the Token next(Token) returns with the reusableToken. If it's not identical then we need to copy all values from the returned Token into the reusableToken, so that the consumer gets the values. This additional copying is what concerns me in terms of performance. \nCurrently e.g. SinkTokenizer always returns a different Token, which means we would always have to copy. We could change SinkTokenizer to not clone, but call Token.reinit(), which copies. But I think there might be a significant amount of filters out there that do similar things. \n",
            "date": "2009-06-16T16:54:07.577+0000",
            "id": 13
        },
        {
            "author": "Uwe Schindler",
            "body": "Grant: But with the new TokenStream API, you must always clone the attributes before caching, because the TokenFilter only has *one* final \"working\" instance per Attribute whose contents change all the time.\nIn this case it would not be better than wrapping next(Token) with the new api and feeding in the current instance token and copy it back into the instance-one if next(Token) returns a different instance.",
            "date": "2009-06-16T16:55:23.751+0000",
            "id": 14
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Currently e.g. SinkTokenizer always returns a different Token, which means we would always have to copy. We could change SinkTokenizer to not clone, but call Token.reinit(), which copies. But I think there might be a significant amount of filters out there that do similar things.\n\nIf SinkTokenizer does this, it would not be very fast even with the old API because it creates a lot of new instances instead of reusing? So the problem here is in SinkTokenizer.",
            "date": "2009-06-16T17:02:00.624+0000",
            "id": 15
        },
        {
            "author": "Shai Erera",
            "body": "Thanks Michael. Given that, I don't think my private scenario will be affected in terms of performance, since my cached token is a different instance then the one that's been passed on the filters, and I just populate the instance passed on the filters with data from the cached token. So besides me copying char[], the consumer will still share the same instance.\n\nPerhaps we should document that as a best practice or something, in light of this proposal?",
            "date": "2009-06-16T17:03:35.313+0000",
            "id": 16
        },
        {
            "author": "Grant Ingersoll",
            "body": "Right, SinkTokenizer is only really cost effective when the number of Tokens that are \"teed\" is roughly less than 1/2, according to my basic tests.  After that, just doing a copy field approach is fine.",
            "date": "2009-06-16T17:15:13.558+0000",
            "id": 17
        },
        {
            "author": "Uwe Schindler",
            "body": "I wanted to add one additional advantage of my Proposal:\nWith it, it is possible (if correctly implemented on the Filter side, must think about it one more time), to mix Filters and TokenStreams together regardless if they implement the new or old API. With the UOE or the current trunk solution, all Filters/Streams in the chain must share the same setting useNewAPI or the same implementation state!\nI would suggest to add a note in the JavaDocs of the deprecated next(Token) method, that it should for optimal performance always return the given Token or null.",
            "date": "2009-06-16T17:20:12.570+0000",
            "id": 18
        },
        {
            "author": "Michael Busch",
            "body": "The in 2.4 released CachingTokenFilter and SinkTokenizer do the cloning, so it was kind of our recommended implementation. Chances are that people have similar implementations. They would see a possible performance hit. Do you think that's ok, Uwe?",
            "date": "2009-06-16T18:15:45.603+0000",
            "id": 19
        },
        {
            "author": "Uwe Schindler",
            "body": "The problem goes further: \nIf the users move from the old Token API to the new one, they will get the same performance decrease. We only have one token instance per tokenizer chain with the new API. So they must also copy the values into their cache and back or think about a completely new implementation of what they have done before.\n\nI will post a patch soon (based on your patch), that implements my thoughts and we could compare.",
            "date": "2009-06-16T18:31:16.405+0000",
            "id": 20
        },
        {
            "author": "Michael Busch",
            "body": "Also what happens if a user starts using the new API and has a TokenStream that adds a different implementation of one or more of the TokenAttribute interfaces?\n\nI think for this case you're proposing that the TokenStream will always add Token right away to the AttributeSource? But that takes away some of the flexibility of the new API?",
            "date": "2009-06-16T18:31:56.046+0000",
            "id": 21
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nIf the users move from the old Token API to the new one, they will get the same performance decrease.\n{quote}\n\nI'm seeing a ~20% performance gain with using the new approach with one Token instance. Because the new implementation clones once in captureState(), and copies then in restoreState() using copyTo(), which is for Token implemented as reinit(), which copies all members.\n\nThe old approach of SinkTokenizer and CachingTokenFilter clones twice. The second clone could be avoided with switching to reinit() also. The old API allows you to do both (double-cloning and clone/copy), the new API forces you to use the more efficient approach.",
            "date": "2009-06-16T18:36:01.260+0000",
            "id": 22
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nAlso what happens if a user starts using the new API and has a TokenStream that adds a different implementation of one or more of the TokenAttribute interfaces? \n{quote}\n\nYou could use the initialize() method I put in there and a user can overwrite it to change the default behavior of putting a Token into the AttributeSource.",
            "date": "2009-06-16T18:47:16.608+0000",
            "id": 23
        },
        {
            "author": "Uwe Schindler",
            "body": "Hallo Michael,\n\nI played a little bit around. This patch implements my proposal:\n- removes all deprecated API and corresponding wrapper from DocInverter and QueryParser.\n- The default TokenStream calls in each of the three possible method the next best one, wrapping the tokens with cloning, as described. Please note: As before, if one extends TokenStream/TokenFilter but does not override one of these methods, indexing will loop and stack overflow. I could add a test in initialize, that tests, if at last one of the methods is overridden (which is a runtime check). An example how this could be done is currently commented out (but was used for some other test).\n- I played a little bit, trying to only register the \"big\" Token instance in the stream, if one does not override incrementToken (see code commented out). But the problem is, that this is then backwards compatible only in one direction: consumers calling incrmentToken succeed always, but some outdated consumers alling next() or next(Token) then fail, because the instance may not be Token, if all producers implement incrementToken and TokenStreams would not register Token as impl. Because this does not work, the TokenStreams and Filters register Token as Impl and use it for wrapping. Because of this, the simple TokenAttribute impls are now useless.\n- There is currently no possibility to change the default Token impl, I will think about it during the night!\n\nI tested the attached patch with core (all tests pass, so \"new\" token streams work correct) and contrib/analyzers (all tests pass, so also old token streams work correct). You can even use mixed impls (not fully tested yet, but you can have e.g. a new-style tokenstream and filter it with an old-style tokenfilter).\n\nWith this patch, all core tokenstreams could be updated, to only implement the new API and remove all next(Token) impls.\n\nUwe",
            "date": "2009-06-17T00:21:25.471+0000",
            "id": 24
        },
        {
            "author": "Michael Busch",
            "body": "I haven't review the patch yet, but I have a quick question:\n\nis your patch backwards-compatible if a user has a TokenStream or -Filter which returns a custom subclass of Token? And then another one in the chain casts to that subclass? Note that Token is not final. Also not sure how common this scenario is, just came to my mind.\n\nAlso, can a user still use the AttributeFactory to use something else but Token?",
            "date": "2009-06-17T00:33:18.885+0000",
            "id": 25
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. is your patch backwards-compatible if a user has a TokenStream or -Filter which returns a custom subclass of Token? And then another one in the chain casts to that subclass? Note that Token is not final. Also not sure how common this scenario is, just came to my mind.\n\nThis is no problem; I can explain:\nReturning something other than Token from next() only makes sense, if the *direct* consumer can handle it. So A TokenStream that returns these tokens must then be wrapped by a TokenFilter that can handle these Tokens. If there would be any other TokenFilter in between, it is not guaranteed, that this TokenFilter also returns the special Token. When you have this direct relation, the calls from the Filter to the prducers method are directly without wrapping (because both implement the old API). At the point where the indexer consumes the TokenFilter on top, the custom Token is uninteresting and can safely copied into a conventional Token (which is done because nextToken!=attributeInstance).\n\nbq. Also, can a user still use the AttributeFactory to use something else but Token?\n\nAs noted in my patch description: This is not possible. One can add additional attributes and use them in his chain (even when old filters are in between, which only handle the Token instance). TokenStream and TokenFilter creates a Token() instance on initialize() and call AddAttributeImpl(). After doing this, it checks, if all Attributes are then subclass of Token, and calls getAttribute(TermAttribute) is called and the result casted to Token (which then should be the same).\n\nOne could change this behaviour if he overrides initialize() in one of his classes, but then another TokenSteam/Filter in the chain also initializing, will see, that one of the instances is *not* Token and will throw a RuntimeException. I tried everything, to be able to handle both pathes (old -> new API, new -> old API), TokenStream and TokenFilter must have a Token instance. In 3.0 or later this can be removed and we will only use the factory to init the attributes.\n\nIn my opinion, this is not a problem, because one could still add custom attributes to his chain and the best: he can mix old and new tokenstreams in one chain as he want. The missing flexibility in modifying the instances of Tokenattributes are in my opinion not important (and one instance initializes faster than 5).",
            "date": "2009-06-17T05:54:32.872+0000",
            "id": 26
        },
        {
            "author": "Michael Busch",
            "body": "I quickly hacked a tool demonstrating my concerns.\n\nRunning the attached tool on trunk+my patch yields the following output:\n\n{noformat}\nnew\ntokenstream --> proper noun\napi\n{noformat}\n\nThe output is identical if the tool is run on Lucene 2.4.\n\n\nRunning the same tool using trunk+uwe's patch yields:\n{noformat}\nnew\ntokenstream\napi\n{noformat}\n\nThis tool might not make much sense, but it shows in what unexpected ways people might use these APIs. It doesn't break API compatibility, but changes runtime behavior - in this case if users have their own subclasses of Token.",
            "date": "2009-06-17T06:44:29.098+0000",
            "id": 27
        },
        {
            "author": "Shai Erera",
            "body": "Doesn't this mean that we need to change all our Core TokenStream/TokenFilter impls to use clone() instead of instantiating a Token themselves (i.e., using one of its ctors)? If we use clone() you shouldn't experience that problem because the type of the cloned Token will be your subclass, and any core filter/stream can still work with Token and just use its methods.\n\nWe can also add a newToken() method to Token, and let the Token extensions return a new Token instance of their type. That is like clone() only it won't copy any characters and initialize fields.\n\nIf we think Token should be extended, I think we should also add proper documentation to TokenStream that mentions this possible way of using and our recommended approach.",
            "date": "2009-06-17T06:52:28.960+0000",
            "id": 28
        },
        {
            "author": "Michael Busch",
            "body": "I don't think we mention subclassing of Token really in the documentation. We also certainly don't prevent it. The tool I wrote works fine with 2.4, if you add other filters to the chain it might not work anymore. But since we don't promise that subclassing of Token works everywhere, that's probably fine.\n\nWe're deprecating the old API anyway, so we shouldn't have to introduce new stuff to fully support subclassing Token.\n\nMy point here is just that this is a very complex API (even though it looks pretty simple). When I wrote the new TokenStream API patch end of last year I thought about all these possibilities of making backwards compatibility more elegant. But I wanted to be certain to not break any runtime behavior or affect performance negatively. Therefore I decided to not mess with the old API, but rather put the burden of implementing both APIs on the committers during the transition phase. I know this is somewhat annoying, on the other hand, how often do we really add new TokenFilters to the core? Often implementing incrementToken() takes 10 minutes if you already have next() implemented. Just copy&paste and change a few things.\n\n",
            "date": "2009-06-17T07:23:40.955+0000",
            "id": 29
        },
        {
            "author": "Michael Busch",
            "body": "But I'll definitely buy Uwe a beer if he comes up with solution that is more elegant and doesn't have the mentioned disadvantages! :)",
            "date": "2009-06-17T07:25:22.567+0000",
            "id": 30
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Michael,\nin principle your test is invalid. It has other tokenfilters in the chain, which the user has no control on. With the two mentioned filters it may work, because they do not change the reuseableToken instance. But the API clearly states, that the reuseableToken must not be used and another one can be returned.\nSo this is really unsupported behaviour. If you remove the filters in between, it would work correct. And this could even fail with 2.4 if you put other tokenfilters in your chain.\n\nIn my opinion, the advantages of the token reuse clearly overweigh the small problems with (unsupported) usage. The API does exactly, what is menthioned in the API Docs for 2.4.1.\n\nThe main advantage is, that you can mix old and new filter instances and you loose nothing...",
            "date": "2009-06-17T07:38:56.321+0000",
            "id": 31
        },
        {
            "author": "Michael Busch",
            "body": "OK, what about this sentence in Token.java:\n\n{code:java}\n  When caching a reusable token, clone it. When injecting a cached token into a stream that can be reset, clone it again.\n{code}\n\nThis double-cloning is exactly what CachingTokenFilter and Tee/Sink do, so they preserve the actual Token class type.\nYou can easily construct an example similar to the tool I attached that uses these streams. \n\n",
            "date": "2009-06-17T07:54:49.636+0000",
            "id": 32
        },
        {
            "author": "Uwe Schindler",
            "body": "OK, I have a solution:\nI write a wrapper class (a reference) that implement all token attribute interfaces but pass this downto the wrapped Token/Subclass-of-Token. Instead of cloning the token when wrapping the return value of next(), I could simply put it into the wrapper. The instance keeps the same, only the delegate is different. Outside users or TokenStreams using the new API, will only see one instance that implements all interfaces.\n\n(in principle the same like your backwards-compatibility thing in the docinverter)\n\nWould this be an idea?",
            "date": "2009-06-17T08:13:30.863+0000",
            "id": 33
        },
        {
            "author": "Michael Busch",
            "body": "For caching:\nI guess you would have to implement the wrapper's clone() method such that it returns what delegate.clone() returns. This would put a clone of the original Token (or subclass) into the cache, instead a clone of the wrapper, which is good. Then the second clone also clones the original Token again and put's it into a second wrapper that the CachingTokenStream owns. Hmm complicated, but should work. \n\nNeed to think more about if all mixes of old and new TokenSteams would work... and if this approach affects performance in any way or changes runtime behavior of corner cases...\n\nGosh, this is like running a huge backwards-compatibility junit test suite in my head every time we consider a different approach. :)\n\nI think you should try it out and see if you run into problems. This should not be much code to write. You might have to do tricks with Tee/Sink, if the sink is wrapped by a filter with the new API, but the tee wraps a stream with the old API, or vice versa.",
            "date": "2009-06-17T08:44:14.551+0000",
            "id": 34
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. I think you should try it out and see if you run into problems. This should not be much code to write. \n\nI am working on that, I have a meeting now, after that. \n\nbq. You might have to do tricks with Tee/Sink, if the sink is wrapped by a filter with the new API, but the tee wraps a stream with the old API, or vice versa.\n\nThis is currently working without any problems, but I want to add a test-case, that explicitely chains some dummy-filters in deprecated and not-deprecated form and looks whats coming out. But it should work.",
            "date": "2009-06-17T08:55:38.853+0000",
            "id": 35
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI am working on that, I have a meeting now, after that. \n{quote}\n\nGood luck. I'm off to bed...",
            "date": "2009-06-17T09:01:59.179+0000",
            "id": 36
        },
        {
            "author": "Uwe Schindler",
            "body": "Attached is a new patch, that implements the last idea:\n- There is no more copying of Tokens, so the API should have the same speed (almost) as before.\n- Per default, the chain of TokenStreams/TokenFilters can be mixed completely (test that explicitely tests this is still missing), the drawback is, that there is only *one* attribute instance called TokenWrapper (package private) that manages the exchange of the Token instance behind.\n- If the user knows, that all tokenizers in his JVM implement incrementToken and do not fallback to next(), he can increase speed by using the static setter setOnlyUseNewAPI(true). In this case, no single TokenWrapper is initialized and code will use the normal Attribute factory to generate the Attributes. If some old code is still available or your consumer calls next(), you will get an UOE during tokenization. The same happens, if you override initialize() and instantiate your attributes manually without super.initialize().\n- When the old API is removed, TokenWrapper and large parts inside TokenStream can be removed and incrementToken() made abstract. This is identical to setting onlyUseNewAPI to true.\n- the api setting can only be static, because the attribute instances are generated during construction of the streams and so a later downgrade to TokenWrapper is not possible.\n\nDocumentation inside this patch enforce, that at least all core tokenizers and consumers are conformant, so one must be able to set TokenStream.setOnlyUseNewAPI to true and then use StandardAnalyzer without any problem. When contrib is transformed, we can extend this to contrib.\n\nBecause the code wraps the old API completely, all converted streams can be changed to only implement only incrementToken() using attributes. Super's TokenStream.next() and next(Token) manage the rest. There is no speed degradion by this, it is safe to remove (and all will be happy)!\n\nUwe",
            "date": "2009-06-17T12:23:11.002+0000",
            "id": 37
        },
        {
            "author": "Uwe Schindler",
            "body": "Sorry, small bug in cloning inside next(): the POSToken-test was failing again. But now it works also correct.",
            "date": "2009-06-17T12:35:33.833+0000",
            "id": 38
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Michael,\nI did not do any performance tests until now, I think you have the better knowledge about measuring tokenization performance. Important would be to compare perf of:\n- Old API with useNewAPI=true\n- Old API with useNewAPI=false\n- My impl with defaults (onlyUseNewAPI=false)\n- My impl with onlyUseNewAPI=true\nFor all tests, you should only use conformant streams (e.g. from core).\nAn good additional test would be to create a chain that has completely implemented incrementToken() and one only suplying next() for some chain entries.\nIs this hard to do?",
            "date": "2009-06-17T13:54:43.259+0000",
            "id": 39
        },
        {
            "author": "Shai Erera",
            "body": "You can run tokenize.alg which invokes the ReadTokenTask, which iterates on a TokenStream. You'll probably need to modify the .alg file to create a different analyzer/token stream each time, and I think this can be done by the \"rounds\" syntax in benchmark.",
            "date": "2009-06-17T14:49:10.667+0000",
            "id": 40
        },
        {
            "author": "Michael Busch",
            "body": "I'm looking at TokenStream.next():\n\n{code:java}\n  public Token next(final Token reusableToken) throws IOException {\n    // We don't actually use reusableToken, but still add this assert\n    assert reusableToken != null;\n    checkTokenWrapper();\n    return next();\n  }\n\n  /** Returns the next token in the stream, or null at EOS.\n   *  @deprecated The returned Token is a \"full private copy\" (not\n   *  re-used across calls to next()) but will be slower\n   *  than calling {@link #next(Token)} instead.. */\n  public Token next() throws IOException {\n    checkTokenWrapper();\n    if (incrementToken()) {\n      final Token token = (Token) tokenWrapper.delegate.clone();\n      Payload p = token.getPayload();\n      if (p != null) {\n        token.setPayload((Payload) p.clone());\n      }\n      return token;\n    }\n    return null;\n  }\n{code}\n\nThis seems like a big performance hit for users of the old API, no? Now every single Token will be cloned, even if they implement next(Token), as soon as the users have one filter in the chain that doesn't implement the new API yet.",
            "date": "2009-06-17T17:55:57.823+0000",
            "id": 41
        },
        {
            "author": "Uwe Schindler",
            "body": "The code is almost identical to before, the old code also copied the token to make it a full private copy.\nThere are three modes of operation:\n- if incrementToken is implemented, docinverter will use it (the code always calls incrementToken, so no indirection)\n- if next(Token) is implemented, the docinverterwill call incrementToken which is forwarded to next(Token), which is cheap\n- if only next() is implemented, the docinverter will call incrementTojen, which forwards to next(Token) and this forwards to next(). But this is identical to before, only one indirection more: the old code got useNewAPI(false) and called next(Token) which forwarded to next()\n\nSo for indexing using the normal indexing components (docinverter), the code is never cloning more that with your code.\n\nThere is one other case: if you have an old consumer calling nextToken(Token), the tokenizer only implemented incrementToken, then you will get a performance degradion. But this is not the indexing case, it is e.g. reusing the tokenizer in a very old e.g. QueryParser. I did not find a good way to pass directly for this special case to incrementToken(). The problem is also, that incrementToken uses the internal buffer and not the supplied buffer.",
            "date": "2009-06-17T18:20:07.134+0000",
            "id": 42
        },
        {
            "author": "Uwe Schindler",
            "body": "Ah I understand the problem: As I told, if a consumer (like a filter() calls next(Token) on the underlying filter), which does not implement this or implements the new API,  he will get a performance decrease because of cloning. I think, we should simply test this with the benchmarker. Mixing old and new API is always a performance decrease.",
            "date": "2009-06-17T18:26:26.580+0000",
            "id": 43
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nAh I understand the problem: As I told, if a consumer (like a filter() calls next(Token) on the underlying filter), which does not implement this or implements the new API,  he will get a performance decrease because of cloning. I think, we should simply test this with the benchmarker. Mixing old and new API is always a performance decrease.\n{quote}\n\nYes that's what I mean. But I think this will almost be the most common use case: I would think most users have chains that mix core streams/filters with custom filters. Also I assume most users who need high performance switched from next() to next(Token) by now. These users will see a performance degradation, which I predict will be similar or worse as going back to using next(), unless they implement the new API in their filters right away.\n\nSo those users will see a performance hit if they just do a drop-in replacement of the lucene jar. ",
            "date": "2009-06-17T18:34:34.597+0000",
            "id": 44
        },
        {
            "author": "Uwe Schindler",
            "body": "I could change the calling chain:\nincrementToken() calls next() calls next(Token), would this be better. next(Token) would per default set the delegate to the reuseable token. hmhm - thinking about it. Where is then the degradion?",
            "date": "2009-06-17T18:39:58.317+0000",
            "id": 45
        },
        {
            "author": "Uwe Schindler",
            "body": "I have a solution to build in some shortcuts:\nin initialize I use reflection (see the earlier patch) to find out, which of the three methods is implemented (check if this.getClass().getMethod(name,params).getDeclaringClass() == TokenStream.class, when this is true, the method was *not* overridden).\nin incrementToken() the method checks if either next(Token) or next() is implemented and calls direct. The same in the other classes. next() should be ideally never called then.\nI will post a patch later.",
            "date": "2009-06-17T19:20:49.093+0000",
            "id": 46
        },
        {
            "author": "Mark Miller",
            "body": "Should I wait to put in the Highlighter update till you guys are done here?",
            "date": "2009-06-17T19:29:09.503+0000",
            "id": 47
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Should I wait to put in the Highlighter update till you guys are done here?\nYou can start with highlighter, if this patch goes through, we can remove the next() methods from all tokenizers.\nFor consumers like the highlighter, there will be no need anymore to switch between old/new api. Just use the new API, it will also work with old tokenizers.\n\n",
            "date": "2009-06-17T19:53:29.432+0000",
            "id": 48
        },
        {
            "author": "Michael Busch",
            "body": "I'm not convinced yet that we will be able to remove the implementations of next() and next(Token). \nMark, I'm not familiar with what changes you need to make to the highlighter, but you should not rely yet on the fact that next() and nextToken() won't have to be implemented anymore.",
            "date": "2009-06-17T20:54:49.683+0000",
            "id": 49
        },
        {
            "author": "Uwe Schindler",
            "body": "Here my solution: The three default methods are now optimized to use the shortest path to the by subclasses implemented iteration method. The implemented iteration methods are determined by reflection in initialize().\nCloning now only done, if next() is directly called by a consumer, in all other cases the reuseableToken is used for passing the attributes around.\n\nThe new TokenStream also checks in initialize, that one of the \"abstract\" methods is overridden. Because of this TestIndexWriter and the inverter singleton state was updated to at least have an empty incrementToken(). Because of this check, nobody can create a TokenStream, that loops indefinite after calling next() because no pseuso-abstract method was overridden. As incrementToken will be abstract in future, it must always be implemented, and this is what I have done.",
            "date": "2009-06-17T21:00:29.685+0000",
            "id": 50
        },
        {
            "author": "Michael Busch",
            "body": "Slightly changes tool yields on 2.4 and identically on trunk + my patch:\n\n{noformat}\nnew\ntokenstream --> proper noun\napi\nnew\ntokenstream --> proper noun\napi\nnew\ntokenstream\napi\n{noformat}\n\nOn trunk + your latest patch:\n{noformat}\nnew\ntokenstream --> proper noun\napi\nnew\ntokenstream\napi\nException in thread \"main\" java.lang.ClassCastException: org.apache.lucene.util.AttributeSource$State\n\tat org.apache.lucene.analysis.SinkTokenizer.next(SinkTokenizer.java:97)\n\tat org.apache.lucene.analysis.TestCompatibility.consumeStream(TestCompatibility.java:97)\n\tat org.apache.lucene.analysis.TestCompatibility.main(TestCompatibility.java:90)\n\n{noformat}\n\nIt runs three tests. The first is good with your patch; the second doesn;t seem to preserve the right Token subclass; the third throws a ClassCastException. I haven't debugged why...",
            "date": "2009-06-17T21:27:00.910+0000",
            "id": 51
        },
        {
            "author": "Michael Busch",
            "body": "You can probably fix CachingTokenFilter and tee/sink to behave correctly. But please remember that a user might have their own implementations of something like a CachingTokenFilter or tee/sink, which must keep working.",
            "date": "2009-06-17T21:37:59.226+0000",
            "id": 52
        },
        {
            "author": "Michael Busch",
            "body": "Btw: SinkTokenizer in my patch has a small bug too. I need to throw a UOE in incrementToken() if it was filled using the old API.\n\nIt should probably also throw a UOE when someone tries to fill it with both, old and new API streams. And that this is not allowed must be made clear in the javadocs.",
            "date": "2009-06-17T21:48:48.281+0000",
            "id": 53
        },
        {
            "author": "Uwe Schindler",
            "body": "Exactly: The problem is in SinkTokenizer. when calling next(Token)  the result is casted to Token, which does not work (the iterator only contains either Tokens or States, dependent on what was added. As SinkTokenizer and TeeTokenFilter may use different APIs it crahes.\nThe problem with the test is, that depending on chaining with old/new APIs the iter may conatin wron type. This can be fixed by removing next(Token) (preferred) or incrementToken() . The problem is that dependent on chaining it is not clear which method is called and the new/old API should not share the same state information.\n\nBecause the problem is related new/old API, we should simply remove the old API from both filters, so they share the same instances in all cases! Then we do not need UOE.\n\nI will look into and check, why the Token in the second test is not preserverd",
            "date": "2009-06-17T21:59:36.147+0000",
            "id": 54
        },
        {
            "author": "Uwe Schindler",
            "body": "The second test does not work, because it always uses per default incrementToken.\n\nBy the way, the APIdocs and behaviour changed with these three classes, TeeTokenFilter, SinkTokenizer and CachingTokenFilter: e.g. getTokens() does not return what is noted. For backwards-compatiblility we should deprecate the current versions of these class [and only let them implement next(Token)]. They can then be used even together with the new API, but they always work on Token instances. When I remove incrementToken from them your test passes complete.\nFor the new API there should be new classes, that use attributesource and restorestate to cache and so on.\n\nBut for current backwards compatibility (you mentioned, somebody have written a similar thing): If the user's class only uses next(Token) it will work as before. The problem is mixed implementations of old/new API and different cache contents. This is not a problem of my proposal!\n\nAgain: We should remove the double implementations everywhere. In these special cases with caches, where the cache should contain a specific class (Tokens or AttributeSource.State), two classes are needed, one deprecated.\n\nBut: what do you think about my latest patch in general?",
            "date": "2009-06-17T22:19:09.559+0000",
            "id": 55
        },
        {
            "author": "Uwe Schindler",
            "body": "Small updates, before I go to sleep. This patch removes the incrementTokenAPI from the three caching classes. It also fixes the double cloning of the payload in next() when the token is cloned directly.\nThere is still one small problem, that your test -- I hate it... :-( -- fails again, if I remove next(Token) from StopFilter or LowerCaseFilter.",
            "date": "2009-06-17T23:10:59.781+0000",
            "id": 56
        },
        {
            "author": "Michael Busch",
            "body": "Go to bed, I'll review later... in meetings now...",
            "date": "2009-06-17T23:19:13.616+0000",
            "id": 57
        },
        {
            "author": "Uwe Schindler",
            "body": "Sorry, last patch was invalid (did not compile), I forgot to to revert some changes before posting.\nAttached patch has still problems in TeeTokenStream, SinkTokenizer and CachingTokenFilter (see before), but fixes:\n- double cloning of payloads\n- the first of your tests works correct, even if i remove next() from StopFilter and/or LowercaseFilter",
            "date": "2009-06-18T06:18:47.142+0000",
            "id": 58
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nFor backwards-compatiblility we should deprecate the current versions of these class [and only let them implement next(Token)]. \n{quote}\n\nI agree. With my patch the Tee/Sink stuff doesn't work in all situations either, when the new API is used. We need to deprecate tee/sink and write a new class that implements the same functionality with the new API.",
            "date": "2009-06-18T09:16:37.684+0000",
            "id": 59
        },
        {
            "author": "Uwe Schindler",
            "body": "OK, we can merge our patches then! At the moement I see no real show-stoppers with the current aproach, have you tested thoroughly and measured performance? All tests from core and contrib/analyzers pass, the problems with your last TestCompatibility.java are Tee/Sink problems.\nThe interesting part (if we stay with my not-so-elegant-anymore solution because of reflections hacks), would be to remove the deprecated next(Token) methods from core streams, which would be a great code cleanup!",
            "date": "2009-06-18T09:39:00.613+0000",
            "id": 60
        },
        {
            "author": "Uwe Schindler",
            "body": "By the way, I tested Solr's token streams also after updating the lucene jar file. All tests pass (only some not related ones fail because of latest changes in Lucene trunk and some compile failures because of changes in no-released APIs).\nSolrs TokenStreams are all programmed with the old-api, but they get inverted using incrementToken from our patch.\nAlso the solr query parser seems to work.",
            "date": "2009-06-18T10:02:19.379+0000",
            "id": 61
        },
        {
            "author": "Uwe Schindler",
            "body": "Again an update: Unified the reuseable tokens in the TokenWrapper.delegate. No it is always set after each action, so no state changes left out.",
            "date": "2009-06-18T12:05:25.308+0000",
            "id": 62
        },
        {
            "author": "Grant Ingersoll",
            "body": "{quote}\nBy the way, I tested Solr's token streams also after updating the lucene jar file. All tests pass (only some not related ones fail because of latest changes in Lucene trunk and some compile failures because of changes in no-released APIs).\nSolrs TokenStreams are all programmed with the old-api, but they get inverted using incrementToken from our patch.\nAlso the solr query parser seems to work. \n{quote}\n\nDid you look at the performance on this?",
            "date": "2009-06-18T14:35:18.465+0000",
            "id": 63
        },
        {
            "author": "Uwe Schindler",
            "body": "I only tested performance with the lucene benchmarker on the various standard analyzers. The tokenizer.alg produces after the patch the same results as before in almost the same time (time variations are bigger than differences). With an unmodified benchmarker, this is clear, benchmarkers tokenizer task call still the deprecated next(Token) and as all core analyzers still implement this directly, so there is no wrapping. I modified the tested tokenstreams and filters in core, that were used, and removed next(Token) and left only incrementToken() avalilable, in this case the speed difference was also not measureable in my configuration (Thinkpad T60, Core Duo, Win32). I also changed some of the filters to implement next(Token) only, others to only incrementToken(), to have a completely mixed old/new API chain, and still the same results (and same tokenization results, as seen in generated indexes for wikipedia). I also changed the benchmarker to use incrementToken(), which was also fine.\n\nTo have a small speed incresase (but I was not able to measure it), I changed all tokenizers to use only incrementToken for the whole chain and changed the benchmarker to also use this method. In this case I was able to TokenStream.setOnlyUseNewAPI(true), which removed the backwards-compatibility-wrapper and the Token instance, so the chain only used the unwrapped simple attributes. In my opinion, tokenization was a little bit faster, faster than without any patch and next(Token). When the old API is completely removed, this will be the default behaviour.\n\nSo I would suggest to review this patch, add some tests for heterogenous tokenizer chains and remove all next(...) implementations from all streams and filters and only implement incrementToken(). Contrib analyzers should then only be rewritten to the new API without the old API.\n\nThe mentioned bugs with Tee/Sink are not related to this bug, but are more serious now, because the tokenizer chain is no longer fixed to on specfic API variant (it supports both mixed together).\n",
            "date": "2009-06-18T19:27:11.413+0000",
            "id": 64
        },
        {
            "author": "Michael Busch",
            "body": "Sorry, Uwe, I was really busy today. I'll try to review the latest patch as soon as I can.",
            "date": "2009-06-19T10:16:04.780+0000",
            "id": 65
        },
        {
            "author": "Uwe Schindler",
            "body": "After committing TrieRange to core, here some updates to the patch (TrieRange used setUseNewAPI in its tests).\nThis patch now also has all next(Token) implementations removed in core (excluding Tee, Sink and CachingToken.)",
            "date": "2009-06-19T12:41:04.817+0000",
            "id": 66
        },
        {
            "author": "Uwe Schindler",
            "body": "During my tests I found a small problem with AttributeSource.getAttributesIterator() & Co.:\nIf you, for example, print TokenStream.toString(), but have all the 6 small interfaces like TermAttribute, FlagsAttribute,.. in the Token class implemented, you will get 6 times the same string, because the iterator enumerates the Token instance 6 times. The same happens when cloning all Attributes or doing a copyTo of all attributes, equals of AttributeSource: They are handled 6 times. So the list of AttributesImplementations should be unique. So in principle the instances should also put into a LinkedHashSet in parallel to the Interface->Implementation mapping which is unique only on the interface side. The getAttributesIterator then should return the iterator on the implementation set. It would fix all double handling and would be faster for our new standard case for backwards compatibility, where e.g. Token is copyTo()'ed 6 times on captureState and so on.",
            "date": "2009-06-19T14:17:18.999+0000",
            "id": 67
        },
        {
            "author": "Mark Miller",
            "body": "How is this patch coming?\n\nDo you think MemoryIndex will still need to be changed to check the TokenStreams API (al la the QueryParser), or is this going to clean it up?",
            "date": "2009-06-23T23:05:33.971+0000",
            "id": 68
        },
        {
            "author": "Uwe Schindler",
            "body": "Michael did not yet respond (I think he is busy).\n\nFor me this patch works perfectly, all tests pass and performance is same as before. The big difference is, that you do not need to implement both APIs in your TokenStreams (the current patch removes the old API completely from core tokenizers). Consumers like the highlighter can simply only use the new API, but will also work when calling the old API. And you can even mix old an new API TokenFilters in one chain.\n\nIn principle this is all we need. Maybe you can also do some tests!",
            "date": "2009-06-23T23:13:39.938+0000",
            "id": 69
        },
        {
            "author": "Uwe Schindler",
            "body": "By the way: Token is no longer deprecated, it is just an implementation of the six standard attributes in one class!",
            "date": "2009-06-23T23:16:21.192+0000",
            "id": 70
        },
        {
            "author": "Mark Miller",
            "body": "Ah, nice - I had to re-implement it for one or two spots, will be nice to just keep the old.\n\nThat sounds great Uwe - I had thought thats where this was going but Michael seemed to have some doubts.\n\nI'll try it out with the Highlighter changes I have made.",
            "date": "2009-06-23T23:35:38.078+0000",
            "id": 71
        },
        {
            "author": "Mark Miller",
            "body": "bq. The big difference is, that you do not need to implement both APIs in your TokenStreams (the current patch removes the old API completely from core tokenizers).\n\nThats a big back compat violation though right ? :)\n",
            "date": "2009-06-23T23:54:24.736+0000",
            "id": 72
        },
        {
            "author": "Michael Busch",
            "body": "Ok another version of my hated compatibility patch. What I did was to copy the exact 2.4 implementations of TeeTokenFilter and SinkTokenizer into the test class and use them in the test I previously had. It performs the test twice: using the old and the new API.\n\nOn my patch it produces:\n{noformat}\nTesting old API...\nnew\ntokenstream --> proper noun\napi\nnew\ntokenstream --> proper noun\napi\nnew\ntokenstream\napi\nTesting new API...\nException in thread \"main\" org.apache.lucene.analysis.TokenStream$UnsupportedTokenStreamAPIException: This stream or filter does not support the new Lucene 2.9 TokenStream API yet.\n\tat org.apache.lucene.analysis.TokenStream.incrementToken(TokenStream.java:114)\n\tat org.apache.lucene.analysis.TestAPICompatibility.consumeStreamNewAPI(TestAPICompatibility.java:145)\n\tat org.apache.lucene.analysis.TestAPICompatibility.test2(TestAPICompatibility.java:119)\n\tat org.apache.lucene.analysis.TestAPICompatibility.main(TestAPICompatibility.java:83)\n{noformat}\n\nOn your patch, Uwe:\n\n{noformat}\nTesting old API...\nThis\nTokenStream\napi\nThis\nTokenStream\napi\nThis\nTokenStream\napi\nTesting new API...\nThis\nTokenStream\napi\nThis\nTokenStream\napi\nThis\nTokenStream\napi\n{noformat}\n\n",
            "date": "2009-06-24T06:45:22.199+0000",
            "id": 73
        },
        {
            "author": "Uwe Schindler",
            "body": "{quote}\nbq. The big difference is, that you do not need to implement both APIs in your TokenStreams (the current patch removes the old API completely from core tokenizers).\n\nThats a big back compat violation though right ? :)\n{quote}\nNo it isn't. It is not completely removed, the abstact TokenStream emulates the old next() methods, so even if all core tokenizers do not implement next(), they still export the next(Token) and next() methods from TokenStream.",
            "date": "2009-06-24T06:59:57.612+0000",
            "id": 74
        },
        {
            "author": "Mark Miller",
            "body": "Ah, okay. Sounds like some magic you have worked here Uwe - great stuff. Very nice to not have to impl twice everywhere.",
            "date": "2009-06-24T11:52:51.008+0000",
            "id": 75
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Michael,\nthis was a small bug in the translation next(Token) -> incrementToken(). If the call to incrementToken itsself calls other filters, that itsself delegate back to next() the wrapper.delegate can change during the incrementToken call. I fixed this, now it works:\n\n{code}\n    [junit] ------------- Standard Output ---------------\n    [junit]\n    [junit] Test old API...\n    [junit] new\n    [junit] tokenstream --> proper noun\n    [junit] api\n    [junit] new\n    [junit] tokenstream --> proper noun\n    [junit] api\n    [junit] new\n    [junit] tokenstream\n    [junit] api\n    [junit]\n    [junit] Test new API...\n    [junit] new\n    [junit] tokenstream --> proper noun\n    [junit] api\n    [junit] new\n    [junit] tokenstream --> proper noun\n    [junit] api\n    [junit] new\n    [junit] tokenstream\n    [junit] api\n{code}\n\nAre you happy now? I think Mark is also very interesting in this and fast progress.\n\nIn my opinion, only the new Tee/Sink/Cache impls are needed and the fix for the duplicate attributes in toString().",
            "date": "2009-06-24T12:42:24.242+0000",
            "id": 76
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. But I'll definitely buy Uwe a beer if he comes up with solution that is more elegant and doesn't have the mentioned disadvantages!  \n\nBy the way: I want to have my beer!!!",
            "date": "2009-06-24T12:48:36.685+0000",
            "id": 77
        },
        {
            "author": "Michael Busch",
            "body": "Sorry, I don't want to be the bad guy here, just trying to mention things that come to my mind. Maybe this one is negligible?\n\nIf the user had extensions of streams/filters that in 2.4 didn't declare next() or next(Token) as final and calls super.next() or super.next(Token), then I think you'll get different behavior or exceptions. E.g.: \n{code:java}\n  private static class ExtendedSinkTokenizer extends SinkTokenizer {\n\t  public Token next() throws IOException {\n\t\t  Token t = super.next();\n\t\t  // do something with t\n\t\t  return t;\n\t  }\n  }\n{code}",
            "date": "2009-06-24T20:11:20.553+0000",
            "id": 78
        },
        {
            "author": "Uwe Schindler",
            "body": "In your example, you mean SinkTokenizer is from the core and only implements incrementToken(), the others are handled by the default in TokenStream (using this reflection-based redirect?)\n\nI tested this with overriding LowerCaseFilter. In my latest patch LowerCaseFilter only implements incrementToken(), next() and next(Token) are handled by the forwarder in TokenStream. If you override next(Token) and consume this stream using incrementToken(), then your overriden next(Token) is never called.\n\nThis exact same backwards-compatibility problem was also there when changed from next() to next(Token). If some core filter only implemented next(Token) and a subclass overrides only next() (because older code-base) and calls super() there, the same happened: the tokenizer code called next(Token), but next() was never called. You have the same problem with our recent deprecations in DocIdSetIterator.\n\nI would no see this as a problem, this problem can be found everywhere in Lucene, where we deprecated (originally abstract) methods and replaced by newer ones calling the old ones as default.\n",
            "date": "2009-06-24T20:41:19.749+0000",
            "id": 79
        },
        {
            "author": "Michael Busch",
            "body": "Not sure if we're talking about the same thing here. The problem is not that next() in the extended class doesn't get called. It gets called, but the super.next() call throws either a ClassCastException, when consuming using the old API, or produces a different result than before uwhen consuming the new API. \n\nThis is the output on your patch:\n{noformat}\nTesting old API...\nnew\ntokenstream --> proper noun\napi\nnew\ntokenstream --> proper noun\napi\nnew\ntokenstream\napi\nnew\ntokenstream --> proper noun\napi\n(new,12,15)\nnew\n(tokenstream,16,27)\ntokenstream\n(api,28,31)\napi\nnull\njava.lang.ClassCastException: org.apache.lucene.util.AttributeSource$State\n\tat org.apache.lucene.analysis.SinkTokenizer.next(SinkTokenizer.java:97)\n\tat org.apache.lucene.analysis.TestCompatibility.consumeStreamOldAPI(TestCompatibility.java:194)\n\tat org.apache.lucene.analysis.TestCompatibility.test2(TestCompatibility.java:142)\n\tat org.apache.lucene.analysis.TestCompatibility.main(TestCompatibility.java:87)\nTesting new API...\nnew\ntokenstream --> proper noun\napi\nnew\ntokenstream --> proper noun\napi\nnew\ntokenstream\napi\nnew\ntokenstream --> proper noun\napi\n(new,12,15)\nnew\n(tokenstream,16,27)\ntokenstream\n(api,28,31)\napi\nnull\nnew\ntokenstream\napi\n{noformat}\n\nOn my patch:\n{noformat}\nTesting old API...\nnew\ntokenstream --> proper noun\napi\nnew\ntokenstream --> proper noun\napi\nnew\ntokenstream\napi\nnew\ntokenstream --> proper noun\napi\n(new,12,15)\nnew\n(tokenstream,16,27)\ntokenstream --> proper noun\n(api,28,31)\napi\nnull\nnew\ntokenstream\napi\nTesting new API...\norg.apache.lucene.analysis.TokenStream$UnsupportedTokenStreamAPIException: This stream or filter does not support the new Lucene 2.9 TokenStream API yet.\n\tat org.apache.lucene.analysis.TokenStream.incrementToken(TokenStream.java:114)\n\tat org.apache.lucene.analysis.TestAPICompatibility.consumeStreamNewAPI(TestAPICompatibility.java:210)\n\tat org.apache.lucene.analysis.TestAPICompatibility.test3(TestAPICompatibility.java:162)\n\tat org.apache.lucene.analysis.TestAPICompatibility.main(TestAPICompatibility.java:93)\norg.apache.lucene.analysis.TokenStream$UnsupportedTokenStreamAPIException: This stream or filter does not support the new Lucene 2.9 TokenStream API yet.\n\tat org.apache.lucene.analysis.TokenStream.incrementToken(TokenStream.java:114)\n\tat org.apache.lucene.analysis.TestAPICompatibility.consumeStreamNewAPI(TestAPICompatibility.java:210)\n\tat org.apache.lucene.analysis.TestAPICompatibility.test4(TestAPICompatibility.java:183)\n\tat org.apache.lucene.analysis.TestAPICompatibility.main(TestAPICompatibility.java:98)\n{noformat}",
            "date": "2009-06-24T21:12:49.673+0000",
            "id": 80
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Not sure if we're talking about the same thing here. The problem is not that next() in the extended class doesn't get called. \n\nI think we are talking about the same. We have exact the same problem, if you override OpenBitSetIterator, but only implement next() instead of nextDoc(). next() is never called, because the new API (nextDoc()) is handling everything. It's exactly the same. You have this always, if you deprecate former abstract methods and make the new one call the old one. I a subclass overrides the new API, the old API is dead for this class. A further (3rd party) subclass then overrides the deprecated method, it is never called -> b\u00e4ng. Other example: Filter.\nWe have this everywhere (DocIdSetIterator next() vs nextDoc(), Filter getBits() vs. getDocIdSet(), TokenStream very old API vs. old API).\n\nbq. It gets called, but the super.next() call throws either a ClassCastException, when consuming using the old API, or produces a different result than before uwhen consuming the new API.\n\nA ClassCastException? Why that? super.next() is always available (at least TokenStream on top of the hierarchy defines it).",
            "date": "2009-06-24T21:27:17.205+0000",
            "id": 81
        },
        {
            "author": "Uwe Schindler",
            "body": "One solution around this poblem would be to always implement both incrementToken() and next(Token) in non-final classes that tend to be overridden (CharTokenizer). In this case you are right. So I should revert the removal of next(Token). But very old code only overriding next() still fails.\n\nBut final things like StandardFilter do not need to have both implementations.",
            "date": "2009-06-24T21:30:40.338+0000",
            "id": 82
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nBy the way: I want to have my beer!!!\n{quote}\nI decided a while ago that you'll definitely get it, because of your admirable tenaciousness. :)",
            "date": "2009-06-24T21:49:51.502+0000",
            "id": 83
        },
        {
            "author": "Uwe Schindler",
            "body": "After thinking one round about it again. Your original patch with the UnsupportedOperationException have the same problem. If one overrides next(Token) in one non-final TokenStream/Filter, the overriden method is never called.\n\nAnd it is also not enough to always override both methods in subclass-able streams/filters. With both batches we break BW compatibility for users that do not override TokenStream or TokenFilter directly.\n\nEven my last comment on java-dev is not correct. Somebody who overrides a Filter (not direct subclass of TokenStream/Filter/Tokenizer, or the in-between classes are also of this user and only override deprecated APIs) and only override next(Token) (e.g. subclass of CharTokenizer), the overriden method will never be called, if the Filter/Stream is top-level in the filter chain :(\n\nI think, we must have somewhere a BW break and clearly document it in the following way in CHANGES.txt:\n- You can mix old and new filters/streams without problems.\n- Such mixed chains can be consumed with both new/old API\n- To have this working, always override the abstract TokenStream/TokenFilter/Tokenizer directly and see all other core classes as \"final\". In all other cases your filters/token streams will have unpredicatable behaviour.\n\nUwe",
            "date": "2009-07-01T13:52:16.417+0000",
            "id": 84
        },
        {
            "author": "Mark Miller",
            "body": "Mr. Busch my friend, I'll buy both you and Uwe *many* beers if you resolve this issue soon!",
            "date": "2009-07-08T17:55:29.815+0000",
            "id": 85
        },
        {
            "author": "Michael Busch",
            "body": "Alright, I hope you are coming to Oakland in November! \n\nI had a few (literally) sleepless nights last week to meet some internal deadlines; but it looks like I'll now have time to work on Lucene, so I'll continue on this issue tonight!",
            "date": "2009-07-08T20:21:58.741+0000",
            "id": 86
        },
        {
            "author": "Uwe Schindler",
            "body": "Attached is a new patch to current trunk. I did a hard work to make it compatible to also test-tag (see the java-dev discussion last weekend: test-tag was not really testing against 2.4, the bw-branch was created in November from trunk, when the new TokenStream API was already committed):\n\n- All tests core, contrib and tag pass correctly (and no changes to bw-branch needed!)\n- Michaels latest TestCompatibility also passes\n- All non-final and abstract Tokenizers implement both next(Token) and incrementToken() for maximum compatibility (but there are still problems mentioned above). This problems are always there and cannot be worked around, even with Michael's initial patch and at other places in Lucene, too (Filters, HitCollector).\n- CachingTokenFilter, TeeTokenizer and SinkTokenizer and the corresponding tests are reverted to 2.4 API and deprecated. There is not yet an implementation in new classes using solely the new API available. This is the only thing on the TODO-list for this case. I suggest the following name: CachingAttributesFilter; but for TeeTokenizer and SinkTokenizer I have no idea :(\n\nIn my opinion, this is ready to commit and the non-deprecated Caching/Sink/Tee things could be a separate issue.\n\nWe also need to write some conclusion or howto for CHANGES.txt, mentioning the problems with the switch to the new API.",
            "date": "2009-07-09T09:14:17.717+0000",
            "id": 87
        },
        {
            "author": "Michael Busch",
            "body": "Thanks for all your hard work here, Uwe.\n\nI think this patch is as good as it can be for achieving the goal of being able to combine the old and the new API.\nAnd I agree that my patch that I posted here has the same potential backwards-compatibility problems regarding inheritance. \n\nI think in the majority of use cases we're fine here. Only the corner cases make me a bit nervous. I think the case I feel most uncomfortable with is when people use Lucene + some external analyzer package + their own subclasses. If they use Lucene 2.9, the external package is not upgraded to the new API yet, but they did upgrade their own classes already to the new API, then they might run into undefined problems. However, I don't even know how many of such \"external analyzer packages\" exist (well, I think Grant mentioned he was working on one...)\n\nAnd I still just have this not-going-away slightly bad feeling in my gut that there are still other corner case problems we haven't thought about yet. What makes this feeling worse is the fact that those problems might not result in exceptions, but in unexpected and hard-to-find search problems, because the wrong tokens were indexed.\n\nThe current patch uses reflection extensively to figure out which of the three APIs the user has implemented. The comments above mention the possible problems. The solution is cool, but also a bit hack-ish (no offense Uwe, you called it that yourself ;) )\n\nSo, having said all this, I'd like other people to chime in here and give their opinion. I'm okay with committing this solution if everyone else is too.\nI think the only solution to not break compatibility at all is to not touch the old API at all and provide APIs that switch on/off using the new API. That's what the code in trunk currently does. It has the major disadvantage that it doesn't allow combining the old and new API in the same chain, and that we have to implement both APIs in core Lucene until the old API is fully removed.\n\nSo Mike, Grant, Mark, or others, could you please comment here?\n\nPS: Uwe, in any case, your solution is cool and I like how cleverly you solved the problems!!\n",
            "date": "2009-07-10T07:49:59.700+0000",
            "id": 88
        },
        {
            "author": "Uwe Schindler",
            "body": "Thanks!\nBefore going to bed yesterday, I remembered the fact, that the attributes iterators list the interfaces and not the instances. I will fix this in a further patch to also have the possibility to iterate over the instances and e.g. only clone instances.\n\nCurrently when you clone or toString(), the big Token instance for all 6 attributes is cloned/printed 6 times. This is the only thing, I forgot yesterday. I will think about it and maybe add an attributesInstancesIterator method or something like that.\n\nWhat do we do with Tee/Sink/Cached?",
            "date": "2009-07-10T08:01:08.266+0000",
            "id": 89
        },
        {
            "author": "Michael Busch",
            "body": "I agree that in any case we should deprecate those three classes and write new ones. I think we need to merge Tee/Sink into one actually. Maybe name it TeeSinkTokenFilter :) And it gets a method like \n\n{code:java}\npublic TokenStream getSinkTokenStream();\n{code}\n\nThat way we can ensure that the sink tokenstream has exactly the same attributes as the TeeSinkTokenFilter. Otherwise, a user could e.g. create the SinkTokenizer first, add some attributes to it, before calling new TeeTokenFilter(in, sink).",
            "date": "2009-07-10T08:06:59.801+0000",
            "id": 90
        },
        {
            "author": "Michael McCandless",
            "body": "I'm still trying to wrap my brain around this issue :)\n\nSo to sum up where things [seem] to stand here:\n\n  * We are switching to separate interface + impl for token\n    attributes.  This is nice because a single class (eg Token.java)\n    can provide multiple attrs, it makes cloning more efficient, etc.\n\n  * We are keeping good old Token.java around; it simply implements all\n    the attrs, and is very convenient to use.\n\nI think this is a great improvement to the new tokenStream API.  It's\nalso a sizable perf gain to clone only one impl that has only the\nattrs you care about.\n\nUwe then extended the original patch:\n\n  * Use reflection on init'ing a TokenStream to determine which of the\n    3 APIs (old, newer, newest) it implements\n\n  * Indexer (and in general any consumer) now only has to consume the\n    new API.  Any parts of the chain that aren't new are automatically\n    wrapped.\n\n  * Core tokenizers/filters (and contrib, when we get to them) only\n    implement the new API; old API is \"wrapped on demand\" if needed\n\n  * One can mix & match old, new tokenizers, though at some perf cost.\n    But that's actually OK since the original patch it's \"all or\n    none\", ie your chain must be entirely new or old\n\n  * I think (?) a chain of all-old-tokenizer/filters and all-new\n    wouldn't see a perf hit?  Wrapping only happens when there's a\n    mismatch b/w two filters in the chain?\n\nI'm tentatively optimistic about the extended patch... (though these\nback-compat, corner cases, etc., are real hard to think about).  I\nagree it's doing alot of \"magic\" under the hood to figure out how to\nbest wrap things, but the appeal of only implementing the new api in\ncore/contrib tokenizers, only consuming new, being free to mix&match,\netc, is strong.\n\nOne concern is: TokenStream.initialize looks spookily heavy weight; eg\nI don't know the \"typical\" cost of reflection.  I think there are\nlikely many apps out there that make a new TokenStream for ever field\nthat's analyzed (ie implement Analyzer.tokenStream not\nreusableTokenStream) and this (introspection every time) could be a\nsizable perf hit.  Uwe was this included in your perf tests?\n",
            "date": "2009-07-10T21:02:56.319+0000",
            "id": 91
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. One concern is: TokenStream.initialize looks spookily heavy weight; eg I don't know the \"typical\" cost of reflection. I think there are likely many apps out there that make a new TokenStream for ever field that's analyzed\n\nAt this point, I'd bet that's still the norm.",
            "date": "2009-07-10T21:14:26.716+0000",
            "id": 92
        },
        {
            "author": "Uwe Schindler",
            "body": "Some updates:\n- Added missing License headers\n- Differentiate between iteration over attributes or attribute impls. The probleme here is, that the unique attribute impls are not known, so a set must be created (which is done on-the fly). As the attributes member is currently protected and can be modified (even by TokenFilters), there is no way around this. Michael: Do you have any idea about that? During cloning, state capturing and toString() only the unique instances should be visited.\n\nMichael: Some more questions: The superinterface Attribute does not contain clone(), but the interface TokenAttribute does. Why these two interfaces (because AttributeImpl has clone). In my opinion, clone() should be part of Attribute and TokenAttribute can be removed.\n\nTo the performance questions:\nThe new Token API uses reflection very intensive, even without the changes from me. Everytime, when you add an attribute, the instance is checked for all implemented interfaces (via reflection), see addAttributeImpl(). This means, creating ne instances of TokenStreams is much more costly than with the old API (with the new \"smart\" TokenStreams and also without). I will prepare a small performance comparison (something like \"time to create one million WhitespaceAnalyzer's TokenStreams\" with old API, new API (as in trunk) and newest API (this patch)).",
            "date": "2009-07-11T14:50:04.777+0000",
            "id": 93
        },
        {
            "author": "Mark Miller",
            "body": "Token is still deprecated in the latest patch - we are not going to deprecate it though, right?",
            "date": "2009-07-11T16:00:25.877+0000",
            "id": 94
        },
        {
            "author": "Uwe Schindler",
            "body": "Mike implemented a nice idea to solve the problems with tokenstreams overriding deprecated methods in LUCENE-1678.\n\nI will try this out here and also fix the problems with # of attribute instances != # of attributes and the iterator problems because of this.",
            "date": "2009-07-13T12:02:45.104+0000",
            "id": 95
        },
        {
            "author": "Uwe Schindler",
            "body": "New patch with cleanup and fixing the last problems and inconsistencies in the\nAttributeSource with captureState and toString and so on, because attributes\ncan now be implemented by more than one impl and one impl can implement more\nthan one attribute (best example: Token). Renamed some methods for that and made sure, that the two Maps holding attributes and instances are private and cannot be modified (to prevent users from making bad things). There are only the public methods and iterators (unmodifiable) available (with changed semantics).\n\nMissing is still the new CachingAttributesFilter and the new TeeSinkTokenizer-combi for the new API (the 3 old ones are already deprecated).\n\nI also removed some of the \"old\" captureState methods with AttributeSource. Also added testcases for that.\n\nI also removed the TokenAttribute and moved clear into Attribute, as the base class AttributeImpl always implements this method.\n\nAfter that 1693 needs only the idea from LUCENE-1678, to detect if in non-final\nclasses, any subclass overrides deprecated methods. Problematic core token\nstreams are:\n- ISOLatin1Filter (no-final and deprecated, so maybe it should not implement\nincrementToken at all) -> would be fixed then.\n- KeywordTokenizer should normally be final, but is not :( -> needs this\nspecial trick\n- StandardTokenizer is the same, should be final, but isn't\n\nContrib analyzers could have this prob, too, but this must be checked when doing the\ntransformation there. If the backwards wrapper is available, we could do\nthis like with the analyzers in LUCENE-1678.\n",
            "date": "2009-07-14T17:36:51.117+0000",
            "id": 96
        },
        {
            "author": "Uwe Schindler",
            "body": "I forgot:\nI also added the TestCompatibilty class as a testcase and implemented ASCIIFoldingFilter with new API.",
            "date": "2009-07-14T17:40:05.375+0000",
            "id": 97
        },
        {
            "author": "Michael Busch",
            "body": "Hi Uwe,\n\nI modified the compatibility test so that it is now a junit and I'd like to commit it too.\n\nIt's failing right now, because it's testing a lot of different combinations. I need to check if all of those different tests are actually valid, because we're saying you can't use Tee/Sink with the new API anymore. \n\nI'm also working on a new TeeSinkTokenizer that will be the replacement. However, we have to make sure that everyone who is upgrading to 2.9 (without code modifications, just after 2.x -> 2.9 jar replacement) and using the old Tee/Sink doesn't see a different behavior.",
            "date": "2009-07-15T05:46:03.357+0000",
            "id": 98
        },
        {
            "author": "Michael Busch",
            "body": "So if people are using streams/filters that implement next(Token) I think the performance should be comparable - even though there's also a (hopefully small) performance hit to expect because of more method calls and if checks.\n\nHowever, if people are using next() in a chain with core streams/filters, then every single token will now be cloned, possibly multiple times, right?\n\n{code:java}\n  /** Returns the next token in the stream, or null at EOS.\n   *  @deprecated The returned Token is a \"full private copy\" (not\n   *  re-used across calls to next()) but will be slower\n   *  than calling {@link #next(Token)} instead. */\n  public Token next() throws IOException {\n    checkTokenWrapper();\n    \n    if (hasIncrementToken) {\n      return incrementToken() ? ((Token) tokenWrapper.delegate.clone()) : null;\n    } else {\n      assert hasReusableNext;\n      final Token token = next(tokenWrapper.delegate);\n      if (token == null) return null;\n      tokenWrapper.delegate = token;\n      return (Token) token.clone();\n    }\n  }\n{code}",
            "date": "2009-07-15T05:49:57.102+0000",
            "id": 99
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. However, if people are using next() in a chain with core streams/filters, then every single token will now be cloned, possibly multiple times, right?\n\nThat was a similar problem in 2.4, whenever you call next() to consume a stream, every token is a new instance. Only cloning WAS not needed but its now needed for BW compatibility (\"full private copy\").\n\nIn my opinion, this is neglectible, as indexing speed is important, and the indexer always uses incrementToken(). If somebody written an own query parser that uses next() to consume speed is not really important. And it is deprecated and in the docs (even in 2.4) stands: \"but will be slower than calling next(Token)\".\n\nThere is one case, when it also affects you (during indexing). If you have an old-style tokenfilter that calls next() on the next stream that is new-api, it would clone. In my opinion, the speed is about the same like before:\n- the 2.4 code created a new uninitialized token instance and the filter then filled it with data, you have to initialize the char arrays and so on.\n- here the token from the TokenWrapper-Attribute is reused (no allocation costs for arrays and so on), but you have to clone the Token (to be full private).\n\nbq. So if people are using streams/filters that implement next(Token) I think the performance should be comparable - even though there's also a (hopefully small) performance hit to expect because of more method calls and if checks.\n\nI found no performance hit, it is about same speed. The varieties between tests is bigger than a measureable performance impact. The other sensitive thing (TokenWrapper): The wrapping using TokenWrapper was in the original indexing code, too (this BackwardsCompatibilityStream private class).\n\nThe if checks are all on final variables, so can be optimized away by the JVM. The method calls are inlined, as far as I have seen.\n\nbq. It's failing right now, because it's testing a lot of different combinations. I need to check if all of those different tests are actually valid, because we're saying you can't use Tee/Sink with the new API anymore. \n\nHave you seen my backwards compatibility test, too? It is a copy of yours (with some variation)? The Lucene24* classes were removed, because Tee/SinkTokenizer werde reverted to their original 2.4 status in the patch (only implement old API).\nAs far as I see (not yet tried out), you try to test new-style-API streams with the old Tee/Sink tokenizer, that is deprecated. You were not able to do this before 2.9 (no new API) and so the bw problem is not there. If you rewrite your streams with new API, you should use TeeSinkTokenizer, too.",
            "date": "2009-07-15T06:11:43.990+0000",
            "id": 100
        },
        {
            "author": "Uwe Schindler",
            "body": "I changed TokenStream to use real final boolean variables to help the optimizer to optimize away the if checks.\n\nBy the way: The problem with initialize() and lateInitialize() [new] is the typical example, why you should never call non-private or non-final methods from a ctor (Sun has a big warning in the documentation). The problem: During initialize() the final fields are not yet initialized, because initialize() is called before the ctor of the actual class (which initializes the fields) is called. Very nasty. So I moved the checks, which methods are available to a private lateInitialize().\n\nIn my opinion, we should remove the protected initialize() from AttributeSource and let every subclass do its initialization using a (possibly) private/final method on its own (must be called in two ctors). What was the idea behind that?",
            "date": "2009-07-15T07:18:48.188+0000",
            "id": 101
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nThere is one case, when it also affects you (during indexing). If you have an old-style tokenfilter that calls next() on the next stream that is new-api, it would clone. In my opinion, the speed is about the same like before:\n{quote}\n\nYes this is exactly what I mean. Not sure if cloning in this case is slower than creating a new empty instance; if yes, it's probably not significant.\n\n{quote}\nAs far as I see (not yet tried out), you try to test new-style-API streams with the old Tee/Sink tokenizer, that is deprecated. You were not able to do this before 2.9 (no new API) and so the bw problem is not there. If you rewrite your streams with new API, you should use TeeSinkTokenizer, too.\n{quote}\n\nYou are right - it fails because it uses a new attribute that will not be cached in the Tee/Sink. So I agree that this is not a valid test if we say that Tee/Sink only supports the old API. I will remove the cases that are invalid.\n\nThinking out loud here again: What if a user uses Lucene in combination with a third-party jar containing TokenStreams, and also own implementations. Are there use cases where it would be necessary for us to provide a switch to run in only-old-API mode?",
            "date": "2009-07-15T07:27:14.045+0000",
            "id": 102
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nIn my opinion, we should remove the protected initialize() from AttributeSource and let every subclass do its initialization using a (possibly) private/final method on its own (must be called in two ctors). What was the idea behind that?\n{quote}\n\nDefinitely. I should start using the nocommit comments Mike puts into early patches. I never wanted the initialize() method to stay for similar reason that you mentioned here. I added it for a test, because I was lazy.\n\nWhat we really should have is an AttributeSource constructor that takes an AttributeFactory. You need to be able to set the factory in the ctor, because TokenStreams usually add attributes in their ctor.",
            "date": "2009-07-15T07:32:05.511+0000",
            "id": 103
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. What we really should have is an AttributeSource constructor that takes an AttributeFactory. You need to be able to set the factory in the ctor, because TokenStreams usually add attributes in their ctor.\n\nI have done this here locally. The ctor that takes an other AttSource uses the factory of the delegate, do we need also a ctor with another AttSource and the factory? The case may be\nI also added this ctor to TokenStream, but not TokenFilter (as it uses the factory from the filtered stream).\n\nI also cleaned up the initialization of TokenStream. Now the tokenWrapper and all the booleans are final.\n\nIf you are interested I could post my current patch.\n\nbq. Thinking out loud here again: What if a user uses Lucene in combination with a third-party jar containing TokenStreams, and also own implementations. Are there use cases where it would be necessary for us to provide a switch to run in only-old-API mode?\n\nThere is still the problem with a TokenStream overriding a deprecated method of a core filter that will be never be called anymore (see LUCENE-1678 which faces the same problem). I will try to fix this here using the same mechanism.\n\nI tested with mixing contrib tokenfilters and core filters. I have seen no problems.",
            "date": "2009-07-15T07:52:45.646+0000",
            "id": 104
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nThere is still the problem with a TokenStream overriding a deprecated method of a core filter that will be never be called anymore (see LUCENE-1678 which faces the same problem). I will try to fix this here using the same mechanism.\n\nI tested with mixing contrib tokenfilters and core filters. I have seen no problems.\n{quote}\n\nYeah that is a good fix for overriding the non-final methods of the core filters.\n\nI guess what I meant here is that my invalid use case could happen in the field: Let's say something like tee/sink lived in the third-party jar and the user upgrades to Lucene 2.9 and also upgrades the own streams/filters, but a version of the third-party jar that has the new implementations is not available yet. The user couldn't simply implement both the new and old API and use such a filter then with the not-updated third party jar, unless there was a only-old-api switch.\n\nBut I'm not sure how realistic this scenario is. I guess we'll find it out sooner or later :)",
            "date": "2009-07-15T08:03:47.545+0000",
            "id": 105
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nHave you seen my backwards compatibility test, too?\n{quote}\n\nOh cool... I guess I should have checked that before... :)\n\nI think if I remove the invalid tests it looks pretty similar to yours, so let's keep the one you have in the patch.\n\nI'm going to bed now... will add the new tee/sink stuff tomorrow.",
            "date": "2009-07-15T08:08:56.419+0000",
            "id": 106
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. But I'm not sure how realistic this scenario is. I guess we'll find it out sooner or later \n\nI think in 2.9 we have a lot of BW breaks (see the long list in CHANGES.txt at the begin). This not so realistic case is just one more :-) There should be a BW note in CHANGES.txt about the new TokenStream API and possible traps (something like: \"we did our best to make it bw-compatible, but there may be the following problems: <list>. You Should upgrade all your TokenStreams as soon as possible, especially if a strange behaviour occurs.\")\n\nDo you have a example of TeeSink and CachingAttributesFilter?\n\nbq. Yeah that is a good fix for overriding the non-final methods of the core filters\n\nI would try to do this now.",
            "date": "2009-07-15T08:16:36.599+0000",
            "id": 107
        },
        {
            "author": "Uwe Schindler",
            "body": "After the whole day thinking about a solution for overriding deprecated methods, I came to one conclusion/solution, that I would create a \"visible\" backwards break (to be noted in CHANGES.txt).\n\nMike's idea from LUCENE-1678 is good, but very complicated for this issue and may lead to unpredicted behavior. And what makes me think, that this will not be a problem for developers, is the fact that there is no JIRA issue about a similar break in the past: When Lucene switched from next() to next(reusableToken), we also had a compatibility method in TokenStream that delegates to next(new Token()). Core streams did *not* implement the old method and the indexer code only called next(Token). If somebody would have overridden only the old next() method of a core tokenstream, this method would have been never called -> bumm we have a break, but nobody realized it. With the new patch, we have the same in 2.9 for incrementToken vs. next(Token) and also next(). In principle the same issue like in LUCENE-1678.\n\nThe good thing is, that most TokenStreams in core are final, except the following ones:\n\n- ISOLatin1Filter\n- KeywordTokenizer\n- StandardTokenizer\n\n...and last but not least the whole structure of subclasses of CharTokenizer. The good thing is (and thanks to the developer!), they are correctly implemented, making their methods incrementToken, next(Token) *final*. Haha, nobody could override them, so the class is not final, but the affected methods. So all subclasses of CharTokenizer are also not affected.\n\nMy latest patch also includes this *final* modifier for the abstract CharTokenizer:\n{code}\npublic final Token next(final Token reusableToken) throws IOException {\n // Overriding this method to make it final as before has no effect for the reflection-wrapper in TokenStream.\n // TokenStream.hasReusableNext is true because of this, but it is never used, as incrementToken() has preference.\n return super.next(reusableToken);\n}\n{code}\n\nSo it is not overrideable and is still compatible (code calling next(Token) will be delegated to incrementToken() by the superclass). For complete correctness also next() should be similar overridden. In both cases the super's method always delegates preferably to incrementToken() so iven that a subclass of TokenStream overrides this method and so hasNext == true and hasReusableNext == true, incrementToken() is still preferred, so everything works.\n\nThis prevents users from overriding next() or next(Token) of core or contrib tokenstreams (which in my opinion nobody has ever done, because if yes, we would have a bug report regarding the last transition).\n\nFor those people, that really have done it (they used one of the tree classes above as super for their own class), the error would not be to detectable. Their TokenStream would simply not work, as next()/next(Token) is never called. To produce a compile error for them (or a runtime error, when they instantiate such a class), I suggest to include this backwards-break (which is better than failing silently). All non-final TokenStreams/Tokenizers/TokenFilters should simply include the code snipplet above to redeclare next() *and* next(Token) as final (only delegating to super) in the first subclass that implements incrementToken(). Instead of failing silently, users will get runtime linker errors (when they replace the lucene jar) or compile errors. We have done a similar change in TokenFilter, because we made the delegate stream final to prevent disturbing the attributes (Mike have done this in LUCENE-1636).\n\nCHANGES.txt would contain this as BW-break together with the other breaks.\n\nAny comments? Michael, what do you think?\n",
            "date": "2009-07-15T16:53:28.893+0000",
            "id": 108
        },
        {
            "author": "Michael Busch",
            "body": "I like the cleanup you did! Good that initialize() is gone now. The only small performance improvement we should probably make is to avoid checking which method in TokenStream is overridden when onlyUseNewAPI==true.\n\n{quote}\nTo produce a compile error for them (or a runtime error, when they instantiate such a class), I suggest to include this backwards-break (which is better than failing silently). All non-final TokenStreams/Tokenizers/TokenFilters should simply include the code snipplet above to redeclare next() and next(Token) as final (only delegating to super) in the first subclass that implements incrementToken().\n{quote}\n\n+1. I think this backwards-compatibility break is acceptable and makes sense. Most likely the final was just forgotten in these three classes in the first place - all the other core classes declare these methods correctly as final. So we can kind of consider this as a bug fix.\n\nAnd I like that they will get a compile or link error, instead of seeing undefined behavior.",
            "date": "2009-07-16T08:42:43.763+0000",
            "id": 109
        },
        {
            "author": "Michael Busch",
            "body": "This is basically your last patch with these changes:\n\n- I removed AttributeSource.setAttributeFactory(factory). Since we have the constructor now that takes the factory as an arg, there should be no need to ever change the factory after a TokenStream was created. It would also lead to problems regarding e.g. Tee/Sink: a user could add attributes to the Tee, then change the factory, then create the sink. How could we then create the same attribute impls for the sink? So I think the right thing to do is to not allow changing the factory after the stream is instantiated.\n\n- I added the initial (untested) version of TeeSinkTokenFilter to demonstrate how I think it should work now. I'll finish tomorrow or Friday (add more javadocs and unit test). I'll also add the CachingAttributeTokenFilter, which is essentially almost the same as the new inner class of TeeSinkTokenFilter. When I have CATF the inner class can probably just extend it.",
            "date": "2009-07-16T08:48:34.472+0000",
            "id": 110
        },
        {
            "author": "Uwe Schindler",
            "body": "Ok looks good. I think you will go to bed now, so the work would not collide. If you start to program again, ask me, that I will post a patch (which makes merging simplier). TortoiseSVN has a problem with merging added files, so when applying your patch I have to remove them first :-(\n\nSome comments:\n- TeeSinkTokenFilter looks good, I think we should also add a test for it (in principle the version of TestTeeTokenFilter from current trunk, not the one reverted to old API from the current patch)\n- I do not understand completely why this WeakReference is needed between Tee and Sink? If it is needed, the code may fail with NPE, when Reference.get() returns null. The idea is, that one can create a Sink for the Tee and throw the Sink away. Tee would then simply not pass the attributes anymore to the sink? If this is the case, the check for Reference.get()==null is really missing.\n- Should I implement CachingAttributesFilter as replacement for CachingTokenFilter, or will you do it together with TeeSink?\n\nI will now start to add all the finals to the missing core analyzers.\n\nbq. The only small performance improvement we should probably make is to avoid checking which method in TokenStream is overridden when onlyUseNewAPI==true\n\nI could disable this for next() and next(Token). In the case of incrementToken, it should really check, that it is enabled, because not doing so would fail hard create endless loops. So the check should be there in all cases. But if onlyUseNewAPI is enabled, I could simply define hasNext and hasReusableNext=false. I will do this.",
            "date": "2009-07-16T10:36:50.080+0000",
            "id": 111
        },
        {
            "author": "Grant Ingersoll",
            "body": "Favor to ask, when this is ready to commit, can you give a few days notice so that the rest of us can look at it before committing?  I've been keeping up with the comments, but not the patches.",
            "date": "2009-07-16T10:44:47.878+0000",
            "id": 112
        },
        {
            "author": "Uwe Schindler",
            "body": "New patch with some more work. First the phantastic news:\n\nAs CachingTokenFilter has no API to access the cached attributes/tokens directly, it does not need to be deprecated, it just switched the internal and hidden impl to incrementToken() and attributes. I also added an additional test in the BW-Testcase, that checks if the caching also works for your strange POSTokens. And it works! You can even mix the consumers, e.g. first use new API to cache tokens and then replay using the old API. really cool. The problem, why the POSToken was not preserved in the past was an error in TokenWrapper.copyTo(). This method created a new Token and copied the contents into it using reinit(). Now it simply creates a clone and let delegate point to it (this is how the caching worked before).\n\nIn principle Tee/SinkTokenizer could also work like this, the only problem with this class is the fact, that it has a public API that exposes the Token instances to the outside. Because of that, there is no way around deprecating.\n\nYour new TeeSinkTokenFilter looks good, it only had one problem:\nIt used addAttributeImpl to add the attribute of the Tee to the new created Sink. Because of this, the sink got the same instance as the parent added. With useOnlyNewAPI, this does not have an effect for the standard attributes, as the ctor already created a Token instance as implementation and added it to the stream, so addAttributeImpl had no effect.\nI changed this to use the getAttributeClassesIterator and added a new attribute instance for each attribute using addAttribute to the sink. As the factory is the same, the attributes are generated in the same way. TeeSinkTokenizer would only *not* work correctly if somebody addes an custom instance using addAttributeImpl in one ctor of another filter in the chain. In this case, the factory would create another impl and restoreState throws IAE. In backwards compatibility mode (default) the new created sink and also the tee have always the default TokenWrapper implementation, so state restoring also works. You only have a problem if you change useOnlyNewAPIU inbetween (which would always create corrupt chains).\n\nAnother idea would be to clone all attribute impls and then add them to the sink - the factory would then not be used?\n\nI started to create a test for the new TeeSinkTokenFilter, but there is one thing missing: The original test created a subclass of SinkTokenizer, overriding add() to filter the tokens added to the sink. This functionality is missing with the new API. The correct workaround would be to plug a filter around the sink and filter the tokens there? The problem is then, that the cache always contains also non-needed tokens (the old impl would not store them in the sink).\n\nMaybe we add the filter to the TeeSinkTokenFilter (getting a State, which would not work, as contents of state pkg-private?). Somehow else? Or leave it as it is and let the user plug the filter on top of the sink (I prefer this)?",
            "date": "2009-07-16T14:02:39.588+0000",
            "id": 113
        },
        {
            "author": "Uwe Schindler",
            "body": "I forgot: I also implemented the final next() methods in all non-final classes.",
            "date": "2009-07-16T14:04:00.212+0000",
            "id": 114
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Favor to ask, when this is ready to commit, can you give a few days notice so that the rest of us can look at it before committing? I've been keeping up with the comments, but not the patches. \n\nNo problem. I want to finish this until the weekend and then you have time to review it. My holidays start next week on monday, so I have only limited time after that.",
            "date": "2009-07-16T14:08:37.785+0000",
            "id": 115
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nAs CachingTokenFilter has no API to access the cached attributes/tokens directly,\n{quote}\n\nOh true :) well, scratch it from the TODO list...\n\nWe knew it'd work conceptually the same for AttributeSource.State; unlike Tee/Sink, which wouldn't even be save to use with the new API if it hadn't the getTokens() method for the reasons I explained above.\n\n{quote}\nAnother idea would be to clone all attribute impls and then add them to the sink - the factory would then not be used?\n{quote}\n\nYes, I thought about this for a while. It would be nice to have this (i. e. cloning an AttributeSource) in general: you could reduce the costs for initializing the TokenStreams with onlyUseNewAPI=false. We just need to keep a static AttributeSource around, that contains the wrapper and the mappings from the 6 default interfaces. Then instead of constructing it every time we just clone that AttributeSource for new TokenStreams.\n\nThe query parser could do the same to keep initialization costs of the TokenStreams minimal, because it always needs the same attributes.\n\nI think it should be easy? We just need to implement clone() for AttributeSource. ",
            "date": "2009-07-17T08:36:15.695+0000",
            "id": 116
        },
        {
            "author": "Michael Busch",
            "body": "I made these changes:\n\n- added clone() to AttributeSource and changed TeeSinkTokenFilter to use it.\n- added a SinkFilter as inner interface of TeeSinkTokenFilter that adds the missing functionality you mentioned, Uwe.",
            "date": "2009-07-17T08:39:05.370+0000",
            "id": 117
        },
        {
            "author": "Uwe Schindler",
            "body": "Patch looks good.\nOnly one thing: If you clone a TokenStream you will not get a TokenStream, only an AttributeSource instance (if TokenStream does not override). For our use case it is ok, because we only want to have the attributes and impls cloned, but it is strange.\nA real clone() method should call super.clone() and then create new maps and copy the old maps into them. Not sure. Or we do not call the method clone() and call it cloneAttributes not returning Object but AttributeSource. E.g. {code}public AttributeSource cloneAttributes(){code}\n\nI will now rewrite the TeeSink-Test to use the new interface and the test should then pass as before with Tee/Sink separate (but I let both tests available, one that tests the old ones and one that tests the new class). I also add a test for the cloning to TestAttributeSource.\n\nThe cloning also speeds up the case with useOnlyNewAPI=true, because the addAttribute-call also uses reflection to find out what interfaces are implemented. In my opinion this cost (the while loop with getSuperclass() and so on) is much more costy than the simple check of the declaring class for a method.\n\nThe patch is still missing some javadocs. If we finished this, are there any other things to do? The optimizations in QueryParser to clone and so on are not really part of this issue, so could be done separately.",
            "date": "2009-07-17T10:25:32.476+0000",
            "id": 118
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nOr we do not call the method clone() and call it cloneAttributes not returning Object but AttributeSource.\n{quote}\n\n+1. Let's do that.\n\n{quote}\nIf we finished this, are there any other things to do? The optimizations in QueryParser to clone and so on are not really part of this issue, so could be done separately.\n{quote}\n\nI agree. I don't think these optimizations are critical at this point. I think updating the javadocs should be the only remaining thing here. (given that everyone else is ok with this patch)\n\nThe other related issues I think will be straightforward... except LUCENE-1448, I have the feeling this will cause some headaches too.... not sure if you read the discussions in 1448 yet, Uwe?",
            "date": "2009-07-17T10:59:55.231+0000",
            "id": 119
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. +1. Let's do that.\n\nOK, I change this locally. And I remove the Cloneable interface again. In principle, this method cloneAttributes() should only be used, to create a new TokenStream that should use the same attributes, but needs different instances. TeeSink is currently the only example for this, but more may follow.\n\nAbout the TokenStream clones in QueryParser: I think, this will not work, as the TokenStream then needs to be really cloned with also setting a new Reader for the input. In my opinion, the reusableTokenStream method of Analyzer should handle this and not QueryParser.\n\nbq. The other related issues I think will be straightforward... except LUCENE-1448, I have the feeling this will cause some headaches too.... not sure if you read the discussions in 1448 yet, Uwe?\n\nWrrrrr, this one is hard. This final offset is not really fitting very good into the current attributes API, it could be an new extra attribute that is only updated at the end of the stream (but the problem is, that it needs to be done when incrementToken returns false.\n\n...and Mike said all others are trivial :(\n\nI will now update as noted before",
            "date": "2009-07-17T11:38:13.650+0000",
            "id": 120
        },
        {
            "author": "Mark Miller",
            "body": "bq. ...and Mike said all others are trivial \n\nHe also said hes willing to skip that one for 2.9 though. \n\nI'd rather not if we can help it though - I started looking at it last night, but I got side tracked before I got very far thinking about it.",
            "date": "2009-07-17T11:43:07.536+0000",
            "id": 121
        },
        {
            "author": "Uwe Schindler",
            "body": "Here my latest patch before I go to bed. I had not much time today, but I implemented parts of the TeeSinkTokenFilter test. The first test and also the performance test are implemented. The performance test is almost as fast as the old Tee/Sink combi (good news). I found a small bug in the new Sink (it did not lazyly created the iterator), but it is fixed and it works as exspected (without calling reset() on the sink first).\n\nThe second test is not implementable with TeeSinkTokenizer and this is a limitation: You are not able to combine different sources into one sink (and this is what the second test does). I am not sure, how you could implement this at all with the new API. It would only work if both tee streams have exactly same attributes, so they could feed their attributes into the same sink.\n\nMichael, any idea? The functionality to feed tokens from two streams into one sink is nice, but how to do it? Or is it just a useless theoretical demonstration in the test?\n\nGood night, Uwe",
            "date": "2009-07-17T20:59:46.551+0000",
            "id": 122
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nMichael, any idea? The functionality to feed tokens from two streams into one sink is nice, but how to do it? Or is it just a useless theoretical demonstration in the test?\n{quote}\n\nI have personally never used the Tee/Sink stuff, so I'm not sure in what kind of ways people use it.\n\nAlso Tee/Sink does not use any internal-only APIs, so everyone could implement it outside of Lucene themselves. Of course this kind of stuff is a bit more tricky with the new API. \n\nIf we want to support multiple tees feeding one sink it think it needs to be based on the Attribute interfaces themselves, not on the impls. Then it doesn't really matter, what kind of impls the sink has. The tee would just try to copy the Attribute's value over. But in this case we'd need to implement the copyTo() method a bit differently, so that you can copy values per Attribute interface. This would probably be a bit slower compared to the current TeeSinkTokenFilter approach.",
            "date": "2009-07-17T21:53:42.417+0000",
            "id": 123
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. I have personally never used the Tee/Sink stuff, so I'm not sure in what kind of ways people use it.\n\nDoes anyone use it?  Given the difficulty of using it, esp since Lucene has been sorting fields before analysis (hence you have to name the fields properly to get one to be indexed before the other), maybe no one is using it.",
            "date": "2009-07-17T22:16:56.065+0000",
            "id": 124
        },
        {
            "author": "Uwe Schindler",
            "body": "In my opinion, this test that I was not able to reimplement using the new TeeSink is very theoretical. In my opinion someone who consumes two streams could always implement this very simple using the two aproaches:\n\n- consume the first stream completely, the  the other\n- consume one token from the first, then one token from the next and so on\n\nWho invented the whole Tee/Sink originally for what? Maybe we should simply ask on java-user who have ever used it. I have never heard of Tee/Sink before Michael confronted me with his strange POSToken tests :-)",
            "date": "2009-07-17T22:30:30.335+0000",
            "id": 125
        },
        {
            "author": "Uwe Schindler",
            "body": "So the simple case of an AppendingTokenStream could easily implemented in the same way (just a filter with not only one but an iterator of delegates). Just have an empty constructor and then add TokenStreams to it (that adds all attributes of each added tokenstream to itsself). Then each call to incrementToken() consumes each added stream until its exhausted. But I do not know if we really need to add this just for 1% of users that could also do it themselves.\n\nIn my opinion the whole Tee/Sink/Appending is very theoretical (look at this test, it is very strange). Whoever would create such a ModuloTokenStream?",
            "date": "2009-07-17T22:42:42.180+0000",
            "id": 126
        },
        {
            "author": "Grant Ingersoll",
            "body": "bq.  Who invented the whole Tee/Sink originally for what?\n\nYonik and I did. \n\nI have used it and would like to use it more.  For the case where the number of tokens captured is roughly less than 50% of the original number of tokens, it is faster than reanalyzing.",
            "date": "2009-07-17T22:44:58.245+0000",
            "id": 127
        },
        {
            "author": "Grant Ingersoll",
            "body": "It is not theoretical at all, even if the test is.  Imagine a stream that identifies phrases, captures them and then tees them to a second field.   Phrases happen infrequently relative to the number of overall tokens, so it is much faster to just buffer those few tokens that are part of the phrase and tee them to your \"phrases\" field.  \n\nThe fact that it may not be used much now is probably due to lack of marketing more so than lack of functionality. ",
            "date": "2009-07-17T22:48:29.210+0000",
            "id": 128
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI have never heard of Tee/Sink before Michael confronted me with his strange POSToken tests \n{quote}\n\nHey, stop insulting my awesome backwards-compatibility tests!! :D",
            "date": "2009-07-17T22:51:59.882+0000",
            "id": 129
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Hey, stop insulting my awesome backwards-compatibility tests!! \n\nThey are phantastic. But I would use a simple FlagsAttribute/Token.flags() to annotate my tokens instead of creating a POSToken subclass...\n\n{quote}\nIt is not theoretical at all, even if the test is. Imagine a stream that identifies phrases, captures them and then tees them to a second field. Phrases happen infrequently relative to the number of overall tokens, so it is much faster to just buffer those few tokens that are part of the phrase and tee them to your \"phrases\" field. \n{quote}\n\nFor this the new TeeSinkTokenFilter is working like a before (only better):\n\n{code}\nTeeSinkTokenFilter tee = new TeeSinkTokenFilter(stream);\ndoc.add(new Field(\"everything\", tee));\ndoc.add(new Field(\"parts1\", new DoSomethingTokenFilter(tee.newSinkTokenStream()));\ndoc.add(new Field(\"parts2\", new DoSomethingOtherTokenFilter(tee.newSinkTokenStream(optionalFilter)));\n{code}\n\nOur problem was more to have two Streams fed into the same Sink. This is not possible anymore, because Sinks are generated by a factory now and are always bound to one Tee.",
            "date": "2009-07-17T23:05:06.150+0000",
            "id": 130
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Given the difficulty of using it, esp since Lucene has been sorting fields before analysis (hence you have to name the fields properly to get one to be indexed before the other), maybe no one is using it.\n\nCan't we fix Tee/Sink so that whichever \"tee\" is pulled from first, does the caching, and then the 2nd one pulls from the cache?\n\nIe right now when you create them you are forced to commit to which is \"primary\" and which is \"secondary\", but if we relax that then it wouldn't be sensitive to the order in which Lucene indexed its fields.\n\nOf course, someday Lucene may index fields concurrently, then Tee/Sink'll get really interesting ;)",
            "date": "2009-07-18T09:01:30.065+0000",
            "id": 131
        },
        {
            "author": "Uwe Schindler",
            "body": "In this case we should rename TeeSink to something like SplitTokenStream (which does not extend TokenStream). One could get then any number of \"sinks\" from it:\n\n{code}\nSplitTokenFilter splitter=new SplitTokenStream(stream); // does not extend TokenStream!!!\nTokenStream stream1=splitter.newSinkTokenStream();\nTokenStream stream2=splitter.newSinkTokenStream();\n...\n{code}\n\nIn this case the caching would be done directly in the splitter and the sinks are only consumers. The first sink that calls to get the attribute states forces the splitter to harvest and cache the input stream (exactly like CachingTokenStream does it). In principle it would be the same like a CachingTokenStream.\n\nBut on the other hand: You can always create a CachingTokenFilter and reuse the same instance for different fields. Because the indexer always calls reset() before consuming, you could re-read it easily. Any additional filters could then plugged in front for each field. In this case the order is not important:\n\n{code}\nTokenStream stream=new CachingTokenFilter(input);\ndoc.add(new Field(\"xyz\", new DoSomethingTokenFilter(stream)));\ndoc.add(new Field(\"abc\", new DoSometingOtherTokenFilter(stream)));\n...\n{code}\n\nThis would not work, if the indexer can consume the different fields in parallel. But with the current state it would even not work with Tee/Sink (not multithread compatible).",
            "date": "2009-07-18T11:04:19.392+0000",
            "id": 132
        },
        {
            "author": "Uwe Schindler",
            "body": "New and final patch. I will be in holidays from tomorrow and have limited time for Lucene. I will respond to comments and if there are major faults with the new API.\n\nThe new patch has some imporvements:\n- TeeSinkTokenizer is now able to also feed multiple tees to one sink. You create first a TeeSinkTokenizer, retrieve a TeeTokenStream from it (newSinkTokenStream()). After that you can add this TeeTokenStream to another tee (addSinkTokenStream()). The test (now similar to the old test) and javadocs demonstrates this.\n- Reflection performance was greatly improved by using caches. Most time was used in AttributeSource.addAttributeImpl() because it iterates through all interfaces of the supplied instance. It caches the found interfaces using a IdentityHashMap<Class<AttributeImpl>,LinkedList<Class<Attribute>>> keyed by the implementation class. Also the default AttributeFactory uses a cache (IdentityHashMap) for the mapping <Class<Attribute>,Class<AttributeImpl>>. So the number of Class.forName() is drastically reduced.\n- Also fixed a bug in addAttributeImpl after refactoring for the cache.\n- TokenStream now has a separate AttributeFactory available, that creates a TokenWrapper for the 6 default attributes. This is now a more clear implementation. The extra checks in next() default impls were removed because of this. Filters now also reuse the tokenWrapper instance already resolved by the input stream.\n\nI did some performance tests with the final impl, analyzing the lorem ipsum text 100000 times with new instances for each time, using reused instances, old/new API for the trunk with latest patch, current trunk and lucene-2.4 (old api only):\n\nThe results (but these test are not very representative due to a variance of +/- 4 sec per run):\n\n{code}\nTesting trunk w/ newest API...\nTime for 100000 runs with new instances (old API): 27.344s\nTime for 100000 runs with reused stream (old API): 21.828s\nTime for 100000 runs with new instances (new API only): 27.297s\nTime for 100000 runs with reused stream (new API only): 24.484s\nTesting trunk w/o newest API...\nTime for 100000 runs with new instances (old API): 22.485s\nTime for 100000 runs with reused stream (old API): 19.047s\nTime for 100000 runs with new instances (new API only): 26.89s\nTime for 100000 runs with reused stream (new API only): 23.719s\nTesting 2.4...\nTime for 100000 runs with new instances (old API): 18.984s\nTime for 100000 runs with reused stream (old API): 18.75s\n{code}\n\nThe cost of creating 100000 new instances on my 32 bit Thinkpad T60 is about 5 sec (no difference between new api and old api). The cost is not caused by reflection, it is caused by building the LinkedHashMaps for the attributes on creation. A little bit faster was the current trunk, because it uses only one LinkedHasMap.\n\nOne interesting thing: Using *only the new api* is little slower during tokenization, because it seems faster to use only *one* instance (Token) instead of 6 instances.\n\nThe cost of creating new instances is smallest with Lucene 2.4, because no attributes are used (in 2.9 it always creates the LinkedHashMaps, even if only the old API was used in current trunk).",
            "date": "2009-07-19T21:23:25.127+0000",
            "id": 133
        },
        {
            "author": "Uwe Schindler",
            "body": "Michael: can you wait for comments and commit & close then? Or should I do this on Wednesday? After closing this, there are some more issues to be notified/closed, because some of the core tokenstreams were already upgraded to latest API.",
            "date": "2009-07-19T21:26:32.289+0000",
            "id": 134
        },
        {
            "author": "Michael Busch",
            "body": "Hi Uwe,\n\nI haven't reviewed the latest patch yet, but it sounds like great improvements you made! \n\nI can commit this after waiting a few days, and will also review during the time. And then I'll work on the related issues, just a bit worried still about 1448. ",
            "date": "2009-07-19T22:10:38.139+0000",
            "id": 135
        },
        {
            "author": "Uwe Schindler",
            "body": "OK!\n\nI forgot to mention: I replaced the lucene JARs in trunk Solr and ran all the tests there, all of them pass! No speed degradion in solr, and all analyzers and query parsers work (and they still use the old API!). Also all Lucene core, Lucene contrib and backwards tests pass.\n\nBy the way: the additional speed degradion even when using the old API with the current trunk version (which should have no speed degradion there when reusing stream) is caused by the latest CharFilters added to the Tokenizers. So please do not say, they come from the new API! :-)",
            "date": "2009-07-19T22:26:49.617+0000",
            "id": 136
        },
        {
            "author": "Uwe Schindler",
            "body": "Forgot CHANGES.txt entries with the backwards-break note and other changes.\n\nWhen starting to transform the contrib streams, we should not forget to do the following (maybe add this note to the JIRA issues):\n- rewrite and replace next(Token)/next() implementations by new API\n- if the class is final, no next(Token)/next() methods needed *(must be removed!!!)*\n- if the class is non-final add the following methods to the class:\n{code}\n/** @deprecated Will be removed in Lucene 3.0. This method is final, as it should\n * not be overridden. Delegates to the backwards compatibility layer. */\npublic final Token next(final Token reusableToken) throws java.io.IOException {\n  return super.next(reusableToken);\n}\n\n/** @deprecated Will be removed in Lucene 3.0. This method is final, as it should\n * not be overridden. Delegates to the backwards compatibility layer. */\npublic final Token next() throws java.io.IOException {\n  return super.next();\n}\n{code}\nAlso the incrementToken() method must be final in this case.",
            "date": "2009-07-19T23:01:27.698+0000",
            "id": 137
        },
        {
            "author": "Michael Busch",
            "body": "I like the cache you added to AttributeSource for speedup!\n\nOne thing we could improve is to add this check, which avoids iterating the foundInterfaces list in case the AttributeImpl has been added previously:\n\n{code:java}\n  /** Adds a custom AttributeImpl instance with one or more Attribute interfaces. */\n  public void addAttributeImpl(final AttributeImpl att) {\n    final Class clazz = att.getClass();\n    if (attributeImpls.containsKey(clazz)) return;\n\n{code}",
            "date": "2009-07-19T23:19:35.935+0000",
            "id": 138
        },
        {
            "author": "Uwe Schindler",
            "body": "Yes, good idea. I added it locally, works good. This is also a speed improvement (the synchronized access to the cache is also removed by this).\n\nI think a new patch is not needed, just add it to your checkout, too!",
            "date": "2009-07-19T23:43:54.018+0000",
            "id": 139
        },
        {
            "author": "Michael Busch",
            "body": "yep, did already and all tests pass!",
            "date": "2009-07-20T00:10:42.973+0000",
            "id": 140
        },
        {
            "author": "Michael McCandless",
            "body": "OK I looked at the latest patch!  It looks good -- only these questions:\n\n  * Are we going to keep Token.java or not?  (Current patch still has\n    it deprecated).\n\n  * Typo (occure) in CHANGES.txt\n\n  * It looks like we are making all core tokenizers/filters final (to\n    foreclose future back-compat problems if we need to change the\n    API)?  If so, we should also make CachingTokenFilter final?\n\n\n",
            "date": "2009-07-21T10:41:34.065+0000",
            "id": 141
        },
        {
            "author": "Grant Ingersoll",
            "body": "One of the things that works really well in Solr is that any time some significant JIRA issue is undertaken, a Wiki page is also generated that effectively documents the ideas in the patch, as well as how to use it and thus results in the final page effectively becoming the documentation.  I know Mike and Uwe have done a ton of work on this, but would it be too much trouble to ask for a Wiki page that describes the current state of the patch?  It is really hard to follow, in JIRA, all the different threads and which ones are still valid and which are not.",
            "date": "2009-07-21T12:50:26.375+0000",
            "id": 142
        },
        {
            "author": "Uwe Schindler",
            "body": "Mark Miller on java-dev:\n\nbq. * Are we going to keep Token.java or not?  (Current patch still has it deprecated).\nI need to know this as well - I have to make a new Token class for the Highlighter package if this one is deprecated. It would seem a convenience to keep it around.",
            "date": "2009-07-21T18:11:10.692+0000",
            "id": 143
        },
        {
            "author": "Uwe Schindler",
            "body": "{quote}\nbq. * Are we going to keep Token.java or not?  (Current patch still has it deprecated).\nI need to know this as well - I have to make a new Token class for the Highlighter package if this one is deprecated. It would seem a convenience to keep it around.\n{quote}\n\nI would also keep it non-deprecated. It is useful also as a convenience class and can be used as one attribute instance for all 6 base attributes (the backwards compatibility layer of the current patch already does this).",
            "date": "2009-07-21T18:16:51.446+0000",
            "id": 144
        },
        {
            "author": "Uwe Schindler",
            "body": "Updated patch that has the following changes:\n- Updated QueryParser.jj instead of .java and rebuilt the parser\n- added final to incrementToken() in CachingTokenFilter\n- added Michael's AttributeSource improvement",
            "date": "2009-07-21T21:59:44.593+0000",
            "id": 145
        },
        {
            "author": "Michael Busch",
            "body": "I agree  - we should keep Token and not deprecate it.",
            "date": "2009-07-21T22:23:23.907+0000",
            "id": 146
        },
        {
            "author": "Michael Busch",
            "body": "Uwe's latest patch with these changes:\n\n- Token is not deprecated anymore. \n- Updated the examples in the 'New TokenStream API\" section in package.html (analysis package) to reflect the changes this patch introduces (Attribute vs. AttributeImpl.)\n- All new files have svn:eol-style=native set now.\n\ntest-core, test-contrib and test-tag all pass. I think this is ready to commit now. \n\nWe might want to improve some javadocs a bit, especially the \"expert\" APIs, such as AttributeFactory. Also  another section about how to improve caching performance would be good. But I think we should open a separate issue for the doc improvements to get the other related issues going. The main APIs are pretty well documented I think.",
            "date": "2009-07-22T01:06:20.968+0000",
            "id": 147
        },
        {
            "author": "Michael Busch",
            "body": "I think the points listed in the description of this issue are still mostly valid. Uwe, do you want to add the changes you made in addition?",
            "date": "2009-07-22T01:08:39.187+0000",
            "id": 148
        },
        {
            "author": "Uwe Schindler",
            "body": "Updated issue description/summary to reflect last state.",
            "date": "2009-07-22T08:58:56.062+0000",
            "id": 149
        },
        {
            "author": "Uwe Schindler",
            "body": "Forgot the new TeeSinkTokenFilter and reformatted the text to use NLs.",
            "date": "2009-07-22T10:20:24.021+0000",
            "id": 150
        },
        {
            "author": "Dave Been",
            "body": "My first post to the list, it appears i should comment here in the JIRA, not reply to email, apologize if i did this wrong:\n\nI've been following this AttributeSource/TokenStream patch thread and reviewing the changes/backwards compatibility issues and the changes.  \nextremely interesting problem/solution.\n\nwhile looking at Uwe's PerfTest3 I noticed an unused allocation in the last run for \"reused stream new API only\"\n\n     for (int i = 0; i < c; i++) {\n        if (i==1000) t = System.currentTimeMillis();\n        tz.reset(new StringReader(text));\n        // Token reusableToken=new Token();   <<<<     This one\n        int num=0;\n        while (tok.incrementToken()) {\n          num++;\n        }\n      }\n\n\njust a small cost, but makes the new reusable api slightly faster\n\nWith extra alloc:\n\nTime for 100000 runs with new instances (old API): 12.75s\nTime for 100000 runs with reused stream (old API): 9.969s\nTime for 100000 runs with new instances (new API only): 13.969s\nTime for 100000 runs with reused stream (new API only): 11.735s  <<<<<<<<<<<<<<<<<<<<<<<<<<\n\n\nWithout extra alloc (changes only the last line's time):\n\nTime for 100000 runs with new instances (old API): 12.593s\nTime for 100000 runs with reused stream (old API): 9.578s\nTime for 100000 runs with new instances (new API only): 13.75s\nTime for 100000 runs with reused stream (new API only): 11.453s     <<<<<<<<<<<<<<<<<<<<<<<<<<\n\n\n\ndave",
            "date": "2009-07-22T19:40:56.423+0000",
            "id": 151
        },
        {
            "author": "Michael Busch",
            "body": "Thanks, Dave... I'll remove that unused allocation before committing.",
            "date": "2009-07-22T20:14:34.170+0000",
            "id": 152
        },
        {
            "author": "Michael Busch",
            "body": "OK, I think we're finally ready to commit here!\n\nI'll wait until Friday - if nobody objects until then, I will commit the latest patch.",
            "date": "2009-07-22T20:15:34.473+0000",
            "id": 153
        },
        {
            "author": "Grant Ingersoll",
            "body": "Checking now.",
            "date": "2009-07-24T01:35:56.856+0000",
            "id": 154
        },
        {
            "author": "Grant Ingersoll",
            "body": "{quote}Token is no longer deprecated, instead it implements all 6 standard\ntoken interfaces (see above). The wrapper for next() and next(Token)\nuses this, to automatically map all attribute interfaces to one\nTokenWrapper instance (implementing all 6 interfaces), that contains\na Token instance. next() and next(Token) exchange the inner Token\ninstance as needed. For the new incrementToken(), only one\nTokenWrapper instance is visible, delegating to the currect reusable\nToken. This API also preserves custom Token subclasses, that maybe\ncreated by very special token streams (see example in Backwards-Test)\n{quote}\n\nToken is no longer deprecated, but all the methods that return it are, right?  I think they are, but it is late, and I may have missed something.  So, what happens in 3.0?  What good does a Token do at that point?\n\nAlso, in looking at incrementToken(), nearly the entire implementation seems to be based on deprecated stuff, doesn't that mean it has to all be re-implemented in 3.0?  Something about that doesn't feel right.\n\nMore later...\n\n",
            "date": "2009-07-24T03:30:07.056+0000",
            "id": 155
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nToken is no longer deprecated, but all the methods that return it are, right? I think they are, but it is late, and I may have missed something. So, what happens in 3.0? What good does a Token do at that point?\n{quote}\n\nIt's what you often asked for: if you don't want to deal with multiple Attributes you can simply add a Token to the AttributeSource and cache the Token reference locally in your stream/filter, because Token now implements all core token attributes.\n\n{quote}\nAlso, in looking at incrementToken(), nearly the entire implementation seems to be based on deprecated stuff, doesn't that mean it has to all be re-implemented in 3.0? Something about that doesn't feel right.\n{quote}\n\nIdeally there wouldn't be an incrementToken() implementation and it would be abstract. However, that's of course not backwards-compatible. Everything is deprecated in the implementation because this code was only written to support backwards-compatibility with the old API.",
            "date": "2009-07-24T07:50:04.132+0000",
            "id": 156
        },
        {
            "author": "Uwe Schindler",
            "body": "And in 3.0 all my phantastic backwards-compatibility stuff is completely removed - and incrementToken() is abstract - What a pity :( The default impl in incrementToken and all other code parts in TokenStream.java will be removed (this is why also new code parts are marked deprectated, it makes it easier to be removed). The code will be about 1/3 of the current size when the backwards layer is removed :-)\nThe biggest problem of the migration to the new API is the backwards stuff, because we have now 3 different TokenStream APIs (next(), next(Token), incrementToken()). Also TokenWrapper is backwards stuff and will be removed.",
            "date": "2009-07-24T07:59:57.014+0000",
            "id": 157
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. if you don't want to deal with multiple Attributes you can simply add a Token to the AttributeSource and cache the Token reference locally in your stream/filter, because Token now implements all core token attributes.\n\nSee the test case for AttributeSource, which tests this.\nBut I think it is not as easy if you have a chain of TokenFilters. Only the first one can add the Token Impl to the AttSource (when the attributes are not yet added). So if one TokenStream adds a TermAttribute and later a Token impl is added, the Token will handle all attributes except the TermAttribute.\nTo force a whole chain to use Token as AttributeImpl, the first created TokenStream (normally the Tokenizer) should set an AttributeFactory, that creates a Token. All filters will then get it from the parent.\nSo in general you can add an Token instance to the AttributeSource but should still reference the attributes by the interfaces.",
            "date": "2009-07-24T08:11:28.414+0000",
            "id": 158
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nSo in general you can add an Token instance to the AttributeSource but should still reference the attributes by the interfaces.\n{quote}\n\nI completely agree. We should discourage users to reference Token and rather use the Attribute interfaces. That's the whole beauty and flexibility about this new API.\n\nHowever, using Token as the actual implementing instance can be convenient to optimize caching or serialization performance.",
            "date": "2009-07-24T08:48:53.276+0000",
            "id": 159
        },
        {
            "author": "Michael Busch",
            "body": "Grant, are you still reviewing? I was going to commit this today... shall I wait?",
            "date": "2009-07-24T18:25:00.401+0000",
            "id": 160
        },
        {
            "author": "Grant Ingersoll",
            "body": "Go ahead, I'm satisfied.",
            "date": "2009-07-24T18:34:26.404+0000",
            "id": 161
        },
        {
            "author": "Michael Busch",
            "body": "Cool thanks for reviewing.\n\nI'll commit later this afternoon.",
            "date": "2009-07-24T18:50:12.870+0000",
            "id": 162
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nI completely agree. We should discourage users to reference Token and rather use the Attribute interfaces. That's the whole beauty and flexibility about this new API.\n{quote}\n\nHas there been any thought into reconsidering the new API's \"experimental\" status then?\nI don't think the WARNING: encourages users to use these interfaces!\n\nor maybe a compromise: maybe modify the javadocs to be a little less scary: does this text have to be FF0000 (red) ?\n",
            "date": "2009-07-24T18:56:47.165+0000",
            "id": 163
        },
        {
            "author": "Grant Ingersoll",
            "body": "Actually, one thing I still don't get:\n\nWhat happens to the attributes that have traditionally been thrown away during indexing?  ie offset, type?  How would one add them into the index like other attributes?  Or, for that matter, exclude them.\n\nI seem to recall there being a loop over attributes somewhere in the posting process, but I can no longer find that code.",
            "date": "2009-07-24T18:57:49.489+0000",
            "id": 164
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI don't think the WARNING: encourages users to use these interfaces!\n{quote}\n\nYeah we should improve that. Let me commit the patch as is and open a new issue for improving the warnings.\nI don't want to touch this patch anymore, it's so big.",
            "date": "2009-07-24T19:29:20.453+0000",
            "id": 165
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nWhat happens to the attributes that have traditionally been thrown away during indexing? ie offset, type? How would one add them into the index like other attributes? Or, for that matter, exclude them.\n{quote}\n\nThe default index format does not make use of some attributes, e.g. type, just as before.\nFlexible indexing will allow to customize the format; then you will be able to store whatever attribute you like in the index.",
            "date": "2009-07-24T19:31:11.224+0000",
            "id": 166
        },
        {
            "author": "Michael Busch",
            "body": "Committed revision 797665.\n\nThanks, Uwe, for all your hard work!!\nAnd thanks to everyone else who helped reviewing here.",
            "date": "2009-07-24T21:48:12.876+0000",
            "id": 167
        },
        {
            "author": "Mark Miller",
            "body": "Not sure what issue it stems from, but Token has a bunch of constructors that are deprecated, but that don't point you to something new.\n\nedit\n\nmust have come from the setBuffer stuff ",
            "date": "2009-07-24T23:56:52.118+0000",
            "id": 168
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nNot sure what issue it stems from, but Token has a bunch of constructors that are deprecated, but that don't point you to something new. \n{quote}\n\nThis is already the case in Lucene 2.4; unrelated to this issue.",
            "date": "2009-07-25T00:10:42.437+0000",
            "id": 169
        },
        {
            "author": "Uwe Schindler",
            "body": "This is a small improvement, related to Grant's comments:\n\nThe TokenStream ctor can have a AttributeFactory, so you can create a subclass of TokenStream that uses a specific AttributeFacory (e.g. using Token instances). Filters do not need this (as they use the factory of the input stream). \nThe factory must therefore be set on the root stream. This is normally a subclass of Tokenizer. The problem: Tokenizer does not have ctors for AttributeFacory, so you are not able to create any Tokenizer using a custom factory, e.g. for using Token as impl.\n\nI will commit this patch shortly.",
            "date": "2009-07-25T09:05:06.367+0000",
            "id": 170
        },
        {
            "author": "Uwe Schindler",
            "body": "Committed revision: 797727",
            "date": "2009-07-25T09:23:32.492+0000",
            "id": 171
        }
    ],
    "component": "modules/analysis",
    "description": "This patch makes the following improvements to AttributeSource and\nTokenStream/Filter:\n\n- introduces interfaces for all Attributes. The corresponding\n  implementations have the postfix 'Impl', e.g. TermAttribute and\n  TermAttributeImpl. AttributeSource now has a factory for creating\n  the Attribute instances; the default implementation looks for\n  implementing classes with the postfix 'Impl'. Token now implements\n  all 6 TokenAttribute interfaces.\n\n- new method added to AttributeSource:\n  addAttributeImpl(AttributeImpl). Using reflection it walks up in the\n  class hierarchy of the passed in object and finds all interfaces\n  that the class or superclasses implement and that extend the\n  Attribute interface. It then adds the interface->instance mappings\n  to the attribute map for each of the found interfaces.\n\n- removes the set/getUseNewAPI() methods (including the standard\n  ones). Instead it is now enough to only implement the new API,\n  if one old TokenStream implements still the old API (next()/next(Token)),\n  it is wrapped automatically. The delegation path is determined via\n  reflection (the patch determines, which of the three methods was\n  overridden).\n\n- Token is no longer deprecated, instead it implements all 6 standard\n  token interfaces (see above). The wrapper for next() and next(Token)\n  uses this, to automatically map all attribute interfaces to one\n  TokenWrapper instance (implementing all 6 interfaces), that contains\n  a Token instance. next() and next(Token) exchange the inner Token\n  instance as needed. For the new incrementToken(), only one\n  TokenWrapper instance is visible, delegating to the currect reusable\n  Token. This API also preserves custom Token subclasses, that maybe\n  created by very special token streams (see example in Backwards-Test).\n\n- AttributeImpl now has a default implementation of toString that uses\n  reflection to print out the values of the attributes in a default\n  formatting. This makes it a bit easier to implement AttributeImpl,\n  because toString() was declared abstract before.\n\n- Cloning is now done much more efficiently in\n  captureState. The method figures out which unique AttributeImpl\n  instances are contained as values in the attributes map, because\n  those are the ones that need to be cloned. It creates a single\n  linked list that supports deep cloning (in the inner class\n  AttributeSource.State). AttributeSource keeps track of when this\n  state changes, i.e. whenever new attributes are added to the\n  AttributeSource. Only in that case will captureState recompute the\n  state, otherwise it will simply clone the precomputed state and\n  return the clone. restoreState(AttributeSource.State) walks the\n  linked list and uses the copyTo() method of AttributeImpl to copy\n  all values over into the attribute that the source stream\n  (e.g. SinkTokenizer) uses. \n\n- Tee- and SinkTokenizer were deprecated, because they use\nToken instances for caching. This is not compatible to the new API\nusing AttributeSource.State objects. You can still use the old\ndeprecated ones, but new features provided by new Attribute types\nmay get lost in the chain. A replacement is a new TeeSinkTokenFilter,\nwhich has a factory to create new Sink instances, that have compatible\nattributes. Sink instances created by one Tee can also be added to\nanother Tee, as long as the attribute implementations are compatible\n(it is not possible to add a sink from a tee using one Token instance\nto a tee using the six separate attribute impls). In this case UOE is thrown.\n\nThe cloning performance can be greatly improved if not multiple\nAttributeImpl instances are used in one TokenStream. A user can\ne.g. simply add a Token instance to the stream instead of the individual\nattributes. Or the user could implement a subclass of AttributeImpl that\nimplements exactly the Attribute interfaces needed. I think this\nshould be considered an expert API (addAttributeImpl), as this manual\noptimization is only needed if cloning performance is crucial. I ran\nsome quick performance tests using Tee/Sink tokenizers (which do\ncloning) and the performance was roughly 20% faster with the new\nAPI. I'll run some more performance tests and post more numbers then.\n\nNote also that when we add serialization to the Attributes, e.g. for\nsupporting storing serialized TokenStreams in the index, then the\nserialization should benefit even significantly more from the new API\nthan cloning. \n\nThis issue contains one backwards-compatibility break:\nTokenStreams/Filters/Tokenizers should normally be final\n(see LUCENE-1753 for the explaination). Some of these core classes are \nnot final and so one could override the next() or next(Token) methods.\nIn this case, the backwards-wrapper would automatically use\nincrementToken(), because it is implemented, so the overridden\nmethod is never called. To prevent users from errors not visible\nduring compilation or testing (the streams just behave wrong),\nthis patch makes all implementation methods final\n(next(), next(Token), incrementToken()), whenever the class\nitsself is not final. This is a BW break, but users will clearly see,\nthat they have done something unsupoorted and should better\ncreate a custom TokenFilter with their additional implementation\n(instead of extending a core implementation).\n\nFor further changing contrib token streams the following procedere should be used:\n\n    *  rewrite and replace next(Token)/next() implementations by new API\n    * if the class is final, no next(Token)/next() methods needed (must be removed!!!)\n    * if the class is non-final add the following methods to the class:\n{code:java}\n      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should\n       * not be overridden. Delegates to the backwards compatibility layer. */\n      public final Token next(final Token reusableToken) throws java.io.IOException {\n        return super.next(reusableToken);\n      }\n\n      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should\n       * not be overridden. Delegates to the backwards compatibility layer. */\n      public final Token next() throws java.io.IOException {\n        return super.next();\n      }\n{code}\nAlso the incrementToken() method must be final in this case\n(and the new method end() of LUCENE-1448)\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1693",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "AttributeSource/TokenStream API improvements",
    "systemSpecification": true,
    "version": ""
}