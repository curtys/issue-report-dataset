{
    "comments": [
        {
            "author": "Uwe Schindler",
            "body": "The ngram things are serious, so also backport.\n\nWe get the non-generic java 1.4 version for solr 1.5 for free.",
            "date": "2010-01-14T23:54:38.278+0000",
            "id": 0
        },
        {
            "author": "Robert Muir",
            "body": "uwe's patch, with the fixes for contrib.\n\nbroken were compounds, n-gram filter, and edge n-gram filter",
            "date": "2010-01-14T23:58:18.436+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "before committing any fix i want to review / add tests for any tokenstreams that do not yet use this BaseTokenStreamTestCase, just to be sure there are no others with this problem.\n\nit may seem trivial but if this clearing does not take place properly, then things like position increment with stopfilter can grow to very large values, overflow, and cause IndexWriter to throw an exception: http://www.lucidimagination.com/search/document/f649a19901d33c75/illegalargumentexception_when_indexwriter_adddocument\n\n",
            "date": "2010-01-15T00:32:13.391+0000",
            "id": 2
        },
        {
            "author": "Robert Muir",
            "body": "Hello Uwe, i did not get time to review all tokenstreams but I converted a ShingleMatrix test to assertTokenStreamContents and found a clearAttributes() bug in it too, so it is also fixed in this one, more tokenstreams with problems might remain.",
            "date": "2010-01-15T03:36:39.650+0000",
            "id": 3
        },
        {
            "author": "Robert Muir",
            "body": "Hi Uwe, PrefixAwareTokenFilter did not clearAttributes() either. I tried all others i could find but I think the rest are ok.",
            "date": "2010-01-15T04:15:41.844+0000",
            "id": 4
        },
        {
            "author": "Uwe Schindler",
            "body": "Some updates to TeeSink test. Changes.txt.",
            "date": "2010-01-15T09:34:14.808+0000",
            "id": 5
        },
        {
            "author": "Uwe Schindler",
            "body": "More updates to TeeSink and also BaseTokenStreamTestCase to still call clearAttributes to force TS to not reuse attributes from previous calls to incrementToken(). This was one of the first traps, Robert investigated in 2.9.",
            "date": "2010-01-15T11:04:20.106+0000",
            "id": 6
        },
        {
            "author": "Robert Muir",
            "body": "i reviewed all code with incrementToken() and found 3 more problems, 2 in highlighter, 1 in memory.\n\ni also fixed all helper tokenstreams in core/contrib tests.",
            "date": "2010-01-15T12:36:57.764+0000",
            "id": 7
        },
        {
            "author": "Uwe Schindler",
            "body": "I'll commit attached patch woith some fixes for typos etc.",
            "date": "2010-01-15T13:42:07.795+0000",
            "id": 8
        },
        {
            "author": "Uwe Schindler",
            "body": "Fixed in trunk revision: 899627",
            "date": "2010-01-15T13:42:36.084+0000",
            "id": 9
        },
        {
            "author": "Uwe Schindler",
            "body": "Patch for 3.0 branch",
            "date": "2010-01-15T14:00:03.703+0000",
            "id": 10
        },
        {
            "author": "Uwe Schindler",
            "body": "Fixed in Lucene 3.0 revision: 899639",
            "date": "2010-01-15T14:21:55.864+0000",
            "id": 11
        },
        {
            "author": "Uwe Schindler",
            "body": "Patch for 2.9 branch. Tests are running...",
            "date": "2010-01-15T14:33:19.666+0000",
            "id": 12
        },
        {
            "author": "Uwe Schindler",
            "body": "There was a new bug in the 2.9 ShingleMatrixFilter because of incorrect Token reuse in a call to this.next(reusableToken). This patch now also contains the merged LUCENE-1939, which was missing.\n\nI commit this now.",
            "date": "2010-01-15T16:08:53.913+0000",
            "id": 13
        },
        {
            "author": "Uwe Schindler",
            "body": "Committed into 2.9 branch revision: 899681",
            "date": "2010-01-15T16:15:01.687+0000",
            "id": 14
        }
    ],
    "component": "modules/analysis",
    "description": "Robert had the idea to use a fake attribute inside BaseTokenStreamTestCase that records if its clear() method was called. If this is not the case after incrementToken(), asserTokenStreamContents fails. It also uses the attribute in TeeSinkTokenFilter, because there a lot of copying, captureState and restoreState() is used. By the attribute, you can track wonderful, if save/restore and clearAttributes is correctly implemented. It also verifies that *before* a captureState() it was also cleared (as the state will also contain the clear call). Because if you consume tokens in a filter, capture the consumed tokens and insert them, the capturedStates must also be cleared before.\n\nIn contrib analyzers are some test that fail to pass this additional assertion. They are not fixed in the attached patch.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2211",
    "issuetypeClassified": "OTHER",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Improve BaseTokenStreamTestCase to uses a fake attribute to check if clearAttributes() was called correctly - found bugs in contrib/analyzers",
    "systemSpecification": true,
    "version": "2.9, 2.9.1, 3.0"
}