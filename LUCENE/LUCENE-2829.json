{
    "comments": [
        {
            "author": "Robert Muir",
            "body": "quickly hacked up patch, \n\nfor the IndexSearcher case, we sum up docFreq ourselves, along the way saving the hashcodes\nof the readers where the term exists into a set.\n\nif this list exists (IndexSearcher case), the scorer then checks the reader's hashcode against this list...\nif we get a collision, worst case we do a wasted seek. but we don't have to keep any hairy references\nto readers or anything.",
            "date": "2010-12-21T20:37:45.195+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "I made a random PK lookup tester (committed to luceneutil), to lookup by docid (unique key) from the luceneutil index.\n\nPre-patch it's 53 usec per lookup and with this patch it's 31 usec -- ~42% faster!",
            "date": "2010-12-21T20:43:07.551+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "right, we just have to not do stupid things like hash hashcodes to make it faster for when the data is hot...\nbut as a start this is safe, hopefully we could do something non-invasive (and backportable) to make it faster.\n\n",
            "date": "2010-12-21T20:47:13.969+0000",
            "id": 2
        },
        {
            "author": "Hoss Man",
            "body": "The patch is over my head, but providing a super optimized solution to the \"primary key\" type lookup problem definitely seems worthwhile -- it has me wondering if a \"PrimaryKeyQuery\" that works like TermQuery bug quits collecting as soon as it finds one matching document would be a good idea?",
            "date": "2010-12-21T20:54:10.685+0000",
            "id": 3
        },
        {
            "author": "Robert Muir",
            "body": "Hoss Man, well I think if you surely know its a PK field you can definitely do something better, starting with a custom collector that does something like what you mentioned, with no PQ at all etc.\n\nBut in this case, though i categorized it as PK, the general problem is this:\n* in lots of cases we do redundant seeks, like to get the docFreq, then to get the DocsEnum\n* in most cases the term dictionary cache helps here because the 2nd time (e.g. getting DocsEnum) is cached.\n\nHere's the problem with \"PK\" or \"PK-ish\" (low freq terms like what wildcards/fuzzies/range queries hit too):\n* our cache doesn't cache \"negative\" hits, the fact that a term *doesnt* exist in some segment.\n* For example in the PK case, if there are 15 segments we always get at most 1 cache hit and \nat least 14 misses when getting the DocsEnum, so we do at least 14 wasted seeks always.\n* For other low frequency terms that don't exist in all segments (very precise dates or what have you) \nthe same idea applies, just to a lesser extent: the PK is the worst.\n",
            "date": "2010-12-21T21:12:07.203+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "I think a cleaner interface may be for the Weight.scorer method to receive the ord of the sub reader in the parent?\n\nThis way TermWeight doesn't need a hash / ReaderView -- it can use just an array.\n\nWe could also make it a struct/class, that contain parent, sub, and ord.  This way TermWeight (and others) could assert they are not invoked on a different parent reader.",
            "date": "2010-12-21T21:15:24.065+0000",
            "id": 5
        },
        {
            "author": "Robert Muir",
            "body": "bq. I think a cleaner interface may be for the Weight.scorer method to receive the ord of the sub reader in the parent?\n\nYes, ideally with the actual df in there. This would save the third seek in the bulkpostings branch.\n\nBut at the same time, i'm worried/don't want this issue to evolve into TermState (LUCENE-2694). I wasn't thinking \nthat this was any kind of end-solution but just an approach we could take that would work against 3.1\n",
            "date": "2010-12-21T21:22:29.910+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "bq. but just an approach we could take that would work against 3.1\n\nWell it's already a 42% speedup so it seems very worthwhile already.\n\nBut, then, passing a struct (parent/sub/ord) is a fairly small change, and, if it \"matches\" the change we will make on LUCENE-2694, then that's great.",
            "date": "2010-12-21T21:27:48.804+0000",
            "id": 7
        },
        {
            "author": "Robert Muir",
            "body": "bq. But, then, passing a struct (parent/sub/ord) is a fairly small change, and, if it \"matches\" the change we will make on LUCENE-2694, then that's great.\n\nOk, that might be a good approach, to fix the it this way in LUCENE-2694 (or actually, preferably add the parent/sub/ord in its own issue!), \nbut in 3.1 we could use the struct to avoid wasted seeks on PK terms... \n\nSeems like backporting the entire termstate thing could be a little tricky/risky for 3.1, with not much to gain there except\nPK lookups anyway, since the multitermqueries there tend to be slow (dominated by term comparison) and don't even work\nper-segment anyway.\n",
            "date": "2010-12-21T21:35:11.426+0000",
            "id": 8
        },
        {
            "author": "Yonik Seeley",
            "body": "Why not keep the TermState cache and use it for all queries except MTQ, while using a different mechanism for MTQ to avoid trashing the cache?\n\nThe cache has a number of advantages that may never be duplicated in a different type of API, including\n- actually cache frequently used terms across different requests\n- cache terms reused in the same request.  term proximity boosting is an example:   +united +states \"united states\"^10\n\nedit: and as robert previously pointed out, if we cached misses as well, then we could avoid needless seeks on segments that don't contain the term.",
            "date": "2010-12-22T14:23:29.430+0000",
            "id": 9
        },
        {
            "author": "Robert Muir",
            "body": "bq. edit: and as robert previously pointed out, if we cached misses as well, then we could avoid needless seeks on segments that don't contain the term.\n\nTrue, this is a good idea, just a little tricker:\n* In trunk, we have TermsEnum.seek(BytesRef text, boolean useCache), defaulting to true.\n* FilteredTermsEnum passes false here, so the multitermqueries don't populate the cache with \n  garbage while enumerating (eg foo*),  only explicitly at the end with cacheTerm() (per-segment) \n  for the ones that were actually accepted. They sum up their docFreq themselves to prevent the \n  first wasted seek in TermQuery. \n* So this solution would make MTQ worse, as it would cause them to trash the caches in the \n  second wasted seek (the docsenum) where they do not today, with negative entries for the \n  segments where the term doesn't exist. Today they do this wasted seek, but they don't \n  trash the cache here. The only solution to prevent that is the PerReaderTermState \n  (or something equally complicated).\n* We would have to look at other places where negative entries would hurt, for example \n  rebuilding spellcheck indexes uses this 'termExists()' method implemented with docFreq. \n  So we would have to likely change spellcheck's code to use a TermsEnum and \n  seek(term, false)... using a termsenum in parallel with the spellcheck dictionary would \n  obviously be more efficient for the index-based spellcheck case (forget about caching)\n  versus docFreq()'ing every term... *but* we cannot assume the spellcheck \"Dictionary\" \n  is actually in term order, (imagine the File-based dictionary case), so we can't \n  implement this today.\n\nOn 3.x i think its slightly less complicated as there is already a hack in the cache to \nprevent sequential termsenums from trashing it (e.g. foo*), and pretty much all the MTQs \njust enumerate sequentially anyway... (except NRQ which doesn't enum many terms \nanyway, likely not a problem).\n\nBut we would have to at least fix the spellcheck case there too I think.\n\nNot saying I don't like your idea... just saying there's more work to do it.\n",
            "date": "2010-12-22T14:46:19.731+0000",
            "id": 10
        },
        {
            "author": "Robert Muir",
            "body": "On further thought Yonik, your idea is really completely unrelated.\n\nWe shouldn't be seeking to terms/relying upon the terms dictionary cache \ninternally when we don't need to...\n\nwhether or not its populated with negative entries for the more general case is unrelated,\neven if we go that route we shouldn't be lazy and rely upon that.\n",
            "date": "2010-12-22T14:58:54.292+0000",
            "id": 11
        },
        {
            "author": "Michael McCandless",
            "body": "bq. The cache has a number of advantages that may never be duplicated in a different type of API\n\n+1 -- I agree we should keep the TermState cache.  It has benefits outside of re-use win a single query.\n\nBut allowing term-lookup-intensive clients like MTQ  to do their own caching (ie pulling the TermState from the enum) is also important.  I think we need both.\n\nOn caching misses... that makes me nervous.  If there are apps out there that do alot of checking for terms that don't exist that can destroy the cache.\n\nThe cache is a great safety net but I think our core queries should be good consumers, when possible, and hold their own TermState.",
            "date": "2010-12-22T16:25:34.151+0000",
            "id": 12
        },
        {
            "author": "Earwin Burrfoot",
            "body": "Term lookup misses can be alleviated by a simple Bloom Filter.\nNo caching misses required, helps both PK and near-PK queries.",
            "date": "2010-12-22T16:46:33.273+0000",
            "id": 13
        },
        {
            "author": "Robert Muir",
            "body": "Bloom filters and negative caches are nice, but please open separate issues!\nI am starting to feel like its mandatory to refactor the entirety of lucene to make a single incremental improvement.\n\nSo, I'd like to proceed with this issue as-is, to make TermWeight explicitly do less seeks.\n",
            "date": "2010-12-22T18:08:45.459+0000",
            "id": 14
        },
        {
            "author": "Earwin Burrfoot",
            "body": "Nobody halts your progress, we're merely discussing.\n\nI, on the other hand, have a feeling that Lucene is overflowing with \"single incremental improvements\" aka \"hacks\", as they are easier and faster to implement than trying to get a bigger picture, and, yes, rebuilding everything :)\nFor example, better term dict code will make this issue (somewhat hackish, admit it?) irrelevant. Whether we implement bloom filters, or just guarantee to keep the whole term dict in memory with reasonable lookup routine (eg. as FST).\n\nHaving said that, I reiterate, I'm not here to stop you or turn this issue into something else.",
            "date": "2010-12-22T19:35:55.522+0000",
            "id": 15
        },
        {
            "author": "Robert Muir",
            "body": "bq. For example, better term dict code will make this issue (somewhat hackish, admit it?) irrelevant. \n\nRight, it is hackish, but what is a worse hack is wasted seeks in our next 3.1 release because we can't\nkeep scope under control and fix small problems without rewriting everything, which means less \ngets backported to our stable branch.\n\nAnyway, I'm just gonna mark this won't fix so I don't have to deal with it anymore.",
            "date": "2010-12-22T19:43:48.994+0000",
            "id": 16
        },
        {
            "author": "Michael McCandless",
            "body": "I think we should commit this, and if/when LUCENE-2694 and/or LUCENE-2831 are committed to 3.x, we can revisit it.",
            "date": "2011-01-04T17:20:24.520+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "I ported this patch to 3.x, and fix a few tests that were illegally passing top readers to Weight.scorer.  There's still a couple nocommits, but I think it's close.",
            "date": "2011-01-06T17:28:52.793+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "New patch.  I added VirtualMethods to Sim to make sure Sim subclasses that don't override idfExplain that takes docFreq are still called.",
            "date": "2011-01-08T19:59:31.924+0000",
            "id": 19
        },
        {
            "author": "Grant Ingersoll",
            "body": "Bulk close for 3.1",
            "date": "2011-03-30T15:49:52.351+0000",
            "id": 20
        }
    ],
    "component": "core/search",
    "description": "For things that are like primary keys and don't exist in some segments (worst case is primary/unique key that only exists in 1)\nwe do wasted seeks.\n\nWhile LUCENE-2694 tries to solve some of this issue with TermState, I'm concerned we could every backport that to 3.1 for example.\n\nThis is a simpler solution here just to solve this one problem in termquery... we could just revert it in trunk when we resolve LUCENE-2694,\nbut I don't think we should leave things as they are in 3.x\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2829",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "improve termquery \"pk lookup\" performance",
    "systemSpecification": true,
    "version": ""
}