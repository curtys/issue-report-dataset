{
    "comments": [
        {
            "author": "Christian Moen",
            "body": "These are very interesting questions, Robert.  Please find my comments below.\n\n{quote}\nshould these parameters continue to be static-final, or configurable?\n{quote}\n\nIt's perhaps possible to make these configurable, but I think we'd be exposing configuration that is most likely to confuse most users rather than help them.\n\nThe values currently uses have been found using some analysis and experimentation, and they can probably be improved both in terms of tuning and with added heuristics -- in particular for katakana compounds (more below).\n\nHowever, changing and improving this requires quite detailed analysis and testing, though.  I think the major case for exposing them is as a means for easily tuning them rather than these parameters being generally useful to users.\n\n{quote}\nshould POS also play a role in the algorithm (can/should we refine exactly what we decompound)?\n{quote}\n\nVery good question and an interesting idea.\n\nIn the case of long kanji words such as \u95a2\u897f\u56fd\u969b\u7a7a\u6e2f (Kansai International Airport), which is a known noun, we can possible use POS info as a hint for applying the Viterbi penalty.  In the case of unknown kanji, Kuromoji unigrams them.  (\u95a2\u897f\u56fd\u969b\u7a7a\u6e2f becomes \u95a2\u897f  \u56fd\u969b  \u7a7a\u6e2f (Kansai International Airport) using search mode.)\n\nKatakana compounds such as \u30b7\u30cb\u30a2\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30a8\u30f3\u30b8\u30cb\u30a2 (senior software engineer) becomes one token without search mode, but when search mode is used, we get three tokens \u30b7\u30cb\u30a2  \u30bd\u30d5\u30c8\u30a6\u30a7\u30a2  \u30a8\u30f3\u30b8\u30cb\u30a2 as you would expect.  It's also the case that \u30b7\u30cb\u30a2\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30a8\u30f3\u30b8\u30cb\u30a2 is an unknown word, but its constituents become known and get the correct POS after search mode. \n\nIn general, unknown words get a noun-POS (\u540d\u8a5e) so the idea of using POS here should be fine.\n\nThere are some problems with the katakana decompounding in search mode.  For example, \u30b3\u30cb\u30ab\u30df\u30ce\u30eb\u30bf\u30db\u30fc\u30eb\u30c7\u30a3\u30f3\u30b0\u30b9 (Konika Minolta Holdings) becomes \u30b3\u30cb\u30ab  \u30df\u30ce\u30eb\u30bf  \u30db\u30fc\u30eb  \u30c7\u30a3\u30f3\u30b0\u30b9  (Konika Minolta horu dings), where we get the additional token \u30db\u30fc\u30eb (also means hall, in Japanese).\n\nTo sum up, I think we can potentially use the noun-POS as a hint when doing the decompounding in search mode, but I'm not sure how much we will benefit from it, but I like the idea.  I think we'll benefit most from an improved heuristic for non-kanji to improve katakana decompounding.\n\nLet me have a tinker and see how I can improve this.\n\n{quote}\nis the Tokenizer the best place to do this, or should we do it in a tokenfilter? or both?\n{quote}\n\nInteresting idea and good point regarding IDF.\n\nIn order do the decompoundning, we'll need access to the lattice and add entries to it before we run the Viterbi.  If we do normal segmentation first then run a decompounding filter, I think we'll need to run the Viterbi twice in order to get the desired results.  (Optimizations are possible, though.)\n\nI'm thinking a possibility could be to expose possible decompounds as part of Kuromoji's Token interface.  We can potentially have something like\n\n{code:title=Token.java}\n\n/**\n * Returns a list of possible decompounds for this token found by a heuristic\n * \n * @return a list of candidate decompounds or null of none is found\n */\nList<Token> getDecompounds() {\n  // ...\n}\n{code}\n\nIn the case of \u30b7\u30cb\u30a2\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30a8\u30f3\u30b8\u30cb\u30a2, the current token would have surface form \u30b7\u30cb\u30a2\u30bd\u30d5\u30c8\u30a6\u30a7\u30a2\u30a8\u30f3\u30b8\u30cb\u30a2, but with tokens \u30b7\u30cb\u30a2, \u30bd\u30d5\u30c8\u30a6\u30a7\u30a2 and \u30a8\u30f3\u30b8\u30cb\u30a2 accessible using {{getDecompounds()}}.\n\nAs a general notice, I should point our that how well the heuristics performs depends on the dictionary/statistical model used (i.e. IPADIC) and if we might want to make different heuristics for each of those we support as needed.",
            "date": "2012-01-27T06:11:40.846+0000",
            "id": 0
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nI'm thinking a possibility could be to expose possible decompounds as part of Kuromoji's Token interface.\n{quote}\n\nI like this idea: I think it would give the most flexibility, we would populate some attribute from \nToken just like we do today for other attributes, and then actual indexing of compounds can be \ncontrolled with a configurable tokenfilter.\n\nLong term, this lets the tokenizer stay a tokenizer and prevents it from growing too complex.",
            "date": "2012-01-27T12:21:49.782+0000",
            "id": 1
        },
        {
            "author": "Christian Moen",
            "body": "Thanks for the feedback.\n\nI'm working on tuning the heuristics to improve accuracy of katakana segmentation in search mode.\n\nI'll keep you posted on results and a patch.  Unit tests will document the cases.",
            "date": "2012-01-27T18:17:08.761+0000",
            "id": 2
        },
        {
            "author": "Christian Moen",
            "body": "I've improved the heuristic and submitted a patch to LUCENE-3730, which covers the issue.\n\nWe can now deal with cases such as \u30b3\u30cb\u30ab\u30df\u30ce\u30eb\u30bf\u30db\u30fc\u30eb\u30c7\u30a3\u30f3\u30b0\u30b9 and many others just fine.  The former becomes \u30b3\u30cb\u30ab  \u30df\u30ce\u30eb\u30bf  \u30db\u30fc\u30eb\u30c7\u30a3\u30f3\u30b0\u30b9 as we'd like.\n\nI think we should apply LUCENE-3730 before changing any defaults -- and also independently of changing any defaults.  I think we should also make sure that the default we use for Lucene is consistent with the Solr's default in {{schema.xml}} for {{text_ja}}.\n\nI'll do additional tests on a Japanese corpus and provide feedback, and we can use this as a basis for how to follow up.  Hopefully, we'll have sufficient and good data to conclude on this.\n",
            "date": "2012-01-30T04:42:18.232+0000",
            "id": 3
        },
        {
            "author": "Christian Moen",
            "body": "I've segmented some Japanese Wikipedia text into sentences (using a naive sentence segmenter) and then segmented each sentence using both normal and search mode with the Kuromoji on Github that has LUCENE-3730 applied.  Segmentation with Kuromoji in Lucene should be similar overall (modulo some differences in punctuation handling).\n\nSearch mode and normal mode segmentation match completely in 90.7% of the sentences segmented and there's a 99.6% match at the token level (when counting normal mode tokens).\n\nFind attached some HTML files with a total of 10,000 sentences that demonstrates the differences in segmentation.\n\nOverall, I think search mode does a decent job.  I've written someone else doing Japanese NLP to get their second opinion, in particular if the kanji splitting should be made somewhat less eager to split three letter words.",
            "date": "2012-02-01T10:08:15.433+0000",
            "id": 4
        },
        {
            "author": "Christian Moen",
            "body": "The latest attached patch introduces a default mode in {{Segmenter}}, which is now {{Mode.SEARCH}}.\n\nThis mode is used by {{KuromojiAnalyzer}} in Lucene without further code changes.  The Solr factory duplicated the default mode, but now retrieves it from {{Segmenter}}.  This way, we set the default mode for both Solr and Lucene in a single place (in {{Segmenter}}), which I find cleaner.\n\nI've also moved some constructors around in {{Segmenter}} and did some minor formatting/style changes.",
            "date": "2012-02-05T05:08:53.857+0000",
            "id": 5
        },
        {
            "author": "Robert Muir",
            "body": "Thanks Christian: I committed this.",
            "date": "2012-02-05T12:44:53.534+0000",
            "id": 6
        }
    ],
    "component": "",
    "description": "Kuromoji supports an option to segment text in a way more suitable for search,\nby preventing long compound nouns as indexing terms.\n\nIn general 'how you segment' can be important depending on the application \n(see http://nlp.stanford.edu/pubs/acl-wmt08-cws.pdf for some studies on this in chinese)\n\nThe current algorithm punishes the cost based on some parameters (SEARCH_MODE_PENALTY, SEARCH_MODE_LENGTH, etc)\nfor long runs of kanji.\n\nSome questions (these can be separate future issues if any useful ideas come out):\n* should these parameters continue to be static-final, or configurable?\n* should POS also play a role in the algorithm (can/should we refine exactly what we decompound)?\n* is the Tokenizer the best place to do this, or should we do it in a tokenfilter? or both?\n  with a tokenfilter, one idea would be to also preserve the original indexing term, overlapping it: e.g. ABCD -> AB, CD, ABCD(posInc=0)\n  from my understanding this tends to help with noun compounds in other languages, because IDF of the original term boosts 'exact' compound matches.\n  but does a tokenfilter provide the segmenter enough 'context' to do this properly?\n\nEither way, I think as a start we should turn on what we have by default: its likely a very easy win.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-3726",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Default KuromojiAnalyzer to use search mode",
    "systemSpecification": true,
    "version": "3.6, 4.0-ALPHA"
}