{
    "comments": [
        {
            "author": "Michael Busch",
            "body": "All core & contrib tests pass.",
            "date": "2009-10-08T00:16:05.344+0000",
            "id": 0
        },
        {
            "author": "Michael Busch",
            "body": "Committed revision 822978.",
            "date": "2009-10-08T00:55:11.725+0000",
            "id": 1
        },
        {
            "author": "Uwe Schindler",
            "body": "Just one question: What happens with indexes that already had compressed fields. Do they behave as before?\n\n*edit*\n\nFrom the patch I see that you also removed the bitmask for testing compression in index format. So an index with compressed fields from 2.9 should behave undefined. In my opinion, support for reading compressed fields should stay available, but not for writing. And: as soon as segments are merged, they should get silently uncompressed (what should be no problem if the special FieldForMerge is no longer used).\n\nAlso the constant bitmask for compression should stay \"reserved\" for futrure use.",
            "date": "2009-10-08T04:27:44.118+0000",
            "id": 2
        },
        {
            "author": "Michael Busch",
            "body": "Users can use CompressionTools#decompress() now. They just must know now which binary fields are compressed.\n\nI don't think the SegmentMerger should uncompress automatically? That'd make <=2.9 indexes suddenly bigger.",
            "date": "2009-10-08T07:26:03.582+0000",
            "id": 3
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nAlso the constant bitmask for compression should stay \"reserved\" for futrure use.\n{quote}\n\nYeah I think you're right, we must make sure that we don't use this bit for something else, as old indexes might have it set to true already. I'll add it back with a deprecation comment saying that we'll remove it in 4.0. (4.0 won't have to be able to read <3.0 indexes anymore).",
            "date": "2009-10-08T08:04:17.102+0000",
            "id": 4
        },
        {
            "author": "Michael Busch",
            "body": "Reopening so that I don't forget to add back the COMPRESS bit.",
            "date": "2009-10-08T08:04:56.999+0000",
            "id": 5
        },
        {
            "author": "Uwe Schindler",
            "body": "In the discussion with Mike, we said, that all pre-2.9 compressed fields should behave as before, e.g. they should automatically decompress. It should not be possibile to create new ones. This is just index compatibility, in the current version it is simply not defined what happens with pre-2.9 fields. The second problem are older compressed fields using the modified Java-UTF-8 encoding, which may not correctly decompress now (if you receive with getByte())\n\nThe problem with your patch: If the field is compressed and you try to get it, you would not hit it (because it is marked as String, not binary). The new self-compressed fields are now should be \"binary\", before they were binary *or* string. See the discussion in LUCENE-652",
            "date": "2009-10-08T08:11:54.842+0000",
            "id": 6
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nThe problem with your patch: If the field is compressed and you try to get it, you would not hit it (because it is marked as String, not binary). The new self-compressed fields are now should be \"binary\", before they were binary or string. See the discussion in LUCENE-652\n{quote}\n\nHmm I see. I should have waited a bit with committing - sorry!\nI'll take care of it tomorrow, it's getting too late now.",
            "date": "2009-10-08T08:28:43.935+0000",
            "id": 7
        },
        {
            "author": "Uwe Schindler",
            "body": "No problem. :-)\n\nI think it should not be a big task to preserve the decompression of previously compressed fields. Just revert FieldsReader changes and modify a little bit.",
            "date": "2009-10-08T08:33:44.141+0000",
            "id": 8
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. I don't think the SegmentMerger should uncompress automatically? That'd make <=2.9 indexes suddenly bigger.\n\nIf we re-add support for compressed fields, we have to also provide the special FieldForMerge again. To get rid of this code (which is the source of the problems behind the whole COMPRESS problem), we could simply let it as it is now without FieldForMerge. If you merge two segmets with compressed data then, without FieldToMerge it gets automatically decompressed and written in uncompressed variant to the new segment. As compressed fields are no longer supported, this is the correct behaviour. During merging, the compress bit must be removed.\n\nThe problem are suddenly bigger indexes, but we should note this in docs: \"As compressed fields are no longer supported, during mering the compression is removed. If you want to compress your fields, do it yourself and store it as binary stored field.\"\n\nJust for confirmation:\nI have some indexes with compress enabled (for some of the documents, since 2.9 we do not compress anymore, newly added docs have no compression anymore [it was never an good idea because of performance]). So I have no possibility to get these fields anymore, because I do not know if they are compressed and cannot do the decompression myself. For me, this data is simply lost.\n\nI think Solr will have the same problem.",
            "date": "2009-10-08T08:46:40.227+0000",
            "id": 9
        },
        {
            "author": "Michael Busch",
            "body": "Adds the compress bit back to FieldsWriter and the uncompress code to FieldsReader.\n\nUwe, could you review the patch?",
            "date": "2009-10-08T22:06:59.818+0000",
            "id": 10
        },
        {
            "author": "Uwe Schindler",
            "body": "I will look into this tomorrow morning. I am to tired now, have to go to bed.\n\nI will also check the implications of not having the FieldForMerge.",
            "date": "2009-10-08T22:19:34.425+0000",
            "id": 11
        },
        {
            "author": "Uwe Schindler",
            "body": "Sorry, I forgot this one, will check tomorrow with some old indexes using compressed fields.",
            "date": "2009-10-11T21:17:24.352+0000",
            "id": 12
        },
        {
            "author": "Michael Busch",
            "body": "I created an index with some compressed binary and String fields with 2.4 and verified that it gets decompressed correctly. The test fails currently on trunk (as expected) and passes with the latest patch.\n\nHowever, there's one issue here: the compressed field gets silently uncompressed during merge, *only* if in the less efficient merge mode that doesn't use FieldsReader#rawDocs() and FieldsWriter#addRawDocuments(). So now this doesn't sound like a great solution that we sometimes uncompress the fields automatically and sometimes don't. \n\nI think we have three options:\n1. Change FieldsWriter#addRawDocuments() to uncompress on-the-fly\n2. Revert the FieldForMerge changes too and never uncompress automatically during merge\n3. Make it possible for the user to uncompress fields with CompressionTools, no matter which UTF format the data was stored with\n\nI don't really want to do 1., because it will have a performance impact for all fields (you have to look at the field bits even in raw merge mode). With 2. we will have to keep most of the compress/uncompress code in Lucene until 4.0, we'll just not make it possible anymore to add Store.COMPRESS fields with 3.0 (that's already how trunk is). For 3. we'd have to add a deprecated isCompressed() method that the user can call.",
            "date": "2009-10-12T06:04:04.221+0000",
            "id": 13
        },
        {
            "author": "Michael Busch",
            "body": "How shall we proceed here? (see my previous comment)",
            "date": "2009-10-14T00:47:01.986+0000",
            "id": 14
        },
        {
            "author": "Uwe Schindler",
            "body": "I still prefer 1, but maybe it's not so good. Else I would implement 2 (even if we need FieldForMerge). Just remove the COMPRES flag that nobody can add any compressed fields anymore.\n\n3 is bad, because it needs you to change your code on the change between 2.9 and 3.0 if you had compressed fields. In 2.9 they were automatically uncompressed, in 3.0 not. This would make it impossible to replace the lucene jar (which is currently possible if you remove all deprecated calls in 2.9).",
            "date": "2009-10-22T21:24:35.597+0000",
            "id": 15
        },
        {
            "author": "Uwe Schindler",
            "body": "If we want to stay with the current patch, we place a warning that indexes can suddenly get bigger on merges. We note this in changes.txt. \n\nIf one wants to regenerate the index with the stored fields decompressed, he could simply use the IndexSplitter contrib module recently added. This command line tool uses addIndexes and therefore merges all segments into a new index. With option 1, they get decompressed.\n\nIf somebody wants real compressed fields again, he has to write code and reindex using CompressableStringTools.",
            "date": "2009-10-23T22:24:47.111+0000",
            "id": 16
        },
        {
            "author": "Michael Busch",
            "body": "I'm actually -1 for option 1). The whole implementation of addRawDocuments() would have to change, and the necessary changes would kind of defeat its purpose.\n\nIf we do 2) nobody will be able to use an index that has compressed fields in 4.0 anymore, and to convert it they have to manually reindex (which might not always be possible).\n\nOf course our policy says that 4.0 must not be able to read <3.0 indexes anymore, however normally users can take a 2.x index, optimize it with 3.x, and then 4.0 can read it without problems. This wouldn't be possible with 2).\n\n",
            "date": "2009-10-23T22:30:54.646+0000",
            "id": 17
        },
        {
            "author": "Uwe Schindler",
            "body": "And how about keeping the current lucene-1960-1.patch? It works for me as I exspected. The only problem is that we do not decompress the fields for sure on optimizing?",
            "date": "2009-10-23T22:35:31.463+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "Can't we detect that we're dealing w/ an older version segment and not use addRawDocuments when merging them (and uncompress when we merge)?",
            "date": "2009-10-23T22:37:15.313+0000",
            "id": 19
        },
        {
            "author": "Michael Busch",
            "body": "Right, because FieldsReader#rawDocs() does not decode the field bits, so it doesn't know which fields are compressed.\n\nIf we want to change that it would have a significant negative performance impact on *all* stored fields.",
            "date": "2009-10-23T22:38:50.909+0000",
            "id": 20
        },
        {
            "author": "Uwe Schindler",
            "body": "Good idea, from where take the version?\n\nOr better, we look into the FieldInfos of the segment-to-merge and look if there is the compressed flag set for one of the fields. If yes, do not use addRawDocuments. It there the possibility to see this flag also or'ed segment-wise (like a field is omitNors is per-segment)?",
            "date": "2009-10-23T22:42:37.775+0000",
            "id": 21
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nCan't we detect that we're dealing w/ an older version segment and not use addRawDocuments when merging them (and uncompress when we merge)?\n{quote}\n\nSo then any 2.x index (including 2.9) would not be merged in the optimized way with 3.x. I'm actually not even sure how much of a slowdown this is. Did you (or anyone else) ever measure that? ",
            "date": "2009-10-23T22:44:17.594+0000",
            "id": 22
        },
        {
            "author": "Uwe Schindler",
            "body": "But this is only one-time. As soon as it is optimized it is fast again. Because of that I said, one could use a tool to enforce optimization or the new IndexSplitter can also do the copy old to new index.",
            "date": "2009-10-23T22:46:35.273+0000",
            "id": 23
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nOr better, we look into the FieldInfos of the segment-to-merge and look if there is the compressed flag set for one of the fields.\n{quote}\n\nFor a second earlier I had the same idea - it would be the most convenient solution. BUT: bummer! no compressed flag in the fieldinfos...\n\nIt's a bit per stored field *instance*.",
            "date": "2009-10-23T22:47:08.591+0000",
            "id": 24
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nBut this is only one-time. As soon as it is optimized it is fast again. Because of that I said, one could use a tool to enforce optimization or the new IndexSplitter can also do the copy old to new index.\n{quote}\n\nThat's right, I'm just trying to make sure we all understand the consequences. Would be nice to know how much longer it takes though.\n\nIf everyone else is ok with this approach I can work on a patch. ",
            "date": "2009-10-23T22:56:01.434+0000",
            "id": 25
        },
        {
            "author": "Uwe Schindler",
            "body": "So the idea is to raise the version number of the stored fields file by one in 3.0. All new or merged segments get this version number? When merging, for all versions before the actual one we do not use addRawDocuments() when copying contents. The current lucene-1960-1.patch stays unchanged.",
            "date": "2009-10-23T23:02:43.643+0000",
            "id": 26
        },
        {
            "author": "Michael Busch",
            "body": "Yes, I believe this would work.",
            "date": "2009-10-23T23:11:30.007+0000",
            "id": 27
        },
        {
            "author": "Uwe Schindler",
            "body": "Then +1 from me!",
            "date": "2009-10-23T23:11:59.705+0000",
            "id": 28
        },
        {
            "author": "Uwe Schindler",
            "body": "I have some large indexes here from 2.9 with compressed XML documents in stored fields. I can compare the optimization time for Lucene 2.9 and Lucene 3.0 with your patch.",
            "date": "2009-10-23T23:30:20.028+0000",
            "id": 29
        },
        {
            "author": "Michael Busch",
            "body": "It was as easy as changing this method in FieldsReader:\n\n{code:java}\n  boolean canReadRawDocs() {\n    // Disable reading raw docs in 2.x format, because of the removal of compressed\n    // fields in 3.0. We don't want rawDocs() to decode field bits to figure out\n    // if a field was compressed, hence we enforce ordinary (non-raw) stored field merges\n    // for <3.0 indexes.\n    return format >= FieldsWriter.FORMAT_LUCENE_3_0_NO_COMPRESSED_FIELDS;\n  }\n{code}\n\nUwe, I made some quick tests and it looks good. But I don't have any indexes with compressed fields (we don't use them), so I'll wait for you to test it out with your indexes that you mentioned.",
            "date": "2009-10-24T01:35:30.585+0000",
            "id": 30
        },
        {
            "author": "Uwe Schindler",
            "body": "Attached is my comparison of an unoptimzed Lucene 2.9 index optimized with 2.9 and optimized with 3.0 with the latest patch. The index was about 5.7 GB big and contained 4 compressed stored fields per document (in addition to ther fields uncompressed) containing XML documents.\n\nAfter optimization with 3.0, the size doubled (which is because of the very good compression of XML documents). The optimization took about double time with 3.0, because the fields were decompressed and no addRawDocuments was used.\n\nTo confirm, that everything worked normal after this initial optimization, I updated 38 documents in both indexes and optimized again. The optimization time of 2.9 was identical, 3.0 took a little bit longer, which is because the uncompressed field created more copy i/o, which you see in the \"time\" output as system time. User time during the initial 2.9 -> 3.0 optimization was much larger.\n\nThe attached document contains all numbers together with the checkindex outputs before and after each step.",
            "date": "2009-10-24T17:11:08.780+0000",
            "id": 31
        },
        {
            "author": "Uwe Schindler",
            "body": "I forgot the infos about the used system:\nSun X4600 Server, Solaris 10 x64, 16 Cores, 32 GB RAM, 64 bit JVM 1.5.0_21, -Xmx1512M, RAID 5",
            "date": "2009-10-24T17:46:27.228+0000",
            "id": 32
        },
        {
            "author": "Uwe Schindler",
            "body": "I do not know if this is a bug in 2.9.0, but it seems that segments with all documents deleted are not automatically removed:\n\n{noformat}\n4 of 14: name=_dlo docCount=5\n  compound=true\n  hasProx=true\n  numFiles=2\n  size (MB)=0.059\n  diagnostics = {java.version=1.5.0_21, lucene.version=2.9.0 817268P - 2009-09-21 10:25:09, os=SunOS,\n     os.arch=amd64, java.vendor=Sun Microsystems Inc., os.version=5.10, source=flush}\n  has deletions [delFileName=_dlo_1.del]\n  test: open reader.........OK [5 deleted docs]\n  test: fields..............OK [136 fields]\n  test: field norms.........OK [136 fields]\n  test: terms, freq, prox...OK [1698 terms; 4236 terms/docs pairs; 0 tokens]\n  test: stored fields.......OK [0 total field count; avg ? fields per doc]\n  test: term vectors........OK [0 total vector count; avg ? term/freq vector fields per doc]\n{noformat}\n\nShouldn't such segments not be removed automatically during the next *commit*/close of IndexWriter?\n\nBut this would be another issue. In my opinion, we are fine with the current approach, the longer optimization time is rectified by the larger index size because of no compression anymore and the more heavyer initial merge without addRawDocument is only 30% slower (one time!).\n\n+1 for committing",
            "date": "2009-10-26T08:56:29.626+0000",
            "id": 33
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I do not know if this is a bug in 2.9.0, but it seems that segments with all documents deleted are not automatically removed\n\nLucene doesn't actually short-circuit this case, ie, if every single doc in a given segment has been deleted, it will still merge it [away] like normal, rather than simply dropping it immediately from the index, which I agree would be a simple optimization.  Can you open a new issue?  I would think IW can drop such a segment immediately (ie not wait for a merge or optimize) on flushing new deletes.",
            "date": "2009-10-26T09:57:44.969+0000",
            "id": 34
        },
        {
            "author": "Uwe Schindler",
            "body": "Will do! -> LUCENE-2010",
            "date": "2009-10-26T10:07:55.847+0000",
            "id": 35
        },
        {
            "author": "Uwe Schindler",
            "body": "For this case: Should we add some testcase in TestBackwardsCompatibility, that tests, that pre 3.0 indexes with compressed fields are correctly uncompressed on optimize and also can be correctly read?\n\nI do not know how to do this and if the current BW test indexes in the zip files contain compressed fields.",
            "date": "2009-10-26T10:19:14.520+0000",
            "id": 36
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Should we add some testcase in TestBackwardsCompatibility\n\n+1, that'd be great\n\nWhat I usually do is make a mod that creates compressed fields, in the prior release (2.9.x), then uncomment the two methods that create new back compat indexes, zip them up, carry them forward to trunk, and modify the trunk test to test them.  Best to also carry forward the code that generated the back compat index, though in this since COMPRESS is removed, you'll have to comment it out w/ a comment stating \"this was used in 2.9 to create field XXX\".",
            "date": "2009-10-26T10:42:18.988+0000",
            "id": 37
        },
        {
            "author": "Uwe Schindler",
            "body": "I am working on it. I already patched the 2.9 version and created the test index.\nIn 3.0 I use if dirName.startsWith(\"29.\") to only do the optimize tests for this index and no other version.",
            "date": "2009-10-26T10:51:08.189+0000",
            "id": 38
        },
        {
            "author": "Uwe Schindler",
            "body": "Modified patch with testcase for backwards compatibility. It was a little bit trick to check if a field was actually compressed in the source index, but it worked with FieldSelectorResult.SIZE and a test-compression/chars*2. The test was done before/after optimize and it is verified that before the field was still compressed (even it is larger in reality, harhar) and uncompressed after optimize.\n\nShould I commit the index creation to the current 2.9 branch or not?\n\nI can commit this, if everybody is happy (because I have the zip files already in my checkout added). Or will you do it, Michael?",
            "date": "2009-10-26T12:15:33.457+0000",
            "id": 39
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Should I commit the index creation to the current 2.9 branch or not?\n\n+1, and ideally also, commented out, in trunk",
            "date": "2009-10-26T12:27:09.469+0000",
            "id": 40
        },
        {
            "author": "Uwe Schindler",
            "body": "OK. I will post new test indexes and a new patch, because I want to also test binary stored fields. I will do it by creating a binary/string field for id%2==0 or 1.\n\nThe patch for the index creating must be committed to 2.9 branch, not the backwards tests (because COMPRESS is undefined there, too!)",
            "date": "2009-10-26T12:54:10.078+0000",
            "id": 41
        },
        {
            "author": "Uwe Schindler",
            "body": "Here updated patch and ZIP index files.\n\nNow also binary fields are tested in the same way. Even document ids get a string compressed field, odd document ids a binary one.\n\nAlso attached is the patch for the 2.9 branch (not BW branch!!!). In trunk, the index creation is commented out, it's just there for reference.",
            "date": "2009-10-26T13:12:33.320+0000",
            "id": 42
        },
        {
            "author": "Uwe Schindler",
            "body": "Small optimization.",
            "date": "2009-10-26T14:18:00.716+0000",
            "id": 43
        },
        {
            "author": "Uwe Schindler",
            "body": "An additional check, that the raw merge mode is disabled for all segments in 2.9 index and enabled after optimizing with 3.0.",
            "date": "2009-10-26T14:44:32.827+0000",
            "id": 44
        },
        {
            "author": "Uwe Schindler",
            "body": "After thinking a little bit about it:\n\nIs it ok to test the size of the compressed field by recompressing it with another target VM? E.g., maybe I created the test 2.9 index with another Java Version (1.5.0_21)  where the deflate function is a little bit different implemented and so the test in 3.0 will fail, because maybe someone with Java 6 ran the test using another libzip?\n\nIn this case, I would add another stored field in the test index, that contains the length of the compressed data during creation of the index in the source VM, to be checked with FieldSelectorResult.SIZE?\n\nOpinions?",
            "date": "2009-10-26T15:27:17.738+0000",
            "id": 45
        },
        {
            "author": "Uwe Schindler",
            "body": "Attached the new indexes and patch that stores the compressed size together with the compressed value as stored field in the 2.9 index. This is now secure and invariant on different in-JDK compression implementations.",
            "date": "2009-10-26T15:53:36.984+0000",
            "id": 46
        },
        {
            "author": "Uwe Schindler",
            "body": "Add an assertion in FieldsReader that checks, that 3.0 indexes have no compressed fields. Also a small test cleanup.\n\nIts ready to commit now!",
            "date": "2009-10-26T17:54:52.641+0000",
            "id": 47
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI can commit this, if everybody is happy (because I have the zip files already in my checkout added). Or will you do it, Michael?\n{quote}\n\nGo ahead!\n\nCool that you added the bw-test. I've been wanting to do that too, but I didn't have time yet. Thank you!",
            "date": "2009-10-26T18:26:41.051+0000",
            "id": 48
        },
        {
            "author": "Uwe Schindler",
            "body": "OK, will do it a later in the evening (MEZ), have no time now. I will also add a good changes.txt entry in behaviour change or like that.",
            "date": "2009-10-26T18:33:55.663+0000",
            "id": 49
        },
        {
            "author": "Uwe Schindler",
            "body": "Committed revision 829972. Thanks also to Michael Busch!",
            "date": "2009-10-26T21:17:45.538+0000",
            "id": 50
        }
    ],
    "component": "",
    "description": "Also remove FieldForMerge and related code.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1960",
    "issuetypeClassified": "OTHER",
    "issuetypeTracker": "OTHER",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Remove deprecated Field.Store.COMPRESS",
    "systemSpecification": true,
    "version": ""
}