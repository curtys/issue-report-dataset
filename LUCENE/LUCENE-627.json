{
    "comments": [
        {
            "author": "Mark Harwood",
            "body": "Works OK for me - just added this to the Highlighter's Junit test and all is well.\nSomething up with your analyzer?\n\n\n\tpublic void testOverlapAnalyzer2() throws Exception\n\t{\n\t\tHashMap synonyms = new HashMap();\n\t\tsynonyms.put(\"ipod\", \"i,pod\");\n\t\tAnalyzer analyzer = new SynonymAnalyzer(synonyms);\n\t\tString srchkey = \"ipod\";\n\n\t\tString s = \"I want an ipod\";\n\t\tQueryParser parser=new QueryParser(\"text\",new WhitespaceAnalyzer());\n\t\tQuery query = parser.parse(srchkey);\n\n\t\tHighlighter highlighter = new Highlighter(new QueryScorer(query));\n\t\tTokenStream tokenStream =\n\t\t\tanalyzer.tokenStream(null, new StringReader(s));\n\t\t// Get 3 best fragments and seperate with a \"...\"\n\t\tString result = highlighter.getBestFragments(tokenStream, s, 3, \"...\");\n\t\tString expectedResult=\"I want an <B>ipod</B>\";\n\t\tassertEquals(expectedResult,result);\n\t}",
            "date": "2006-07-14T02:33:26.000+0000",
            "id": 0
        },
        {
            "author": "Yonik Seeley",
            "body": "The start and end offsets also overlap... I wonder if that's what is different.\n\nSolr's analyzer output:\nhttp://localhost:8983/solr/admin/analysis.jsp?name=text&verbose=on&val=iPod\n\nterm position 1, 2\nterm text \ti, pod/ipod\nterm type    word, word/word\nsource start,end (0,1),\t(1,4)/(0,4)\n\nI'll try and come up with a junit test that demonstrates it.",
            "date": "2006-07-14T02:42:57.000+0000",
            "id": 1
        },
        {
            "author": "Yonik Seeley",
            "body": "OK, here we go... the following test fails with\nExpected :ipod <B>foo</B>\nActual  :iiPod <B>foo</B>\n\n\n  public void testOverlapAnalyzer2() throws Exception\n  {\n\n    String s = \"iPod foo\";\n    // the token stream for the string above:\n    TokenStream ts = new TokenStream() {\n      Iterator iter;\n      {\n        List lst = new ArrayList();\n        Token t;\n        t = new Token(\"i\",0,1);\n        lst.add(t);\n        t = new Token(\"pod\",1,4);\n        t.setPositionIncrement(0);\n        lst.add(t);\n        t = new Token(\"ipod\",0,4);\n        lst.add(t);\n        t = new Token(\"foo\",5,8);\n        lst.add(t);\n        iter = lst.iterator();\n      }\n      public Token next() throws IOException {\n        return iter.hasNext() ? (Token)iter.next() : null;\n      }\n    };\n\n    String srchkey = \"foo\";\n\n    QueryParser parser=new QueryParser(\"text\",new WhitespaceAnalyzer());\n    Query query = parser.parse(srchkey);\n\n    Highlighter highlighter = new Highlighter(new QueryScorer(query));\n\n// Get 3 best fragments and seperate with a \"...\"\n    String result = highlighter.getBestFragments(ts, s, 3, \"...\");\n    String expectedResult=\"ipod <B>foo</B>\";\n    assertEquals(expectedResult,result);\n  }",
            "date": "2006-07-14T03:05:29.000+0000",
            "id": 2
        },
        {
            "author": "Mark Harwood",
            "body": "The problem appears to be because the \"pod\" token advances the start position to 1 while the next token \"ipod\" takes a step back (to 0)\n\nI've found if you just arrange the tokens to be emitted in start pos order all is fine - see below\n\n\n\t  public void testOverlapAnalyzer2() throws Exception\n\t  {\n\n\t    String s = \"iPod foo\";\n\t    // the token stream for the string above:\n\t    TokenStream ts = new TokenStream() {\n\t      Iterator iter;\n\t      {\n\t        List lst = new ArrayList();\n\t        Token t;\n                             //moved this token to start\n\t        t = new Token(\"ipod\",0,4);\n\t        lst.add(t);\n\t        t = new Token(\"i\",0,1);\n\t        lst.add(t);\n\t        t = new Token(\"pod\",1,4);\n\t        t.setPositionIncrement(0);\n\t        lst.add(t);\n\t        t = new Token(\"foo\",5,8);\n\t        lst.add(t);\n\t        iter = lst.iterator();\n\t      }\n\t      public Token next() throws IOException {\n\t        return iter.hasNext() ? (Token)iter.next() : null;\n\t      }\n\t    };\n\n\t    String srchkey = \"foo\";\n\n\t    QueryParser parser=new QueryParser(\"text\",new WhitespaceAnalyzer());\n\t    Query query = parser.parse(srchkey);\n\n\t    Highlighter highlighter = new Highlighter(new QueryScorer(query));\n\n//\t Get 3 best fragments and seperate with a \"...\"\n\t    String result = highlighter.getBestFragments(ts, s, 3, \"...\");\n                         //had to upper case the P in the test here\n\t    String expectedResult=\"iPod <B>foo</B>\";\n\t    assertEquals(expectedResult,result);\n\t  }\n",
            "date": "2006-07-14T03:22:02.000+0000",
            "id": 3
        },
        {
            "author": "Mark Harwood",
            "body": "Position increments wrong in my  above code but the solution of sequencing start offsets correctly still holds true:\n\n\t        t = new Token(\"ipod\",0,4);\n\t        t.setPositionIncrement(0);\n\t        lst.add(t);\n\t        t = new Token(\"i\",0,1);\n\t        t.setPositionIncrement(0);\n\t        lst.add(t);\n\t        t = new Token(\"pod\",1,4);\n\t        lst.add(t);\n\t        t = new Token(\"foo\",5,8);\n\t        lst.add(t);\n",
            "date": "2006-07-14T03:26:01.000+0000",
            "id": 4
        },
        {
            "author": "Yonik Seeley",
            "body": "The original token stream is a valid one though right?  If so, it seems like the fix should be in the highlighter module.\n",
            "date": "2006-07-14T03:31:22.000+0000",
            "id": 5
        },
        {
            "author": "Mark Harwood",
            "body": ">>The original token stream is a valid one though right? \n\nI don't think so, see below...\n\n       List lst = new ArrayList(); \n        Token t; \n        t = new Token(\"i\",0,1); \n        lst.add(t); \n        t = new Token(\"pod\",1,4); \n        t.setPositionIncrement(0); \n        lst.add(t); \n        t = new Token(\"ipod\",0,4); \n!! Missing a t.setPositionIncrement(0) here.\n        lst.add(t); \n        t = new Token(\"foo\",5,8); \n        lst.add(t); \n        iter = lst.iterator(); \n\nHaving fixed the above I believe this change below is all that is required to fix the highlighter:\n\nTokenGroup.java\n\n\tboolean isDistinct(Token token)\n\t{\n//\t\treturn token.startOffset()>=endOffset;\n\t\treturn token.getPositionIncrement()>0;\n\t}\n\nAll my Junit  tests pass with this change - can you verify this is true for you too?\nThis change would break highlighting for any analyzers that had position increments but whose offsets somehow overlapped - text would potentially be duplicated in the same way you originally reported your problem. I can't verify this to be true for CJK analyzers etc so feel a little uneasy about committing this. \n\nCheers\nMark",
            "date": "2006-07-14T05:10:04.000+0000",
            "id": 6
        },
        {
            "author": "Yonik Seeley",
            "body": ">>The original token stream is a valid one though right?\n> I don't think so, see below...\n\nAh, right... I constructed the wrong one first.  I wanted pod and ipod in the same position... so the token stream looks like \"i\" (\"pod\"|\"ipod\") \"foo\".\nNow this token-stream is correct, I believe, but the same problem happens.\n\nA work-around is to swap the order that \"pod\" and \"ipod\" tokens appear, but it seems like any such workaround should be put into the highlighter rather than external to it.\n\n\n  public void testOverlapAnalyzer2() throws Exception\n  {\n\n    String s = \"iPod foo\";\n    // the token stream for the string above:\n    TokenStream ts = new TokenStream() {\n      Iterator iter;\n      {\n        List lst = new ArrayList();\n        Token t;\n        t = new Token(\"i\",0,1);\n        lst.add(t);\n        t = new Token(\"pod\",1,4);\n        lst.add(t);\n        t = new Token(\"ipod\",0,4);\n        t.setPositionIncrement(0);   // pod and ipod occupy the same token position.\n        lst.add(t);\n        t = new Token(\"foo\",5,8);\n        lst.add(t);\n        iter = lst.iterator();\n      }\n      public Token next() throws IOException {\n        return iter.hasNext() ? (Token)iter.next() : null;\n      }\n    };\n\n    String srchkey = \"foo\";\n\n    QueryParser parser=new QueryParser(\"text\",new WhitespaceAnalyzer());\n    Query query = parser.parse(srchkey);\n\n    Highlighter highlighter = new Highlighter(new QueryScorer(query));\n\n// Get 3 best fragments and seperate with a \"...\"\n    String result = highlighter.getBestFragments(ts, s, 3, \"...\");\n    String expectedResult=\"iPod <B>foo</B>\";\n    assertEquals(expectedResult,result);\n  }",
            "date": "2006-07-14T05:25:06.000+0000",
            "id": 7
        },
        {
            "author": "Yonik Seeley",
            "body": "I just tried the change to isDistinct, and it didn't work for the corrected tokenstream I posted.\nI agree that it doesn't seem like the right fix anyway.   It seems like things should be based on startOffset and endOffset.",
            "date": "2006-07-14T07:40:51.000+0000",
            "id": 8
        },
        {
            "author": "Yonik Seeley",
            "body": "Here's another fun one :-)\n\njunit.framework.ComparisonFailure: \nExpected :zooPod<B>Foo</B>\nActual  :zoozooPod<B>zooPodFoo</B>\n\n\n  public void testOverlapAnalyzer3() throws Exception\n  {\n\n    String s = \"zooPodFoo\";\n    // the token stream for the string above:\n    TokenStream ts = new TokenStream() {\n      Iterator iter;\n      {\n        List lst = new ArrayList();\n        Token t;\n        t = new Token(\"zoo\",0,3);\n        lst.add(t);\n        t = new Token(\"pod\",3,6);\n        lst.add(t);\n        t = new Token(\"zoopod\",0,6);\n        t.setPositionIncrement(0);\n        lst.add(t);\n        t = new Token(\"foo\",6,9);\n        lst.add(t);\n        t = new Token(\"zoopodfoo\",0,9);\n        t.setPositionIncrement(0);        \n        lst.add(t);\n        iter = lst.iterator();\n      }\n      public Token next() throws IOException {\n        return iter.hasNext() ? (Token)iter.next() : null;\n      }\n    };\n\n    String srchkey = \"foo\";\n\n    QueryParser parser=new QueryParser(\"text\",new WhitespaceAnalyzer());\n    Query query = parser.parse(srchkey);\n\n    Highlighter highlighter = new Highlighter(new QueryScorer(query));\n\n// Get 3 best fragments and seperate with a \"...\"\n    String result = highlighter.getBestFragments(ts, s, 3, \"...\");\n    String expectedResult=\"zooPod<B>Foo</B>\";\n    assertEquals(expectedResult,result);\n  }\n\n",
            "date": "2006-07-14T07:58:17.000+0000",
            "id": 9
        },
        {
            "author": "Yonik Seeley",
            "body": "It seems like maybe the only way to handle some of this stuff is two passes... one to collect the scores & offsets of matches, and then a second pass to generate the fragments.",
            "date": "2006-07-14T09:04:05.000+0000",
            "id": 10
        },
        {
            "author": "Mark Harwood",
            "body": ">>It seems like maybe the only way to handle some of this stuff is two passes\n\nThe highlighter does not expect token positions to \"rewind\" in this manner. I'm not sure where this ends. Imagine an analyzer, which having considered and emitted tokens for a whole document, chooses to append some  tokens positioned which  has offsets referencing much earlier sections of the document. (Why, I'm not sure but there's nothing to say this couldn't happen).\n\n>>It seems like maybe the only way to handle some of this stuff is two passes\n\nMaybe a special \"OrderFixer\" TokenStream could be used by to wrap \"rewinding\" token streams such as yours and then accumulate all tokens in a  buffer before then sorting and outputting them in ascending start offset order. If the Highlighter ignored position increment and just used offsets (as it does currently) I suspect all would be OK\n\n",
            "date": "2006-07-14T13:53:51.000+0000",
            "id": 11
        },
        {
            "author": "Yonik Seeley",
            "body": "Right... I'm not sure the highlighter should be expected to handle all cases, but WordDelimiterFilter doesn't seem *that* complex or atypical.\n\n\"a-b-c\" would be indexed as \"a\",\"b\",\"c\"/\"abc\"\n(c and abc occupy the same token position)\n\nAnother thing I ran across is the addition of non-scoring tokens to a TokenGroup... this ends up highlighting the widest token in the token group, rather than the widest that matched.  I was able to get around this by checking that score>0 in TokenGroup, but was this indended?\n",
            "date": "2006-07-14T14:51:19.000+0000",
            "id": 12
        },
        {
            "author": "Yonik Seeley",
            "body": "I'm going with the OrderFixer approach for now... if startOffsets are equal, but endOffsets are not, does it matter which comes first?",
            "date": "2006-07-14T16:58:06.000+0000",
            "id": 13
        },
        {
            "author": "Yonik Seeley",
            "body": "Here is a patch that makes the tests work after tokens are re-ordered.\nI basically keep track of the start and end offsets of what matches, not just of the TokenGroup, so highlighting is more specific.\n\nSorry about the messy patch - my IDE collapses the tabs so I don't even see them (I only realize there are tabs after I do a diff).",
            "date": "2006-07-14T20:41:32.000+0000",
            "id": 14
        },
        {
            "author": "Yonik Seeley",
            "body": "So Mark, does this patch look OK?\nWithout it, even if I order the tokens by startOffset, I get things like\nHiHi-Speed <em>USB</em>\n\nWordDelimiterFilter (that's what is producing these types of tokens) is widely used in Solr-land, so I'm eager to get this fixed.\n",
            "date": "2006-07-15T19:40:18.000+0000",
            "id": 15
        },
        {
            "author": "Mark Harwood",
            "body": "Hi Yonik,\nBeen away for a little while and just managed to catch up with your changes. Thanks for this.\n\nLooking at the patch all seems good. I would just change a couple of things:\n\n1) TokenGroup.getTotalScore should just return your new \"tot\" variable\n2) TokenGroup.clear() needs to reinitialize \"tot\" to zero.\n\n\nOther than that all looks good to me. Let me know if you're happy and I'll commit it.\n\nThanks again",
            "date": "2006-07-15T20:52:04.000+0000",
            "id": 16
        },
        {
            "author": "Yonik Seeley",
            "body": "> 1) TokenGroup.getTotalScore should just return your new \"tot\" variable\n\nOK, I had considered that change before, but because all the vars were package protected rather than private, I wasn't sure if it was safe.\n\n> 2) TokenGroup.clear() needs to reinitialize \"tot\" to zero.\n\nOops... that one I missed.\n\n> Other than that all looks good to me. Let me know if you're happy and I'll commit it.\n\nLooks fine to me... please do!\n",
            "date": "2006-07-15T21:14:26.000+0000",
            "id": 17
        },
        {
            "author": "Yonik Seeley",
            "body": "Closing... works fine for me now.\nThanks!",
            "date": "2006-07-16T03:19:47.000+0000",
            "id": 18
        },
        {
            "author": "Kerang Lv",
            "body": "Hi Yonik, \nI'm trying to add support for some overlapping bigram analyzer, e.g. the CJKAnalyzer(http://svn.apache.org/repos/asf/lucene/java/trunk/contrib/analyzers/src/java/org/apache/lucene/analysis/cjk/) onto your patch.\n\nWith your patch, the following test fails with:\nExpected :?<B>??</B>??<B>??</B>???\nActual :?<B>??????</B>\n\npublic void testOverlapAnalyzer4() throws Exception\n{\n    String s = \"??????????\";\n    // the token stream for the string above:\n    TokenStream ts = new TokenStream() {\n      Iterator iter;\n      {\n        List lst = new ArrayList();\n        Token t;\n        t = new Token(\"??\",0,2);\n        lst.add(t);\n        t = new Token(\"??\",1,3);\n        lst.add(t);\n        t = new Token(\"??\",2,4);\n        lst.add(t);\n        t = new Token(\"??\",3,5);\n        lst.add(t);\n        t = new Token(\"??\",4,6);\n        lst.add(t);\n        t = new Token(\"??\",5,7);\n        lst.add(t);\n        t = new Token(\"??\",6,8);\n        lst.add(t);\n        t = new Token(\"??\",7,9);\n        lst.add(t);\n        t = new Token(\"??\",8,10);\n        lst.add(t);\n        iter = lst.iterator();\n      }\n      public Token next() throws IOException {\n        return iter.hasNext() ? (Token)iter.next() : null;\n      }\n    };\n\n    String srchkey = \"?? ??\";\n\n    QueryParser parser=new QueryParser(\"text\",new WhitespaceAnalyzer());\n    Query query = parser.parse(srchkey);\n\n    Highlighter highlighter = new Highlighter(new QueryScorer(query));\n\n    // Get 3 best fragments and seperate with a \"...\"\n    String result = highlighter.getBestFragments(ts, s, 3, \"...\");\n    String expectedResult=\"?<B>??</B>??<B>??</B>???\";\n    assertEquals(expectedResult,result);\n} \n\nWith some overlapping bigram analyzer, the current token's startOffset is the previous token's endOffset - 1, so the TokenGroup.isDistinct(token) returns false the most time, which lead to bad range tokenText.\n\nHere is a patch that makes the tests work.",
            "date": "2006-09-12T08:12:52.000+0000",
            "id": 19
        },
        {
            "author": "Mark Harwood",
            "body": "Hi Kerang, can you supply a patch/tests against the latest version?\n\nI committed a change to highlighter on the 16th August with what I beleived to be a fix for  this problem:   http://issues.apache.org/jira/browse/LUCENE-645\n\nLooking at the patch you provided it seems to be old code which is missing support for this fix.\n\nCheers\nMark",
            "date": "2006-09-12T21:57:00.000+0000",
            "id": 20
        },
        {
            "author": "Kerang Lv",
            "body": "Hi Mark, sorry for the long time missing!\n\nHere is the test, it fails again with the lastest version (Revision 450719):\n\nExpected :A<B>BC</B>DE<B>FG</B>HIJ\nActual:A<B>BCDEFG</B>HIJ\n\npublic void testOverlapAnalyzer4() throws Exception \n{ \n    String s = \"ABCDEFGHIJ\"; \n    // the token stream for the string above: \n    TokenStream ts = new TokenStream() { \n      Iterator iter; \n      { \n        List lst = new ArrayList(); \n        Token t; \n        t = new Token(\"AB\",0,2); \n        lst.add(t); \n        t = new Token(\"BC\",1,3); \n        lst.add(t); \n        t = new Token(\"CD\",2,4); \n        lst.add(t); \n        t = new Token(\"DE\",3,5); \n        lst.add(t); \n        t = new Token(\"EF\",4,6); \n        lst.add(t); \n        t = new Token(\"FG\",5,7); \n        lst.add(t); \n        t = new Token(\"GH\",6,8); \n        lst.add(t); \n        t = new Token(\"HI\",7,9); \n        lst.add(t); \n        t = new Token(\"IJ\",8,10); \n        lst.add(t); \n        iter = lst.iterator(); \n      } \n      public Token next() throws IOException { \n        return iter.hasNext() ? (Token)iter.next() : null; \n      } \n    }; \n\n    String srchkey = \"BC FG\"; \n\n    QueryParser parser=new QueryParser(\"text\",new WhitespaceAnalyzer()); \n    Query query = parser.parse(srchkey); \n\n    Highlighter highlighter = new Highlighter(new QueryScorer(query)); \n\n    // Get 3 best fragments and seperate with a \"...\" \n    String result = highlighter.getBestFragments(ts, s, 3, \"...\"); \n    String expectedResult=\"A<B>BC</B>DE<B>FG</B>HIJ\"; \n    assertEquals(expectedResult,result); \n} \n",
            "date": "2006-09-28T06:57:47.000+0000",
            "id": 21
        },
        {
            "author": "Mark Harwood",
            "body": "Thanks for the test Kerang.\n\nI no longer have a clear view as to what is expected behaviour here and whether this is a test that needs to pass.\n\nIt seems to conflict with the expected results for Yonik's test method \"testOverlapAnalyzer2\".\nIn that test, (like yours) for a cluster of overlapping tokens with search terms identified at the beginning and end, Yonik expects the whole cluster from search term 1's start offset to search term 2's end offset to be surrounded by one highlight tag. Your test expected 2 tags.\n\nWho is right?\n\nThis is a snippet from Yonik's test:\n    query = new QueryParser(\"text\",new WhitespaceAnalyzer()).parse(\"hi speed\");\n    highlighter = new Highlighter(new QueryScorer(query));\n    result = highlighter.getBestFragments(getTS2(), s, 3, \"...\");\n    assertEquals(\"<B>Hi-Speed</B>10 foo\",result);\n\nand yours:\n\n      String srchkey = \"BC FG\"; \n      String expectedResult=\"A<B>BC</B>DE<B>FG</B>HIJ\"; \n\nI don't really have an opinion either way so I'll turn it over to you\n\nCheers\nMark\n\n\n",
            "date": "2006-09-28T19:17:00.000+0000",
            "id": 22
        },
        {
            "author": "Yonik Seeley",
            "body": "I agree with Kerang about the expected behavior (of this specific case at least).\nThe test case of mine quoted above was not what I was shooting for, but was an acceptable unintended side-effect of fixing the other cases.\n\nSo I'm fine with this case being changed to\n    query = new QueryParser(\"text\",new WhitespaceAnalyzer()).parse(\"hi speed\");\n    highlighter = new Highlighter(new QueryScorer(query));\n    result = highlighter.getBestFragments(getTS2(), s, 3, \"...\");\n    assertEquals(\"<B>Hi</B>-<B>Speed</B>10 foo\",result);",
            "date": "2006-09-28T21:11:08.000+0000",
            "id": 23
        },
        {
            "author": "Chris Harris",
            "body": "I'm here two years after the last comment, trying to create a sense of closure for myself regarding the above conversation:\n\n * It appears that Mark Harwood committed a slightly modified version of Yonik's 2006-07-14 01:41 PM patch in r422031 and r422302.\n * It's not clear to me whether there was any actual code or test suite changes that eventually resulted from Mark, Yonik and Kerang's subsequent discussion about what, exactly, should be considered correct highlighting of multiple tokens.\n",
            "date": "2009-01-10T00:35:44.380+0000",
            "id": 24
        },
        {
            "author": "David Smiley",
            "body": "[~ryguasu] HighlighterTest was affected by r422302.\n\nI think that _either_ {{<B>Hi</B>-<B>Speed</B>10 foo}} OR {{<B>Hi-Speed</B>10 foo}} are acceptable results from a boolean query of \"hi\" and \"speed\".",
            "date": "2014-12-31T22:01:07.277+0000",
            "id": 25
        }
    ],
    "component": "core/other",
    "description": "The lucene highlighter has problems when tokens that overlap are generated.\n\nFor example, if analysis of iPod generates the tokens \"i\", \"pod\", \"ipod\" (with pod and ipod in the same position),\nthen the highlighter will output this as iipod, regardless of if any of those tokens are highlighted.\n\nDiscovered via http://issues.apache.org/jira/browse/SOLR-24\n",
    "hasPatch": false,
    "hasScreenshot": false,
    "id": "LUCENE-627",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "highlighter problems with overlapping tokens",
    "systemSpecification": true,
    "version": "2.1"
}