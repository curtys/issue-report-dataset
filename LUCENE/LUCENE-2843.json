{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "Attached patch.\n\nStill some nocommits but I think it's close... though I still need to\nget indexDivisor working for var gap.  Note that this patch changes\nthe index format, even for Standard codec (using fixed gap terms\nindex).\n\nA few tests fail because they assume Standard codec supports ord...\n\nTo properly test the alternatives we now have for the terms index\n(including fixed vs variable, and then different index term selectors\nfro the variable case), I created a new fun testing codec,\nMockRandomCodec.  It randomly pairs up a postings impl with a terms\nindex impl.  EG it sometimes uses an IndexTermPolicy that randomly\npicks index terms.  We may now be able to remove the other Mock*\ncodecs...\n",
            "date": "2011-01-03T11:09:52.047+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "As a first test, I just made a policy that's identical to the fixed\ngap terms index, ie, it just picks every 32nd term as the index term.\nSo this is really a test of the packed int/bytes vs FST.\n\nOn the 10M Wikipedia test index, the resulting terms index files (=\nRAM used by SegmentReader) is ~38% smaller (~52% once optimized -- FST\n\"scales up\" well).\n\nHere's the query perf vs trunk:\n\n||Query||QPS base||QPS vargap||Pct diff||||\n|spanFirst(unit, 5)|17.13|16.75|{color:red}-2.2%{color}|\n|\"unit state\"~3|5.31|5.20|{color:red}-2.1%{color}|\n|spanNear([unit, state], 10, true)|4.59|4.52|{color:red}-1.4%{color}|\n|\"unit state\"|7.86|7.77|{color:red}-1.1%{color}|\n|+nebraska +state|204.74|202.85|{color:red}-0.9%{color}|\n|+unit +state|11.37|11.30|{color:red}-0.6%{color}|\n|doctimesecnum:[10000 TO 60000]|9.74|9.76|{color:green}0.2%{color}|\n|unit~1.0|21.70|21.82|{color:green}0.6%{color}|\n|unit*|26.18|26.55|{color:green}1.4%{color}|\n|state|29.29|29.75|{color:green}1.6%{color}|\n|uni*|15.06|15.32|{color:green}1.7%{color}|\n|unit state|10.73|10.93|{color:green}1.9%{color}|\n|unit~2.0|21.05|21.45|{color:green}1.9%{color}|\n|un*d|77.10|79.65|{color:green}3.3%{color}|\n|u*d|26.41|28.81|{color:green}9.1%{color}|\n|united~1.0|102.27|116.88|{color:green}14.3%{color}|\n|united~2.0|25.47|31.18|{color:green}22.4%{color}|\n\nIt's great that for the seek intensive fuzzy queries, the FST-based\nseeking is substantially faster.  For other queries the term seek time\nis in the noise.\n\nI think we should make this (VariableGapTermsIndex) terms index impl\nthe default (for Standard codec).\n",
            "date": "2011-01-03T11:57:05.565+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "I like this idea, it would be interesting to see what other type of policies we can come up with.\n\nJust curious, how would the 'let FST decide' work? Do we have some quick way to know that \nselecting term X versus term Y would reduce the number of states/transitions? Isn't the resulting\nFST size also dependent upon the output value (terms dict file pointer)? And if we optimize\nthis locally (X versus Y) does it tend to hold globally?\n",
            "date": "2011-01-03T13:06:52.091+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Just curious, how would the 'let FST decide' work?\n\nThe FST builder is able to prune-as-it-builds.  EG it prunes a node if the number of unique terms going through it is less than N.  Alternatively, it prunes if the node just before had < N nodes coming through.  To do this we'd pass all terms to the builder, and specify the prune threshold.  So the FST would be \"bushy\"/deep when terms are a high density, and shallowish elsewhere.\n\n{quote}\nIsn't the resulting\nFST size also dependent upon the output value (terms dict file pointer)? And if we optimize\nthis locally (X versus Y) does it tend to hold globally?\n{quote}\n\nYes, very much so -- the more stuff you store in the output the bigger the FST.  But we only store the long file pointer into the main terms dict for this usage, and the FST is efficient (delta-codes the long values).  But, I'm not trying in anyway to minimize that net size (in bytes)...",
            "date": "2011-01-03T19:20:17.614+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "New patch, resolving all nocommits.  I think it's ready to commit.\n\nI cutover StandardCodec to the VariableGapTermsIndex, with 'every 32' as the index term selection policy.  We could lower 32 to eg 20, since FST uses so much less RAM than packed ints/bytes, but for now I'm just leaving it at 32 to be safe.\n\nThe \"let FST builder pick the indexed terms\" turns out not to be very easy to do w/ this API.  I put a big comment explaining this in the var gap writer.  It's also not clear we'd want to use this approach, since the resulting term index density can then vary substantially.",
            "date": "2011-01-04T11:35:41.688+0000",
            "id": 4
        },
        {
            "author": "Earwin Burrfoot",
            "body": "And we're nearing a day when we keep the whole term dictionary in memory (as Sphinx does for instance).\nAt that point a gazillion of term lookup-related hacks (like lookup cache) become obsolete :)\nTerm dictionary itself can also be memory-mapped after this, instead of being \"read\" and \"built\" from disk, which makes new segment opening near-instantaneous.",
            "date": "2011-01-09T08:57:08.203+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "In-memory terms dict would be great.  I agree it'd fundamentally change how we execute eg the automaton queries (suddenly we can just intersect against the terms dict instead of doing the seek/next thing); FuzzyQuery might be a direct search through the terms dict instead of first building the LevN DFA; respelling similarly...\n\nBut, I suspect we'll always have to support the \"on-disk only\" option because some apps seem to have an insane number of terms.",
            "date": "2011-01-09T11:07:15.326+0000",
            "id": 6
        },
        {
            "author": "Earwin Burrfoot",
            "body": "As I said, there's already a search server with strictly in-memory (in mmap sense. it can theoretically be paged out) terms dict AND widespread adoption. Their users somehow manage.\n\nMy guess is that's because people with \"insane number of terms\" store various crap like unique timestamps as terms. With CSF (\"attributes\" in Sphinx lingo), and some nice filters that can work over CSF, there's no longer any need to stuff your timestamps in the same place you stuff your texts. That can be reflected in documentation, and then, suddenly, we can drop \"on-disk only\" support.",
            "date": "2011-01-09T11:50:43.774+0000",
            "id": 7
        },
        {
            "author": "Robert Muir",
            "body": "bq. As I said, there's already a search server with strictly in-memory (in mmap sense. it can theoretically be paged out) terms dict AND widespread adoption. Their users somehow manage\n\nI don't like the reasoning that, just because sphinx does it and their 'users manage', that makes it ok.\nsphinx also requires mysql, which only when started supporting *real* utf-8?! (not that 3-byte crap they tried to pass off instead)\n\nI don't think we should really be looking there for inspiration.",
            "date": "2011-01-09T12:23:58.265+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "Yes doc values should cut back on these large term dicts.\n\nBut, I'm not a fan of pure disk-based terms dict.  Expecting the OS to make good decisions on what gets swapped out is risky -- Lucene is better informed than the OS on which data structures are worth spending RAM on (norms, terms index, field cache, del docs).\n\nIf indeed the terms dict (thanks to FSTs) becomes small enough to \"fit\" in RAM, then we should load it into RAM (and do away w/ the terms index).",
            "date": "2011-01-09T14:32:12.465+0000",
            "id": 9
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. Their users somehow manage. \n\nThat neglects to count those who are not users because they could not manage with the limitations ;-)\n\nAnyway, being able to optionally keep the term dict in memory, per-field, if it's below a certain limits (terms/memory or whatever) would be very cool!",
            "date": "2011-01-09T15:11:11.115+0000",
            "id": 10
        },
        {
            "author": "Earwin Burrfoot",
            "body": "bq. I don't like the reasoning that, just because sphinx does it and their 'users manage', that makes it ok.\nI'm in no way advocating it as an all-round better solution. It has it's wrinkles just as anything else.\nMy reasoning is merely that alternative exists, and it is viable. As proven by pretty high-profile users.\nThey have memory-resident term dictionary, and it works, I heard no complaints regarding this ever.\n\nbq. sphinx also requires mysql\nHave you read anything at all? It has an integration ready, for the layman user who just wants to stick a fulltext search into their little app, but it is in no way reliant on it.\nSphinx is a direct alternative to Solr.\n\n{quote}\nBut, I'm not a fan of pure disk-based terms dict. Expecting the OS to make good decisions on what gets swapped out is risky - Lucene is better informed than the OS on which data structures are worth spending RAM on (norms, terms index, field cache, del docs).\nIf indeed the terms dict (thanks to FSTs) becomes small enough to \"fit\" in RAM, then we should load it into RAM (and do away w/ the terms index).\n{quote}\nThat's a bit delusional. If a system is forced to swap out, it'll swap your explicitly managed RAM just as likely as memory-mapped files. I've seen this countless times.\nBut then, you have a number of benefits - like sharing filesystem cache when opening same file multiple times, offloading things from Java heap (which is almost always a good thing), fastest load-into-memory times possible.\n\n\nSorry, if I sound offending at times, but, damn, there's a whole world of simple and efficient code lying ahead in that direction :)",
            "date": "2011-01-09T15:26:13.239+0000",
            "id": 11
        },
        {
            "author": "Robert Muir",
            "body": "bq. Have you read anything at all?\n\nNope, havent looked at their code... i think i stopped at the documentation when i saw how they analyzed text!\n\nbq. Sorry, if I sound offending at times, but, damn, there's a whole world of simple and efficient code lying ahead in that direction\n\nSo where is the problem?\n\nYou can make your own all-on-disk impl, or all-in-ram impl and contribute it? And you dont have to implement terms dict cache,\nthats contained in the implementation?\n\nMy problem is that we shouldnt assume all users can fit all their terms in RAM.\n\nI think its great to offer alternative impls that work all in ram, and maybe if termsdict < X where X is some configurable value,\neven consider using these automatically in standardcodec... but i don't see any benefit of 'forcing' this when we have this\nwhole flexible indexing thing!\n",
            "date": "2011-01-09T15:57:32.076+0000",
            "id": 12
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. My reasoning is merely that alternative exists, and it is viable. As proven by pretty high-profile users.\n\nActually, I sort of agree.  I read the \"in memory\" too fast and didn't realize you were talking about memory mapped.\nThere are other parts of sphinx that are kept directly in memory (not memory mapped) and do limit it's single-node scalability too much IMO.\nUnfortunately, Java has additional overhead wrt mmap,  and you also can't do some stuff that you could do in C.  All this means is that trade-offs that made sense for C/C++ solutions may or may not make sense for Java solutions.",
            "date": "2011-01-09T15:58:58.760+0000",
            "id": 13
        },
        {
            "author": "Robert Muir",
            "body": "bq. Unfortunately, Java has additional overhead wrt mmap\n\nIts not just that, you cant assume mmap even \"works\" (32-bit platform, even some troubles on 64-bit windows).\nBecause this is a search engine library, not just a server on 64-bit linux only, then we need to support\nother situations like 32-bit users doing desktop search.\n\nIn other words, Test2BTerms in src/test should pass on my 32-bit windows machine with whatever we default to.",
            "date": "2011-01-09T16:20:27.085+0000",
            "id": 14
        },
        {
            "author": "Earwin Burrfoot",
            "body": "bq. Nope, havent looked at their code... i think i stopped at the documentation when i saw how they analyzed text!\nAll my points are contained within their documentation. No need to look at the code (it's as shady as Lucene's).\nIn the same manner, Lucene had crappy analyzis for years, until you've taken hold of (unicode) police baton.\nSo let's not allow color differences between our analyzers affect our judgement on other parts of ours : )\n\nbq. In other words, Test2BTerms in src/test should pass on my 32-bit windows machine with whatever we default to.\nI'm questioning is there any legal, adequate reason to have that much terms.\nI'm agreeing on mmap+32bit/mmap+windows point for reasonable amount of terms though :/\n\nA hybrid solution, with term-dict being loaded completely into memory (either via mmap, or into arrays) on per-field basis, is probably best in the end, however sad it may be.",
            "date": "2011-01-09T16:56:16.101+0000",
            "id": 15
        },
        {
            "author": "Robert Muir",
            "body": "bq. A hybrid solution, with term-dict being loaded completely into memory (either via mmap, or into arrays) on per-field basis, is probably best in the end, however sad it may be.\n\nWhats the sad part again? why does it bother you if there is another alternative codec setup or terms dict implementation if you aren't using it?\nShould we also only have RAMDirectory and MMapDirectory and its sad that we have NIOFSDirectory?\n",
            "date": "2011-01-09T17:13:40.373+0000",
            "id": 16
        },
        {
            "author": "Michael McCandless",
            "body": "bq. If a system is forced to swap out, it'll swap your explicitly managed RAM just as likely as memory-mapped files.\n\nIn fact, even if it's not under any real memory pressure the OS will swap out your not-recently-accessed RAM.  Net/net this is a good policy, if your metric is total throughput accomplished by all programs.\n\nBut if your metric is latency to search queries, this is an awful policy.\n\nFortunately OSs (at least Windows & Linux) give you some tunability here.  Unfortunately, the tunable is global and it defaults \"badly\" for those programs that do make a careful distinction b/w what data structures are best held in RAM and what data is best left on disk.\n\nIf I could I would offer an option to pin these pages, so the OS cannot swap them out, but I don't think we can do (easily) that from javaland (and I think you'd have to be root).  Lacking pinning the best (approximation) we can do is pull these ourselves into RAM.",
            "date": "2011-01-10T12:03:31.264+0000",
            "id": 17
        },
        {
            "author": "Toke Eskildsen",
            "body": "I see that the VariableGapTermsIndexReader/Writer is now the default (or at least an experimental default) in trunk. This means that ord() and consequently seek() are not available. Are you, Michael, planning on adding these later on or are they gone for good?\n\nIf they are gone for good, it does represent a bit of a problem for me as I use ord() and seek() for a memory-efficient hierarchical faceting system. Not having those in the default reader/writer means that most indexes \"out there\" will not support accessing terms by ordinals and that my code won't work on them unless they are re-build. Boo hoo for me, but not implementing the interface fully in the default implementation seems wrong. Or maybe the interface should be changed?",
            "date": "2011-02-02T15:05:44.366+0000",
            "id": 18
        },
        {
            "author": "Robert Muir",
            "body": "bq. Or maybe the interface should be changed?\n\n+1, ord is not an interface, its an implementation detail specific\nto only certain basic implementations that shouldn't be in TermsEnum.\n\ni would much prefer if this were some attribute, or somehow exposed\nvia those implementations' TermStates... as in my opinion its really \nactually an implementation detail of TermState, not even terms.\n",
            "date": "2011-02-02T15:15:29.205+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "Toke, the FixedGapTermsIndexWriter/Reader supports ord, but requires more RAM for the terms index and may cause some queries to run slower.\n\nCan you describe how your faceting system is using ord?",
            "date": "2011-02-02T15:26:39.181+0000",
            "id": 20
        },
        {
            "author": "Robert Muir",
            "body": "Also, if faceting wants to exploit a codec-specific implementation detail,\nits far more interesting to evaluate things like changing VariableGapIndex's \nFST output to be a pair including max(docFreq) for the block it indexes.\n\nThen, faceting that wants to get the top-10 terms by docfreq, would \ninstead work an FSTEnum, and only go to disk for the top-10 blocks... this\nwould actually be a change in complexity order no?\n",
            "date": "2011-02-02T15:37:36.970+0000",
            "id": 21
        },
        {
            "author": "Toke Eskildsen",
            "body": "Thank you. I will use the FixedGap-version myself, but that only works when I'm the one controlling the index build, right?\n\nAs for the faceting system then the principle really simple: Instead of holding terms (BytesRefs) in memory, I just hold their ordinals. As the terms themselves only need to be resolved when the final faceting result is to be returned, seeking for a few hundred or thousand terms by their ordinal has worked very well so far (no guarantees for old hardware such as spinning disks though).\n\nThe memory savings over holding BytesRefs in memory of course varies with term lengths. There are some numbers at https://sbdevel.wordpress.com/2010/10/11/hierarchical-faceting/ if someone finds it interesting and LUCENE-2369 has some measurements of the same principle applied to sorting.",
            "date": "2011-02-02T15:47:50.912+0000",
            "id": 22
        },
        {
            "author": "Toke Eskildsen",
            "body": "Robert, there is already OrdTermState to hold the ord, but the ordinal itself is only interesting if the corresponding term can be seeked from it. Upon further inspection I see that the method {code}TermsIndexReaderBase.supportsOrd(){code} is coupled logically to {code}seek(long ord){code} and {code}ord(){code} so support for ordinals does not seem like something one can expect.\n\nAs for the FSTEnum-idea then I don't understand how it can work with faceting where the terms to return are defined by the documents from a search? ...But maybe we should discuss that elsewhere.",
            "date": "2011-02-02T16:11:23.203+0000",
            "id": 23
        },
        {
            "author": "Robert Muir",
            "body": "bq. Robert, there is already OrdTermState to hold the ord, but the ordinal itself is only interesting if the corresponding term can be seeked from it. \n\nYou can seek to any arbitrary TermState (even if its not holding ord), but it might hold other things you don't care about.\n\nbq. As for the FSTEnum-idea then I don't understand how it can work with faceting where the terms to return are defined by the documents from a search? ...But maybe we should discuss that elsewhere.\n\nIn the general case, if you are using something like a priority queue to get the top-N terms (even if you are filtering by the documents from a search), this number would mean that once your priority queue is full, you can tell that an entire block of low freq terms is not-competitive to enter the PQ, without going to disk?\n",
            "date": "2011-02-02T16:18:58.609+0000",
            "id": 24
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Thank you. I will use the FixedGap-version myself, but that only works when I'm the one controlling the index build, right?\n\nRight, but, this is fair?  I mean, it's easy (in Lucene 4.0) to pick the appropriate codec per field.  So, if people want to use your faceting package, and you explain that it requires using a certain Codec, that seems OK?\n\n{quote}\nAs for the faceting system then the principle really simple: Instead of holding terms (BytesRefs) in memory, I just hold their ordinals. As the terms themselves only need to be resolved when the final faceting result is to be returned, seeking for a few hundred or thousand terms by their ordinal has worked very well so far (no guarantees for old hardware such as spinning disks though).\n{quote}\nOK that makes sense... impressive that seeking up to a few thousand terms is giving you good perf.  You could also load DocTermsIndex in FieldCache, but of course then all terms data & ords are RAM resident (and the point of LUCENE-2369 is to have low memory overhead).",
            "date": "2011-02-03T15:24:11.172+0000",
            "id": 25
        },
        {
            "author": "Toke Eskildsen",
            "body": "Michael, I think I'll use a TermsEnum-wrapper which keeps every N BytesRef or TermState and uses that for ordinal-based random access. That's a simple configurable RAM/speed tradeoff that is Codec-agnostic. Question is what the lower bound of the amount of methods implemented by a given Codec is? Is anything besides ord/seek optional?",
            "date": "2011-02-04T07:26:30.163+0000",
            "id": 26
        }
    ],
    "component": "core/index",
    "description": "PrefixCodedTermsReader/Writer (used by all \"real\" core codecs) already\nsupports pluggable terms index impls.\n\nThe only impl we have now is FixedGapTermsIndexReader/Writer, which\npicks every Nth (default 32) term and holds it in efficient packed\nint/byte arrays in RAM.  This is already an enormous improvement (RAM\nreduction, init time) over 3.x.\n\nThis patch adds another impl, VariableGapTermsIndexReader/Writer,\nwhich lets you specify an arbitrary IndexTermSelector to pick which\nterms are indexed, and then uses an FST to hold the indexed terms.\nThis is typically even more memory efficient than packed int/byte\narrays, though, it does not support ord() so it's not quite a fair\ncomparison.\n\nI had to relax the terms index plugin api for\nPrefixCodedTermsReader/Writer to not assume that the terms index impl\nsupports ord.\n\nI also did some cleanup of the FST/FSTEnum APIs and impls, and broke\nout separate seekCeil and seekFloor in FSTEnum.  Eg we need seekFloor\nwhen the FST is used as a terms index but seekCeil when it's holding\nall terms in the index (ie which SimpleText uses FSTs for).\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2843",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Add variable-gap terms index impl.",
    "systemSpecification": true,
    "version": ""
}