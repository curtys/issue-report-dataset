{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "This looks great!\n\nOne alternate approach here would be to create a WikipediaDocMaker\n(implementing DocMaker interface) that pulls directly from the XML\nfile and feeds documents into the alg.\n\nThen, to make a line file, one could create an alg that pulls docs\nfrom WikipediaDocMaker and uses WriteLineDoc task to create the\nline-by-line file.\n\nOne benefit of this approach is creating docs of a certain size (10\ntokens, 100 tokens, etc) would become a one-step process (single alg)\ninstead of what I think is a 2-step process now (make first line file,\nthen reprocess into second line file).  Another benefit would be you\ncould make wikipedia tasks that pull directly from the XML file and\nnot even use a line file as an intermediary.\n\nSteve do you think this would be a hard change?  I think it should be\neasy, except, I'm not sure how to do this w/ SAX since SAX is \"in\ncontrol\".  You sort of need coroutines.  Or maybe one thread is\nrunning SAX and putting doc data into a shared queue, and then the other\nthread (the normal \"main\" thread that benchmark runs) would pull from\nthis queue?\n",
            "date": "2007-08-01T15:21:43.217+0000",
            "id": 0
        },
        {
            "author": "Steven Parkes",
            "body": "I can look at what it would take to avoid the line file ... but ... what about the overhead of the XML parser? I don't tend to think of XML parsers as \"light\". Would bundling that into the test be a concern?\n\nI guess it's not an issue if you're just using this to create an index and then are going to do your performance measurements on the queries of the index. But for measuring index performance, I would probably be cautious of bundling in the XML processing (until proven insignificant).",
            "date": "2007-08-01T15:28:14.844+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "\n> I can look at what it would take to avoid the line file ... but\n> ... what about the overhead of the XML parser? I don't tend to think\n> of XML parsers as \"light\". Would bundling that into the test be a\n> concern?\n\nRight I too would not consider XML parsing overhead \"light\".  So tests\nthat are sensitive to the XML parsing cost should first create a line\nfile.\n\nBut, this is the case regardless of which approach we use (ie, both\napproaches allow you use a line file -- the WriteLineDocTask writes a\nline file from any DocMaker).  It's just that the new approach would\nbuy us more flexibility for those people who don't need (or want) to\nuse the line file as an intermediary.\n",
            "date": "2007-08-01T16:33:34.702+0000",
            "id": 2
        },
        {
            "author": "Doron Cohen",
            "body": "> But, this is the case regardless of which approach we use (ie, both\n> approaches allow you use a line file -- the WriteLineDocTask writes a\n> line file from any DocMaker).  It's just that the new approach would\n> buy us more flexibility for those people who don't need (or want) to\n> use the line file as an intermediary.\n\nSo there would now be two alternative ways to index wiki data:\n(1) using the proposed WikiDocMaker directly to feed AddDoc task.\n(2) using line file after first running WriteLineDocTask when the \ndoc maker was WikiDocMaker.\n\nI like this approach.\n\nThis means that WikiDocMaker would read the data straight from \ntemp/enwiki-20070527-pages-articles.xml. So the extract-enwiki \ntarget in build.xml would no longer be needed, right?\n\n",
            "date": "2007-08-01T18:59:30.365+0000",
            "id": 3
        },
        {
            "author": "Doron Cohen",
            "body": "Mmm... an additional advantage of this is not needing to extract \nthe entire enwiki collection in order to index it - setting the \nrepetition count to 100 for AddDocTask in alternative 1 or for \nWriteLineDocTask in alternative 2 would  mean that only 100 \ndocs from the huge file are extracted.",
            "date": "2007-08-01T19:05:01.210+0000",
            "id": 4
        },
        {
            "author": "Steven Parkes",
            "body": "Sounds good. New patch soon.",
            "date": "2007-08-06T20:27:35.718+0000",
            "id": 5
        },
        {
            "author": "Steven Parkes",
            "body": "Okay. Here's an update to the patch.\n\nChanges:\n\n1) EnwikiDocMaker replaces ExtractWikipedia\n\n2) A sample algorithm is provided (and used by the build.xml file, which could be removed if desired\n\n3) A bug in LineDocMaker is fixed (it was storing both the title and date in the title field (small enough that it doesn't need its own JIRA(?))\n\n4) LineDocMaker was made derivable-from\n\nMuch of the code in LineDocMaker is useful in EnwikiDocMaker so I made it so (it's inheritance for impl, not abstraction so it could be changed, of course)\n\n5) Made LineDocMaker and WriteLineDocTask multicharater safe\n\nOr at least I tried to. Wikipedia has non-ascii characters in it. To make LineDocMaker work as a base class, I made it use an explicit FileInputStream which is required so that SAX can extract the encoding correctly. I made WriteLineDocTask always write UTF-8 so that I can get non-ASCII in the output file. Seems like UTF-8 is the best encoding for line files? At the same time, I made LineDocMaker assume UTF-8 (unless told otherwise by a derived class like EnwikiDocMaker) so that the line files created by EnwikiDocMaker/WriteLineDocTask can be read by LineDocMaker w/o loss.",
            "date": "2007-08-07T00:53:29.482+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good; a few comments:\n\n  * In conf/wikipedia.alg:\n\n    - The comment says \"Reuters\" but should say \"Wikipedia\"\n\n    - It's only processing 1 doc?  I think you should change the \": 1\"\n      to \": *\"?\n \n    - Maybe rename this to conf/extractEnWikipedia.alg?\n\n  * When I tried to run this I hit OOM (on Linux).  Then I changed the\n    line in conf/wikipedia.alg to this:\n\n      {WriteLineDoc() > : *\n\n    And OOM went away and I was able to produce the full line file.\n    That change tells benchmark not to record PerfTask details.  So I\n    think we should make that change too.\n",
            "date": "2007-08-08T17:59:06.358+0000",
            "id": 7
        },
        {
            "author": "Steven Parkes",
            "body": "All agreed and fixed. Thanks.",
            "date": "2007-08-08T21:14:39.136+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "Super, new patch looks good.  I will commit!",
            "date": "2007-08-09T08:31:50.520+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "\nOK, committed with these small changes:\n\n  * Replaced conf/wikipedia.alg -> conf/extractWikipedia.alg in the\n    comment in that file.\n\n  * Moved doc.maker line up under the \"# Where to get documents from:\"\n    comment\n\n  * In build.xml, removed the extract-enwiki target so that \"ant\n    enwiki\" does the right thing.\n\nThanks Steve!\n",
            "date": "2007-08-09T08:58:35.486+0000",
            "id": 10
        }
    ],
    "component": "",
    "description": "Create a line per article rather than a file. Consume with indexLineFile task.",
    "hasPatch": false,
    "hasScreenshot": false,
    "id": "LUCENE-971",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Create enwiki indexable data as line-per-article rather than file-per-article",
    "systemSpecification": true,
    "version": ""
}