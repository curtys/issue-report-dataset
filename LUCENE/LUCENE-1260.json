{
    "comments": [
        {
            "author": "Karl Wettin",
            "body": "\n * Simlarity#getNormCodec()\n * Simlarity#setNormCodec(NormCodec)\n * Similarity$NormCodec\n * Similarity$DefaultNormCodec\n * Similarity$SimpleNormCodec (binsearches over a sorted float[])\n\nI also depricated Similarity#getNormsTable() and replaced the only use I could find of it - in TermScorer. Could not spont any problems with performance or anything with that.",
            "date": "2008-04-07T20:20:08.104+0000",
            "id": 0
        },
        {
            "author": "Karl Wettin",
            "body": "I suppose it would be possible to implement a NormCodec that would listen to encodeNorm(float) while one is creating a subset of the index in order to find all norm resolution sweetspots for that corpus using some appropriate algorithm. Mean shift?.\n\nPerhaps it even would be possible to compress it down to n bags from the start and then allow for it to grow in case new documents with other norm requirements are added to the store.\n\nI haven't thought too much about it yet, but it seems to me that norm codec has more to do with the physical store (Directory) than Similarity and should perhaps be moved there instead? I have no idea how, but I also want to move it to the instance scope so I can have multiple indices with unique norm span/resolutions created from the same classloader.",
            "date": "2008-04-07T23:00:57.009+0000",
            "id": 1
        },
        {
            "author": "Hoss Man",
            "body": "bq. I haven't thought too much about it yet, but it seems to me that norm codec has more to do with the physical store (Directory) than Similarity and should perhaps be moved there instead?\n\nAs long as the norm remains a fixed size (1 byte) then it doesn't really matter whether it's tied to Similarity's or the store itself -- it would be nice if the Index could tell you which normDecoder to use, but it's not any more unreasonable to expect the application to keep track of this (if it's not the default encoding) since applications already have to keep track of things like which Analyzer is \"compatible\" with querying this index.\n\nIf we want norms to be more flexible, so tat apps can pick not only the encoding but also the size... then things get more interesting, but it's still feasible to say \"if you customize this, you have to make your reading apps and your writing apps smart enough to know about your customization.\"\n\nbq. I also want to move it to the instance scope so I can have multiple indices with unique norm span/resolutions created from the same classloader.\n\nI agree, it's a good goal.\n",
            "date": "2008-04-08T20:45:21.860+0000",
            "id": 2
        },
        {
            "author": "Karl Wettin",
            "body": "{quote}\nAs long as the norm remains a fixed size (1 byte) then it doesn't really matter whether it's tied to Similarity's or the store itself - it would be nice if the Index could tell you which normDecoder to use, but it's not any more unreasonable to expect the application to keep track of this (if it's not the default encoding) since applications already have to keep track of things like which Analyzer is \"compatible\" with querying this index.\n\nIf we want norms to be more flexible, so tat apps can pick not only the encoding but also the size... then things get more interesting, but it's still feasible to say \"if you customize this, you have to make your reading apps and your writing apps smart enough to know about your customization.\"\n{quote}\n\nI like the idea of an index that is completely self aware of norm encoding, what payloads mean, et c. \n\n{quote}\nI also want to move it to the instance scope so I can have multiple indices with unique norm span/resolutions created from the same classloader.\n{quote}\n\nMy use case is really about document boost and not normalization. \n\nSo another solution to this is to introduce a (variable bit sized?) document boost file and completely separate it from the norms instead of as now where  normalization and document boost is baked up as the same thing. I think there would be no need to touch the norms encoding then, that the default resolution is good enough for /normalization/. It would fix several caveats with norms as I see it. \n\n",
            "date": "2008-04-09T18:24:29.444+0000",
            "id": 3
        },
        {
            "author": "Hoss Man",
            "body": "bq. My use case is really about document boost and not normalization.\n\nbq. So another solution to this is to introduce a (variable bit sized?) document boost file and completely separate it from the norms instead...\n\n1) \"norms\" is a vague term.  currently \"lengthNorm\" is folded in with \"field boosts\" and \"doc boosts\" to form a generic \"fieldNorm\" ... I assumed you were interested in a more general way to improve the resolution of \"fieldNorm\"\n\n2) your description of general purpose variable sized document boosting sounds exactly like LUCENE-1231 ... in the long run utilities using LUCENE-1231 (or something like it) to replace \"field boosts\" and \"length norms\" might make the most sense as a way to eliminate the current static Norm encoding and put more flexibility in the hands of users",
            "date": "2008-04-10T00:15:00.176+0000",
            "id": 4
        },
        {
            "author": "Karl Wettin",
            "body": "{quote}\n1) \"norms\" is a vague term. currently \"lengthNorm\" is folded in with \"field boosts\" and \"doc boosts\" to form a generic \"fieldNorm\" ... I assumed you were interested in a more general way to improve the resolution of \"fieldNorm\"\n{quote}\n\nI still am but mainly because it is the simplest and only way to get better document boost resolution at the moment.\n\n\n",
            "date": "2008-04-10T01:03:17.078+0000",
            "id": 5
        },
        {
            "author": "Karl Wettin",
            "body": "I notice there is a tyop in the patch. And there is no test case for SimpleNormCodec. I'll come up with that too.",
            "date": "2008-04-10T01:03:27.370+0000",
            "id": 6
        },
        {
            "author": "Karl Wettin",
            "body": "Fixed some typos and added some tests. Perhaps it needs new javadocs too?",
            "date": "2008-04-11T13:01:56.406+0000",
            "id": 7
        },
        {
            "author": "Karl Wettin",
            "body": "This is a retroactive ASL blessing of the patch posted 11/Apr/08 06:01 AM",
            "date": "2008-04-11T13:03:30.792+0000",
            "id": 8
        },
        {
            "author": "Karl Wettin",
            "body": "New patch additionally includes:\n\n * Lots of javadocs with warnings\n * Similarity#readNormCodec(Directory):NodeCodec\n * Similarity#writeNormCodec(Directory, NodeCode)\n",
            "date": "2008-04-24T23:32:12.101+0000",
            "id": 9
        },
        {
            "author": "Karl Wettin",
            "body": "I think I've takes this as far as it can without refactoring it out of the static scope.",
            "date": "2008-04-24T23:38:42.001+0000",
            "id": 10
        },
        {
            "author": "Karl Wettin",
            "body": "I'd like to see this committed in 2.4, but I don't have core access.",
            "date": "2008-08-24T12:20:02.679+0000",
            "id": 11
        },
        {
            "author": "Yonik Seeley",
            "body": "This solves a particular usecase nicely, but is it really generic enough and durable enough to put in core?\nThis essentially adds a new file into the index, but it's not really part of the index.  It wouldn't work with any possible upcoming similarity-per-field to give different NormCodecs per field, and it requires the user to handle their own management of the file (using lucene addIndexes to copy from one place to another won't grab this file, etc).",
            "date": "2008-08-24T14:45:11.328+0000",
            "id": 12
        },
        {
            "author": "Karl Wettin",
            "body": "The file is just something secondary I added on \"request\", personally I use a hardcoded codec. All it does is to allow a simple way in to change the current static norm translation table.",
            "date": "2008-08-24T21:09:48.132+0000",
            "id": 13
        },
        {
            "author": "Johan Kindgren",
            "body": "Wouldn't the simplest solution be to refactor out the static methods, replace them with instance methods and remove the getNormDecoder method? This would enable a pluggable behavior without introducing a new Codec.\nWould cause minor changes to 11 classes in the core, and would also clean up the code from static stuff.\n\nAs described in LUCENE-1261.",
            "date": "2009-05-06T21:43:10.112+0000",
            "id": 14
        },
        {
            "author": "Karl Wettin",
            "body": "bq. Wouldn't the simplest solution be to refactor out the static methods, replace them with instance methods and remove the getNormDecoder method? This would enable a pluggable behavior without introducing a new Codec.\n\nHi Johan,\n\nfeel free to post a patch!\n\n",
            "date": "2009-06-02T11:39:41.785+0000",
            "id": 15
        },
        {
            "author": "Johan Kindgren",
            "body": "Removed 'static' keyword to enable a pluggable behavior for encoding/decoding norms. Our business-case for this is to fix scoring when using NGrams. If a word is split into three parts, the norm for these parts would then become ~0.3125 (don't remember exactly) in the current implementation. A search for the exact same word would then generate a score of less than 1.0. With a pluggable norm-calculation, we could use a norm-table with values 0-100 and get a better scoring.\n\nMinor changes in 11 core-classes and some tests. Also minor changes in analyzers, instantiated, memory and miscellaneous.",
            "date": "2009-06-21T15:48:44.554+0000",
            "id": 16
        },
        {
            "author": "Karl Wettin",
            "body": "Hi Johan,\n\ndidn't try it out yet but the patch looks nice and clean. +1 from me. Let's try to convince some of the old -1:ers. \n\nYONIK? See, it's not just me. ; )\n\nI do however still think it's nice with the serializable codec interface as in the previous patches in order for all applications to use the index as intended (Luke and what not). 256 bytes stored to a file and by default backed by a binary search or so unless there is a registred codec that handles it algorithmic. I'll copy and paste that in as an alternative suggestion ASAP.\n\n(I think the next move should be to allow for per field variable norms resolution, but that is a whole new issue.)",
            "date": "2009-06-22T12:15:06.586+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "Has anyone tested performance of this last patch?  One thing that concerns me is this change to TermScorer:\n\n{code}\n-    return norms == null ? raw : raw * SIM_NORM_DECODER[norms[doc] & 0xFF]; // normalize for field\n+    return norms == null ? raw : raw * getSimilarity().decodeNorm(norms[doc]); // normalize for field\n{code}\n\nthough it could easily be in practice that it doesn't matter.",
            "date": "2009-11-09T18:10:11.619+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "I think this is a reasonable change, but we probably should wait for 3.1 as long as 3.0 comes out soonish.",
            "date": "2009-11-09T18:13:06.373+0000",
            "id": 19
        },
        {
            "author": "Johan Kindgren",
            "body": "Regarding the performance of the TermScorer, there could be two things to handle to ensure that the Jvm will inline the code: \n1. In the Scorer base-class, make the field 'similarity' final. (Shouldn't be any problem since it's imutable?)\n2. In the Similarity, make the internal decoder array final. That's really up to the implementor, but the default implementations should perhaps use a final field. Also add a note in the javadoc of this?\nWould you like me to create another patch with the above changes? Maybe there could other optimizations, haven't really looked at optimizing the code yet.",
            "date": "2009-11-10T05:59:56.401+0000",
            "id": 20
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Would you like me to create another patch with the above changes?\n\nYes, please -- then I'll run some basic perf tests. Thanks!",
            "date": "2009-11-10T09:45:31.001+0000",
            "id": 21
        },
        {
            "author": "Johan Kindgren",
            "body": "Added 'final' modifier to the Similarity field where it was used.\n\nThe norm-array in Similarity was already made 'final', so there's no change there. I think there could be further refactoring of the use of the Similarity instance, but that is perhaps out of the scope for this issue. I hope this will pass the performance-tests!",
            "date": "2009-11-11T21:43:06.086+0000",
            "id": 22
        },
        {
            "author": "Michael McCandless",
            "body": "Performance looks good -- I tested with query \"1\" on a 5M doc wikipedia index and any difference appears to be in the noise.\n\nBut, the current patch breaks back-compat (eg {{ant test-tag -Dtestcase=TestNorms}} fails) -- I think we have to put back the static methods (mark them deprecated), and then find new names for the instance methods?",
            "date": "2009-11-12T01:24:04.918+0000",
            "id": 23
        },
        {
            "author": "Johan Kindgren",
            "body": "Haven't really thought about the back-compat question yet, but that's of course an important aspect. When is the 3.0 release planned? I noticed that there were a couple of issues still open, and that release will already break the compatibility...\nMaybe this kind of change should be tested for a couple of weeks before bringing it to a release, if the 3.0 is impending.\n\nWould you like me to continue working with this, or do you already have suggestions for new names of the instance methods?",
            "date": "2009-11-12T07:06:35.939+0000",
            "id": 24
        },
        {
            "author": "Uwe Schindler",
            "body": "The 3.0 release will *not* break backwards compatibility for users that upgraded to 2.9.1 and got rid of deprecation warnings. 3.0 release cicle will start at the weekend, most issues are organizational ones, the rest is finished soon.\n\nI tend to add the deprecated static method and leave this as 3.1.",
            "date": "2009-11-12T07:11:45.741+0000",
            "id": 25
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Would you like me to continue working with this, or do you already have suggestions for new names of the instance methods?\n\nYes, please, could you turnaround a new patch?\n\nHmm, naming is always the hardest part.... maybe decodeNormValue/encodeNormValue?  normToByte/byteToNorm?  getEncodedNorm/getDecodedNorm?  Something else?",
            "date": "2009-11-12T09:53:38.695+0000",
            "id": 26
        },
        {
            "author": "Michael McCandless",
            "body": "Johan are you working up a new patch here?  (to fix the back compat issue)",
            "date": "2009-11-17T11:14:53.789+0000",
            "id": 27
        },
        {
            "author": "Johan Kindgren",
            "body": "I haven't had time so far to create a new patch, and I will be away for the next couple of days. Feel free to modify my patch if you like to finish up this issue. 'decodeNormValue' sounds fine by me!\nOtherwise I hope I can come up with a patch by the end of the week (probably late, sunday).",
            "date": "2009-11-17T12:31:29.056+0000",
            "id": 28
        },
        {
            "author": "Johan Kindgren",
            "body": "I've added the old static methods again, but made them deprecated.\n\nIn contrib/misc there is still a reference to the static encodeNorm method, maybe that should be replaced with Similarity.getDefaultSimilarity().encodeNormValue(f)? This call to the static method is only done if no similarity is passed to the FieldNormModifier.\n\nI added a short javadoc description to the static methods, not sure if that is enough? (I guess they will be removed, so the relevant javadoc is probably in the instance methods?)",
            "date": "2009-11-22T19:14:13.989+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good!  Thanks Johan.  I'll commit in a day or two...",
            "date": "2009-11-22T21:41:02.125+0000",
            "id": 30
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Johan!",
            "date": "2009-11-24T20:27:10.756+0000",
            "id": 31
        },
        {
            "author": "Robert Muir",
            "body": "I think there are serious traps here, that if you supply Similarity to IWConfig etc rather than\n setting the global static Similarity.setDefault, your code will have no effect.\n\nThe biggest offendor can be seen in the patch:\n{noformat}\n      final float norm = docState.similarity.computeNorm(fieldInfo.name, fieldState);\n-      norms[upto] = Similarity.encodeNorm(norm);\n+      norms[upto] = Similarity.getDefault().encodeNormValue(norm);\n{noformat}\n\nshouldnt that simply call docState.similarity.encodeNormValue?\n\nThere are other problems with decode too.\nI think we need to review all places where we use the static Similarity.getDefault() carefully.\n\n",
            "date": "2011-01-07T04:54:46.925+0000",
            "id": 32
        },
        {
            "author": "Robert Muir",
            "body": "Here's a patch for the general case, and it also adds a warning\nthat you should set your similarity with Similarity.setDefault, especially if you omit norms.\n\nWe can backport this to 3.x\n\nThe other cases involve fake norms, which I think we should completely remove in trunk\nwith LUCENE-2846, then there is no longer an issue and we can remove the warning in trunk.\n",
            "date": "2011-01-08T17:20:52.029+0000",
            "id": 33
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Here's a patch for the general case, and it also adds a warning that you should set your similarity with Similarity.setDefault, especially if you omit norms. \n\nIs there no way to remove this stupid static default and deprecate Similarity.(g|s)etDefault()? Can we not use the Similarity from IndexWriter for the case of NormsWriter?",
            "date": "2011-01-08T17:31:18.610+0000",
            "id": 34
        },
        {
            "author": "Robert Muir",
            "body": "bq. Is there no way to remove this stupid static default and deprecate Similarity.(g|s)etDefault()? Can we not use the Similarity from IndexWriter for the case of NormsWriter?\n\nI think this is totally what we should try to do in trunk, especially after LUCENE-2846.\n\nIn this case, i want to fix the issue in a backwards-compatible way for lucene 3.x\nThe warning is a little crazy I know, really people shouldnt rely upon their encoder being used for *fake norms*.\nBut i think its fair to document the corner case, just because its not really fixable easily in 3.x\n\nFor trunk, here is what i suggest:\n* LUCENE-2846: remove all uses of fake norms. We never fill fake norms anymore at all, once we fix this issue. If you have a non-atomic reader with two segments, and one has no norms, then the whole norms[] should be null. this is consistent with omitTF. So, for example MultiNorms would never create fake\nnorms.\n* LUCENE-2854: Mike is working on some issues i think where BooleanQuery uses this static or some other silliness with Similarity, i think we can clean that up there.\n* finally at this point, I would like to remove Similarity.getDefault/setDefault alltogether. I would prefer instead that IndexSearcher has a single 'DefaultSimilarity' that is the default value if you don't provide one, and likewise with IndexWriterConfig.\n",
            "date": "2011-01-08T17:47:14.277+0000",
            "id": 35
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. For trunk, here is what i suggest:\nI didn't follow the entire thread here but is it worth all the effort what robert is suggesting or should we simply land docvalues branch and make norms a DocValues field? The infrastructure is already there, its integrated into codec and gives users the freedom to use any Type they want. ",
            "date": "2011-01-09T11:14:55.270+0000",
            "id": 36
        },
        {
            "author": "Robert Muir",
            "body": "bq. I didn't follow the entire thread here but is it worth all the effort what robert is suggesting or should we simply land docvalues branch and make norms a DocValues field? The infrastructure is already there, its integrated into codec and gives users the freedom to use any Type they want.\n\nSimon, the the problem is encode/decode is in Similarity (instead of somewhere else).\n\nSo, you would have the same problem with DocValues!",
            "date": "2011-01-09T11:39:42.029+0000",
            "id": 37
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. So, you would have the same problem with DocValues!\nhmm, not sure if I understand this correctly. how values are encoded / decoded depends on the DocValues implementation which can be customized since it is exposed via codec. That means that users of the API always operate on float and the encoding and decoding happens inside codec and per field. So encode/decode in Sim would be obsolet, right?",
            "date": "2011-01-09T13:04:44.976+0000",
            "id": 38
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nhmm, not sure if I understand this correctly. how values are encoded / decoded depends on the DocValues implementation which can be customized since it is exposed via codec. That means that users of the API always operate on float and the encoding and decoding happens inside codec and per field. So encode/decode in Sim would be obsolet, right?\n{quote}\n\nthe issues remaining here involve mostly \"fake norms\", for the omitNorms case (also \"empty norms\" I think).\nSo, the stuff I listed must be fixed regardless, to clean up the fake norms case, it does not matter if \"real norms\" are encoded with CSF or not.\n\nDoing things like cleaning up how we deal with fake norms, and removing Similarity.get/setDefault is completely unrelated to DocValues... its just stuff we must fix.\n\nAs long as we have these statics like Similarity.get/setDefault, its not even useful to think about things like flexible scoring or per-field SImilarity...!\n",
            "date": "2011-01-09T13:20:08.549+0000",
            "id": 39
        },
        {
            "author": "Michael McCandless",
            "body": "I think we need to stop faking norms, independent of whether/when we cutover to CSF to store norms / index stats?\n\nIe the two issues are orthogonal (and both are important!).",
            "date": "2011-01-09T13:42:49.154+0000",
            "id": 40
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. I think we need to stop faking norms, independent of whether/when we cutover to CSF to store norms / index stats? \n\n+1, it was only intended to be a short-term thing for back compat (see way back to LUCENE-448)",
            "date": "2011-01-09T13:56:03.627+0000",
            "id": 41
        },
        {
            "author": "Robert Muir",
            "body": "(Updating fix-version correctly, also).\n\nI think its safe to mark this resolved... the issues are totally cleared up in 4.0, \nand only some (documented) corner cases in 3.x where we still use the default sim.",
            "date": "2011-01-13T12:00:23.346+0000",
            "id": 42
        },
        {
            "author": "Grant Ingersoll",
            "body": "Bulk close for 3.1",
            "date": "2011-03-30T15:49:54.140+0000",
            "id": 43
        }
    ],
    "component": "core/search",
    "description": "The static span and resolution of the 8 bit norms codec might not fit with all applications. \n\nMy use case requires that 100f-250f is discretized in 60 bags instead of the default.. 10?\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1260",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Norm codec strategy in Similarity",
    "systemSpecification": true,
    "version": "2.3.1"
}