{
    "comments": [
        {
            "author": "Christian Moen",
            "body": "Kuromoji - a Japanese morphological analyzer",
            "date": "2011-07-12T07:20:11.449+0000",
            "id": 0
        },
        {
            "author": "Christian Moen",
            "body": "Kuromoji Solr integration",
            "date": "2011-07-12T07:20:44.206+0000",
            "id": 1
        },
        {
            "author": "Christian Moen",
            "body": "MD5 hashes for the attachments are as follows:\n{code}\nMD5 (kuromoji-0.7.6.tar.gz) = 70d3d2f69f0511b86ebe11484cbe1313\nMD5 (kuromoji-solr-0.5.3.tar.gz) = b9a54698c9aebc264845e64d3904642d\n{code}",
            "date": "2011-07-12T07:22:07.005+0000",
            "id": 2
        },
        {
            "author": "Christian Moen",
            "body": "Attaching a brief presentation",
            "date": "2011-07-12T07:24:39.641+0000",
            "id": 3
        },
        {
            "author": "Simon Willnauer",
            "body": "WOW this is awesome. It seems we need to file some IP clearance here since this is a substantial contribution not developed in the ASF source control or on the mailing list. I will figure out the process here. \n\nI looked briefly at the sources here and I think we need to put this into a patch rather into a tar.gz. Some of the files don't have an apache header and some of the files state a copyright in the ASL 2 header. Basically for the code grant you need to put \"our\" ASL header into each file.\nWe also need to apply these sources to our source tree so it is very likely that this goes under /modules/analysis/common can you try to create a patch against trunk? if its is too much of a hassle you can also move the solr integration to a different issue. \n\nthanks simon",
            "date": "2011-07-12T09:17:50.984+0000",
            "id": 4
        },
        {
            "author": "Christian Moen",
            "body": "Thanks a lot, Simon.  I wasn't sure when we'd update the headers as part of the process, so thanks for clarifying that, too.\n\nKuromoji downloads IPADIC as part of its build (from our server in Japan) to make its data structures, which it bundles into its jar file (becomes 11M, but can be made a lot smaller).  Building also requires more than default heap-space, so it's build is a little convoluted and different from the other code in /modules/analysis/common.\n\nKuromoji is also usable independently from search, although, even though search perhaps is its most important application.  Would it be a good idea that I make a patch that puts it in /modules/analysis/kuromoji for now and that we take things from there?\n\nThe quickest way to get Kuromoji in there would be to check the jar file /modules/analysis/kuromoji/lib, but I'm not sure that's a good way to go.\n\nI'll follow up in whatever way you prefer.  Thanks again! :)\n\n\n",
            "date": "2011-07-12T10:51:41.270+0000",
            "id": 5
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nI looked briefly at the sources here and I think we need to put this into a patch rather into a tar.gz. Some of the files don't have an apache header and some of the files state a copyright in the ASL 2 header. Basically for the code grant you need to put \"our\" ASL header into each file.\n{quote}\n\nBut these things are separate, right? Can't he just fix the license headers and upload a new .tar.gz?\n\nI don't see anywhere that says a code grant should be a patch, this puts a burden on Christian to do all\nthe work, and our trunk moves too fast. Lets defer creating a patch until the code grant stuff is over... anyone could then turn it into a patch.\n",
            "date": "2011-07-12T11:51:05.229+0000",
            "id": 6
        },
        {
            "author": "Mark Miller",
            "body": "bq. But these things are separate, right?\n\nRight - looks like all we need is the ASF copyright in the files. The rest can easily be handled after the grant goes through.",
            "date": "2011-07-12T12:20:03.345+0000",
            "id": 7
        },
        {
            "author": "Christian Moen",
            "body": "Thanks, Robert and Mark.\n\nI'll upload new tarballs where the standard ASF license notice is being used in all Java source files and I've also removed author tags to comply better with code standards.  I've removed any Atilika Inc. copyrights from NOTICE.txt in both tarballs.",
            "date": "2011-07-12T13:32:13.718+0000",
            "id": 8
        },
        {
            "author": "Christian Moen",
            "body": "Now uses standard ASF license notice in all Java source files.\n\n{code}\nMD5 (kuromoji-0.7.6-asf.tar.gz) = a84f016bd5162e57423a1da181c25f36\n{code}",
            "date": "2011-07-12T13:33:31.313+0000",
            "id": 9
        },
        {
            "author": "Christian Moen",
            "body": "Now uses standard ASF license notice in all Java source files.\n\n{code}\nMD5 (kuromoji-solr-0.5.3-asf.tar.gz) = a3e7d5afba64ec0843be6d4dbb95be1c\n{code}",
            "date": "2011-07-12T13:33:57.259+0000",
            "id": 10
        },
        {
            "author": "Uwe Schindler",
            "body": "Code looks cool. I think we should first do the legal stuff and then produce patches. Robert is currently developing another morphological analyzer (Lucene-Gosen, https://code.google.com/p/lucene-gosen/), but this one uses a LGPL library that cannot be included with Lucene/Solr. The Lucene part has lots of cool attributes and additional TokenFilters, so maybe we combine lucene-gosen with this one (your Apache-2.0 and his TokenFilters+Attributes)? That would be really cool.",
            "date": "2011-07-12T13:58:43.569+0000",
            "id": 11
        },
        {
            "author": "Christian Moen",
            "body": "Thanks, Uwe!\n\nI think we definitely should work together and combine the great work that Robert, Koji & co. have been doing on Lucene-GoSen with Kuromoji to make a highly attractive Japanese linguistics offering that is also an integrated part of Lucene/Solr.\n\nThe attributes do indeed look very nice -- excellent job!  I have several improvements in mind for Kuromoji (and other Japanese related code) and I'm looking forward to working with you to improve some of these things.\n\nAdditional to its license, an issue with GoSen (and Sen) used to be its segmentation quality.  To my knowledge, these analyzers still don't support so-called \"unknown words\" which means that words that are not in the dictionaries are treated second-rate, which impacts negatively on segmentation quality.\n\n\n\n",
            "date": "2011-07-12T14:32:15.440+0000",
            "id": 12
        },
        {
            "author": "Koji Sekiguchi",
            "body": "Hi Christian, it's been a long time. Contribution of Kuromoji to Lucene/Solr sounds really nice! As already Uwe mentioned, lucene-gosen has really good TokenFilters, those are org.apache packages and Apache License. It will be nice if this Japanese tokenizer uses them. Plus, lucene-gosen can use not only IPADIC, but also NAIST JDIC. I'd like the tokenizer to choose dictionary in the future release.",
            "date": "2011-07-12T14:58:30.847+0000",
            "id": 13
        },
        {
            "author": "Christian Moen",
            "body": "\u4e45\u3057\u3076\u308a\u3067\u3059\u3088\u306d\u3002 Thanks a lot, Koji. :)\n\nI completely agree.  If we can get Kuromoji into the codebase, I'm more than happy to submit patches for your filters so that they will work with Kuromoji.\n\nKuromoji has preliminary support for UniDic and it sounds like a good idea to join effort on this as well.  We could support them all; IPADIC, NAIST JDIC and UniDic.\n",
            "date": "2011-07-12T15:35:14.955+0000",
            "id": 14
        },
        {
            "author": "Christian Moen",
            "body": "Please let me know if you need paperwork from me to follow up on this.  Thanks again.",
            "date": "2011-07-13T08:14:47.859+0000",
            "id": 15
        },
        {
            "author": "Simon Willnauer",
            "body": "Hey Christian, I attache the IP-Clearance form for this code donation. What we need to wrap up this process is \n\n* a code grant (http://www.apache.org/licenses/software-grant.txt)\n* a CLA from each of you\n* a list of all contributor if there are more than the two of you (http://www.apache.org/licenses/icla.txt)\n* a CLA from the company owning the IP (http://www.apache.org/licenses/cla-corporate.txt)\n\n\nThe CLA should go to the secretary, I still need to figure out where the code grant needs to go.",
            "date": "2011-07-15T08:44:57.637+0000",
            "id": 16
        },
        {
            "author": "Simon Willnauer",
            "body": "koji, I took the issue until the code grant is due etc.",
            "date": "2011-07-15T21:13:01.826+0000",
            "id": 17
        },
        {
            "author": "Christian Moen",
            "body": "Thanks, Simon.  Please let me know where I should send the code grant and I'll file the paperwork.",
            "date": "2011-07-18T05:07:14.618+0000",
            "id": 18
        },
        {
            "author": "Christian Moen",
            "body": "Hello again, Simon. Has there been any update as to where I should send the code grant? Many thanks.",
            "date": "2011-07-21T02:27:33.282+0000",
            "id": 19
        },
        {
            "author": "Simon Willnauer",
            "body": "Christian, apparently we just handle this as the CLA. You fill it out, scan it and send it to secretary@apache.org. Make sure you use the ICLA details when you file it.\n\nlet me know once you those are send.",
            "date": "2011-07-21T05:57:15.989+0000",
            "id": 20
        },
        {
            "author": "Simon Willnauer",
            "body": "I am going to be away for 2 weeks if somebody wants to continue driving this code grant. please do. Otherwise @christian sorry for the break I will continue once I am back or here and there if I find a computer :)\n\nsimon",
            "date": "2011-07-22T09:55:51.474+0000",
            "id": 21
        },
        {
            "author": "Christian Moen",
            "body": "Hello Simon.  I'll file the paperwork over the next couple of days by email and copy you.  Have a brilliant vacation! :)",
            "date": "2011-07-22T10:02:40.968+0000",
            "id": 22
        },
        {
            "author": "Christian Moen",
            "body": "Hello again, Simon.  I've filed the paperwork and copied you on email.  Hope you're enjoying your vacation!",
            "date": "2011-08-02T06:44:27.144+0000",
            "id": 23
        },
        {
            "author": "Simon Willnauer",
            "body": "Christina, thanks for filing the paper work, I just called out a vote on dev@l.a.o hope to get this done soon!\n\nsimon",
            "date": "2011-08-08T08:23:53.203+0000",
            "id": 24
        },
        {
            "author": "Simon Willnauer",
            "body": "Christian, I see a couple of files in the resource folders that don't have a license header, we need to make sure that all files do have an ASL 2 license header before we can finish the IP clearance process. Yet, I don't know much about this segmenter but I guess it works based on a dictionary, no? If so where are the dictionary files since I only see resource files in the test folder but maybe I miss something?\n\nsimon",
            "date": "2011-08-09T08:53:16.520+0000",
            "id": 25
        },
        {
            "author": "Christian Moen",
            "body": "Please see {{NOTICE.txt}} for information on the dictionaries.\n\nKindly let me know which files that require a license header and how I should proceed to provide a revised version.  Do you prefer a complete tarball or can I attach the filed individually to this JIRA?\n\nThanks!\n",
            "date": "2011-08-09T09:03:42.111+0000",
            "id": 26
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. Please see NOTICE.txt for information on the dictionaries.\nso those dictionaries are not ASL licensed, right? I need to check with legal if we can include them into our distribution at all so we need to figure that out first. ",
            "date": "2011-08-09T09:17:12.408+0000",
            "id": 27
        },
        {
            "author": "Christian Moen",
            "body": "Correct.  You should definitely check this with legal.  I've tried to point this out in the description and in my email with the secretary as well.  If there are questions or concerns my legal counsel can possibly assist, but I guess this is something the ASF has to consider by itself.\n",
            "date": "2011-08-09T09:32:24.121+0000",
            "id": 28
        },
        {
            "author": "Simon Willnauer",
            "body": "FYI - I created an issue on legal to categorize the IPADIC license LEGAL-97",
            "date": "2011-08-17T07:42:07.748+0000",
            "id": 29
        },
        {
            "author": "Robert Muir",
            "body": "Now that we have some feedback on LEGAL-97, what is the next step we need to do to move forward with this feature?",
            "date": "2011-09-20T22:59:09.457+0000",
            "id": 30
        },
        {
            "author": "Simon Willnauer",
            "body": "According to LEGAL-97 we can include the dict files. That means we can finish this code donation and get everything in shape for a commit. I will finish the paper work once I am back from traveling.\n\n",
            "date": "2011-09-21T09:03:02.903+0000",
            "id": 31
        },
        {
            "author": "Christian Moen",
            "body": "Thanks for the follow-up, Robert and Simon.  I've started working on a patch.",
            "date": "2011-09-22T08:34:26.418+0000",
            "id": 32
        },
        {
            "author": "Simon Willnauer",
            "body": "here is an updated ip-clearance file. Since this is the first time I do this I would appreciate some feedback or help from other with more experience here. Grant, does that look fine to you?\n\nI think if we are ok with this we can go ahead and call the vote on incubator.",
            "date": "2011-09-26T10:26:32.017+0000",
            "id": 33
        },
        {
            "author": "Robert Muir",
            "body": "Just a ping... whats our next step?",
            "date": "2011-11-03T14:09:24.021+0000",
            "id": 34
        },
        {
            "author": "Grant Ingersoll",
            "body": "File looks good to me.  You need to check in the file to https://svn.apache.org/repos/asf/incubator/public/trunk/site-author/ip-clearance and then call a vote on general@incubator.apache.org (there should be examples of this in the archives for that list).  Vote is lazy consensus, so don't expect too much feedback.  Once that vote passes, then the code can be committed.",
            "date": "2011-11-08T21:02:54.905+0000",
            "id": 35
        },
        {
            "author": "Simon Willnauer",
            "body": "I committed the file to the incubator ip-clearance in revision 1199470. I will go ahead an call an incubator vote now. thanks grant\n",
            "date": "2011-11-08T21:39:06.178+0000",
            "id": 36
        },
        {
            "author": "Simon Willnauer",
            "body": "I send the vote to general@incubator ...we will see in 72h! thanks folks",
            "date": "2011-11-08T21:51:47.716+0000",
            "id": 37
        },
        {
            "author": "Simon Willnauer",
            "body": "here is an initial patch. nothing special just basic integration into the modules/analysis tree. I added a task taht downloads the dicts and puts them in place so I could run the tests. all passing for me... still lots of work but its a start",
            "date": "2011-11-11T22:54:36.175+0000",
            "id": 38
        },
        {
            "author": "Robert Muir",
            "body": "looks like we want to add the Lucene analyzer/tokenizer and solr factories from kuromoji-solr-0.5.3-asf.tar.gz\n\nI'd say once we get stuff going, maybe just download the dictionary, build it, and when committing commit\nthe built dictionary under resources/ folder (this is where the script puts it).\n\nI think for this kind of feature it might be hard to iterate with patches, we should maybe try to get it \nin SVN (trunk) initially and iterate with smaller issues. The code looks pretty clean to me already.\n\nThe produced jar file is somewhat large but I think its still reasonable, so I think we should look past\nthis for now? working with Sen before I know some ways we can shrink this a lot, but that would be best\non a future issue.\n\nSome java6 apis are here (e.g. unicode normalization). Christian can you confirm this is only for the \ndictionary-build stage? It looked to me like its only needed for ipadic/unidic parsing, but not\ncustom dictionary support.\n\nIf its only for the build stage, personally I think thats fine for 3.x too, because I'm suggesting we \ncommit a 'built' dictionary and we tell people if they want to compile the dictionary themselves they \nneed java6? We could put the dictionary-building under a tools/ directory thats java6-only, or we could \ndepend on ICU for just the tools/ piece (i think we already have such hacks for generating jflex rules\nfor StandardTokenizer) and be fine on java5.\n\n+1 for the GraphVizFormatter... \n",
            "date": "2011-11-12T01:56:33.510+0000",
            "id": 39
        },
        {
            "author": "Simon Willnauer",
            "body": "+1 to all your comments. For 3.x lets figure this out somewhere else... first iterate on trunk and when we have it at a reasonable stage we backport it to 3.x. The vote succeeded so we are good to go!",
            "date": "2011-11-13T11:22:22.004+0000",
            "id": 40
        },
        {
            "author": "Simon Willnauer",
            "body": "marking fix version 4.0 - lets open a new issue for backporting...",
            "date": "2011-11-13T11:25:30.926+0000",
            "id": 41
        },
        {
            "author": "Christian Moen",
            "body": "Thanks a lot, Simon!\n\nRobert, I agree completely with your comments.  The Unicode normalization is only done at dictionary build time.  Simon has turned it on by default -- its previous default was off.  Perhaps it makes sense to have it on in Lucene's case...\n\nSimon, the TokenizerRunner class doesn't seem to be included in the patch, which might be fine.  It's not strictly necessary for Lucene, but I think it's useful to keep it there so the analyzer can easily be run from the command line.  The DebugTokenizer and GraphvizFormatter is there already, which aren't strictly necessary either, but sometimes quite useful, so I'm think we should add the TokenizerRunner as well -- at least for now.\n\nTests didn't pass in my case, but I'll look more into this soon.  My tomorrow is very busy, but I'll have time for this on Wednesday.\n",
            "date": "2011-11-14T14:23:15.009+0000",
            "id": 42
        },
        {
            "author": "Robert Muir",
            "body": "I created a branch here (https://svn.apache.org/repos/asf/lucene/dev/branches/lucene3305) \nwith an initial import of this code, only minor tweaks to get things working in the build so far.\n\n",
            "date": "2012-01-03T05:17:27.420+0000",
            "id": 43
        },
        {
            "author": "Christian Moen",
            "body": "Thanks, Robert.\n\nI've built the branch.  I needed to do {{ant test -Dargs=\"-Dfile.encoding=UTF-8\"}} in order to make all the Kuromoji tests pass as some of them assume UTF-8 file encoding. (MacRoman is default on my system.)\n\nI really appreciate the efforts yourself and Simon have put it.  I also hope to make some meaningful contributions to make sure Kuromoji integrates and works works well with Solr and Lucene.",
            "date": "2012-01-03T22:49:22.308+0000",
            "id": 44
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nI've built the branch. I needed to do ant test -Dargs=\"-Dfile.encoding=UTF-8\" in order to make all the Kuromoji tests pass as some of them assume UTF-8 file encoding. (MacRoman is default on my system.)\n{quote}\n\nThis sounds like a bug in the build, you shouldn't have to do that (it should be set already). However, my default encoding is UTF-8 so thats why i didn't catch it. I'll look into this.",
            "date": "2012-01-03T22:52:57.019+0000",
            "id": 45
        },
        {
            "author": "Christian Moen",
            "body": "Patch to fix zero wordid issue.  Backport of fix from kuromoji 0.7.7-SNAPSHOT on Github.",
            "date": "2012-01-11T09:25:28.501+0000",
            "id": 46
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Christian,\nthanks for the fix. I will aply the patch to the branch. The tests testYabottai() and testTsukitosha() are not hurting, but have no meaning for our variant, because wordid=0 and last wordid have different words (because we presort the whole dictionary for the FST). To make the test really use wordid=0, I should lookup the actual dictionary entries of first and last word.",
            "date": "2012-01-11T10:07:33.146+0000",
            "id": 47
        },
        {
            "author": "Uwe Schindler",
            "body": "Committed development branch revision: 1229948\nThanks Christian!",
            "date": "2012-01-11T10:12:44.575+0000",
            "id": 48
        },
        {
            "author": "Robert Muir",
            "body": "Thank you for fixing that bug!\n\nBy the way, I've been reviewing the differences between mecab and kuromoji. In general the differences seem fine to me, \nactually in Kuromoji's favor (at least for search). Most revolve around middle-dot:\n\n{noformat}\nsentence: \u79c1\u304c\u30a8\u30c9\u30ac\u30fc\u30fb\u30c9\u30ac\u3067\u3059\u3002\nmecab: [\u79c1, \u304c, \u30a8\u30c9\u30ac\u30fc\u30fb\u30c9\u30ac, \u3067\u3059]\nkuromoji: [\u79c1, \u304c, \u30a8\u30c9\u30ac\u30fc, \u30c9\u30ac, \u3067\u3059]\n{noformat}\n\nSo I think these are improvements, at least for search (e.g. Kuromoji splits the first/last name here).\n\nBut, there is often funkiness revolving caused by the normalizeEntries option, which, if\nan entry is not NFKC-normalized, it adds an NFKC-normalized entry with the same costs etc. \n\nHowever, I think in some cases this skews the costs because e.g. half-width and full-width numbers have different costs.\nSo by adding normalized entries with the full-width cost, we sometimes get worse tokenization.\n\n{noformat}\nsentence: Windows95\u5bfe\u5fdc\u306e\u30b2\u30fc\u30e0\u3092\u52d5\u304b\u3057\u305f\u3044\u306e\u3067\u3059\u3002\nmecab: [Windows, 95, \u5bfe\u5fdc, \u306e, \u30b2\u30fc\u30e0, \u3092, \u52d5\u304b\u3057, \u305f\u3044, \u306e, \u3067\u3059]\nkuromoji: [Windows, 9, 5, \u5bfe\u5fdc, \u306e, \u30b2\u30fc\u30e0, \u3092, \u52d5\u304b\u3057, \u305f\u3044, \u306e, \u3067\u3059]\n{noformat}\n\nI changed the default locally of 'normalizeEntries' to false and it seemed to totally fix this, and all\nthe differences vs. mecab then seemed positive. \n\nI think we should disable normalizeEntries by default so that no costs are potentially skewed... opinions?\n",
            "date": "2012-01-11T16:44:25.087+0000",
            "id": 49
        },
        {
            "author": "Christian Moen",
            "body": "The middle dot character (nakaguro) is treated as character class SYMBOL in order to provoke a split.  This is by design and we override IPADIC in this case since we feel the split behaviour is more reasonable for most applications.\n\nHaving said this, I'd expect input\n\n{noformat}\n\u79c1\u304c\u30a8\u30c9\u30ac\u30fc\u30fb\u30c9\u30ac\u3067\u3059\u3002\n{noformat}\n\nto produce segmentation\n{noformat}\n\u79c1\u3000\u304c\u3000\u30a8\u30c9\u30ac\u30fc\u3000\u30fb\u3000\u30c9\u30ac\u3000\u3067\u3059\u3000\u3002\n{noformat}\n\nThe middle dot \u30fb seems to have been removed in your case.  Are you deliberately removing it somewhere?\n\n\nYou're right about the NFKC-normalization.  It's turned off by default in the Kuromoji on Github.  I think disabling this is a reasonable default, but I think it's a good idea to have the option of doing NFKC-normalization prior to segmentation in the Tokenizer/Analyzer (Lucene).\n",
            "date": "2012-01-11T17:17:43.088+0000",
            "id": 50
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nThe middle dot \u30fb seems to have been removed in your case. Are you deliberately removing it somewhere?\n{quote}\n\nJust in my debugging :)\n\n(Separately: i did add an option to doTokenize to not emit punctuation tokens, and the lucene analyzer uses it by default, otherwise\nindex size and searches are affected by many tokens like \"\u3002\"... but thats unrelated here)\n\n{quote}\nYou're right about the NFKC-normalization. It's turned off by default in the Kuromoji on Github. I think disabling this is a reasonable default, but I think it's a good idea to have the option of doing NFKC-normalization prior to segmentation in the Tokenizer/Analyzer (Lucene).\n{quote}\n\nYeah i agree, we can add a charfilter that uses the incremental normalization api.\n",
            "date": "2012-01-11T18:04:38.280+0000",
            "id": 51
        },
        {
            "author": "Robert Muir",
            "body": "updated patch, showing differences from branch and trunk. \n\nI think this is ready to commit to trunk.",
            "date": "2012-01-12T19:13:29.375+0000",
            "id": 52
        },
        {
            "author": "Robert Muir",
            "body": "I committed this to trunk. \n\nI'll let hudson chew on it a bit before backporting to branch 3.x, but in general\nI think we've hammered on this enough that its ready to be backported too.\n\nIts a big contribution so I'm sure minor things might pop up but we can just open new issues...\n\nBig thanks to Christian for the contribution... this is awesome!",
            "date": "2012-01-12T20:11:39.837+0000",
            "id": 53
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. I committed this to trunk.\n\nYAY! thanks everyone!",
            "date": "2012-01-12T20:40:46.197+0000",
            "id": 54
        },
        {
            "author": "Robert Muir",
            "body": "Yes, thanks also to Uwe for lots of work compressing data and refactoring, and Mike for tuning the fsts.",
            "date": "2012-01-12T20:45:28.036+0000",
            "id": 55
        },
        {
            "author": "Robert Muir",
            "body": "backported to 3.x. Thanks again Christian!!! Such an awesome addition!",
            "date": "2012-01-14T17:40:41.120+0000",
            "id": 56
        },
        {
            "author": "Christian Moen",
            "body": "Thanks for excellent work integrating Kuromoji, Robert.  Also thanks to everybody who has made helped and made this happen. ",
            "date": "2012-01-15T14:07:54.782+0000",
            "id": 57
        },
        {
            "author": "Shad Storhaug",
            "body": "I am working on porting this code over to .NET (LUCENENET-567). All is good with the Analyzer, Filters, etc. since they have good tests.\n\nHowever, there are only 2 tests for the \"tools\", and neither one tests the {{DictionaryBuilder.Main(String[] args)}} method (or runs any of the I/O code). Documentation is scant, and it is difficult to work out what should be in the input directory in order to do and end-to-end test of this tool.\n\nCould you add some tests so we have better code coverage, and so I can verify it all works after the translation to .NET? Or at least provide a zip file with the files that are required as input to the tool?\n\n",
            "date": "2017-07-23T00:29:45.912+0000",
            "id": 58
        }
    ],
    "component": "modules/analysis",
    "description": "Atilika Inc. (\u30a2\u30c6\u30a3\u30ea\u30ab\u682a\u5f0f\u4f1a\u793e) would like to donate the Kuromoji Japanese morphological analyzer to the Apache Software Foundation in the hope that it will be useful to Lucene and Solr users in Japan and elsewhere.\n\nThe project was started in 2010 since we couldn't find any high-quality, actively maintained and easy-to-use Java-based Japanese morphological analyzers, and these become many of our design goals for Kuromoji.\n\nKuromoji also has a segmentation mode that is particularly useful for search, which we hope will interest Lucene and Solr users.  Compound-nouns, such as \u95a2\u897f\u56fd\u969b\u7a7a\u6e2f (Kansai International Airport) and \u65e5\u672c\u7d4c\u6e08\u65b0\u805e (Nikkei Newspaper), are segmented as one token with most analyzers.  As a result, a search for \u7a7a\u6e2f (airport) or \u65b0\u805e (newspaper) will not give you a for in these words.  Kuromoji can segment these words into \u95a2\u897f \u56fd\u969b \u7a7a\u6e2f and \u65e5\u672c \u7d4c\u6e08 \u65b0\u805e, which is generally what you would want for search and you'll get a hit.\n\nWe also wanted to make sure the technology has a license that makes it compatible with other Apache Software Foundation software to maximize its usefulness.  Kuromoji has an Apache License 2.0 and all code is currently owned by Atilika Inc.  The software has been developed by my good friend and ex-colleague Masaru Hasegawa and myself.\n\nKuromoji uses the so-called IPADIC for its dictionary/statistical model and its license terms are described in NOTICE.txt.\n\nI'll upload code distributions and their corresponding hashes and I'd very much like to start the code grant process.  I'm also happy to provide patches to integrate Kuromoji into the codebase, if you prefer that.\n\nPlease advise on how you'd like me to proceed with this.  Thank you.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-3305",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Kuromoji code donation - a new Japanese morphological analyzer",
    "systemSpecification": true,
    "version": ""
}