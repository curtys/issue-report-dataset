{
    "comments": [
        {
            "author": "Grant Ingersoll",
            "body": "This is a fairly trivial start to to this, but it creates the sinks package in the contrib/Analysis section and adds a simple TokenRangeSinkTokenizer and test.  This can be used to siphon off tokens that fall in a range.  All it does is count the tokens that go by and add those that fall in the range.  It might be useful for documents that you know have certain structures.  For instance, if you know the first 5 tokens of your docs are X.\n\nMore to follow.",
            "date": "2007-12-04T16:17:33.429+0000",
            "id": 0
        },
        {
            "author": "Grant Ingersoll",
            "body": "Expanding to add in some other analysis capabilities",
            "date": "2007-12-05T14:03:15.859+0000",
            "id": 1
        },
        {
            "author": "Grant Ingersoll",
            "body": "Adds to the sinks package:\nDateRecognizerSinkTokenizer which only adds dates that can be parsed by a DateFormat object\n\nTokenRangeSinkTokenizer as described earlier\n\nTokenTypeSinkTokenizer only adds to the sink if the token type is a specific value.\n\nAdds the payloads package, which contains\nNumericPayloadTokenFilter -- Assigns a predefined float-based payload to a Token if the type matches the specified input type of the Token.  As a use case, this could be used to assign a payload for all tokens that are marked as \"bold\" or some other value.",
            "date": "2007-12-06T03:44:31.842+0000",
            "id": 2
        },
        {
            "author": "Grant Ingersoll",
            "body": "Thought of another possibly useful payload convenience TokenFilter that adds the Token type as the payload.  I've always wondered why we don't make more use of the type attribute on a Token.",
            "date": "2007-12-06T03:57:36.411+0000",
            "id": 3
        },
        {
            "author": "Grant Ingersoll",
            "body": "Committed",
            "date": "2007-12-07T12:57:41.218+0000",
            "id": 4
        }
    ],
    "component": "modules/analysis, modules/other",
    "description": "With the advent of the new TeeTokenFilter and SinkTokenizer, there now exists some interesting new things that can be done in the analysis phase of indexing.  See LUCENE-1058.\n\nThis patch provides some new implementations of SinkTokenizer that may be useful.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1077",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "New Analysis  Contributions",
    "systemSpecification": true,
    "version": ""
}