{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "Attached patch.\n\nI also fixed int numTerms -> long numTerms which was a latent bug.",
            "date": "2010-08-04T21:45:23.364+0000",
            "id": 0
        },
        {
            "author": "Yonik Seeley",
            "body": "Ah sweet, nice idea!",
            "date": "2010-08-04T21:53:47.833+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "i think this patch as-is is a good improvement (at least as a defensive measure against \"noise\" terms and other things). it also seems to buy more savings on the non-latin data i tested (60kb -> 40kb). +1 to commit\n\n{quote}\nIn the future we could do crazier things. EG there's no real reason why the indexed terms must be regular (every N terms), so, we could instead pick terms more carefully, say \"approximately\" every N, but favor terms that have a smaller net prefix\n{quote}\n\nI think we should explore this in the future. \"randomly\" selecting every N terms isn't optimal when allowing a \"fudge\" of the interval maybe +/- 5 or 10% could intentionally select terms that differ very quickly from their previous term, without wasting a bunch of cpu or unbalancing the terms index... \n\nif additional smarts like this could save enough size on average maybe we could rethink lowering the default interval of 128?",
            "date": "2010-08-05T01:34:56.942+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "bq. if additional smarts like this could save enough size on average maybe we could rethink lowering the default interval of 128?\n\n+1\n\nWith flex we've already very substantially reduced the RAM cost, GC load and init time of the terms index.  We no longer create a TermInfo, Term, String objects per indexed term.\n\nSo, I think even without these further mods we could drop the default index interval to maybe 32.",
            "date": "2010-08-05T10:22:50.054+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "New patch, changes default TII to 32.",
            "date": "2010-08-05T10:29:57.147+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "Allowing the term index interval to be variable (eg to find \"short min prefix\" terms to further improve savings from this opto, or to balance index terms by net docFreq of terms in each block) makes seek-by-ord more complex.\n\nIe, today, because indexed terms are every N, seek by ord is a straight divide/mod.  But with variable spaced index terms, I think we'd have to hold the ord of each indexed term in RAM, probably as packed int array so added RAM cost shouldn't be too bad.  Then a lookup by ord would require a binary search then scan.",
            "date": "2010-08-05T10:33:49.106+0000",
            "id": 5
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nBut with variable spaced index terms, I think we'd have to hold the ord of each indexed term in RAM, probably as packed int array so added RAM cost shouldn't be too bad. Then a lookup by ord would require a binary search then scan.\n{quote}\n\nWell, I was suggesting this mainly to try to justify lowering default TII, but if we can lower it \nseparately as you propose, but keep TII constant and not have to deal with this tradeoff, I think thats great!\n",
            "date": "2010-08-05T15:10:30.413+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "Unfortunately, it's not quite as simple to do this on 3.x.\n\nOn trunk, an indexed term points to the file pointer for that term's entry in the tis file, and we then read the absolute (term and frq/prx pointers) at that entry.\n\nBut in 3.x, all entries are deltas (even the seek points), and, the indexed term points to the entry *after* itself in the tis file, which requires that the term in memory be the full term (unless we change 3.x's index format to also store the absolutes at the seek points).",
            "date": "2010-08-07T00:45:46.432+0000",
            "id": 7
        },
        {
            "author": "Simon Willnauer",
            "body": "Reopening this because this optimization is not safe for all BytesRef comparators. Reverse unicode order already breaks it. See LUCENE-2622 for details. The non-distinguishable suffix must be determined by the actually used Comparator otherwise the assumption might be wrong for non-standard sort order.",
            "date": "2010-09-15T10:25:19.887+0000",
            "id": 8
        },
        {
            "author": "Robert Muir",
            "body": "Should we really change StandardCodec to support this [non-binary order]?\n\nReally if you have anything but regular unicode order, other things in lucene will break too, such as queries.\nThe test just doesnt test these. Try changing the order of PreFlexCodec's comparator...\n\nCan't we just fix the test not to use StandardCodec? I mean we aren't taking any feature away here.",
            "date": "2010-09-15T11:46:47.792+0000",
            "id": 9
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. Should we really change StandardCodec to support this [non-binary order]?\nI'm not sure if we should do, but we should at least document the limitation. People who work with that level do also read doc strings - if they don't let them be doomed but if you run into the bug we had in LUCENE-2622 you will have a super hard time to figure out what is going on without knowing lucene very very well. \n\n\nbq. Can't we just fix the test not to use StandardCodec? I mean we aren't taking any feature away here. \n\n+1 I think we should fix this test ASAP with either using byte sort order or add some MockCodec (what robert has suggested). \n",
            "date": "2010-09-15T12:59:23.493+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "After I commit the simple renaming of standard codec's terms dicts (LUCENE-2647), I plan to make this suffix-stripping opto private to StandardCodec (I think by refactoring SimpleTermsIndexWriter to add a method that can alter the indexed term before it's written).\n\nSince StandardCodec hardwires the term sort to unicode order, the opto is safe there.\n\nIn general, if a codec uses a different term sort (such as this test's codec) it's conceivable a different opto could apply.  EG I think this test could prune suffix based on the term after the index term.  But, it makes no sense to spend time exploring this until a \"real\" use case arrives... this is just a simple test to assert that a codec is in fact free to customize the sort order.\n\nAlso, there are other fun optos we could explore w/ terms index.  EG we could \"wiggle\" the index term selection a bit, so it wouldn't be fixed to every N, to try to find terms that are small after removing the useless suffix.  Separately, we could choose index terms according to docFreq -- eg one simple policy would be to plant an index term on term X if either 1) term X's docFreq is over a threshold, or, 2) it's been > N terms since the last indexed terms.  This could be a powerful way to even further reduce RAM usage of the terms index, because it'd ensure that high cost terms (ie, many docs/freqs/positions to visit) are in fact fast to lookup.  The low freq terms can afford a higher seek time since it'll be so fast to enum the docs.",
            "date": "2010-09-16T10:07:12.924+0000",
            "id": 11
        },
        {
            "author": "Simon Willnauer",
            "body": "{quote}\nAfter I commit the simple renaming of standard codec's terms dicts (LUCENE-2647), I plan to make this suffix-stripping opto private to StandardCodec (I think by refactoring SimpleTermsIndexWriter to add a method that can alter the indexed term before it's written).\n{quote}\nMike what about factoring out a method like \n{code}\nprotected short indexTermPrefixLen(BytesRef lastTerm, BytesRef currentTerm){\n  ...\n}\n\n{code}\n\nthen we can simply override that method if there is a comparator which can not utilize / breaks this opto?",
            "date": "2010-09-16T10:40:57.693+0000",
            "id": 12
        },
        {
            "author": "Michael McCandless",
            "body": "I think that's good!  I'll take that approach.",
            "date": "2010-09-16T10:56:13.021+0000",
            "id": 13
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nAlso, there are other fun optos we could explore w/ terms index. EG we could \"wiggle\" the index term selection a bit, so it wouldn't be fixed to every N, to try to find terms that are small after removing the useless suffix. Separately, we could choose index terms according to docFreq - eg one simple policy would be to plant an index term on term X if either 1) term X's docFreq is over a threshold, or, 2) it's been > N terms since the last indexed terms. This could be a powerful way to even further reduce RAM usage of the terms index, because it'd ensure that high cost terms (ie, many docs/freqs/positions to visit) are in fact fast to lookup. The low freq terms can afford a higher seek time since it'll be so fast to enum the docs.\n{quote}\n\nit would be great to come up with a heuristic that balances all 3 of these: \nBecause selecting % 32 is silly if it would give you \"abracadabra\" when the previous term is \"a\" and a fudge would give you a smaller index term (of course it depends too, on what the next index term would be, and the docfreq optimization too).\n\nIt sounds tricky, but right now we are just selecting index terms with no basis at all (essentially random). then we are trying to deal with bad selections by trimming wasted suffixes, etc.\n",
            "date": "2010-09-16T13:00:20.585+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "Attached patch -- factors out the opto into a separate method (indexedTermPrefixLength), and leaves the opto enabled by default.  Then I fixed TestExternalCodecs to override & disable the opto, and the test then passes w/ the seed that failed before.",
            "date": "2010-09-19T10:48:06.995+0000",
            "id": 15
        },
        {
            "author": "Simon Willnauer",
            "body": "Mike this looks good, just a little nit-picking\n\n{code}\nprotected int indexedTermPrefixLength(final BytesRef priorTerm, final BytesRef indexedTerm) {\n    // As long as codec sorts terms in unicode codepoint\n    // order, we can safely strip off the non-distinguishing\n    // suffix to save RAM in the loaded terms index.\n    final int idxOffset = indexedTerm.offset;\n    final int priorOffset = priorTerm.offset;\n    final int limit = Math.min(priorTerm.length, indexedTerm.length);\n    for(int byteIdx=0;byteIdx<limit;byteIdx++) {\n      if (priorTerm.bytes[priorOffset+byteIdx] != indexedTerm.bytes[idxOffset+byteIdx]) {\n        return byteIdx+1;\n      }\n    }\n    return Math.min(1+priorTerm.length, indexedTerm.length);;\n}\n{code}\n\nI know this is not super hot code but maybe we wanna safe the Math.min() method call if not necessary and public member deref is also an unnecessary indirection. On a low termIndexInterval and lot of long terms like shingles we might save some cycles :D\n\nI mean that would't make a big difference just mentioning it though.\n",
            "date": "2010-09-19T11:04:23.215+0000",
            "id": 16
        },
        {
            "author": "Michael McCandless",
            "body": "Sure -- we can fix those.  You wanna take a stab?",
            "date": "2010-09-19T12:20:16.616+0000",
            "id": 17
        },
        {
            "author": "Simon Willnauer",
            "body": "attached nit-picky iteration. Both failures from LUCENE-2622 pass now - I will commit soon if nobody objects",
            "date": "2010-09-19T12:48:01.306+0000",
            "id": 18
        },
        {
            "author": "Simon Willnauer",
            "body": "Committed revision 998675.",
            "date": "2010-09-19T14:35:43.432+0000",
            "id": 19
        }
    ],
    "component": "core/index",
    "description": "This idea came up when discussing w/ Robert how to improve our terms index...\n\nThe terms dict index today simply grabs whatever term was at a 0 mod 128 index (by default).\n\nBut this is wasteful because you often don't need the suffix of the term at that point.\n\nEG if the 127th term is aa and the 128th (indexed) term is abcd123456789, instead of storing that full term you only need to store ab.  The suffix is useless, and uses up RAM since we load the terms index into RAM.\n\nThe patch is very simple.  The optimization is particularly easy because terms are now byte[] and we sort in binary order.\n\nI tested on first 10M 1KB Wikipedia docs, and this reduces the terms index (tii) file from 3.9 MB -> 3.3 MB = 16% smaller (using StandardAnalyzer, indexing body field tokenized but title / date fields untokenized).  I expect on noisier terms dicts, especially ones w/ bad terms accidentally indexed, that the savings will be even more.\n\nIn the future we could do crazier things.  EG there's no real reason why the indexed terms must be regular (every N terms), so, we could instead pick terms more carefully, say \"approximately\" every N, but favor terms that have a smaller net prefix.  We can also index more sparsely in regions where the net docFreq is lowish, since we can afford somewhat higher seek+scan time to these terms since enuming their docs will be much faster.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2588",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "terms index should not store useless suffixes",
    "systemSpecification": true,
    "version": ""
}