{
    "comments": [
        {
            "author": "Jason Rutherglen",
            "body": "Solr used CHM as an LRU, however it turned out to be somewhat\nless than truly LRU? I'd expect Google Collections to offer a\nconcurrent linked hash map however no dice?\nhttp://code.google.com/p/google-collections/ \n\nMaybe there's a way to build a concurrent LRU using their CHM?",
            "date": "2009-11-16T23:27:00.036+0000",
            "id": 0
        },
        {
            "author": "Earwin Burrfoot",
            "body": "There's no such thing in Google Collections. However, look at this - http://code.google.com/p/concurrentlinkedhashmap/",
            "date": "2009-11-17T01:16:34.581+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "Since Solr already has already created a concurrent LRU, I think we simply reuse that?  Is there any reason not  to?\n\nI don't think we need absolutely truly LRU for the terminfo cache.",
            "date": "2009-11-17T19:14:34.117+0000",
            "id": 2
        },
        {
            "author": "Mark Miller",
            "body": "We should prob compare with google's (its apache 2 licensed, so why not)\n\nSolr has two synchronized lru caches - LRUCache, which is basically just a synchronized LinkedHashMap, and FastLRUCache which I believe tries to minimize the cost of gets - however, unless you have a high hit ratio, it was tested as slower than LRUCache.",
            "date": "2009-11-17T19:27:50.688+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "bq. We should prob compare with google's (its apache 2 licensed, so why not)\n\nWell, that's just hosted on code.google.com (ie it's not \"Google's\"), and reading its description it sounds sort of experimental (though they do state that they created a \"Production Version\").  It made me a bit nervous... however, it does sound people use it in \"production\".\n\nI think FastLRUCache is probably best for Lucene, because it scales up well w/ high number of threads?  My guess is it's slower cost for low hit rates is negligible to Lucene, but I'll run some perf tests.\n\nIt looks like ConcurrentLRUCache (used by FastLRUCache, but the latter does other solr-specific things) is the right low-level one to use for Lucene?",
            "date": "2009-11-18T01:35:49.541+0000",
            "id": 4
        },
        {
            "author": "Mark Miller",
            "body": "bq. Well, that's just hosted on code.google.com (ie it's not \"Google's\"), \n\nAh - got that vibe, but it didn't really hit me.\n\nbq. though they do state that they created a \"Production Version\"\n\nRight - thats what I was thinking we might try. Though the whole, trying this from scratch to learn is a bit scary too ;) But hey, I'm not recommending, just perhaps trying it.\n\nbq. I think FastLRUCache is probably best for Lucene, because it scales up well w/ high number of threads?\n\nIndeed - though if we expect a low hit ratio, we might still compare it with regular old synchronized LinkedHashMap to be sure. In certain cases, puts become quite expensive I think.\n\nbq. It looks like ConcurrentLRUCache (used by FastLRUCache, but the latter does other solr-specific things) is the right low-level one to use for Lucene?\n\nRight.",
            "date": "2009-11-18T01:51:35.421+0000",
            "id": 5
        },
        {
            "author": "Mark Miller",
            "body": "When/If yonik finally pops up here, he will have some good info to add I think.",
            "date": "2009-11-18T01:56:36.188+0000",
            "id": 6
        },
        {
            "author": "Uwe Schindler",
            "body": "Should this ConcurrentLRUCache not better be fitted into the o.a.l.util.cache package?\n\nAbout the Solr implementation: The generification has a \"small\" problem: get(), contains(), remove() and other by-key-querying methods should use Object as type for the key, not the generic K, because it is not bad to test with contains any java type (it would just return false). The sun generic howto explains that, also this one: [http://smallwig.blogspot.com/2007/12/why-does-setcontains-take-object-not-e.html]\n\nVery funny video about that: [http://www.youtube.com/watch?v=wDN_EYUvUq0] (explaination starts at 4:35)",
            "date": "2009-11-18T07:40:19.817+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "I'll work out a simple perf test to compare the options...",
            "date": "2009-11-18T13:15:42.805+0000",
            "id": 8
        },
        {
            "author": "Earwin Burrfoot",
            "body": "> Well, that's just hosted on code.google.com (ie it's not \"Google's\"), and reading its description it sounds sort of experimental (though they do state that they created a \"Production Version\"). It made me a bit nervous... however, it does sound people use it in \"production\".\n\nI run it in production for several months (starting from 'experimental' version) as a cache for Filters. No visible problems.",
            "date": "2009-11-18T16:18:02.501+0000",
            "id": 9
        },
        {
            "author": "Yonik Seeley",
            "body": "The Solr one could be simplified a lot for Lucene... no need to keep some of the statistics and things like \"isLive\".\n\nTesting via something like the double barrel approach will be tricky.  The behavior of ConcurrentLRUCache (i.e. the cost of puts) depends on the access pattern - in the best cases, a single linear scan would be all that's needed.  In the worst case, a subset of the  map needs to go into a priority queue.  It's all in markAndSweep... that's my monster - let me know if the comments don't make sense.\n\nHow many entries must be removed to be considered a success also obviously affects whether a single linear scan is enough.  If that's often the case, some other optimizations can be done such as not collecting the entries for further passes:\n{code}\n          // This entry *could* be in the bottom group.\n          // Collect these entries to avoid another full pass... this is wasted\n          // effort if enough entries are normally removed in this first pass.\n          // An alternate impl could make a full second pass.\n{code}",
            "date": "2009-11-18T16:29:02.277+0000",
            "id": 10
        },
        {
            "author": "Yonik Seeley",
            "body": "Here's a simplified version of Solr's ConcurrentLRUCache.",
            "date": "2009-11-18T17:53:44.515+0000",
            "id": 11
        },
        {
            "author": "Uwe Schindler",
            "body": "Looks good! Can this cache subclass the abstract (Map)Cache; it is in the correct package but does not subclass Cache?",
            "date": "2009-11-18T18:02:56.258+0000",
            "id": 12
        },
        {
            "author": "Yonik Seeley",
            "body": "Here's a new version extending Cache<K,V>",
            "date": "2009-11-18T18:37:52.917+0000",
            "id": 13
        },
        {
            "author": "Uwe Schindler",
            "body": "As PriorityQueue is generified since Lucene 3.0, I added missing generics. The class now compiles without unchecked warnings. I also removed lots of casts and parameterized the missing parts. Also added K type for inner map.\n\nNice work, even if I do not understand it completely :-)",
            "date": "2009-11-18T19:26:46.989+0000",
            "id": 14
        },
        {
            "author": "Uwe Schindler",
            "body": "Sorry a small problem with cast. Will upload new patch, soon.",
            "date": "2009-11-18T19:40:07.004+0000",
            "id": 15
        },
        {
            "author": "Yonik Seeley",
            "body": "New patch attached - while refreshing my memory on the exact algorithm, I noticed a bug :-)\nThings won't work well after 2B accesses since Integer.MAX_VALUE is used instead of Long.MAX_VALUE.\nNeed to go fix Solr now too :-)",
            "date": "2009-11-18T19:44:05.396+0000",
            "id": 16
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nThings won't work well after 2B accesses since Integer.MAX_VALUE is used\n{quote}\n\nFrom ReentrantLock javadocs:\n\"This lock supports a maximum of 2147483648 recursive locks by the same thread.\"\n\nI think you only use the lock for markAndSweep and everything else uses atomics, but ConcurrentHashMap uses ReentrantLocks internally for each segment. So overall, things wil probably run longer than 2B ops, but not sure how long.",
            "date": "2009-11-18T20:05:12.670+0000",
            "id": 17
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Yonik, thaks, that you used my class, but I found one type erasure problem in the PQueue, because thee Heap is erasured to Object[] by javac. The getValues() tries to cast this array -> ClassCastException. This is described here: http://safalra.com/programming/java/wrong-type-erasure/\nThe same happens in myInsertWithOverflow().\n\nWill fix.",
            "date": "2009-11-18T20:06:06.776+0000",
            "id": 18
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. \"This lock supports a maximum of 2147483648 recursive locks by the same thread.\"\n\nI read this as a maximum of recursive locks (which this class won't do at all)... not the total number of times one can successfully lock/unlock the lock.\n\nThis cache impl should be able to support 1B operations per second for almost 300 years (i.e. the time it would take to overflow a long).",
            "date": "2009-11-18T20:11:54.241+0000",
            "id": 19
        },
        {
            "author": "Paul Smith",
            "body": "bq. This cache impl should be able to support 1B operations per second for almost 300 years (i.e. the time it would take to overflow a long).\n\nHopefully Sun has released Java 7 by then. :)",
            "date": "2009-11-18T20:16:56.341+0000",
            "id": 20
        },
        {
            "author": "Uwe Schindler",
            "body": "Patch that fixes the bug in javac with typed arrays (because of that it does not allow generic array creation - the problem is that heap is a generic array in PQ, but implemented as Object[]).\n\nI fixed the PQueue by returning a List<CacheEntry<K,V>> values() and also made the private maxSize in the PriorityQueue protected. So it does not need to implement an own insertWithOverflow. As this class moves to Lucene Core, we should not make such bad hacks. \n\nWe need a good testcase for the whole cache class. It was hard to me to find a good test that hits the PQueue at all (its only used in special cases). Hard stuff :(",
            "date": "2009-11-18T20:25:06.636+0000",
            "id": 21
        },
        {
            "author": "Uwe Schindler",
            "body": "Updated patch, adds missing @Overrides, we added in 3.0 and also makes the private PQ implement Iterable, the markAndSweep code is now synactical sugar :-)",
            "date": "2009-11-19T08:51:50.304+0000",
            "id": 22
        },
        {
            "author": "Michael McCandless",
            "body": "First cut at a benchmark.  First, download\nhttp://concurrentlinkedhashmap.googlecode.com/files/clhm-production.jar\nand put into your lib subdir, then run \"ant -lib\nlib/clhm-production.jar compile-core\", then run it something like\nthis:\n\n{code}\njava -server -Xmx1g -Xms1g -cp build/classes/java:lib/clhm-production.jar org.apache.lucene.util.cache.LRUBench 4 5.0 0.0 1024 1024\n{code}\n\nThe args are:\n\n  * numThreads\n\n  * runSec\n\n  * sharePct -- what %tg of the terms should be shared b/w the threads\n\n  * cacheSize\n\n  * termCountPerThread -- how many terms each thread will cycle through\n\nThe benchmark first sets up arrays of strings, per thread, based\ntermsCountPerThread & sharePct.  Then each thread steps through the\narray, and for each entry, tries to get the string, and if it's not\npresent, puts it.  It records the hit & miss count, and prints summary\nstats in the end, doing 3 rounds.\n\nTo mimic Lucene, each entry is tested twice in a row, ie, the 2nd time\nwe test the entry, it should be a hit.  Ie we expect a hit rate of 50%\nif sharePct is 0.\n\nHere's my output from the above command line, using Java 1.6.0_14 (64\nbit) on OpenSolaris:\n\n{code}\nnumThreads=4 runSec=5.0 sharePct=0.0 cacheSize=1024 termCountPerThread=1024\n\nLRU cache size is 1024; each thread steps through 1024 strings; 0 of which are common\n\nround 0\n  sync(LinkedHashMap): Mops/sec=2.472 hitRate=50.734\n  DoubleBarreLRU: Mops/sec=20.502 hitRate=50\n  ConcurrentLRU: Mops/sec=17.936 hitRate=84.409\n  ConcurrentLinkedHashMap: Mops/sec=1.248 hitRate=50.033\n\nround 1\n  sync(LinkedHashMap): Mops/sec=2.766 hitRate=50.031\n  DoubleBarreLRU: Mops/sec=17.66 hitRate=50\n  ConcurrentLRU: Mops/sec=17.82 hitRate=83.726\n  ConcurrentLinkedHashMap: Mops/sec=1.266 hitRate=50.331\n\nround 2\n  sync(LinkedHashMap): Mops/sec=2.714 hitRate=50.168\n  DoubleBarreLRU: Mops/sec=17.912 hitRate=50\n  ConcurrentLRU: Mops/sec=17.866 hitRate=84.156\n  ConcurrentLinkedHashMap: Mops/sec=1.26 hitRate=50.254\n{code}\n\nNOTE: I'm not sure about the correctness of DoubleBarrelLRU -- I just\nquickly wrote it.\n\nAlso, the results for ConcurrentLRUCache are invalid (its hit rate is\nway too high) -- I think this is because its eviction process can take\na longish amount of time, which temporarily allows the map to hold way\ntoo many entries, and means it's using up alot more transient RAM than\nit should.\n\nIn theory DoubleBarrelLRU should be vulnerable to the same issue, but\nin practice it seems to affect it much less (I guess because\nCHM.clear() must be very fast).\n\nI'm not sure how to fix the benchmark to workaround that... maybe we\nbring back the cleaning thread (from Solr's version), and give it a\nhigh priority?\n\nAnother idea: I wonder whether a simple cache-line like cache would be\nsufficient.  Ie, we hash to a fixed slot and we evict whatever is\nthere.\n",
            "date": "2009-11-20T14:39:23.709+0000",
            "id": 23
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. Also, the results for ConcurrentLRUCache are invalid (its hit rate is\nway too high) - I think this is because its eviction process can take\na longish amount of time, which temporarily allows the map to hold way\ntoo many entries, and means it's using up alot more transient RAM than\nit should.\n\nYep - there's no hard limit.  It's not an issue in practice in Solr since doing the work to generate a new entry to put in the cache is much more expensive than cache cleaning (i.e. generation will never swamp cleaning).  Seems like a realistic benchmark would do some amount of work on a cache miss?  Or perhaps putting it in lucene and doing real benchmarks?\n\nbq. Another idea: I wonder whether a simple cache-line like cache would be sufficient. Ie, we hash to a fixed slot and we evict whatever is\nthere.\n\nWe need to balance the overhead of the cache with the hit ratio and the cost of a miss. for the String intern cache, the cost of a miss is very low, hence lowering overhead but giving up hit ratio is the right trade-off.  For this term cache, the cost of a miss seems relatively high, and warrants increasing overhead to increase the hit ratio.\n",
            "date": "2009-11-20T22:31:20.897+0000",
            "id": 24
        },
        {
            "author": "Yonik Seeley",
            "body": "Aside: a singe numeric range query will be doing many term seeks (one at the start of each enumeration).  It doesn't look like these will currently utilize the cache - can someone refresh my memory on why this is?  We should keep the logic that prevents the cache while iterating over terms with a term enumerator, but it seems like using the cache for the initial seek would be nice.",
            "date": "2009-11-20T23:03:43.227+0000",
            "id": 25
        },
        {
            "author": "Uwe Schindler",
            "body": "The initial seek should really be optimized, this also affects the new AutomatonTermEnum for the future of RegEx queries, WildCardQueries and maybe FuzzyQueries with DFAs. With the automaton enum, depending of the DFA, there can be lot's of seeks (LUCENE-1606).",
            "date": "2009-11-20T23:11:33.407+0000",
            "id": 26
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. a singe numeric range query will be doing many term seeks (one at the start of each enumeration). It doesn't look like these will currently utilize the cache - can someone refresh my memory on why this is?\n\nYou're right -- here's the code/comment:\n\n{code}\n  /** Returns an enumeration of terms starting at or after the named term. */\n  public SegmentTermEnum terms(Term term) throws IOException {\n    // don't use the cache in this call because we want to reposition the\n    // enumeration\n    get(term, false);\n    return (SegmentTermEnum)getThreadResources().termEnum.clone();\n  }\n{code}\n\nI think this is because \"useCache\" (the 2nd arg to get) is overloaded\n-- if you look at get(), if useCache is true and you have a cache hit,\nit doesn't do it's \"normal\" side-effect of repositioning the\nthread-private TermEnum.  So you'd get incorrect results.\n\nIf get had a 2nd arg \"repositionTermEnum\", to decouple caching from\nrepositioning, then we could make use of the cache for NRQ (& soon\nAutomatonTermEnum as well), though, this isn't so simple because the\ncache entry (just a TermInfo) doesn't store the term's ord.  And we\ndon't want to add ord to TermInfo since, eg, this sucks up alot of\nextra RAM storing the terms index.  Probably we should make a new\nclass that's used for caching, and not reuse TermInfo.\n\nThis was also done before NumericRangeQuery, ie, all MTQs before NRQ\ndid a single seek.\n\nBTW the flex branch fixes this -- TermsEnum.seek always checks the\ncache.\n",
            "date": "2009-11-21T10:47:18.181+0000",
            "id": 27
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nbq. Also, the results for ConcurrentLRUCache are invalid (its hit rate is way too high) - I think this is because its eviction process can take a longish amount of time, which temporarily allows the map to hold way too many entries, and means it's using up alot more transient RAM than it should.\n\nYep - there's no hard limit. It's not an issue in practice in Solr since doing the work to generate a new entry to put in the cache is much more expensive than cache cleaning (i.e. generation will never swamp cleaning).  Seems like a realistic benchmark would do some amount of work on a cache miss? Or perhaps putting it in lucene and doing real benchmarks?\n{quote}\n\nI agree the test is synthetic, so the blowup we're seeing is a worse\ncase sitatuion, but are you really sure this can never be hit in\npractice?\n\nEG as CPUs gain more and more cores... it becomes more and more\npossible with time that the 1 thread that's trying to do the cleaning\nwill be swamped by the great many threads generating.  Then if the CPU\nis over-saturated (too many threads running), that 1 thread doing the\ncleaning only gets slices of CPU time vs all the other threads that\nmay be generating...\n\nIt makes me nervous using a collection that, in the \"perfect storm\",\nsuddenly consumes way too much RAM.  It's a leaky abstraction.\n\nThat said, I agree the test is obviously very synthetic.  It's not\nlike a real Lucene installation will be pushing 2M QPS through Lucene\nany time soon...\n\nBut still I'm more comfortable w/ the simplicity of the double-barrel\napproach.  In my tests its performance is in the same ballpark as\nConcurrentLRUCache; it's much simpler; and the .clear() calls appear\nin practice to very quickly free up the entries.\n\n{quote}\nbq. Another idea: I wonder whether a simple cache-line like cache would be sufficient. Ie, we hash to a fixed slot and we evict whatever is there.\n\nWe need to balance the overhead of the cache with the hit ratio and the cost of a miss. for the String intern cache, the cost of a miss is very low, hence lowering overhead but giving up hit ratio is the right trade-off. For this term cache, the cost of a miss seems relatively high, and warrants increasing overhead to increase the hit ratio.\n{quote}\n\nOK I agree.\n\nYet another option... would be to create some sort of \"thread-private\nQuery scope\", ie, a store that's created & cleared per-Query where\nLucene can store things.  When a Term's info is retrieved, it'd be\nstored here, and then that \"query-private\" cache is consulted whenever\nthat Term is looked up again within that query.  This would be the\n\"perfect cache\" in that a single query would never see its terms\nevicted due to other queries burning through the cache...\n\nThough, net/net I suspect the overhead of creating/pulling from this\nnew cache would just be an overall search slowdown in practice.\n",
            "date": "2009-11-21T11:03:39.670+0000",
            "id": 28
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. I agree the test is synthetic, so the blowup we're seeing is a worse case sitatuion, but are you really sure this can never be hit in practice?\n\nI'm personally comfortable that Solr isn't going to hit this for it's uses of the cache... it's simply the relative cost of generating a cache entry vs doing some cleaning.\n\nbq. But still I'm more comfortable w/ the simplicity of the double-barrel approach. In my tests its performance is in the same ballpark as ConcurrentLRUCache;\n\nBut it wouldn't be the same performance in Lucene - a cache like LinkedHashMap would achieve a higher hit rate in real world scenarios.\n",
            "date": "2009-11-21T14:10:15.961+0000",
            "id": 29
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. BTW the flex branch fixes this - TermsEnum.seek always checks the cache.\n\nCan we fix this for trunk, too? But I think *this* issue talks about trunk.",
            "date": "2009-11-21T20:08:35.829+0000",
            "id": 30
        },
        {
            "author": "Robert Muir",
            "body": "Hi, I applied automaton patch and its benchmark (LUCENE-1606) against the flex branch, and kept with the old TermEnum api.\n\nI tested two scenarios, an old index created with 3.0 (trunk) and a new index created with flex branch.\nin both cases, its slower than trunk, but I assume this is due to flex branch not being optimized yet?... (last i saw it used new String() placeholder for utf conversion)\n\nbut i think it is fair to compare the flex branch with itself, with old idx versus new idx. I can only assume with a new idx it is using the caching.\nthese numbers are stable on HEAD and do not deviate much.\nfeel free to look at the benchmark code over there and suggest improvements if you think there is an issue with it.\n\n||Pattern||Iter||AvgHits||AvgMS (old idx)||AvgMS (new idx)||\n|N?N?N?N|10|1000.0|86.6|70.2|\n|?NNNNNN|10|10.0|3.0|2.0|\n|??NNNNN|10|100.0|12.5|7.2|\n|???NNNN|10|1000.0|86.9|34.8|\n|????NNN|10|10000.0|721.2|530.5|\n|NN??NNN|10|100.0|8.3|4.0|\n|NN?N*|10|10000.0|149.1|143.2|\n|?NN*|10|100000.0|1061.4|836.7|\n|*N|10|1000000.0|16329.7|11480.0|\n|NNNNN??|10|100.0|2.7|2.2|\n\n",
            "date": "2009-11-21T20:28:35.115+0000",
            "id": 31
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nbq. BTW the flex branch fixes this - TermsEnum.seek always checks the cache.\n\nCan we fix this for trunk, too? But I think this issue talks about trunk.\n{quote}\n\nRight, this issue is about trunk (not flex API).  I think we could fix this (see my suggestions above), basically decoupling \"useCache\" from \"seekTheEnum\"... but we have to fix the terminfo cache to also store the term's ord.  I'll try out this approach...",
            "date": "2009-11-22T10:35:22.619+0000",
            "id": 32
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. in both cases, its slower than trunk, but I assume this is due to flex branch not being optimized yet?\n\nThe automaton benchmark looks great -- I'll dig into why the flex branch\nis slower in both of these cases.\n\nThe first case tests old API on top of an old index, which I'm\nsurprised to see not matching trunk's performance.  The flex changes\nare supposed to \"optimize\" that case by directly using the old (trunk)\ncode.\n\nThe second test tests old API emulated over a flex index, which I'm\nalso surprised to see is not faster than trunk -- there must be\nsomething silly going on in the API emulation.\n\nI'll dig...\n\nWhen I tested MTQs (TermRangeQuery, WildcardQuery), using flex API on\nflex index, they were reasonably faster, so I'll also try to get\nautomaton's FilteredTermEnum cutover to the flex API, and test that.\n",
            "date": "2009-11-22T10:38:13.200+0000",
            "id": 33
        },
        {
            "author": "Michael McCandless",
            "body": "Attached patch; all tests pass:\n\n  * Switches the terms dict cache away from per-thread cache to shared\n    (DoubleBarrelLRU) cache\n\n  * Still uses the cache when seeking the term enum\n\nHowever, I'm baffled: I re-ran the BenchWildcard test and saw no\nmeasurable improvement in ????NNN query (yet, I confirmed it's now\nstoring into and then hitting on the cache), but I did see a gain in\nthe *N query (from ~4300 msec before to ~3500 msec) which I can't\nexplain because that query doens't use the cache at all (just the\nlinear scan).  I'm confused....\n\nRobert maybe you can try this patch plus automaton patch and see if\nyou see this same odd behavior?\n",
            "date": "2009-11-23T11:44:07.698+0000",
            "id": 34
        },
        {
            "author": "Michael McCandless",
            "body": "I ended up subclassing TermInfo (had to remove its \"final\" modifier) to make a TermInfoAndOrd class that adds \"int termOrd\", and then fixed TermInfosReader.get to create that class and put/get into cache.\n\nNow, in get we always consult the cache, and store into it for the non-sequential case, but now the 2nd boolean arg is \"mustSeekEnum\".  So if we have a cache hit, but must seek the enum, we fall through and do the existing scan/seek logic, but avoid the binary search through the terms index since we can use the ord from the cache hit.",
            "date": "2009-11-23T11:48:39.766+0000",
            "id": 35
        },
        {
            "author": "Robert Muir",
            "body": "bq. Robert maybe you can try this patch plus automaton patch and see if you see this same odd behavior?\n\nconfirmed, though on my machine, it is 4 second avg *N versus 6 second avg *N :)\n\nI havent looked at the code, but fyi, even the smart mode is always \"in term order\" traversal, its just skipping over terms.\nI think numeric range might do \"out of order\"? Uwe can confirm.\nI don't know if this matters, either.\n",
            "date": "2009-11-23T12:31:34.761+0000",
            "id": 36
        },
        {
            "author": "Uwe Schindler",
            "body": "Have you tried aut with NRQ, too? If not I will run a comparison with a integer index before/after this patch and measure query times. For lower precSteps it should get faster, as seeking the TermEnum is optimized.\n\nTo your patch: Looks good, I would only add @Overrides to the DoubleBarrelCache. What do we do with Yonik/mine's cache?",
            "date": "2009-11-23T12:33:07.830+0000",
            "id": 37
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. think numeric range might do \"out of order\"? Uwe can confirm.\n\nNo its also in order. It starts with highest precision (first lower end, then upper end sub-range), which has a shift value of 0. This is smaller that lower prec terms with a bigger shift value that come later. And of course each sub-range is in ascending order because the terms are :-)",
            "date": "2009-11-23T12:41:50.312+0000",
            "id": 38
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, thanks. so both the enums behave in a very similar way.",
            "date": "2009-11-23T12:42:48.469+0000",
            "id": 39
        },
        {
            "author": "Michael McCandless",
            "body": "bq. To your patch: Looks good, I would only add @Overrides to the DoubleBarrelCache.\n\nAhh right will do -- not quite in Java 5 mode yet ;)\n\nbq. What do we do with Yonik/mine's cache?\n\nSolr's ConcurrentLRUCache makes me somewhat nervous, that it can blow up under high (admittedly, rather synthetic, by today's standards) load.\n\nbq. Have you tried aut with NRQ, too?\n\nI haven't; that'd be great if you could & report back.\n\n{quote}\nbq. Robert maybe you can try this patch plus automaton patch and see if you see this same odd behavior?\n\nconfirmed, though on my machine, it is 4 second avg *N versus 6 second avg *N \n{quote}\n\nWeird -- I can't explain why this full scan is faster but the skipping scan is not.\n\nbq. I havent looked at the code, but fyi, even the smart mode is always \"in term order\" traversal, its just skipping over terms.\n\nRight -- but that skipping variant is now pulling from cache.  Let's see what NRQ results look like... though it does quite a bit less seeking than eg the ????NNN query.",
            "date": "2009-11-23T13:06:38.723+0000",
            "id": 40
        },
        {
            "author": "Uwe Schindler",
            "body": "I updated the patch to add overrides. I also had to add one SupressWarnings, because the get() method does an unchecked cast (because it modifies the map, which is not in the contract of get(), but it's safe, because it only adds the key to the second map, if the first map already contains it, and therefore the key has correct type).\n\nI will start now my tests with NRQ.",
            "date": "2009-11-23T13:21:53.358+0000",
            "id": 41
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Uwe!\n\nI attached another one: made DBLRU final, tweaked javadocs, fixed spelling in the saturation comment, add you guys to the CHANGES entry.",
            "date": "2009-11-23T13:47:26.739+0000",
            "id": 42
        },
        {
            "author": "Yonik Seeley",
            "body": "What about replacing the expensive division with a comparison with zero?\n\nif (putCount.decrementAndGet()==0) {\n  secondary.clear();\n  swapCount.getAndIncrement();\n  putCount.set(maxSize);\n}\n\nAlso, I don't think there's a need for swapCount?  A simple \"volatile boolean swapped\" should do?\n",
            "date": "2009-11-23T14:05:28.214+0000",
            "id": 43
        },
        {
            "author": "Uwe Schindler",
            "body": "I tested with an 5 mio doc index containing trie ints, but it seems that trie does not really profit from the seeking cache. With the default precStep of 4 no difference (max. 16 seeks per query), and with precStep of 1 (max. 64 seeks per query) it was even a little bit slower on average (???). The test compares also with FieldCacheRangeFilter which is always faster (because no deletes, optimized index), also the field cache loading time did not really change (linear scan in term enum).\n\nPrecisionStep: 4\ntrunk:\nloading field cache time: 6367.667678 ms\navg number of terms: 68.1\nTRIE:       best time=6.323709 ms; worst time=414.367469 ms; avg=201.18463369999998 ms; sum=32004735\nFIELDCACHE: best time=64.770523 ms; worst time=265.487652 ms; avg=155.5479675 ms; sum=32004735\n\npatch:\nloading field cache time: 6295.055377 ms\navg number of terms: 68.1\nTRIE:       best time=5.288102 ms; worst time=415.290771 ms; avg=195.72079685 ms; sum=32004735\nFIELDCACHE: best time=65.511957 ms; worst time=202.482438 ms; avg=138.69083925 ms; sum=32004735\n\n---\n\nPrecisionStep: 1\ntrunk:\nloading field cache time: 6416.105399 ms\navg number of terms: 19.85\nTRIE:       best time=6.51228 ms; worst time=410.624255 ms; avg=192.33796475 ms; sum=32002505\nFIELDCACHE: best time=65.349088 ms; worst time=211.308979 ms; avg=143.71657580000002 ms; sum=32002505\n\npatch:\nloading field cache time: 6809.792026 ms\navg number of terms: 19.85\nTRIE:       best time=6.814832 ms; worst time=436.396525 ms; avg=205.6526038 ms; sum=32002505\nFIELDCACHE: best time=64.939539 ms; worst time=277.474371 ms; avg=142.58939345 ms; sum=32002505",
            "date": "2009-11-23T14:10:47.577+0000",
            "id": 44
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nWhat about replacing the expensive division with a comparison with zero?\n{code}\nif (putCount.decrementAndGet()==0) { secondary.clear(); swapCount.getAndIncrement(); putCount.set(maxSize); }\n{code}\n{quote}\n\nGood idea!  I'll do that.\n\nbq. Also, I don't think there's a need for swapCount? A simple \"volatile boolean swapped\" should do?\n\nExcellent -- I'll do that too.\n",
            "date": "2009-11-23T16:27:18.460+0000",
            "id": 45
        },
        {
            "author": "Michael McCandless",
            "body": "Uwe, why do you see so much variance in your times?  EG best for trie is 5-6 msec, but avg is ~190-205 msec.",
            "date": "2009-11-23T16:30:17.587+0000",
            "id": 46
        },
        {
            "author": "Uwe Schindler",
            "body": "Because of random ranges on the whole range. If you only request a very short range, it is faster (less seeks because maybe only highest precision affected for very short ranges) vs a full range query which may seek the maximum count. It is reproduceable, because the random seed was identical.",
            "date": "2009-11-23T16:34:07.324+0000",
            "id": 47
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Because of random ranges on the whole range. \n\nAhh, OK.\n\nI wonder if your test is getting any cache hits at all -- if you do random ranges, and never repeat queries, then likely your hit rate is quite low?",
            "date": "2009-11-23T17:59:54.268+0000",
            "id": 48
        },
        {
            "author": "Michael McCandless",
            "body": "New patch, folding in Yonik's suggestions, adding a unit test (carried\nover from TestSimpleLRUCache -- I'll \"svn mv\" when I commit it), and\ndeprecating SimpleLRUCache.\n",
            "date": "2009-11-23T18:15:16.602+0000",
            "id": 49
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. I wonder if your test is getting any cache hits at all - if you do random ranges, and never repeat queries, then likely your hit rate is quite low?\n\nI am quite sure that also Robert's test is random (as he explained).\n\nI fixed the test to only test few queries and repeat them quite often. For precStep=4 and long values, I got about 28 seeks per query, but there was no speed improvement. Maybe 28 seeks / query is too less for an effect. The number of terms seen per query was 70, so about 2.5 terms/seek which is typical for precStep=4 with this index value density (5 Mio random number in the range 2^-63..2^63). It is also important, that the random ranges hit many documents (in avg 1/3 of all docs), so most time in my opinion is used in collecting the results. Maybe I should try shorter and limited ranges.\n\nRobert: How many term enum seeks did your queries produce?\n\nCurrently I am indexing a 100 Mio docs, precStep=1, long values index (64 terms per doc). Let's see what happens here.\n\nIf you deprecate SimpleLRUCache, you can also deprecate the MapCache abstract super class. But I wouldn't like to deprecate these classes, as I for myself use them in my own code for e.g. caching queries etc. And even if you deprecate the Map, why remove the tests, they should stay alive until the class is removed?\n\nUwe",
            "date": "2009-11-23T20:12:25.624+0000",
            "id": 50
        },
        {
            "author": "Uwe Schindler",
            "body": "I changed my benchmark to better show the seek caching effect. For NRQ the overall improvement has no neglectible effect.\n\nI chenged the rewrite mode of the NRQ to SCORING_BOOLEAN_QUEY and then just rewrote the queries to BQ and measured time. So no TermDocs/Collecting was in effect:\n\ntrunk: avg number of terms: 68.537; avg seeks=28.838; best time=1.022756 ms; worst time=17.036802 ms; avg=1.8388833272 ms\npatch: avg number of terms: 68.537; avg seeks=28.838; best time=1.066616 ms; worst time=12.80917 ms; avg=1.6932529156 ms\n\nYou see the effect of the caching. The code ran 5000 rewrites with each query repeated 20 times.",
            "date": "2009-11-23T21:10:26.081+0000",
            "id": 51
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I am quite sure that also Robert's test is random (as he explained).\n\nIt's not random -- it's the specified pattern, parsed to\nWildcardQuery, run 10 times, then take best or avg time.\n\n{quote}\nI fixed the test to only test few queries and repeat them quite often. For precStep=4 and long values, I got about 28 seeks per query, but there was no speed improvement. Maybe 28 seeks / query is too less for an effect. The number of terms seen per query was 70, so about 2.5 terms/seek which is typical for precStep=4 with this index value density (5 Mio random number in the range 2^-63..2^63). It is also important, that the random ranges hit many documents (in avg 1/3 of all docs), so most time in my opinion is used in collecting the results. Maybe I should try shorter and limited ranges.\n{quote}\n\nOK... it sounds like the differences may simply be in the noise for NRQ.\n\nbq. If you deprecate SimpleLRUCache, you can also deprecate the MapCache abstract super class. But I wouldn't like to deprecate these classes, as I for myself use them in my own code for e.g. caching queries etc.\n\nHmm... I felt like because nothing in Lucene uses SimpleLRUCache\nanymore, we should deprecate & remove it.  I don't think we should be\nin the business of creating/exporting (as public classes) such\ncollections, unless we continue to use them.\n\nI even wonder why we don't put these classes into oal.index, and make\nthem package private.  Ie, I think we make them public only to share\nthem across packages within lucene, not because we want/expect apps to\nconsume them.  The term \"public\" is heavily overloaded,\nunfortunately.\n\nAlso, I already put a strong note to this effect in\nDoubleBarrelLRUCache, ie we reserve future right to up and remove the\nclass.\n\nbq. And even if you deprecate the Map, why remove the tests, they should stay alive until the class is removed?\n\nOh good point -- I'll resurrect & keep it.\n",
            "date": "2009-11-23T21:17:17.587+0000",
            "id": 52
        },
        {
            "author": "Michael McCandless",
            "body": "bq. You see the effect of the caching. The code ran 5000 rewrites with each query repeated 20 times.\n\nOK so it sounds like NRQ in practice won't see a boost from this (it's too good already -- does too little seeking ;), though with this [contrived] test you were able to show the new seek cache is having a small positive effect.",
            "date": "2009-11-23T21:18:23.201+0000",
            "id": 53
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. For NRQ the overall improvement has no neglectible effect. \n\nPerhaps it's just the ratio of seeks to TermDocs.next() for the relatively large indexes you were testing against?",
            "date": "2009-11-23T21:27:05.555+0000",
            "id": 54
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Perhaps it's just the ratio of seeks to TermDocs.next() for the relatively large indexes you were testing against?\n\nExactly thats the difference to AutomatonQuery: The AutomatonTermEnum does lot's of seeking in the TermEnum to not scan all terms. But each term normally has the same (small) docFreq. For NRQ, some terms have a very high docFreq (e.g. up to 100,000 for this large index in lowest precision), so most of the time in the query is enumerating docs.",
            "date": "2009-11-23T22:24:03.523+0000",
            "id": 55
        },
        {
            "author": "Uwe Schindler",
            "body": "Just one question: The cache is initialized with max 1024 entries. Why that number. If we share the cache between multiple threads, maybe we should raise the max size. Or make it configureable?\n\nThe entries in the cache are not very costly, why not use 8192 or 16384, MTQs would be happy with that?",
            "date": "2009-11-24T10:25:28.519+0000",
            "id": 56
        },
        {
            "author": "Michael McCandless",
            "body": "Well, I just kept 1024 since that's what we currently do ;)\n\nOK I just did a rough tally -- I think we're looking at ~100 bytes (on\n32 bit JRE) per entry, including CHMs HashEntry, array in CHM,\nTermInfoAndOrd, Term & its String text.\n\nNot to mention DBLRU has 2X multiplier at peak, so 200 bytes.\n\nSo at 1024 we're looking at ~200KB peak used by this cache already,\nper segment which is able to saturate that cache... so for a 20\nsegment index you're at ~4MB additional RAM consumed... so I don't\nthink we should increase this default.\n\nAlso, I don't think this cache is/should be attempting to achieve a\nhigh hit rate *across* queries, only *within* a single query when that\nquery resolves the Term more than once.\n\nI think caches that wrap more CPU, like Solr's query cache, are where\nthe app should aim for high hit rate.\n\nMaybe we should even decrease the default size here -- what's\nimportant is preventing in-fligh queries from evicting one another's\ncache entries.\n\nFor NRQ, 1024 is apparently already plenty big for that (relatively\nfew seeks occur).\n\nFor automaton query, which does lots of seeking, once flex branch\nlands there is no need for the cache (each lookup is done only once,\nbecause the TermsEnum actualEnum is able to seek).  Before flex lands,\nthe cache is important, but only for automaton query I think.\n\nAnd honestly I'm still tempted to do away with this cache altogether\nand create a \"query scope\", private to each query while it's running,\nwhere terms dict (and other places that need to, over time) could\nstore stuff.  That'd give a perfect within-query hit rate and wouldn't\ntie up any long term RAM...\n",
            "date": "2009-11-24T11:03:23.443+0000",
            "id": 57
        },
        {
            "author": "Uwe Schindler",
            "body": "{quote}\nAnd honestly I'm still tempted to do away with this cache altogether\nand create a \"query scope\", private to each query while it's running,\nwhere terms dict (and other places that need to, over time) could\nstore stuff. That'd give a perfect within-query hit rate and wouldn't\ntie up any long term RAM...\n{quote}\n\nWith Query Scope you mean a whole query, so not only a MTQ? If you combine multiple AutomatonQueries in a BooleanQuery it could also profit from the cache (as it is currently).\n\nI think until Flex, we should commit this and use the cache. When Flex is out, we may think of doing this different.",
            "date": "2009-11-24T11:17:41.079+0000",
            "id": 58
        },
        {
            "author": "Michael McCandless",
            "body": "bq. With Query Scope you mean a whole query, so not only a MTQ? If you combine multiple AutomatonQueries in a BooleanQuery it could also profit from the cache (as it is currently).\n\nRight, I think the top level query would open up the scope... and free it once it's done running.\n\nbq. I think until Flex, we should commit this and use the cache. When Flex is out, we may think of doing this different.\n\nOK let's go with the shared cache for now, and revisit once flex lands.  I'll open a new issue...\n\nBut should we drop cache to maybe 512?  Typing up 4MB RAM (with cache size 1024) for a \"normal\" index is kinda alot...",
            "date": "2009-11-24T11:43:02.277+0000",
            "id": 59
        },
        {
            "author": "Michael McCandless",
            "body": "OK I opened LUCENE-2093 to track the \"query private scope\" idea.",
            "date": "2009-11-24T11:45:27.452+0000",
            "id": 60
        },
        {
            "author": "Uwe Schindler",
            "body": "I would keep it as it is, because we already minimized memory requirements, because before the cache was per-thread.",
            "date": "2009-11-24T11:46:29.348+0000",
            "id": 61
        },
        {
            "author": "Robert Muir",
            "body": "i am still triyng to figure out the use case.\n\nbq. With Query Scope you mean a whole query, so not only a MTQ? If you combine multiple AutomatonQueries in a BooleanQuery it could also profit from the cache (as it is currently).\n\nisn't there a method i can use to force these to combine into one AutomatonQuery (I can use union, intersection, etc)?\nI haven't done this, but we shouldnt create a private scoped-cache for something like this?",
            "date": "2009-11-24T11:51:30.745+0000",
            "id": 62
        },
        {
            "author": "Uwe Schindler",
            "body": "...not only AutomatonQueries can be combined, they can also be combined with other queries and then make use of the cache.",
            "date": "2009-11-24T11:55:25.207+0000",
            "id": 63
        },
        {
            "author": "Robert Muir",
            "body": "Uwe i just wonder if the cache would in practice get used much. ",
            "date": "2009-11-24T12:00:29.422+0000",
            "id": 64
        },
        {
            "author": "Uwe Schindler",
            "body": "For testing we could add two AtomicIntegers to the cache that counts hits and requests to get a hit rate, only temporary to not affect performance.",
            "date": "2009-11-24T12:03:21.216+0000",
            "id": 65
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I would keep it as it is, because we already minimized memory requirements, because before the cache was per-thread.\n\nOK let's leave it at 1024, but with flex (which automaton query no longer needs the cache for), I think we should drop it and/or cutover to query-private scope.  I don't think sucking up 4 MB of RAM for this rather limited purpose is warranted.  I'll add a comment on LUCENE-2093.",
            "date": "2009-11-24T13:21:25.332+0000",
            "id": 66
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Uwe i just wonder if the cache would in practice get used much.\n\nThis cache (mapping Term -> TermInfo) does get used alot for \"normal\"\natomic queries we first hit the terms dict to get the docFreq (to\ncompute idf), then later hit it again with the exact same term, to\nget the TermDocs enum.\n\nSo, for these queries our hit rate is 50%, but, it's rather overkill\nto be using a shared cache for this (query-private scope is much\ncleaner).  EG a large automaton query running concurrently with other\nqueries could evict entries before they read the term the 2nd time.\n\nExisting MTQs (except NRQ) which seek once and then scan to completion\ndon't hit the cache (though, I think they do double-load each term,\nwhich is wasteful; likely this is part of the perf gains for flex).\n\nNRQ doens't do enough seeking wrt iterating/collecting the docs for\nthe cache to make that much a difference.\n\nThe upcoming automaton query should benefit.... however in testing we\nsaw only the full linear scan benefit, which I'm still needing to get\nto the bottom of.\n",
            "date": "2009-11-24T13:28:33.554+0000",
            "id": 67
        },
        {
            "author": "Robert Muir",
            "body": "Thanks mike, thats what I was missing\nhitting the terms dict twice in the common case explains it to me :)\n",
            "date": "2009-11-24T13:33:44.220+0000",
            "id": 68
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nbq. I am quite sure that also Robert's test is random (as he explained).\n\nIt's not random - it's the specified pattern, parsed to\nWildcardQuery, run 10 times, then take best or avg time.\n{quote}\n\nWoops -- I was wrong here -- Robert's test is random: on each iteration, it replaces any N's in the pattern w/ a random number 0-9.\n\nStill baffled on why the linear scan shows gains w/ the cache... digging.",
            "date": "2009-11-24T17:17:33.161+0000",
            "id": 69
        },
        {
            "author": "Robert Muir",
            "body": "bq. Woops - I was wrong here - Robert's test is random: on each iteration, it replaces any N's in the pattern w/ a random number 0-9.\n\nYeah, the terms are equally distributed 0000000-9999999 though, just a \"fill\"\nThe wildcard patterns themselves are filled with random numbers.\n\nThis is my basis for the new wildcard test btw, except maybe 1-10k, definitely want over 8192 :)\nunless you have better ideas?",
            "date": "2009-11-24T17:22:39.265+0000",
            "id": 70
        },
        {
            "author": "Michael McCandless",
            "body": "bq. This is my basis for the new wildcard test btw, except maybe 1-10k, definitely want over 8192\n\nSounds great :)",
            "date": "2009-11-24T17:31:49.045+0000",
            "id": 71
        },
        {
            "author": "Michael McCandless",
            "body": "OK as best I can tell, the reason why linear scan shows so much faster\nwith the new cache, is some kind of odd GC problem when you use\nLinkedHashMap.... the DoubleBarrelLRUCache doesn't tickle GC in this\nway.\n\nIf you turn on -verbose:gc when running the bench, you see this\nhealthy GC behavior during warmup (running wildcard \"*\"):\n\n{code}\n[GC 262656K->15659K(1006848K), 0.0357409 secs]\n[GC 278315K->15563K(1006848K), 0.0351360 secs]\n[GC 278219K->15595K(1006848K), 0.0150112 secs]\n[GC 278251K->15563K(1006848K), 0.0054310 secs]\n{code}\nAll minor collections, all fairly fast, all rather effective (~270 MB\ndown to ~15 MB).\n\nBut then when the test gets to the the *N query:\n\n{code}\n[GC 323520K->33088K(1022272K), 0.0377057 secs]\n[GC 338432K->78536K(990592K), 0.1830592 secs]\n[GC 344776K->118344K(1006336K), 0.1205320 secs]\n[GC 384584K->158080K(987264K), 0.2340810 secs]\n[GC 400640K->194264K(979136K), 0.2139520 secs]\n[GC 436824K->230488K(989760K), 0.2017131 secs]\n[GC 463192K->266501K(969152K), 0.1932188 secs]\n[GC 499205K->301317K(989632K), 0.1918106 secs]\n[GC 530437K->335541K(990080K), 0.1907594 secs]\n[GC 564661K->369749K(990528K), 0.1905007 secs]\n[GC 599445K->404117K(990208K), 0.1922657 secs]\n[GC 633813K->438477K(991680K), 0.1994350 secs]\n[GC 670157K->474250K(991040K), 0.2073795 secs]\n[GC 705930K->508842K(992832K), 0.2061273 secs]\n[GC 742570K->543770K(991936K), 0.1980306 secs]\n[GC 777498K->578634K(994560K), 0.1975961 secs]\n[GC 815818K->614010K(993664K), 0.2042480 secs]\n[GC 851194K->649434K(996096K), 0.1940145 secs]\n[GC 889754K->686551K(995264K), 0.1991030 secs]\n[Full GC 686551K->18312K(995264K), 0.1838671 secs]\n[GC 258632K->54088K(997120K), 0.0735258 secs]\n[GC 296456K->90280K(996288K), 0.1382187 secs]\n[GC 332648K->126512K(998592K), 0.1427443 secs]\n[GC 371888K->163096K(997824K), 0.1472803 secs]\n{code}\n\nThe minor collections are not nearly as effective -- way too many\nobjects are for some reason being marked as live (even though they are\nnot) and promoted to the older generation, thus making the minor\ncollection much more costly and also requiring major collection every\nso often.\n\nNow here's the really crazy thing: if I move the *N query up to be the\nfirst query the benchmark runs, GC is healthy:\n\n{code}\n[GC 323868K->17216K(1027840K), 0.0060598 secs]\n[GC 322496K->17128K(1006016K), 0.0062586 secs]\n[GC 322408K->17160K(1027712K), 0.0008879 secs]\n[GC 321672K->17192K(1027776K), 0.0003269 secs]\n[GC 321704K->18669K(1028608K), 0.0012964 secs]\n[GC 324205K->18741K(1027968K), 0.0104134 secs]\n[GC 324277K->18613K(1029632K), 0.0083720 secs]\n[GC 326261K->18677K(1029056K), 0.0003520 secs]\n{code}\n\nAnd the query runs about as fast as w/ the new cache.\n\nSo..... somehow, running the other queries sets object state up to\nconfuse GC later.  I'm pretty sure it's the linking that the\nLinkedHashMap (in SimpleLRUCache) is doing, because if I forcefully\nturn off all caching, GC acts healthy again, and that query runs as\nfast as it does w/ the patch.\n\nDoubleBarrelLRUCache doens't tickle GC in this way, so the *N query\nruns fast with it.\n\nSheesh!!\n",
            "date": "2009-11-24T18:23:28.929+0000",
            "id": 72
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote} And honestly I'm still tempted to do away with this\ncache altogether and create a \"query scope\", private to each\nquery while it's running, where terms dict (and other places\nthat need to, over time) could store stuff. That'd give a\nperfect within-query hit rate and wouldn't tie up any long term\nRAM... {quote}\n\nSounds better than the caching approach which if I recall\ncorrectly, Michael B. noted was kind of a hack (i.e. this isn't\nreally a cache, isn't it just a convenient way of recalling\nimmediately previously read data whose scope is really within\nthe query itself).",
            "date": "2009-11-24T18:24:54.891+0000",
            "id": 73
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Sounds better than the caching approach\n\nRight, the cache is a blunt (but effective) tool for simply sharing\ninformation w/in a single query.  LUCENE-1195 was the original issue.\n\nI'd rather make the private query scope (LUCENE-2093) and then remove\nthis cache, but, that's a bigger change.\n\nAlso, once flex lands, the MTQ's will require the cache alot less\n(because the TermsEnum API can produce a docs() directly w/o going\nback to the terms dict); my guess is we can drop the cache size to\nsomething relatively tiny (32) and get most of the gains.\n",
            "date": "2009-11-24T18:48:21.099+0000",
            "id": 74
        },
        {
            "author": "Robert Muir",
            "body": "Mike, I think you also might be seeing strangeness with that wildcard test due to the fact that most Terms automaton seeks to do not actually exist.\ninstead its simply 'the next possible subsequence', according to the DFA, and it relies on sort order of TermEnum to do the rest...\n",
            "date": "2009-11-24T19:06:21.504+0000",
            "id": 75
        },
        {
            "author": "Michael McCandless",
            "body": "New patch attached -- restores (deprecated) TestSimpleLRUCache.  I think this one is ready to commit?",
            "date": "2009-11-24T19:11:10.968+0000",
            "id": 76
        },
        {
            "author": "Uwe Schindler",
            "body": "Should we additionally deprecate the SimpleMapCache, as it is an abstract super class no longer used? The LinkedHashMap based cache simply extends this class and initializes it with an LinkedHashMap instance in the ctor.",
            "date": "2009-11-24T19:29:45.030+0000",
            "id": 77
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nMike, I think you also might be seeing strangeness with that wildcard test due to the fact that most Terms automaton seeks to do not actually exist.\ninstead its simply 'the next possible subsequence', according to the DFA, and it relies on sort order of TermEnum to do the rest...\n{quote}\n\nHang on -- the weirdness I was seeing was for the *N query, which does\nfull linear scan...  as best I can tell, the weird GC problems with\nLinkedHashMap completely explain that weirdness (and boy was\nit weird!).\n\nBut it sounds like you're talking about the seek-intensive ????NNN\nquery?  In that case it's only 1 in 10 seek'd terms that don't exist\n(though it is a rather contrived test).\n\nI guess if we created a much more sparse index, then re-ran that\nquery, we'd see many more seeks to non-existent terms.\n\nBut I think in general seek to non-existent term is harmless, because,\nsince it did not exist, it's not like you (or the app) will turnaround\nand ask for that term's docFreq, the TermDocs, etc.  Ie, we don't\ncache that 'I could not find term XXX', but I think we don't need to.\n",
            "date": "2009-11-24T19:31:41.204+0000",
            "id": 78
        },
        {
            "author": "Michael McCandless",
            "body": "bq. hould we additionally deprecate the SimpleMapCache, as it is an abstract super class no longer used?\n\nWoops, sorry, you said that already and I forgot -- I'll do that.",
            "date": "2009-11-24T19:33:12.923+0000",
            "id": 79
        },
        {
            "author": "Michael McCandless",
            "body": "Also deprecates SimpleMapCache.",
            "date": "2009-11-24T19:35:33.182+0000",
            "id": 80
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nBut it sounds like you're talking about the seek-intensive ????NNN\nquery? In that case it's only 1 in 10 seek'd terms that don't exist\n(though it is a rather contrived test).\n{quote}\n\nI guess I worded this wrong. you are right only 1 in 10 seek'ed terms.\nBut it will read a lot of useless terms too. This is because it does not try to seek until there is a mismatch.\n\nfirst it will seek to \\u0000\\u0000\\u0000\\u0000NNN\nedit: this will return 00000000 which fails, then it will seek to 0000NNN\nthis will be a match\n* since this was a match, next it will read sequentially the next term, which will not match, so it must seek again.\nnow it must backtrack and will try 0001NNN, match, it will do the sequential thing again.\n\nperhaps this optimization of 'don't seek unless you encounter a mismatch' is not helping the caching?\n(sorry i cant step thru this thing in my mind easily)",
            "date": "2009-11-24T19:43:31.378+0000",
            "id": 81
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nI guess I worded this wrong. you are right only 1 in 10 seek'ed terms.\nBut it will read a lot of useless terms too. This is because it does not try to seek until there is a mismatch.\n\nfirst it will seek to \\u0000\\u0000\\u0000\\u0000NNN\nedit: this will return 00000000 which fails, then it will seek to 0000NNN\nthis will be a match\n\nsince this was a match, next it will read sequentially the next term, which will not match, so it must seek again.\nnow it must backtrack and will try 0001NNN, match, it will do the sequential thing again.\nperhaps this optimization of 'don't seek unless you encounter a mismatch' is not helping the caching?\n(sorry i cant step thru this thing in my mind easily)\n{quote}\n\nSo it sort of plays ping pong w/ the terms enum API, until it finds an\nintersection.  (This is very similar to how filters are applied\ncurrently).\n\nIn this case, I agree it should not bother with the \"first=true\" case\n-- it never matches in this particular test -- it should simply seek\nto the next one.  Inside the term enum API, that seek will fallback to\na scan, anyway, if it's \"close\" (within the same index block).\n\nSo I guess if there's a non-empty common suffix you should just always seek?\n\nWe should test performance of that.\n",
            "date": "2009-11-24T20:14:56.606+0000",
            "id": 82
        },
        {
            "author": "Robert Muir",
            "body": "bq. So it sort of plays ping pong w/ the terms enum API\n\nping-pong, i like it.\n\nbq. So I guess if there's a non-empty common suffix you should just always seek?\n\nthe suffix is just for quick comparison, not used at all in seeking.\n\nin general, you can usually compute the next spot to go, even on a match.\nif its a regex of \"ab[cd]\" and the enum is at abc, its pretty stupid to compute abd and seek to it, so I don't. (as long as there is match, just keep reading).\n\notherwise I am seeking the whole time, whenever a term doesn't match, I calculate the next spot to go to. \n\nwe can work it on that other issue if you want, i don't mean to clutter this one up... happy to see you improve the *N case here :)\n\nedit: remove the * for simplicity",
            "date": "2009-11-24T20:20:32.693+0000",
            "id": 83
        },
        {
            "author": "Michael McCandless",
            "body": "OK let's move this over to LUCENE-1606?  I'll respond there.",
            "date": "2009-11-24T20:35:19.815+0000",
            "id": 84
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks all!",
            "date": "2009-11-27T15:33:18.964+0000",
            "id": 85
        }
    ],
    "component": "core/index",
    "description": "Right now each thread creates its own (thread private) SimpleLRUCache,\nholding up to 1024 terms.\n\nThis is rather wasteful, since if there are a high number of threads\nthat come through Lucene, you're multiplying the RAM usage.  You're\nalso cutting way back on likelihood of a cache hit (except the known\nmultiple times we lookup a term within-query, which uses one thread).\nIn NRT search we open new SegmentReaders (on tiny segments) often\nwhich each thread must then spend CPU/RAM creating & populating.\n\nNow that we are on 1.5 we can use java.util.concurrent.*, eg\nConcurrentHashMap.  One simple approach could be a double-barrel LRU\ncache, using 2 maps (primary, secondary).  You check the cache by\nfirst checking primary; if that's a miss, you check secondary and if\nyou get a hit you promote it to primary.  Once primary is full you\nclear secondary and swap them.\n\nOr... any other suggested approach?\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2075",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Share the Term -> TermInfo cache across threads",
    "systemSpecification": true,
    "version": ""
}