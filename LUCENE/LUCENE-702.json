{
    "comments": [
        {
            "author": "Ning Li",
            "body": "A possible solution to this issue is to check, when writing segment infos to \"segments\" in directory d,\nwhether dir of a segment info is d, and only write if it is. Suggestions?\n\nThe following is my comment on this issue from the mailing list documenting how Lucene could\nproduce an inconsistent index if addIndexes(Directory[]) does not run to its completion.\n\n\"This makes me notice a bug in current addIndexes(Directory[]). In current addIndexes(Directory[]),\nsegment infos in S are added to T's \"segmentInfos\" upfront. Then segments in S are merged to T\nseveral at a time. Every merge is committed with T's \"segmentInfos\". So if a reader is opened on T\nwhile addIndexes(Directory[]) is going on, it could see an inconsistent index.\"\n",
            "date": "2006-11-01T17:19:18.000+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "\nThat seems like a reasonable approach?  At least the index would be\nconsistent (ie, loadable).\n\nThough, if you hit disk full part way through, then some of your\nindexes were added and some where not.  How do you \"recover\" after you\nfree up your disk space?  If you just re-add all indexes then you have\nduplicates in the index.\n\nIs it possible instead to not reference (in the written segments info\nfile) those segments that were carried over from the input\nreaders/directories?  This would make the operation transactional, so\nthat if we crashed part way through, then the index rolls back to\nwhere it was before the addIndexes call.  And user could free up disk\nspace and try again, without creating dups.\n\n(One problem with this is that the orphan'd segments that had been\nwritten would not actually get deleted automatically; lockless commits\nwould fix that though because it recomputes on instantiating a reader\nwhich files are unreferenced and then deletes them).\n",
            "date": "2006-11-01T21:00:16.000+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "I think we should try to make all of the addIndexes calls (and more\ngenerally any call to Lucene) \"transactional\".  Meaning, if the call\nis aborted (machine crashes, disk full, jvm killed, neutrino hits CPU,\netc.) then your index just \"rolls back\" to where it was at the start\nof the call.  Ie, it is consistent and none of the incoming documents\nwere added.\n\nThis way your index is fine after the crash, and, you can fix the\ncause of the crash and re-run the addIndexes call and you won't get\nduplicate documents.\n\nTo achieve this, each of the three addIndexes methods would need to 1)\nnot commit a new segments file until the end, and 2) not delete any\nsegments referenced by the initial segments file (segmentInfos) until\nthe end.\n\nWe have three methods now for addIndexes:\n\n  * For addIndexes(IndexReader[]): this method is I think already\n    transactional.  We create a merger, add all readers to it, do the\n    merge, and only at the end commit the new segments file & remove\n    old segments.\n\n  * For addIndexes(Directory[]): this method can currently corrupt the\n    index if aborted.  However, because all merging is done only on\n    the newly added segments, I think the fix is simply to not commit\n    the new segments file until the end?\n\n  * For addIndexesNoOptimize(Directory[]): this method can also\n    currently corrupt the index if aborted.  To fix this I think we\n    need to not only prevent committing a new segments file until the\n    end, but also to prevent deletion of any segments in the original\n    segments file.  This is because it's able (I think?) to merge\n    both old and new segments in its step 3.  This would normally\n    result in deleting those old segments that were merged.\n\n    Note that this will increase the temporary disk usage used during\n    the call, because old segments must remain on disk even if they\n    have been merged, but I think this is the right tradeoff\n    (transactional vs temporary disk usage)?\n\nAlso note that we would need the fixes from lockless LUCENE-701 to\nproperly delete orphan'd segments after an abort.  Without the\nIndexFileDeleter I think they would stick around indefinitely.\n\nDoes this approach sound reasonable/right?  Any feedback?\n",
            "date": "2006-11-07T23:19:58.000+0000",
            "id": 2
        },
        {
            "author": "Ning Li",
            "body": "> I think we should try to make all of the addIndexes calls (and more\n> generally any call to Lucene) \"transactional\".\n\nAgree. A transactional semantics would be better.\n\nThe approach you described for three addIndexes looks good.\n\naddIndexes(IndexReader[]) is transactional but has two commits: one\nwhen existing segments are merged at the beginning, the other at the\nend when all segment/readers are merged.\n\naddIndexes(Directory[]) can be fixed to have a similar behaviour:\nfirst commit when existing segments are merged at the beginning, then\nat the end when all old/new segments are merged.\n\naddIndexesNoOptimize(Directory[]), on the other hand, does not merge\nexisting segments at the beginning. So when fixed, it will only have\none commit at the end which captures all the changes.\n",
            "date": "2006-11-08T01:55:12.000+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "\nI've attached a patch with changes as described below.  I will commit\nin a few days if no one objects!\n\nAll unit tests pass.\n\nThis patch fixes addIndexes to 1) not corrupt the index on exception,\n2) be transactional (either all or nothing was actually added), and 3)\nto leave the writer instance in an consistent state (meaning, on an\nexception, the segmentInfos state is restored to its starting point,\nso that it matches what's actually in the index, and any unreferenced,\npresumably partially written, files in the index are removed).\n\nI've also fixed IndexWriter.mergeSegments() and IndexReader.commit()\nto keep the instance \"consistent\" on exception as well (ie, even when\nnot being called from addIndexes).  Meaning, on an exception, any\nchanges to segmentInfos state are rolled back, any opened readers (in\nthe merger) are closed, and non-committed files are removed.\n\nI've added two unit tests (one for IndexWriter.addIndexes* and one for\nIndexReader.close) that expose these various cases as failures in the\ncurrent Lucene sources, which then pass with this patch.\n\nThese unit tests use a new handy MockRAMDirectory that can simulate\ndisk full, inject random IOExceptions, measures peak disk usage, etc.\n \nHere is the summary of the approach:\n\n  * Added private methods startTransaction(), rollbackTransaction()\n    and commitTransaction() to IndexWriter.  During a transaction\n    (inTransaction = true), I block changes to the index that alter\n    how it was at the start of the transaction: we don't write any new\n    segments_N files (but, do update the segmentInfos in memory, and\n    do write / merge new segment files); we are not allowed to delete\n    any segment files that were in the index at the start (added\n    \"protectedSegments\" to track this during a transaction).\n\n  * For the addIndexes methods, I first call startTransaction(), then\n    do the actual work in a try block, and in a finally block then\n    call either commitTransaction() or rollbackTransaction().\n\n  * Fixed mergeSegments to respect whether it's in a transaction (and\n    not commit/delete).  Also, fixed this method so that if it's not\n    in a transaction (ie, being called by optimize or\n    maybeMergeSegments) it still (in a try/finally) leaves\n    segmentInfos \"consistent\" on hitting an exception, and removes any\n    partial files that had been written.\n\n  * Fixed IndexReader.commit with similar rollback logic to reset\n    internal state if commit has failed.\n\nIt is actually possible to get an IOException from\naddIndexes(Directory[]) yet see that all docs were in fact added. This\nhappens when the disk full is hit in building the CFS file on the\nfinal optimize.  In this case, the index is consistent, but that\nsegment will remain in non-CFS format and will show all docs as added.\n\nVarious other small changes:\n\n  * Changed RAMDirectory, and its createOutput method, to be\n    non-final.  Also changed private -> protected for some of its\n    instance variables.\n\n  * Created MockRAMDirectory subclass.\n\n  * Moved the RAMDirectory.getRecomputedSizeInBytes() into\n    MockRAMDirectory.\n\n  * Changed IndexFileDeleter to use a HashSet (instead of Vector) to\n    track pending files since with these changes the same file can be\n    added to the pending set many times.\n\n  * Added some javadocs around temporary disk usage by these methods\n    (this came up on java-user recently).  Also included a check in\n    one of the unit tests to assert this.\n\n  * In SegmentInfos, separately track (in memory only) which\n    generation we will use for writing vs which generation was last\n    successfully read.  These are almost always the same, but can\n    differ when a commit failed while writing the next segments_N\n    file.\n\n  * Changed package protected IndexFileDeleter.commitPendingFiles() to\n    actually delete the files (previously you also had to separately\n    call deleteFiles).\n\n  * Added SegmentInfos.getNextSegmentFileName()\n\n  * Added SegmentInfos.clone() to also copy the contents of the vector\n    (ie, each SegmentInfo).\n\n  * Added SegmentInfo.reset(), which is used to \"copy back\" a previous\n    clone of a SegmentInfo (IndexReader uses this to rollback).\n\n  * Added package protected SegmentReader.getSegmentName()\n\n  * Fixed a small bug in IndexFileDeleter that was failing to remove\n    un-referenced CFS file on a segment that wasn't successfully\n    converted to a CFS file (and added case to TestIndexFileDelter to\n    expose the bug first).\n\n  * Fixed a few minor typos.\n",
            "date": "2006-12-11T17:09:45.000+0000",
            "id": 4
        },
        {
            "author": "Ning Li",
            "body": "It looks good. My two cents:\n\n1 In the two rollbacks in mergeSegments (where inTransaction is false), the segmentInfos' generation is not always rolled back. So something like this could happen: two consecutive successful commits write segments_3 and segments_5, respectively. Nothing is broken, but it'd be nice to roll back completely (even for the IndexWriter instance) when a commit fails.\n\n2 Code serving two purposes are (and has been) mixed in mergeSegments: one to merge segments and create compound file if necessary, the other to commit or roll back when inTransaction is false. It'd be nice if the two could be separated: optimize and maybeMergeSegments call mergeSegmentsAndCommit, which creates a transaction, calls mergeSegments and commits or rolls back; mergeSegments doesn't deal with commit or rollback. However, currently the non-CFS version is committed first even if useCompoundFile is true. Until that's changed, mergeSegments probably has to continue serving both purposes.\n",
            "date": "2006-12-12T00:25:04.000+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks for the review Ning!\n\n> 1 In the two rollbacks in mergeSegments (where inTransaction is\n> false), the segmentInfos' generation is not always rolled back. So\n> something like this could happen: two consecutive successful commits\n> write segments_3 and segments_5, respectively. Nothing is broken,\n> but it'd be nice to roll back completely (even for the IndexWriter\n> instance) when a commit fails.\n\nThis is actually intentional: I don't want to write to the same\nsegments_N filename, ever, on the possibility that a reader may be\nreading it.  Admittedly, this should be quite rare (filling up disk\nand then experiencing contention, only on Windows), but still I wanted\nto keep \"write once\" even in this case.\n\n> 2 Code serving two purposes are (and has been) mixed in\n> mergeSegments: one to merge segments and create compound file if\n> necessary, the other to commit or roll back when inTransaction is\n> false. It'd be nice if the two could be separated: optimize and\n> maybeMergeSegments call mergeSegmentsAndCommit, which creates a\n> transaction, calls mergeSegments and commits or rolls back;\n> mergeSegments doesn't deal with commit or rollback. However,\n> currently the non-CFS version is committed first even if\n> useCompoundFile is true. Until that's changed, mergeSegments\n> probably has to continue serving both purposes.\n\nI agree, mergeSegments is doing two different things now: merging\nsegments, and, committing this change (in 2 steps, for the CFS case)\ninto the index (if not in a transaction).\n\nI had in fact tried putting a transaction up at the\noptimize/maybeMergeSegments level, and it worked, but there was one\nsevere drawback: the max temp free space required would be [up to] 2X\nthe starting size of the segments-to-be-merged because the original\nsegments would be there (1X), the newly merged separate segment files\nwould be there (another 1X) and the just-created CFS segment file\nwould also be there (another 1X).\n\nWhereas the max temp space required now is 1X the starting size of\nsegments-to-be-merged.\n\nSo I think this (doing 2 commits with the current source before my\npatch) is intentional, to keep the temp free space required at 1X.\n\nIt's also [relatively] OK to commit that first time, at least\nfunctionally: the index is consistent and has all docs.  The only\ndownside is that if you hit disk full or other exception when creating\nthe CFS file then your index has one segment not in compound format (I\nwill call this out in the javadocs).\n\nOK I've added a unit-test that explicitly tests max temp space\nrequired by just optimize and asserts that it's at most 1X starting\nindex size.  Will attach a patch shortly!\n",
            "date": "2006-12-12T12:51:03.000+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "OK I attached a new patch with changes to only javadocs & unit tests:\n\n  * Fixed the disk full unit test to use \"richer\" documents so indexes\n    shrink less on merging/optimizing (ie make the test case \"harder\"\n    to satisfy the disk usage check).\n\n  * Added new test case for temp disk usage of optimize.  Verified\n    that it fails if we put a transaction around mergeSegments call in\n    optimize (as described above).\n\n  * Fixed javadocs for addIndexes(*): we actually require up to 2X the\n    total input size of all indices.  Fixed unit test to assert this.\n\n  * Fixed javadocs in IndexWriter's optimize, addIndexes(*),\n    addDocument to describe disk usage and index state after an\n    IOException is thrown.\n\n  * Improved how MockRAMDirectory tracks/enforces max usage.\n\n  * Other small fixes to unit test.\n",
            "date": "2006-12-12T12:52:56.000+0000",
            "id": 7
        },
        {
            "author": "Ning Li",
            "body": "> This is actually intentional: I don't want to write to the same\n> segments_N filename, ever, on the possibility that a reader may be\n> reading it.  Admittedly, this should be quite rare (filling up disk\n> and then experiencing contention, only on Windows), but still I wanted\n> to keep \"write once\" even in this case.\n\nIn IndexWriter, the rollbackTransaction call in commitTransaction could\ncause write to the same segment_N filename, right?\n\nThe \"write once\" semantics is not kept for segment names or .delN. This\nis ok because no reader will read the old versions.",
            "date": "2006-12-12T20:00:48.000+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "> In IndexWriter, the rollbackTransaction call in commitTransaction could\n> cause write to the same segment_N filename, right?\n\nGood catch -- you are correct!  OK, I fixed rollbackTransaction to do\nthe same in-place rollback of the segmentInfos that I do in\nmergeSegments (patch attached).  This means if another attempt is made\nto commit, using this same IndexWriter instance after it had received\nan exception, it will write to a new segments_N file.\n\n> The \"write once\" semantics is not kept for segment names or\n> .delN. This is ok because no reader will read the old versions.\n\nRight, I think these cases are less important because readers would never try to\nopen those partially written files (since no segments_N references them).",
            "date": "2006-12-12T21:55:38.000+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "Closing all issues that were resolved for 2.1.",
            "date": "2007-02-27T18:10:35.683+0000",
            "id": 10
        }
    ],
    "component": "core/index",
    "description": "This is a spinoff of LUCENE-555\n\nIf the disk fills up during this call then the committed segments file can reference segments that were not written.  Then the whole index becomes unusable.\n\nDoes anyone know of any other cases where disk full could corrupt the index?\n\nI think disk full should worse lose the documents that were \"in flight\" at the time.  It shouldn't corrupt the index.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-702",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Disk full during addIndexes(Directory[]) can corrupt index",
    "systemSpecification": true,
    "version": "2.1"
}