{
    "comments": [
        {
            "author": "Simon Willnauer",
            "body": "linked issues",
            "date": "2009-12-28T17:13:30.542+0000",
            "id": 0
        },
        {
            "author": "Simon Willnauer",
            "body": "Here is a first proposal of a slightly new API that rather decouples the \"isTokenChar\" predicate and the \"normalize\" method instead of enforceing CharTokenizer subclasses and overriding methods. \nI introduced a new class TokenCharFunctions that is passed to the constructor of CharTokenizer and is used internally. The patch preserves full backwards compat and provides a clean way to move towards a consistent codepoint based API with would still provide the flexibility to make use of the old and buggy behavior even if the char based methods are removed.\n\nI consider this patch as a basis for the discussion how to solve this problem. other approaches would be using reflection like the TokenStream BWCompat layer does but I personally prefer not to use reflection and rather use delegation in favor of inheritance.\n\nlooking forward to see some comments. ",
            "date": "2009-12-28T17:30:28.945+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "Hello, first comment is that I really like how the IO-handling is done in CharacterUtils.\n\nThis solves a problem across more than CharTokenizer, other tokenizers in lucene contrib that do NOT extend CharTokenizer have the same logic and also need to be fixed.\n\nSo we could reuse this code in other places too, such as CJKTokenizer. I think we could also reuse this code to fix some unrelated problems in the n-gram tokenizers (at a glance, i do not see how the n-gram tokenizer io-handling even works correctly at all)\n",
            "date": "2009-12-28T18:08:56.932+0000",
            "id": 2
        },
        {
            "author": "Robert Muir",
            "body": "Hello Simon, another option very similar to yours (I am not sure if it really would work, but just thinking out loud somewhat) could be:\n\n{code}\n/** this method will be declared abstract in Lucene 4.0 */\npublic int isTokenChar(int ch) {\n  throw UOE();\n}\n\n/** @deprecated will be removed in Lucene 4.0 */\npublic int isTokenChar(char ch) {\n  return isTokenChar((int)ch);\n}\n{code}\n\nand do the same for normalize(). The rest would be the same as your patch:\n* Use CharacterUtils for io-buffering\n* Use CharacterUtils for character/codepoint iteration.\n* Use Version to decide which method to call instead of reflection: this should not be conditional upon each call to isTokenChar() but instead two private inner classes or whatever.\n\nThe difference would be that the api would appear more natural in my opinion, and once deprecations are removed we would end out with an abstract class with the int-equivalent of what we have now.\n\nIf someone attempts to use a CharTokenizer that does *not* support int-based methods (only implements the char-based methods) with Version.LUCENE_31 then this would throw UOE, which in my opinion is correct, as it does not support the behavior of that version.\n\nedit: I changed the deprecation of isTokenChar(char) to 4.0. The index back compat would still exist because using CharacterUtils with isTokenChar(int) is the same thing.\n",
            "date": "2009-12-28T19:11:54.156+0000",
            "id": 3
        },
        {
            "author": "Uwe Schindler",
            "body": "+1 @ Roberts comment! I like this more and it would work correctly as proposed with UOE..",
            "date": "2009-12-28T22:17:53.656+0000",
            "id": 4
        },
        {
            "author": "Robert Muir",
            "body": "I thought about this some (the idea i mentioned earlier, not Simon's patch), but i am worried about one thing:\n\nConsider LetterTokenizer, which is non-final subclass of CharTokenizer.\nLets say you make LetterAndNumberTokenizer which extends LetterTokenizer, but you do not implement the int-based method.\n\n{code}\npublic boolean isTokenChar(char c) {\n  return super.isTokenChar(c) || Character.isNumber(c);\n}\n{code}\n\nwe have fixed LetterTokenizer so it has isTokenChar(int), but that means if someone tries to use this LettterAndNumberTokenizer with Version.LUCENE_31, it will not work, because it will not throw UOE, and silently discard numbers...  since it will call the LetterTokenizer int-based method.\n\nof course it will work correctly with Version.LUCENE_30, so it is not a back compat problem, but it will not throw UOE and silently behave incorrectly for LUCENE_31 until the 'int' method is implemented.\n\nso i think this is a problem in this design, and i do not know how to fix without reflection.",
            "date": "2009-12-28T23:34:31.855+0000",
            "id": 5
        },
        {
            "author": "Simon Willnauer",
            "body": "Hey guys thanks for your comments.\nwhen I started thinking about this issue I had a quick chat with robert and we figured that his solution could be working so I implemented it.\nYet, i found 2 problems with it.\n1. If a user calls super.isTokenChar(char) and the super class has implemented the int method the UOE will never be thrown and the code does not behave like \"expected\" from the user perspective. - This is what robert explained above. We could solve this problem with reflection which leads to the second problem.\n\n2. If a Tokenizer like LowerCaseTokenizer only overrides normalize(char|int) it relies on the superclass implementation of isTokenChar. Yet if we solve problem 1. the user would be forced to override the isTokenChar to just call super.isTokenChar otherwise the reflection code will raise an exception that the int method is not implemented in the concrete class or will use the char API - anyway it will not do what is expected. \n\nWorking around those two problem was the cause of a new API for CharTokenizer. My personal opinion is that inheritance is the wrong tool for changing behavior I used delegation (like a strategy) to on the one hand define a clear \"new\" API and decouple the code changing the behavior of the Tokenizer from the tokenizer itself. Inheritance for me is for extending a class and delegation is for changing behavior in this particular problem. \nDecoupling the old from the new has several advantages over a reflection / inheritance based solution. \n1. if a user does not provide a delegation impl he want to use the old API\n2. if a user does provide a delegation impl he has still the ability to choose between charprocessing in 3.0 style or 3.1 style\n3. no matter what is provided a user has full flexibility to choose the combination of their choice - old char processing - new int based api (maybe minor though)\n4. we can leave all tokeinizer subclasses as their are and define new functions that implement their behavior in parallel. those functions can be made final from the beginning and which prevents users from subclassing them. (all of the existing ones should be final in my opinion - like LowerCaseTokenizer which should call Character.isLetter in the isTokenCodePoint(int) directly instead of subclassing another function.)\n\nAs a user I would expect lucene to revise their design decisions made years ago when there is a need for it like we have in this issue. It is easier to change behavior in user code by swapping to a new api instead of diggin into an workaround implementation of an old api silently calling a new API.\n\n",
            "date": "2009-12-29T12:47:11.052+0000",
            "id": 6
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\n1. If a user calls super.isTokenChar(char) and the super class has implemented the int method the UOE will never be thrown and the code does not behave like \"expected\" from the user perspective. - This is what robert explained above. We could solve this problem with reflection which leads to the second problem.\n\n2. If a Tokenizer like LowerCaseTokenizer only overrides normalize(char|int) it relies on the superclass implementation of isTokenChar. Yet if we solve problem 1. the user would be forced to override the isTokenChar to just call super.isTokenChar otherwise the reflection code will raise an exception that the int method is not implemented in the concrete class or will use the char API - anyway it will not do what is expected. \n{quote}\n\ni do not think this is true, what i was trying to do was modify the design i proposed so that we did not need reflection at all: but i think this is impossible. \n\nin the design you propose under the new api, subclassing is impossible, which I am not sure I like either.\n\n#2 is no problem at all, instead the reflection code to address #1 must be implemented with these conditions \n\n* A is the class implementing method isTokenChar(int)\n* B is the class implementing method isTokenChar(char)\n* B is a subclass of A\n* A is not CharTokenizer\n\n",
            "date": "2009-12-29T13:54:40.060+0000",
            "id": 7
        },
        {
            "author": "Simon Willnauer",
            "body": "{quote}\n#2 is no problem at all, instead the reflection code to address #1 must be implemented with these conditions\n\n    * A is the class implementing method isTokenChar(int)\n    * B is the class implementing method isTokenChar(char)\n    * B is a subclass of A\n    * A is not CharTokenizer\n{quote}\n\nok here is a scenario:\n{code}\nclass MySmartDeseretTokenizer extends LetterTokenizer {\n  \n  public boolean isTokenChar(char c) {\n    // we trust that DeseretHighLow surrogates are never unpaired\n    return super.isTokenChar(c) || isDeseretHighLowSurrogate(c);\n  }\n\n  public char nomalize(char c) {\n    if(isDeseretHighSurrogate(c))\n      return c;\n    if(isDeseretLowSurrogate(c))\n     return lowerCaseDeseret('\\ud801', c)[1];\n    return Character.toLowercase(c);\n  }\n\n  public int normalize(int c) {\n    return Character.toLowerCase(c);\n  }\n}\n\n{code}\n\nif somebody has similar code like this they might want to preserve compat because they have different versions of their app. Yet the old app only supports deseret high surrogates but the new one accepts all letter supplementary chars due to super.isTokenChar(int). This scenario will break our reflection solution and users might be disappointed though as the new api is there to bring the unicode support. I don't say this scenario exists but it could be a valid one for a very special usecase. \n\nI don't say my proposal is THE way to go but I really don't want to use reflection - this would make things worse IMO. \nLets find a solution that fits to all scenarios.\n\nbq. in the design you propose under the new api, subclassing is impossible, which I am not sure I like either.\n\nHmm, that is not true. You can still subclass and pass your impl up to the superclass. I haven't implemented that yet but this is def. possible.",
            "date": "2009-12-29T18:03:58.146+0000",
            "id": 8
        },
        {
            "author": "Robert Muir",
            "body": "Simon, I don't think your example is a problem.\n\nI am proposing my original design, with no reflection, driven by Version only.\n\nThere is only one exception where reflection is used... that is during ctor to determine if:\n* you subclass a tokenizer that implements int-based methods\n* you have only implemented char-based methods\n* you request VERSION >= 3.1\n\nin this case, the reflection is only used in the ctor to throw UOE!\n\nif someone wants to support VERSION 3.1 in their app, they simply implement the int-based methods.\nto support lower versions, they do nothing, they do not need to implement char-based methods, they get the backwards compat automatically, as long as they supply the correct version. this is guaranteed by CharacterUtils.\n\nI am only proposing using reflection to enforce the throwing of UOE, in the case that someone requests VERSION 3.1, but has not implemented int.\n\nif they want to support Version <= 3.1, this is fine, it will work with their char-based stuff automatically.\n\nI think it would be easiest if i modified your patch to illustrate this, so i'll do it in a few days.\n",
            "date": "2009-12-30T04:58:10.527+0000",
            "id": 9
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. This issue is blocked by: ...\nI give up... ",
            "date": "2010-01-03T14:10:16.021+0000",
            "id": 10
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. I give up... \n\nHehe :-)",
            "date": "2010-01-03T14:33:20.148+0000",
            "id": 11
        },
        {
            "author": "Uwe Schindler",
            "body": "this linkage is less hard.",
            "date": "2010-01-03T21:03:07.115+0000",
            "id": 12
        },
        {
            "author": "Uwe Schindler",
            "body": "{quote}\nThere is only one exception where reflection is used... that is during ctor to determine if:\n\n- you subclass a tokenizer that implements int-based methods \n- you have only implemented char-based methods \n- you request VERSION >= 3.1 \n{quote}\n\nWith LUCENE-2188, this is easy and no performance problem. Just define two static final fields for both char-based methods and check in the ctor if this.getClass() overrides the char-based method. In this case throw UOE. The result is cached for the class and further instantiations of the same class will not use reflection anymore:\n\n{code}\nprivate static final VirtualMethod<CharTokenizer> isTokenCharMethod =\n    new VirtualMethod<CharTokenizer>(CharTokenizer.class, \"isTokenChar\", char.class);\nprivate static final VirtualMethod<CharTokenizer> normalizeMethod =\n    new VirtualMethod<CharTokenizer>(CharTokenizer.class, \"normalize\", char.class);\n...\npublic CharTokenizer(...) {\n  super(...)\n  if (matchVersion.onOrAfter(Version.LUCENE_31) && (\n   isTokenCharMethod.isOverriddenAsOf(this.getClass()) || normalizeMethod.isOverriddenAsOf(this.getClass())\n  ) throw new IAE(\"For matchVersion >= LUCENE_31, CharTokenizer subclasses must not override isTokenChar(char) or normalize(char).\"):\n}\n{code}",
            "date": "2010-01-03T21:15:53.232+0000",
            "id": 13
        },
        {
            "author": "Simon Willnauer",
            "body": "I updated the patch to make use of the nice reflection utils and ported all subclasses of CharTokenizer to the int based API.\nDue to the addition of Version to CharTokenizer ctors this patch creates a lot of usage of deprecated API.\nYet, I haven't changed all the usage of the deprecated ctors, this should be done in another issue IMO.",
            "date": "2010-01-15T21:00:24.858+0000",
            "id": 14
        },
        {
            "author": "Uwe Schindler",
            "body": "Have not looked detailed into it yet, but it looks correct. I am not sure about the overhead of passing each char through the proxy class. My idea would be to declare CharFunction as a private interface and let CharTokenizer implement it (invisible to the outside, so it can be removed in later versions). The ctor then passes \"this\" as CharFunction if >=3.1 and a new proxy instance of the interface for the deprecated case. By this at least the new stuff does not have extra method calls.\n\nThe VirtualMethod stuff looks ok, thanks for using it as suggested here! :-)",
            "date": "2010-01-15T22:04:53.310+0000",
            "id": 15
        },
        {
            "author": "Simon Willnauer",
            "body": "Uwe, using an interface doesn't work though as I can not reduce the public visibility in CharTokeinzer. Yet, this patch tries to solve it with an abstract class.\nTo be honest I would rather say we duplicate the code and use a simple boolean switch in incrementToken. Not that nice but def. faster.\n\nwhat do you think?",
            "date": "2010-01-15T23:04:47.821+0000",
            "id": 16
        },
        {
            "author": "Uwe Schindler",
            "body": "+1, because this is very speed-sensitive.",
            "date": "2010-01-16T00:25:41.081+0000",
            "id": 17
        },
        {
            "author": "Simon Willnauer",
            "body": "This version \"duplicates\" the incrementToken method to prevent any unnecessary method call inside the loop. \nI also use the Character static methods directly where possible without the CharacterUtils indirection. \n\n ",
            "date": "2010-01-16T02:10:45.177+0000",
            "id": 18
        },
        {
            "author": "Simon Willnauer",
            "body": "Short update: I found a bug in the latest version which was untested I will update soon with a speed comparison between the current version and the version using the proxy class.",
            "date": "2010-01-27T13:53:37.643+0000",
            "id": 19
        },
        {
            "author": "Simon Willnauer",
            "body": "Added CHANGES.TXT entry and fixed 2 supplementary chars related bugs in the new version of incrementToken(). Testcases added for the bugs.",
            "date": "2010-01-28T13:08:47.227+0000",
            "id": 20
        },
        {
            "author": "Simon Willnauer",
            "body": "I did run following benchmark alg file against the latest patch (specialized old and new methods), the patch with the proxy methods and the old 3.0 code. The outcome shows that the specialized code is about ~8% faster than the proxy class based code so I would rather keep the specialized code as this class is performance sensitive though\n\n.alg file\n{code}\nanalyzer=org.apache.lucene.analysis.WhitespaceAnalyzer\ncontent.source=org.apache.lucene.benchmark.byTask.feeds.ReutersContentSource\ncontent.source.forever=false\n{ \"Rounds\" { \"ReadTokens\" ReadTokens > : *  NewRound ResetSystemErase} : 10\nRepAll\n{code}\n\n10 Rounds with the latest patch\n{code}\n     [java] ------------> Report All (11 out of 12)\n     [java] Operation          round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem\n     [java] Rounds_10              0        1            0         0.00       14.83     5,049,432     66,453,504\n     [java] ReadTokens_Exhaust -   0 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   2.07 -  34,558,000 -   55,705,600\n     [java] ReadTokens_Exhaust     1        1            0         0.00        1.40    41,865,312     60,555,264\n     [java] ReadTokens_Exhaust -   2 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   1.22 -  34,393,904 -   63,176,704\n     [java] ReadTokens_Exhaust     3        1            0         0.00        1.24    15,440,624     64,487,424\n     [java] ReadTokens_Exhaust -   4 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   1.22 -   7,540,512 -   65,601,536\n     [java] ReadTokens_Exhaust     5        1            0         0.00        1.21    50,174,760     67,239,936\n     [java] ReadTokens_Exhaust -   6 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   1.19 -  22,202,768 -   67,174,400\n     [java] ReadTokens_Exhaust     7        1            0         0.00        1.19    20,591,672     68,812,800\n     [java] ReadTokens_Exhaust -   8 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   1.18 -  63,749,984 -   69,009,408\n     [java] ReadTokens_Exhaust     9        1            0         0.00        1.19    22,331,600     68,943,872\n{code}\n\n10 rounds with Proxy Class\n{code}\n     [java] ------------> Report All (11 out of 12)\n     [java] Operation          round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem\n     [java] Rounds_10              0        1            0         0.00       16.33     5,021,144     67,436,544\n     [java] ReadTokens_Exhaust -   0 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   2.34 -  44,649,496 -   59,244,544\n     [java] ReadTokens_Exhaust     1        1            0         0.00        1.53    36,681,952     61,472,768\n     [java] ReadTokens_Exhaust -   2 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   1.37 -  13,863,688 -   64,094,208\n     [java] ReadTokens_Exhaust     3        1            0         0.00        1.34    50,247,864     65,470,464\n     [java] ReadTokens_Exhaust -   4 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   1.36 -  14,922,888 -   66,322,432\n     [java] ReadTokens_Exhaust     5        1            0         0.00        1.36     5,718,296     67,371,008\n     [java] ReadTokens_Exhaust -   6 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   1.32 -  54,583,776 -   67,502,080\n     [java] ReadTokens_Exhaust     7        1            0         0.00        1.33    35,739,800     68,943,872\n     [java] ReadTokens_Exhaust -   8 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   1.32 -  24,985,688 -   69,861,376\n     [java] ReadTokens_Exhaust     9        1            0         0.00        1.29    64,138,112     69,730,304\n{code}\n\n10 rounds with current trunk\n{code}\n     [java] ------------> Report All (11 out of 12)\n     [java] Operation          round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem\n     [java] Rounds_10                   0        1            0         0.00       15.19     5,040,928     66,256,896\n     [java] ReadTokens_Exhaust -   0 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   2.15 -  39,548,440 -   55,443,456\n     [java] ReadTokens_Exhaust     1        1            0         0.00        1.43    28,088,544     60,096,512\n     [java] ReadTokens_Exhaust -   2 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   1.27 -  16,004,088 -   61,800,448\n     [java] ReadTokens_Exhaust     3        1            0         0.00        1.25    51,034,016     63,045,632\n     [java] ReadTokens_Exhaust -   4 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   1.24 -  23,371,056 -   63,504,384\n     [java] ReadTokens_Exhaust     5        1            0         0.00        1.24    12,964,368     65,208,320\n     [java] ReadTokens_Exhaust -   6 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   1.25 -   6,598,128 -   65,601,536\n     [java] ReadTokens_Exhaust     7        1            0         0.00        1.23    50,932,464     67,239,936\n     [java] ReadTokens_Exhaust -   8 -  -   1 -  -  -  - 0 -  -  - 0.00 -  -   1.24 -  20,433,136 -   67,305,472\n     [java] ReadTokens_Exhaust     9        1            0         0.00        1.23    63,638,552     68,812,800\n\n{code}",
            "date": "2010-01-28T13:14:39.784+0000",
            "id": 21
        },
        {
            "author": "Uwe Schindler",
            "body": "I'll commit this, if nobody objects in a day.",
            "date": "2010-01-28T14:47:24.953+0000",
            "id": 22
        },
        {
            "author": "Robert Muir",
            "body": "hello, a few very minor nitpicks:\n* it seems the javadoc for isTokenChar()/isTokenInt() is backwards, the int-based api refers to 'character' but the char-based api refers to codepoint (which is wrong). perhaps it would be best to refer to all char-based methods as working on 'utf-16 code unit' and int-based apis as 'codepoint'.\n* we might want to insert a note/warning on the char-based methods, consistent with the JDK javadocs, \"Note this method cannot handle supplementary characters...\" for example, like: http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Character.html#getType%28char%29 I think its important to include the link to the JDK explanation of what a supplementary character is, also. http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Character.html#supplementary\n* if possible we might want to include some class-level wordage on how the whole thing works. If you implement the int-based API, you can use your class with all Lucene Versions, and bw layer will make it work correctly with old indexes. If you only stay with the char-based API, then you can only use your CharTokenizer for Version <= 3.0. We can also mention it is unnecessary to implement both, only the int-based api!!!\n\nsorry for being picky about the javadocs, trying to think of ways to prevent confusion on the user list, as i anticipate a bunch of people probably just use Version.LUCENE_CURRENT and have no idea what a supplementary character is.\n\ngreat work.",
            "date": "2010-01-28T16:30:30.923+0000",
            "id": 23
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. we might want to insert a note/warning on the char-based methods, consistent with the JDK javadocs, \"Note this method cannot handle supplementary characters...\" for example, like: http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Character.html#getType%28char%29 I think its important to include the link to the JDK explanation of what a supplementary character is, also. http://java.sun.com/j2se/1.5.0/docs/api/java/lang/Character.html#supplementary \n\nFor that a link using javadoc {@link Character#supplementary} would be good. I will fix this here, as I already have the patcxh applied and will commit it later.\n\nbq. if possible we might want to include some class-level wordage on how the whole thing works. If you implement the int-based API, you can use your class with all Lucene Versions, and bw layer will make it work correctly with old indexes. If you only stay with the char-based API, then you can only use your CharTokenizer for Version <= 3.0. We can also mention it is unnecessary to implement both, only the int-based api!!! \n\n++++++1. The old TokenStream API had a check for these problems, right?\n\n",
            "date": "2010-01-28T17:24:21.142+0000",
            "id": 24
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. For that a link using javadoc {@link Character#supplementary} would be good. I will fix this here, as I already have the patcxh applied and will commit it later.\n\nUwe I will take care of it and upload another patch. Thanks for being picky rob!",
            "date": "2010-01-28T19:00:47.725+0000",
            "id": 25
        },
        {
            "author": "Uwe Schindler",
            "body": "here other javadoc fixes",
            "date": "2010-01-28T19:01:48.533+0000",
            "id": 26
        },
        {
            "author": "Simon Willnauer",
            "body": "Robert / Uwe,\n\nI worked on the documentation on method and class level. I would appreciate a wording review.\n\nthanks",
            "date": "2010-01-28T21:25:40.004+0000",
            "id": 27
        },
        {
            "author": "Robert Muir",
            "body": "hello simon, thanks for adding this additional wording.",
            "date": "2010-01-28T21:36:53.572+0000",
            "id": 28
        },
        {
            "author": "Uwe Schindler",
            "body": "OK, will commit that tomorrow. Thanks Simon & Robert!",
            "date": "2010-01-28T22:58:11.818+0000",
            "id": 29
        },
        {
            "author": "Uwe Schindler",
            "body": "Committed revision: 904401\nThanks Simon!",
            "date": "2010-01-29T07:45:55.978+0000",
            "id": 30
        }
    ],
    "component": "modules/analysis",
    "description": "CharTokenizer is an abstract base class for all Tokenizers operating on a character level. Yet, those tokenizers still use char primitives instead of int codepoints. CharTokenizer should operate on codepoints and preserve bw compatibility. ",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2183",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Supplementary Character Handling in CharTokenizer",
    "systemSpecification": true,
    "version": ""
}