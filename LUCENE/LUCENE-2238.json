{
    "comments": [
        {
            "author": "Erik Hatcher",
            "body": "+1",
            "date": "2010-01-29T05:27:51.547+0000",
            "id": 0
        },
        {
            "author": "Simon Willnauer",
            "body": "+1 I will commit this later today if nobody objects",
            "date": "2010-01-29T07:48:42.308+0000",
            "id": 1
        },
        {
            "author": "Simon Willnauer",
            "body": "committed in revision 904521\n\nthanks robert",
            "date": "2010-01-29T15:45:27.107+0000",
            "id": 2
        }
    ],
    "component": "modules/analysis",
    "description": "The ChineseAnalyzer, ChineseTokenizer, and ChineseFilter (not the smart one, or CJK) indexes chinese text as individual characters and removes english stopwords, etc.\n\nIn my opinion we should simply deprecate all of this in favor of StandardAnalyzer, StandardTokenizer, and StopFilter, which does the same thing.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2238",
    "issuetypeClassified": "OTHER",
    "issuetypeTracker": "OTHER",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "deprecate ChineseAnalyzer",
    "systemSpecification": true,
    "version": ""
}