{
    "comments": [
        {
            "author": "Shyamal Prasad",
            "body": "Patch fixes Javadoc with suggested text, adds test cases to motivate change.",
            "date": "2009-12-16T18:55:05.034+0000",
            "id": 0
        },
        {
            "author": "Robert Muir",
            "body": "Hi Shyamal, I am not sure we should document this behavior, but instead improve standard analyzer.\n\nLike you said it is hard to make everyone happy, but we now have a mechanism to improve things, that is based on that Version constant you provide.\nFor example, in a future release we hope to be able to use Jflex 1.5, which has greatly improved unicode support.\n\nyou can try your examples against unicode segmentation standards here to get a preview of what this might look like: http://unicode.org/cldr/utility/breaks.jsp\n",
            "date": "2009-12-18T01:19:51.255+0000",
            "id": 1
        },
        {
            "author": "Shyamal Prasad",
            "body": "Hi Robert, I presume that when you say we should \"instead improve standard analyzer\" you mean the code should work more like the original Javadoc states it should? Or are you suggesting that moving to Jflex 1.5 \n\nThe problem I observed was that the current JFlex rules don't implement what the Javadoc says is the  behavior of the tokenizer. I'd be happy to spend some time on this if I could get some direction on where I should focus.",
            "date": "2009-12-19T00:43:05.736+0000",
            "id": 2
        },
        {
            "author": "Robert Muir",
            "body": "bq. Hi Robert, I presume that when you say we should \"instead improve standard analyzer\" you mean the code should work more like the original Javadoc states it should?\n\nShyamal I guess what I am saying is I would prefer the javadoc of StandardTokenizer to be a little vague as to exactly what it does.\nI would actually prefer it have less details than it currently has: in my opinion it starts getting into nitty-gritty details of what could be considered Version-specific.\n\nbq. I'd be happy to spend some time on this if I could get some direction on where I should focus.\n\nIf you have fixes to the grammar, I would prefer this over 'documenting buggy behavior'. LUCENE-2074 gives us the capability to fix bugs without breaking backwards compatibility.",
            "date": "2009-12-19T01:07:26.214+0000",
            "id": 3
        },
        {
            "author": "Shyamal Prasad",
            "body": "Hi Robert,\n\nIt's been a while but  I finally got around to working on the grammar. Clearly, much of this is an opinion, so I finally stuck to the one minor change that I believe is arguably an improvement. Previously comma separated fields containing digits would be mistaken for numbers and combined into a single token. I believe this is a mistake because part numbers etc. are rarely comma separated, and regular text that is comma separated is not uncommon. This is also the problem I ran into in real life when using Lucene :)\n\nThis patch stops treating comma separated tokens as numbers when they contain digits.\n\nI did not included the patched Java file since I don't know what  JFlex version I should use to create it  (I used JFlex 1.4.3, and test-tag passes with JDK 1.5/1.6; I presume the Java 1.4 compatibility comment in the generated file is now history?).\n\nLet me know if this is headed in a useful direction.\n\nCheers!\nShyamal",
            "date": "2010-02-24T02:13:56.762+0000",
            "id": 4
        },
        {
            "author": "Robert Muir",
            "body": "bq. Clearly, much of this is an opinion, so I finally stuck to the one minor change that I believe is arguably an improvement. Previously comma separated fields containing digits would be mistaken for numbers and combined into a single token. I believe this is a mistake because part numbers etc. are rarely comma separated, and regular text that is comma separated is not uncommon.\n\nI don't think it really has to be, i actually am of the opinion StandardTokenizer should follow unicode standard tokenization. then we can throw subjective decisions away, and stick with a standard.\n\nIn this example, i think the change would be bad, as the comma is treated differently depending upon context, as it is a decimal separator and thousands separator in many languages, including English. so, the treatment of the comma depends upon the previous character.\n\nthis is why in unicode, the comma has the Mid_Num Word_Break property.\n",
            "date": "2010-02-24T03:39:29.861+0000",
            "id": 5
        },
        {
            "author": "Shyamal Prasad",
            "body": "{quote}\nI don't think it really has to be, i actually am of the opinion StandardTokenizer should follow unicode standard tokenization. then we can throw subjective decisions away, and stick with a standard.\n{quote}\n\nYep, I see I am going for the wrong ambition level and only tweaking the existing grammar. I'll take a crack at understanding unicode standard tokenization, as you'd suggested originally,  and try and produce something as soon as I get a chance. I see your point.\n\nCheers!\nShyamal",
            "date": "2010-02-24T22:58:57.747+0000",
            "id": 6
        },
        {
            "author": "Robert Muir",
            "body": "bq. I'll take a crack at understanding unicode standard tokenization, as you'd suggested originally, and try and produce something as soon as I get a chance.\n\nI would love it if you could produce a grammar that implemented UAX#29!\n\nIf so, in my opinion it should become the StandardAnalyzer for the next lucene version. If I thought I could do it correctly, I would have already done it, as the support for the unicode properties needed to do this is now in the trunk of Jflex!\n\nhere are some references that might help: \nThe standard itself: http://unicode.org/reports/tr29/\n\nparticularly the \"Testing\" portion: http://unicode.org/reports/tr41/tr41-5.html#Tests29\n\nUnicode provides a WordBreakTest.txt file, that we could use from Junit, to help verify correctness: http://www.unicode.org/Public/UNIDATA/auxiliary/WordBreakTest.txt\n\nI'll warn you I think it might be hard, but perhaps its not that bad. In particular the standard is defined in terms of \"chained\" rules, and Jflex doesnt support rule chaining, but I am not convinced we need rule chaining to implement WordBreak (maybe LineBreak, but maybe WordBreak can be done easily without it?) \n\nSteven Rowe is the expert on this stuff, maybe he has some ideas.",
            "date": "2010-02-24T23:07:12.618+0000",
            "id": 7
        },
        {
            "author": "Robert Muir",
            "body": "btw, here is some statement that seems to confirm my suspicions, from the standard:\n\nIn section 6.3, there is an example of the grapheme cluster boundaries converted into a simple regex (the kind we could do easily in jflex now that it has the properties available).\n\nThey make this statement: Such a regular expression can also be turned into a fast, deterministic finite-state machine. Similar regular expressions are possible for Word boundaries. Line and Sentence boundaries are more complicated, and more difficult to represent with regular expressions.",
            "date": "2010-02-24T23:15:12.905+0000",
            "id": 8
        },
        {
            "author": "Steve Rowe",
            "body": "I wrote word break rules grammar specifications for JFlex 1.5.0-SNAPSHOT and both Unicode versions 5.1 and 5.2 - you can see the files here:\n\nhttp://jflex.svn.sourceforge.net/viewvc/jflex/trunk/testsuite/testcases/src/test/cases/unicode-word-break/\n\nThe files are {{UnicodeWordBreakRules_5_\\*.\\*}} - these are written to: parse the Unicode test files; run the generated scanner against each composed test string; output the break opportunities/prohibitions in the same format as the test files; and then finally compare the output against the test file itself, looking for a match.  (These tests currently pass.)\n\nThe .flex files would need to be significantly changed to be used as a StandardTokenizer replacement, but you can get an idea from them how to implement the Unicode word break rules in (as yet unreleased version 1.5.0) JFlex syntax.",
            "date": "2010-02-24T23:24:09.514+0000",
            "id": 9
        },
        {
            "author": "Robert Muir",
            "body": "Steven, thanks for providing the link.\n\nI guess this is the point where I also say, I think it would be really nice for StandardTokenizer to adhere straight to the standard as much as we can with jflex (I realize in 1.5, we won't have > 0xffff support). Then its name would actually make sense.\n\nIn my opinion, such a transition would involve something like renaming the old StandardTokenizer to EuropeanTokenizer, as its javadoc claims:\n{code}\nThis should be a good tokenizer for most European-language documents\n{code}\n\nThe new StandardTokenizer could then say\n{code}\nThis should be a good tokenizer for most languages.\n{code}\n\nAll the english/euro-centric stuff like the acronym/company/apostrophe stuff could stay with that \"EuropeanTokenizer\" or whatever its called, and it could be used by the european analyzers.\n\nbut if we implement the Unicode rules, I think we should drop all this english/euro-centric stuff for StandardTokenizer. Otherwise it should be called *StandardishTokenizer*.\n\nwe can obviously preserve the backwards compat with Version, as Uwe has created a way to use a different grammar for a different Version.\n\nI expect some -1 to this, waiting comments :)",
            "date": "2010-02-24T23:46:03.375+0000",
            "id": 10
        },
        {
            "author": "Shyamal Prasad",
            "body": "Robert Muir wrote:\n{quote}\nI would love it if you could produce a grammar that implemented UAX#29!\n\nIf so, in my opinion it should become the StandardAnalyzer for the next lucene version. If I thought I could do it correctly, I would have already done it, as the support for the unicode properties needed to do this is now in the trunk of Jflex!\n{quote}\n\nI'm not smart enough to know if I should even try to do it at all (leave alone correctly), but am always willing to learn! Thanks for the references, I will certainly give it an honest try.\n\n/Shyamal",
            "date": "2010-02-25T22:43:41.717+0000",
            "id": 11
        },
        {
            "author": "Steve Rowe",
            "body": "(stole Robert's comment to change the issue description)",
            "date": "2010-04-30T22:18:03.876+0000",
            "id": 12
        },
        {
            "author": "Steve Rowe",
            "body": "Patch implementing a UAX#29 tokenizer, along with most of Robert's TestICUTokenizer tests (left out tests for Thai, Lao, and breaking at 4K chars, none of which are features of this tokenizer) - I re-upcased the downcased expected terms, and un-normalized the trailing greek lowercase sigma one of the expected terms in testGreek().",
            "date": "2010-05-06T05:01:02.946+0000",
            "id": 13
        },
        {
            "author": "Steve Rowe",
            "body": "I want to test performance relative to StandardTokenizer and ICUTokenizer, and also consider switching from lookahead chaining to single regular expression per term type to improve performance.",
            "date": "2010-05-06T05:07:22.908+0000",
            "id": 14
        },
        {
            "author": "Steve Rowe",
            "body": "I ran contrib/benchmark over 10k Reuters docs with tokenization-only analyzers; Sun JDK 1.6, Windows Vista/Cygwin; best of five:\n\n||Operation||recsPerRun||rec/s||elapsedSec||\n|StandardTokenizer|1262799|655,318.62|1.93|\n|ICUTokenizer|1268451|536,116.25|2.37|\n|UAX29Tokenizer|1268451|524,586.88|2.42|\n\nI think UAX29Tokenizer is slower than StandardTokenizer because it does the lookahead/chaining thing.  Still, decent performance.",
            "date": "2010-05-07T14:30:46.228+0000",
            "id": 15
        },
        {
            "author": "Robert Muir",
            "body": "Hi Steve, this is great progress!\n\nLooking at the code/perf, is there anyway to avoid the CharBuffer.wrap calls in updateAttributes()?\n\nIt seems since you are just appending, it might be better to use some \"append\" like:\n{noformat}\nint newLength = termAtt.length() + <length of text you are appending from zzBuffer>)\nchar bufferWithRoom[] = termAtt.resizeBuffer(newLength);\nSystem.arrayCopy(from zzBuffer into bufferWithRoom, starting at termAtt.length());\ntermAtt.setLength(newLength);\n{noformat}\n\n",
            "date": "2010-05-07T15:22:56.593+0000",
            "id": 16
        },
        {
            "author": "Steve Rowe",
            "body": "I added your change removing CharBuffer.wrap(), Robert, and it appears to have sped it up, though not as much as I would like:\n\n||Operation||recsPerRun||rec/s||elapsedSec||\n|StandardTokenizer|1262799|647,589.23|1.95|\n|ICUTokenizer|1268451|526,328.22|2.41|\n|UAX29Tokenizer|1268451|558,788.99|2.27|\n\nI plan on attempting to rewrite the grammar to eliminate chaining/lookahead this weekend.\n\n*edit*: fixed the rec/s, which were from the worst of five instead of the best of five - the elapsedSec, however, were correct.",
            "date": "2010-05-08T16:51:36.176+0000",
            "id": 17
        },
        {
            "author": "Steve Rowe",
            "body": "Attached a patch that removes lookahead/chaining.  All tests pass.\n\nUAX29Tokenizer is now in the same ballpark performance-wise as StandardTokenizer:\n\n||Operation||recsPerRun||rec/s||elapsedSec||\n|StandardTokenizer|1262799|658,737.06|1.92|\n|ICUTokenizer|1268451|542,768.94|2.34|\n|UAX29Tokenizer|1268451|668,661.56|1.90|\n",
            "date": "2010-05-09T05:07:20.870+0000",
            "id": 18
        },
        {
            "author": "Robert Muir",
            "body": "Hi Steven: this is impressive progress!\n\nWhat do you think the next steps should be?\n* should we look at any tailorings to this? The first thing that comes to mind is full-width forms, which have no WordBreak property\n* is it simple, or would it be messy, to apply this to the existing grammar (English/EuroTokenizer)? Another way to say it, is it possible for\n  English/EuroTokenizer (StandardTokenizer today) to instead be a tailoring to UAX#29, for companies,acronym, etc, such that if it encounters\n  say some hindi or thai text it will behave better?\n",
            "date": "2010-05-10T12:06:34.817+0000",
            "id": 19
        },
        {
            "author": "Steve Rowe",
            "body": "bq. should we look at any tailorings to this? The first thing that comes to mind is full-width forms, which have no WordBreak property\n\nLooks like Latin full-width letters are included (from http://www.unicode.org/Public/5.2.0/ucd/auxiliary/WordBreakProperty.txt):\n\nFF21..FF3A    ; ALetter # L&  [26] FULLWIDTH LATIN CAPITAL LETTER A..FULLWIDTH LATIN CAPITAL LETTER Z\nFF41..FF5A    ; ALetter # L&  [26] FULLWIDTH LATIN SMALL LETTER A..FULLWIDTH LATIN SMALL LETTER Z\n\nBut as you mention in a code comment in TestICUTokenizer, there are no full-width WordBreak:Numeric characters, so we could just add these to the {NumericEx} macro, I think.\n\nWas there anything else you were thinking of?\n\nbq. is it simple, or would it be messy, to apply this to the existing grammar (English/EuroTokenizer)? Another way to say it, is it possible for English/EuroTokenizer (StandardTokenizer today) to instead be a tailoring to UAX#29, for companies,acronym, etc, such that if it encounters say some hindi or thai text it will behave better?\n\nNot sure about difficulty level, but it should be possible.\n\nNaming will require some thought, though - I don't like EnglishTokenizer or EuropeanTokenizer - both seem to exclude valid constituencies.\n\nWhat do you think about adding tailorings for Thai, Lao, Myanmar, Chinese, and Japanese?  (Are there others like these that aren't well served by UAX#29 without customizations?)\n\nI'm thinking of leaving UAX29Tokenizer as-is, and adding tailorings as separate classes - what do you think?",
            "date": "2010-05-10T13:24:58.903+0000",
            "id": 20
        },
        {
            "author": "Steve Rowe",
            "body": "One other thing, Robert: what do you think of adding URL tokenization? \n\nI'm not sure whether it's more useful to have the domain and path components separately tokenized.  But maybe if someone wants that, they could add a filter to decompose?  \n\nIt would be impossible to do post-tokenization composition to get back the original URL, however, so I'm leaning toward adding URL tokenization.",
            "date": "2010-05-10T13:30:40.157+0000",
            "id": 21
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nBut as you mention in a code comment in TestICUTokenizer, there are no full-width WordBreak:Numeric characters, so we could just add these to the {NumericEx} macro, I think.\n\nWas there anything else you were thinking of?\n{quote}\n\nNo, that's it!\n\nbq. Naming will require some thought, though - I don't like EnglishTokenizer or EuropeanTokenizer - both seem to exclude valid constituencies.\n\nWhat valid constituencies do you refer to? In general the acronym,company,possessive stuff here are very english/euro-specific.\nBugs in JIRA get opened if it doesn't do this stuff right on english, but it doesn't even work at all for a lot of languages.\nPersonally I think its great to rip this stuff out of what should be a \"default\" language-independent tokenizer based on \nstandards (StandardTokenizer), and put it into the language-specific package that it belongs. Otherwise we have to \nworry about these sort of things overriding and screwing up UAX#29 rules for words in real languages.\n\nbq. What do you think about adding tailorings for Thai, Lao, Myanmar, Chinese, and Japanese? (Are there others like these that aren't well served by UAX#29 without customizations?)\n\nIt gets a little tricky: we should be careful about how we interpret what is \"reasonable\" for a language-independent default tokenizer. \nI think its \"enough\" to output the best indexing unit that is possible and relatively unambiguous to identify. I think this is a shortcut\nwe can make, because we are trying to tokenize things for information retrieval, not for other purposes. The approach for Lao, \nMyanmar, Khmer, CJK, etc in ICUTokenizer is to just output syllables as indexing unit, since words are ambiguous. Thai is based \non words, not syllables, in ICUTokenizer, which is inconsistent from this, but we get this for free, so its just a laziness thing.\n\nBy the way: none of those syllable-grammars in ICUTokenizer used chained rules, so you are welcome to steal what you want!\n\nbq. I'm thinking of leaving UAX29Tokenizer as-is, and adding tailorings as separate classes - what do you think?\n\nWell, either way I again strongly feel this logic should be tied into \"Standard\" tokenizer, so that it has better unicode behavior. I think\nit makes sense for us to have a reasonable, language-independent, standards-based tokenizer that works well for most languages.\n I think it also makes sense to have English/Euro-centric stuff thats language-specific, sitting in the analysis.en package just like we\n do with other languages.\n",
            "date": "2010-05-10T13:54:12.780+0000",
            "id": 22
        },
        {
            "author": "Robert Muir",
            "body": "bq. One other thing, Robert: what do you think of adding URL tokenization?\n\nI think I would lean towards not doing this, only because of how complex a URL can be these days. It also\nstarts to get a little ambiguous and will likely interfere with other rules (generating a lot of false positives).\n\nI guess I don't care much either way, if its strict and standards-based, it probably won't cause any harm.\nBut if you start allowing things like http urls without the http:// being present, its gonna cause some problems.",
            "date": "2010-05-10T13:58:24.373+0000",
            "id": 23
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nbq. Naming will require some thought, though - I don't like EnglishTokenizer or EuropeanTokenizer - both seem to exclude valid constituencies.\n\nWhat valid constituencies do you refer to?\n{quote}\n\nWell, we can't call it English/EuropeanTokenizer (maybe EnglishAndEuropeanAnalyzer?  seems too long), and calling it either only English or only European seems to leave the other out.  Americans, e.g., don't consider themselves European, maybe not even linguistically (however incorrect that might be).\n\nbq. In general the acronym,company,possessive stuff here are very english/euro-specific.\n\nRight, I agree.  I'm just looking for a name that covers the languages of interest unambiguously.  WesternTokenizer?  (but \"I live east of the Rockies - can I use WesternTokenizer?\"...)  Maybe EuropeanLanguagesTokenizer?  The difficulty as I see it is the messy intersection between political, geographic, and linguistic boundaries.\n\nbq. Bugs in JIRA get opened if it doesn't do this stuff right on english, but it doesn't even work at all for a lot of languages.  Personally I think its great to rip this stuff out of what should be a \"default\" language-independent tokenizer based on standards (StandardTokenizer), and put it into the language-specific package that it belongs. Otherwise we have to worry about these sort of things overriding and screwing up UAX#29 rules for words in real languages.\n\nI assume you don't mean to say that English and European languages are not real languages :) .\n\n{quote}\nbq. What do you think about adding tailorings for Thai, Lao, Myanmar, Chinese, and Japanese? (Are there others like these that aren't well served by UAX#29 without customizations?)\n\nIt gets a little tricky: we should be careful about how we interpret what is \"reasonable\" for a language-independent default tokenizer. I think its \"enough\" to output the best indexing unit that is possible and relatively unambiguous to identify. I think this is a shortcut we can make, because we are trying to tokenize things for information retrieval, not for other purposes. The approach for Lao, Myanmar, Khmer, CJK, etc in ICUTokenizer is to just output syllables as indexing unit, since words are ambiguous. Thai is based on words, not syllables, in ICUTokenizer, which is inconsistent from this, but we get this for free, so its just a laziness thing.\n{quote}\n\nI think that StandardTokenizer should contain tailorings for CJK, Thai, Lao, Myanmar, and Khmer, then - it should be able to do reasonable things for all languages/scripts, to the greatest extent possible.\n\nThe English/European tokenizer can then extend StandardTokenizer (conceptually, not in the Java sense).\n\n{quote}\nbq. I'm thinking of leaving UAX29Tokenizer as-is, and adding tailorings as separate classes - what do you think?\n\nWell, either way I again strongly feel this logic should be tied into \"Standard\" tokenizer, so that it has better unicode behavior. I think it makes sense for us to have a reasonable, language-independent, standards-based tokenizer that works well for most languages. I think it also makes sense to have English/Euro-centric stuff thats language-specific, sitting in the analysis.en package just like we\ndo with other languages.\n{quote}\n\nI agree that stuff like giving \"O'Reilly's\" the <APOSTROPHE> type, to enable so-called StandardFilter to strip out the trailing /'s/, is stupid for all non-English languages.\n\nIt might be confusing, though, for a (e.g.) Greek user to have to go look at the analysis.en package to get reasonable performance for her language.\n\nMaybe an EnglishTokenizer, and separately a EuropeanAnalyzer?  Is that what you've been driving at all along??? (Silly me....  Sigh.)",
            "date": "2010-05-10T16:35:38.587+0000",
            "id": 24
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nbq. What do you think about adding tailorings for Thai, Lao, Myanmar, Chinese, and Japanese?\n\nBy the way: none of those syllable-grammars in ICUTokenizer used chained rules, so you are welcome to steal what you want!\n{quote}\n\nThanks, I will!  Of course now that you've given permission, it won't be as much fun...",
            "date": "2010-05-10T16:45:36.136+0000",
            "id": 25
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nbq. One other thing, Robert: what do you think of adding URL tokenization?\n\nI think I would lean towards not doing this, only because of how complex a URL can be these days. It also starts to get a little ambiguous and will likely interfere with other rules (generating a lot of false positives).\n{quote}\n\nI have written standards-based URL tokenization routines in the past.  I agree it's very complex, but I know it's do-able.\n\nDo you have some examples of false positives?  I'd like to add tests for them.\n\nbq. I guess I don't care much either way, if its strict and standards-based, it probably won't cause any harm.  But if you start allowing things like http urls without the http:// being present, its gonna cause some problems.\n\nYup, I would only accept strictly correct URLs.\n\nNow that international TLDs are a reality, it would be cool to be able to identify them.\n",
            "date": "2010-05-10T16:49:14.796+0000",
            "id": 26
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nI assume you don't mean to say that English and European languages are not real languages  .\n{quote}\n\nI think the heuristics I am talking about that are in StandardTokenizer today, that don't really even work*,\nshouldn't have a negative effect on other languages, thats all. \n\n\n{quote}\nI agree that stuff like giving \"O'Reilly's\" the <APOSTROPHE> type, to enable so-called StandardFilter to strip out the trailing /'s/, is stupid for all non-English languages.\n\nIt might be confusing, though, for a (e.g.) Greek user to have to go look at the analysis.en package to get reasonable performance for her language.\n{quote}\n\nfyi, GreekAnalyzer didn't even use this stuff until 3.1 (it omitted StandardFilter).\n\nBut I don't think it matters where we put the \"western\" tokenizer, as long as its not StandardTokenizer.\nI don't really even care too much about the stuff it does honestly, I don't consider it very important, nor very\naccurate, only the source of many jira bugs* and hassle and confusion (invalidAcronym etc). \nJust seems to be more trouble than its worth.\n\n* LUCENE-1438\n* LUCENE-2244\n* LUCENE-1787\n* LUCENE-1403\n* LUCENE-1100\n* LUCENE-1556\n* LUCENE-571\n* LUCENE-34\n* LUCENE-1068\n* i stopped at this point, i think this is enough examples\n",
            "date": "2010-05-10T16:51:26.388+0000",
            "id": 27
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nYup, I would only accept strictly correct URLs.\n\nNow that international TLDs are a reality, it would be cool to be able to identify them.\n{quote}\n\n+1. This is in my opinion, the way such things in *Standard* Tokenizer should work. \nPerhaps too strict for some folks tastes, but correct!\n",
            "date": "2010-05-10T16:53:55.987+0000",
            "id": 28
        },
        {
            "author": "Marvin Humphrey",
            "body": "I find that it works well to parse URLs as multiple tokens, so long as the\nquery parser tokenizes them as phrases rather than individual terms.  That\nallows you to hit on URL substrings, so e.g. a document containing\n'http://www.example.com/index.html' is a hit for 'example.com'.\n\nHappily, no special treatment for URLs also makes for a simpler parser.",
            "date": "2010-05-10T18:37:48.518+0000",
            "id": 29
        },
        {
            "author": "Steve Rowe",
            "body": "Good point, Marvin - indexing URLs makes no sense without query support for them.  (Is this a stupid can of worms for me to have opened?)  I have used Lucene tokenizers for other things than retrieval (e.g. term vectors as input to other processes), and I suspect I'm not alone. The ability to extract URLs would be very nice.\n\nIdeally, URL analysis would produce both the full URL as a single token, and as overlapping tokens the hostname, path components, etc.  However, I don't think it's a good idea for the tokenizer to output overlapping tokens - I suspect this would break more than a few things.\n\nA filter that breaks URL type tokens into their components, and then adds them as overlapping tokens, or replaces the full URL with the components, should be easy to write, though.\n",
            "date": "2010-05-10T18:52:42.683+0000",
            "id": 30
        },
        {
            "author": "Robert Muir",
            "body": "bq. A filter that breaks URL type tokens into their components, and then adds them as overlapping tokens, or replaces the full URL with the components, should be easy to write, though.\n\nNot sure, for this to really work for non-english, it should recognize and normalize punycode representations of international domain names, etc.\n\nSo while its a good idea, maybe it is a can of worms, and better to leave it alone for now?",
            "date": "2010-05-10T19:26:21.115+0000",
            "id": 31
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nbq. A filter that breaks URL type tokens into their components, and then adds them as overlapping tokens, or replaces the full URL with the components, should be easy to write, though.\n\nNot sure, for this to really work for non-english, it should recognize and normalize punycode representations of international domain names, etc.\n\nSo while its a good idea, maybe it is a can of worms, and better to leave it alone for now?\n{quote}\n\nDo you mean URL-as-token should not be attempted now?  Or just this URL-breaking filter?\n",
            "date": "2010-05-10T20:59:04.588+0000",
            "id": 32
        },
        {
            "author": "Robert Muir",
            "body": "bq. Do you mean URL-as-token should not be attempted now? Or just this URL-breaking filter?\n\nWe can always add tailorings later, as Uwe has implemented Version-based support.\n\nPersonally I see no problems with this patch, and I think we should look at tying this in as-is as the new StandardTokenizer, still backwards compatible thanks to Version support (we can just invoke EnglishTokenizerImpl in that case).\n\nI still want to rip StandardTokenizer out of lucene core and into modules. I think thats not too far away and its probably better to do this afterwards?, but we can do it before that time if you want, doesn't matter to me.\n\nIt will be great to have StandardTokenizer working for non-European languages out of box!\n",
            "date": "2010-05-14T21:40:30.904+0000",
            "id": 33
        },
        {
            "author": "Steve Rowe",
            "body": "I think UAX29Tokenizer should remain as-is, except that I think there are some valid letter chars (Lao/Myanmar, I think) that are being dropped rather than returned as singletons, as CJ chars are now.  I need to augment the tests and make sure that valid word/number chars are not being dropped.  Also, I want to add full-width numeric chars to the {NumericEx} macro.\n\nA separate replacement StandardTokenizer class should have standards-based email and url tokenization - the current StandardTokenizer gets part of the way there, but doesn't support some valid emails, and while it recognizes host/domain names, it doesn't recognize full URLs.  I want to get this done before anything in this issue is committed.\n\nThen (after this issue is committed), in separate issues, we can add EnglishTokenizer (for things like acronyms and maybe removing posessives (current StandardFilter), and then as needed, other language-specific tokenizers.\n\nbq. I still want to rip StandardTokenizer out of lucene core and into modules. I think thats not too far away and its probably better to do this afterwards?, but we can do it before that time if you want, doesn't matter to me.\n\nI'll finish the UAX29Tokenizer fixes this weekend, but I think it'll take me a week or so to get the URL/email tokenization in place.",
            "date": "2010-05-14T21:58:26.687+0000",
            "id": 34
        },
        {
            "author": "Steve Rowe",
            "body": "Currently in StandardTokenizer there is a hack to allow contiguous Thai chars to be sent in a block to the ThaiWordFilter, which then uses the JDK BreakIterator to generate words.  \n\nRobert, were you thinking of not supporting that in the StandardTokenizer replacement in the short term?",
            "date": "2010-05-14T22:01:15.579+0000",
            "id": 35
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nCurrently in StandardTokenizer there is a hack to allow contiguous Thai chars to be sent in a block to the ThaiWordFilter, which then uses the JDK BreakIterator to generate words.\nRobert, were you thinking of not supporting that in the StandardTokenizer replacement in the short term?\n{quote}\n\nYou don't need any special support.\n\nI don't know how this hack founds its way in, but from a Thai tokenization perspective the only thing it is doing is preventing StandardTokenizer from splitting thai on non-spacing marks (like it does wrongly for other languages).\n\nSo UAX#29 itself is the fix...",
            "date": "2010-05-14T22:24:37.018+0000",
            "id": 36
        },
        {
            "author": "Robert Muir",
            "body": "bq. except that I think there are some valid letter chars (Lao/Myanmar, I think) that are being dropped rather than returned as singletons\n\nDo you have any examples?",
            "date": "2010-05-14T22:26:10.935+0000",
            "id": 37
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nbq. except that I think there are some valid letter chars (Lao/Myanmar, I think) that are being dropped rather than returned as singletons\n\nDo you have any examples?\n{quote}\n\nI imported your tests from TestICUTokenizer, but I left out Lao, Myanmar and Thai because I didn't plan on adding tailorings like those you put in for ICUTokenizer.  However, I think Lao had zero tokens output, so if you just import the Lao test from TestICUTokenizer you should see the issue.\n",
            "date": "2010-05-14T22:28:41.019+0000",
            "id": 38
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nbq. Currently in StandardTokenizer there is a hack to allow contiguous Thai chars to be sent in a block to the ThaiWordFilter, which then uses the JDK BreakIterator to generate words.\nRobert, were you thinking of not supporting that in the StandardTokenizer replacement in the short term?\n\nI don't know how this hack founds its way in, but from a Thai tokenization perspective the only thing it is doing is preventing StandardTokenizer from splitting thai on non-spacing marks (like it does wrongly for other languages).\n\nSo UAX#29 itself is the fix...\n{quote}\n\nAFAICT, UAX#29 would output individual Thai chars, just like CJ.  Is that appropriate?",
            "date": "2010-05-14T22:31:20.707+0000",
            "id": 39
        },
        {
            "author": "Robert Muir",
            "body": "bq. However, I think Lao had zero tokens output, so if you just import the Lao test from TestICUTokenizer you should see the issue.\n\nOk, I will take a look. The algorithm there has some handling for incorrectly ordered unicode, for example combining characters before the base form when they should be after... so it might be no problem at all",
            "date": "2010-05-14T22:32:51.193+0000",
            "id": 40
        },
        {
            "author": "Robert Muir",
            "body": "bq. AFAICT, UAX#29 would output individual Thai chars, just like CJ. Is that appropriate?\n\nWhat is a Thai character? :). According to the standard, it should be outputting phrases as there is nothing to delimit them... you can see this by pasting some text into http://unicode.org/cldr/utility/breaks.jsp",
            "date": "2010-05-14T22:36:31.781+0000",
            "id": 41
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nbq. AFAICT, UAX#29 would output individual Thai chars, just like CJ. Is that appropriate?\n\nWhat is a Thai character? . According to the standard, it should be outputting phrases as there is nothing to delimit them... you can see this by pasting some text into http://unicode.org/cldr/utility/breaks.jsp\n{quote}\n\nYeah, your Thai text \"\u0e01\u0e32\u0e23\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e15\u0e49\u0e2d\u0e07\u0e41\u0e2a\u0e14\u0e07\u0e27\u0e48\u0e32\u0e07\u0e32\u0e19\u0e14\u0e35. \u0e41\u0e25\u0e49\u0e27\u0e40\u0e18\u0e2d\u0e08\u0e30\u0e44\u0e1b\u0e44\u0e2b\u0e19? \u0e51\u0e52\u0e53\u0e54\" breaks at space and punctuation and nowhere else.  This test should be put back into TestUAX29Tokenizer with the appropriate expected output.",
            "date": "2010-05-14T22:40:49.992+0000",
            "id": 42
        },
        {
            "author": "Robert Muir",
            "body": "Hmm i ran some tests, I think i see your problem.\n\nI tried this:\n{code}\n  public void testThai() throws Exception {\n    assertAnalyzesTo(a, \"\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\", new String[] { \"\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\" });\n  }\n{code}\n\nThe reason you get something different than the unicode site, is because recently? these have [:WordBreak=Other:]\nInstead anything that needs a dictionary or whatever is identified by [:Line_Break=Complex_Context:]\nYou can see this mentioned in the standard:\n\n{noformat}\nIn particular, the characters with the Line_Break property values of Contingent_Break (CB), \nComplex_Context (SA/South East Asian), and XX (Unknown) are assigned word boundary property \nvalues based on criteria outside of the scope of this annex. \n{noformat}\n\nIn ICU, i noticed the default rules do this:\n$dictionary   = [:LineBreak = Complex_Context:];\n$dictionary $dictionary\n\n(so they just stick together with this chained rule)",
            "date": "2010-05-14T22:48:15.417+0000",
            "id": 43
        },
        {
            "author": "Robert Muir",
            "body": "bq. Yeah, your Thai text \"\u0e01\u0e32\u0e23\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e15\u0e49\u0e2d\u0e07\u0e41\u0e2a\u0e14\u0e07\u0e27\u0e48\u0e32\u0e07\u0e32\u0e19\u0e14\u0e35. \u0e41\u0e25\u0e49\u0e27\u0e40\u0e18\u0e2d\u0e08\u0e30\u0e44\u0e1b\u0e44\u0e2b\u0e19? \u0e51\u0e52\u0e53\u0e54\" breaks at space and punctuation and nowhere else. This test should be put back into TestUAX29Tokenizer with the appropriate expected output.\n\nBut why does it fail for my test (listed above) with only a single thai phrase (nothing is output)? \nDo you think it is because of Complex_Context or is there an off-by-one bug somehow?",
            "date": "2010-05-14T22:49:42.650+0000",
            "id": 44
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nbq. Yeah, your Thai text \"\u0e01\u0e32\u0e23\u0e17\u0e35\u0e48\u0e44\u0e14\u0e49\u0e15\u0e49\u0e2d\u0e07\u0e41\u0e2a\u0e14\u0e07\u0e27\u0e48\u0e32\u0e07\u0e32\u0e19\u0e14\u0e35. \u0e41\u0e25\u0e49\u0e27\u0e40\u0e18\u0e2d\u0e08\u0e30\u0e44\u0e1b\u0e44\u0e2b\u0e19? \u0e51\u0e52\u0e53\u0e54\" breaks at space and punctuation and nowhere else. This test should be put back into TestUAX29Tokenizer with the appropriate expected output.\n\nBut why does it fail for my test (listed above) with only a single thai phrase (nothing is output)?\nDo you think it is because of Complex_Context or is there an off-by-one bug somehow?\n{quote}\n\nDefinitely Complex_Content.  I'll add that in, and this should address Thai, Myanmar, Khmer, Tai Le, etc.",
            "date": "2010-05-14T23:40:05.922+0000",
            "id": 45
        },
        {
            "author": "Steve Rowe",
            "body": "New patch addressing the following issues:\n\n* On #lucene-dev, Uwe mentioned that methods in the generated scanner should be (package) private, since unlike the current StandardTokenizer, UAX29Tokenizer is not hidden behind a facade class. I added JFlex's %apiprivate option to fix this issue.\n* Thai, Lao, Khmer, Myanmar and other scripts' characters are now kept together, like the ICU UAX#29 implementation, using rule [:Line_Break = Complex_Context:]+.\n* Added the Thai test back from Robert's TestICUTokenizer.\n* Added full-width numeric characters to the {NumericEx} macro, so that they can be appropriately tokenized, just like full-width alpha characters are now.\n\nI couldn't find any suitable Lao test text (mostly because I don't know Lao at all), so I left out the Lao test in TestICUTokenizer, because Robert mentioned on #lucene that its characters are not in logical order.\n\n*edit* Complex_Content --> Complex_Context\n*edit #2* Added bullet about full-width numerics issue",
            "date": "2010-05-15T16:12:38.862+0000",
            "id": 46
        },
        {
            "author": "Robert Muir",
            "body": "bq. I couldn't find any suitable Lao test text (mostly because I don't know Lao at all), so I left out the Lao test in TestICUTokenizer, because Robert mentioned on #lucene that its characters are not in logical order.\n\nOnly some of my icu tests contain \"screwed up lao\".\n\nBut you should be able to use \"good text\" and it should do the right thing.\nHere's a test\n{code}\nassertAnalyzesTo(a, \"\u0eaa\u0eb2\u0e97\u0eb2\u0ea5\u0eb0\u0e99\u0eb0\u0ea5\u0eb1\u0e94 \u0e9b\u0eb0\u0e8a\u0eb2\u0e97\u0eb4\u0e9b\u0eb0\u0ec4\u0e95 \u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99\u0ea5\u0eb2\u0ea7\", \nnew String[] { \"\u0eaa\u0eb2\u0e97\u0eb2\u0ea5\u0eb0\u0e99\u0eb0\u0ea5\u0eb1\u0e94\", \"\u0e9b\u0eb0\u0e8a\u0eb2\u0e97\u0eb4\u0e9b\u0eb0\u0ec4\u0e95\", \"\u0e9b\u0eb0\u0e8a\u0eb2\u0e8a\u0ebb\u0e99\u0ea5\u0eb2\u0ea7\" });\n{code}",
            "date": "2010-05-15T16:20:19.966+0000",
            "id": 47
        },
        {
            "author": "Steve Rowe",
            "body": "New patch:\n\n* added Robert's Lao test (thanks, Robert).\n* added a javadoc comment about UAX29Tokenizer not handling supplementary characters (thanks to Uwe for bringing this up on #lucene), with a pointer to Robert's ICUTokenizer.\n",
            "date": "2010-05-15T22:52:07.283+0000",
            "id": 48
        },
        {
            "author": "Steve Rowe",
            "body": "This patch contains the benchmarking implementation I've been using.  I'm  pretty sure we don't want this stuff in Lucene, so I'm including it here only for reproducibility by others.  I have hardcoded absolute paths to the ICU4J jar and the contrib/icu jar in the script I use to run the benchmark ({{lucene/contrib/benchmark/scripts/compare.uax29.analyzers.sh}}), so if anybody tries to run this stuff, they will have to first modify that script.\n\nOn #lucene, Robert suggested comparing the performance of the straight ICU4J RBBI against UAX29Tokenizer, so I took his ICUTokenizer and associated classes, stripped out the script-detection logic, and made something I named RBBITokenizer, which is included in this patch.\n\nTo run the benchmark, you have to first run \"ant jar\" in {{lucene/}} to produce the lucene core jar, and then again in {{lucene/contrib/icu/}}.  Then in {{contrib/benchmark/}}, run {{scripts/compare.uax29.analyzers.sh}}.\n\nHere are the results on my machine (Sun JDK 1.6.0_13; Windows Vista/Cygwin; best of five):\n\n||Operation||recsPerRun||rec/s||elapsedSec||\n|ICUTokenizer|1268451|548,638.00|2.31|\n|RBBITokenizer|1268451|568,047.94|2.23|\n|StandardTokenizer|1262799|644,614.06|1.96|\n|UAX29Tokenizer|1268451|640,631.81|1.98|\n\n",
            "date": "2010-05-16T01:13:30.717+0000",
            "id": 49
        },
        {
            "author": "Robert Muir",
            "body": "bq. Here are the results on my machine (Sun JDK 1.6.0_13; Windows Vista/Cygwin; best of five):\n\nThis is really cool, I think its a great benchmark to know, I played with it and saw similar results.\n\n* For Lucene Tokenizer-ish (forward-iteration) purposes, JFlex is quite a bit faster than RBBI for unicode segmentation.\n* Supporting unicode segmentation in StandardTokenizer doesn't slow it down in comparison to the current implementation.\n* The script detection/delegation in ICU doesn't really cost that tokenizer much; though, the benchmark is reuters, and it cheats for Latin-1 (see bottom of ScriptIterator.java).\n",
            "date": "2010-05-16T05:39:51.448+0000",
            "id": 50
        },
        {
            "author": "Steve Rowe",
            "body": "Robert, what do you think of <SOUTHEAST_ASIAN> for the token type for Thai, Khmer, Lao, etc. Complex_Context runs?",
            "date": "2010-05-16T16:08:37.735+0000",
            "id": 51
        },
        {
            "author": "Steve Rowe",
            "body": "Sequences of South East Asian scripts are now assigned term type <SOUTHEAST_ASIAN> by UAX29Tokenizer.  I think UAX29Tokenizer is now a complete untailored UAX#29 implementation.  \n\nFor the future StandardTokenizer replacement, I plan on making a copy of the UAX29Tokenizer grammar and adding email/URL tokenization, and maybe Southeast Asian tailorings converted from those in ICUTokenizer.",
            "date": "2010-05-17T00:22:28.751+0000",
            "id": 52
        },
        {
            "author": "Steve Rowe",
            "body": "As of r591, JFlex now has code in the generated yyreset() method to resize the internal scan buffer (zzBuffer) back down to its initial size if it has grown.  This is exactly the same workaround code in the reset() method in the UAX29Tokenizer grammar.\n\nThis patch just removes the scan buffer size check and reallocation code from reset() in the .jflex file, as well as the .java file generated with r591 JFlex.",
            "date": "2010-05-17T05:49:09.863+0000",
            "id": 53
        },
        {
            "author": "Robert Muir",
            "body": "bq. This patch just removes the scan buffer size check and reallocation code from reset() in the .jflex file, as well as the .java file generated with r591 JFlex.\n\nWe have this code in our existing StandardTokenizer .jflex files, should we open an issue and fix these (we would have to ensure that we use a jflex > r591 for generation?) \n\nAdditionally shouldn't we regen WikipediaTokenizer etc too, I noticed it doesnt even have the hack in its .jflex file.\n",
            "date": "2010-05-17T12:01:23.503+0000",
            "id": 54
        },
        {
            "author": "Uwe Schindler",
            "body": "Yeah we should regen all jflex files when pathing this (ant jflex does this automatically, so we dont need to care). Removing the hack from StandardTokenizers jflex file should be done in an issue, but it also does not hurt if the hack stays in code.\n\nChecking the jflex version is hard to do, i think about it, maybe there is an ANT trick. Is the version noted somewhere in a class file as constant?\n\nI think we should simply reopen LUCENE-2384 (its part of 3x and trunk)",
            "date": "2010-05-17T12:08:15.693+0000",
            "id": 55
        },
        {
            "author": "Steve Rowe",
            "body": "bq. Yeah we should regen all jflex files when pathing this (ant jflex does this automatically, so we dont need to care). Removing the hack from StandardTokenizers jflex file should be done in an issue, but it also does not hurt if the hack stays in code.\n\nAgreed.  I was thinking since Robert is moving StandardTokenizer that the regen could wait until afterward.\n\nbq. Checking the jflex version is hard to do, i think about it, maybe there is an ANT trick. Is the version noted somewhere in a class file as constant?\n\nRelease version is, I think, but we're using an unreleased version ATM.  Hmm, for the SVN checkout, maybe the .svn/entries file could be checked or something?  If we go that route (and I think it's probably not a good idea), we should instead maybe be \"svn up\"ing the checkout?\n\nbq. I think we should simply reopen LUCENE-2384 (its part of 3x and trunk)\n\n+1",
            "date": "2010-05-17T12:15:11.125+0000",
            "id": 56
        },
        {
            "author": "DM Smith",
            "body": "{quote}\nbq.Naming will require some thought, though - I don't like EnglishTokenizer or EuropeanTokenizer - both seem to exclude valid constituencies.\nWhat valid constituencies do you refer to?\n{quote}\n{quote}\nWell, we can't call it English/EuropeanTokenizer (maybe EnglishAndEuropeanAnalyzer? seems too long), and calling it either only English or only European seems to leave the other out. Americans, e.g., don't consider themselves European, maybe not even linguistically (however incorrect that might be).\n{quote}\n\nTongue in cheek:\nBy and large, these are Romance languages (i.e. latin derivatives). And the constructs that are being considered for special processing for the most part are fairly recent additions to the languages. So how about *ModernRomanceAnalyzer*?",
            "date": "2010-05-17T14:10:54.735+0000",
            "id": 57
        },
        {
            "author": "Steve Rowe",
            "body": "My daughter likes the Lady Gaga song \"Bad Romance\" - why not *BadRomanceAnalyzer*?  Advertizing slogans: \"It slices your text when it's supposed to dice it, but it always apologizes afterward - how can you stay mad?\"; \"Who knew that analysis could have such catchy lyrics?\" :)",
            "date": "2010-05-17T22:06:57.612+0000",
            "id": 58
        },
        {
            "author": "Steve Rowe",
            "body": "Updated to trunk.  Tests pass.\n\nThis patch removes the jflex-* target dependencies on init, since init builds Lucene, which isn't a necessity prior running JFlex.",
            "date": "2010-05-27T06:01:01.575+0000",
            "id": 59
        },
        {
            "author": "Steve Rowe",
            "body": "Maven plugin including a mojo that generates a file containing a JFlex macro that accepts all valid ASCII top-level domains (TLDs), by downloading the IANA Root Zone Database, parsing the HTML file, and outputting ASCIITLD.jflex-macro into the analysis/common/src/java/org/apache/lucene/analysis/standard/ source directory; this file is also included in the patch.\n\nTo run the Maven plugin, first run \"mvn install\" from the lucene-buildhelper-maven-plugin/ directory, then from the {{src/java/org/apache/lucene/analysis/standard/}} directory, run the following command:\n\n{code}\nmvn org.apache.lucene:lucene-buildhelper-maven-plugin:generate-jflex-tld-macros\n{code}\n\nExecution is not yet hooked into build.xml, but this goal should run before JFlex runs.",
            "date": "2010-05-27T06:23:06.308+0000",
            "id": 60
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Steven,\n\nlooks cool, I have some suggestions:\n- Must it be a maven plugin? From what I see, the same code could be done as a simple Java Class with main() like Roberts ICU converter. The external dependency to httpclient can be replaces by simply java.net.HttpUrlConnection and the URL itsself (you can even set the no-cache directives). Its much easier from ant to invoke a java method as a build step. So why not refactor a little bit to use a main() method that acceps the target directory.\n- You use the HTML root zone database from IANA. The format of this file is hard to parse and may change suddenly. BIND administrators know, that there is also the root zone file available for BIND in the standardized named-format @ [http://www.internic.net/zones/root.zone] (ASCII only, as DNS is ASCII only). You just have to use all rows that are not comments and contain \"NS\" as second token. The nameservers behind are not used, just use the DNS name before. This should be much easier to do. A python script may also work well.\n- You can write the Last-Modified-Header of the HTTP-date (HttpURLConnection.getLastModified()) also into the generated file.\n- The database only contains the punycode enabled DNS names. But users use the non-encoded variants, so you should decode punycode, too [we need ICU for that :( ] and create patterns for that, too.\n- About changes in analyzer syntax because of regeneration: This should not be a problem, as the IANA only *adds* new zones to the file and very seldom removes some (like old yugoslavian zones). As eMails and Webadresses should *not* appear in tokenized text *before* they are in the zone file, its no problem that they suddenly later are marked as \"URL/eMail\" (as they cannot appear before). So in my opinion we can update the zone database even in minor Lucene releases without breaking analyzers.\n\n\nFine idea!",
            "date": "2010-05-27T07:18:33.444+0000",
            "id": 61
        },
        {
            "author": "Steve Rowe",
            "body": "bq. Must it be a maven plugin? [...] Its much easier from ant to invoke a java method as a build step.\n\nLucene's build could be converted to Maven, though, and this could be a place for build-related stuff.\n\nMaven Ant Tasks allows for Ant to call full Maven builds without a Maven installation: http://maven.apache.org/ant-tasks/examples/mvn.html\n\nbq. From what I see, the same code could be done as a simple Java Class with main() like Roberts ICU converter. [snip]\n\nI hadn't seen Robert's ICU converter - I'll take a look.\n\nbq. A python script may also work well.\n\nPerl is my scripting language of choice, not Python, but yes, a script would likely do the trick, assuming there are no external (Java) dependencies.  (And as you pointed out, HttpComponents, the only dependency of the Maven plugin, does not need to be a dependency.)\n\nbq. You use the HTML root zone database from IANA. The format of this file is hard to parse and may change suddenly. BIND administrators know, that there is also the root zone file available for BIND in the standardized named-format @ http://www.internic.net/zones/root.zone (ASCII only, as DNS is ASCII only).\n\nI think I'll stick with the HTML version for now - there are no decoded versions of the internationalized TLDs and no descriptive information in the named-format version.  I agree the HTML format is not ideal, but it took me just a little while to put together the regexes to parse it; when the format changes, the effort to fix will likely be similarly small.\n\nbq. You can write the Last-Modified-Header of the HTTP-date (HttpURLConnection.getLastModified()) also into the generated file.\n\nExcellent idea, I searched the HTML page source for this kind of information but it wasn't there.\n\nbq. The database only contains the punycode enabled DNS names. But users use the non-encoded variants, so you should decode punycode, too [we need ICU for that :( ] and create patterns for that, too.\n\nI agree.  However, I looked into what's required to do internationalized domain names properly, and it's quite complicated.  I plan on doing what you suggest eventually, both for TLDs and all other domain labels, but I'd rather finish the ASCII implementation and deal with IRIs in a separate follow-on issue.\n\nbq. About changes in analyzer syntax because of regeneration: This should not be a problem, as the IANA only adds new zones to the file and very seldom removes some (like old yugoslavian zones). As eMails and Webadresses should not appear in tokenized text before they are in the zone file, its no problem that they suddenly later are marked as \"URL/eMail\" (as they cannot appear before). So in my opinion we can update the zone database even in minor Lucene releases without breaking analyzers.\n\n+1\n",
            "date": "2010-05-27T13:53:09.679+0000",
            "id": 62
        },
        {
            "author": "Uwe Schindler",
            "body": "Here my patch with the TLD-macro generator:\n- Uses zone database from DNS (downloaded)\n- Outputs correct platform dependent newlines, else commits with SVN fail\n- Has no comments :(\n- Is included into build.xml. Run ant gen-tlds in modules/analysis/common\n\nThe resulting macro is almost identical, 4 TLDs are missing, but the file on internic.net is actual (see last mod date). The comments are not available, of course.",
            "date": "2010-05-27T15:19:01.012+0000",
            "id": 63
        },
        {
            "author": "Uwe Schindler",
            "body": "Small update (dont output lastMod date if internic.net gave none)",
            "date": "2010-05-27T15:47:18.722+0000",
            "id": 64
        },
        {
            "author": "Uwe Schindler",
            "body": "Updated patch.\n\nI had not seen that the previous jflex generator version had a bug in missing locale in String.toUpperCase (turkish i!). This version uses Character.toUpperCase() [non-locale-aware] and also only iterates over tld.charAt() [what was the reason for the strange substring stuff?]. This is fine, as the TLDs only contain [\\-A-Za-z0-9] (Standard for domain names and the regex enforces this, so no supplementary chars.\n\nThis patch also creates correct macro (single escaping).",
            "date": "2010-06-01T08:10:23.089+0000",
            "id": 65
        },
        {
            "author": "Steve Rowe",
            "body": "bq. This version uses Character.toUpperCase() [non-locale-aware] and also only iterates over tld.charAt() [what was the reason for the strange substring stuff?].\n\nI looked for Character.toUpperCase(), didn't find it (no idea why), so went with the strange substring stuff to use the String version instead ...\n\nI plan on integrating your patch with mine, to make a single one, including a definition for a StandardTokenizer replacement.  I have implemented URL, Email and Host rules, just gotta write some tests now.\n",
            "date": "2010-06-01T23:01:06.647+0000",
            "id": 66
        },
        {
            "author": "Steve Rowe",
            "body": "New patch incorporating Uwe's JFlex TLD macro generation patch (with a few small adjustments), and also including a jflex grammar for a new class: NewStandardTokenizer.  This grammar adds recognition of URLs, e-mail addresses, and host names and IP addresses (both v4 and v6) to the UAX29Tokenizer grammar.  \n\nThis is a work in progress -- testing for http: scheme URLs and e-mail addresses is included, but there is no testing yet for the file:, https:, or ftp: schemes.\n\nI have dropped the idea of recognizing mailto: URIs, because these seem more complicated than they are worth (mailto: URIs can include multiple email addresses, comments, full email bodies, etc.).  E-mail addresses within mailto: URIs should still be recognized.\n\nWARNING: I had to invoke Ant with a 900MB heap ({{ANT_OPTS=-Xmx900m ant jflex}} on Windows Vista, 64 bit Sun JDK 1.5.0_22) in order to allow the JFlex generation process to complete for NewStandardTokenizer; the process also took a minute or two to finish.\n\n*edit*: Sun 1. -> Sun JDK 1.5.0_22",
            "date": "2010-06-07T08:00:48.287+0000",
            "id": 67
        },
        {
            "author": "Steve Rowe",
            "body": "URL testing for NewStandardTokenizer is now complete.\n\nI have dropped the <HOST> token type, since it seems to me that, e.g., both of the following strings should be interpretable as URLs, given that they effectively refer to the same resource (when interpreted in the context of the HTTP URI scheme), and the first is clearly not just a host name:\n\n{code}\nexample.com/\n\nexample.com\n{code}\n\nBoth of the above are now marked by NewStandardTokenizer with type <URL>.\n\nNewStandardTokenizer is not quite finished; I plan on stealing Robert's Southeast Asian (Lao, Myanmar, Khmer) syllabification routines from ICUTokenizer and incorporating them into NewStandardTokenizer.  Once that's done, I think we can make NewStandardTokenizer the new StandardTokenizer.\n",
            "date": "2010-06-10T04:27:06.292+0000",
            "id": 68
        },
        {
            "author": "Robert Muir",
            "body": "bq. NewStandardTokenizer is not quite finished; I plan on stealing Robert's Southeast Asian (Lao, Myanmar, Khmer) syllabification routine\n\nCurious, what is your plan here? Do you plan to somehow \"jflex-#include\" these into the grammar so that these are longest-matched instead of the Complex_Context rule? \n\nHow to handle the cases where the grammar cannot be forward-only deterministic matching? (at least i don't see how it could be, but maybe). e.g. the lao cases where some backtracking is needed... and the combining class reordering needed for real-world text?\n\nCurious what would you plan to index for Thai, words? a grammar for TCC?\n\nAlso, some of these syllable techniques are probably not very good for search without doing a \"shingle\" later... in some cases it may perform OK like single ideographs or tibetan syllables do with the grammar you have. For others (Khmer, etc) I think the shingling is likely mandatory since they are really only a bit better than indexing grapheme clusters.\n\nAs far as needing punctuation for shingling, the similar problem already exists. For example, after tokenizing, some discarding of information (punctuation) has been lost and its too late to do a nice shingle. practical cheating/workarounds exist for CJK (you could look at the offset or something and cheat, to figure out that they were adjacent), but for something like Tibetan the type of punctuation itself is important: the tsheg being unambiguous syllable separator, but ambiguous word separator, but the shad or whitespace being both. \n\nHere is the paper I brought up at ehatcher's house recently when we were discussing tibetan, that recommends this syllable bigram technique, where the shingling is dependent on the original punctuation: http://terpconnect.umd.edu/~oard/pdf/iral00b.pdf\n\nOne alternative for the short term would be to make a tokenfilter that hooks into the ICUTokenizer logic but looks for Complex_Context, or similar. I definitely agree it would be best if standardtokenizer worked the best out of box without doing something like this.\n\nFinally, I think its worth considering a lot of this as a special case of a larger problem that affects even english. For a lot of users, punctuation such as the hyphen in english might have some special meaning and they might want to shingle or something else in that case too. Its a general problem with tokenstreams that the tokenizer often discards this information and the filters are left with only a partial picture. Some ideas to improve it would be to make use of properties like [:Terminal_Punctuation=Yes:] somehow, or to try to integrate Sentence segmentation.\n",
            "date": "2010-06-11T16:28:49.799+0000",
            "id": 69
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nbq. NewStandardTokenizer is not quite finished; I plan on stealing Robert's Southeast Asian (Lao, Myanmar, Khmer) syllabification routine\n\nCurious, what is your plan here? Do you plan to somehow \"jflex-#include\" these into the grammar so that these are longest-matched instead of the Complex_Context rule?\n{quote}\n\nSorry, I haven't looked at the details yet, but roughly, yes, what you said.\n\nbq. How to handle the cases where the grammar cannot be forward-only deterministic matching? (at least i don't see how it could be, but maybe). e.g. the lao cases where some backtracking is needed... and the combining class reordering needed for real-world text?\n\nI was thinking of trying to make regex versions of all of these, and failing that, recognize chunks that need special handling, and do that outside of matching in methods in the tokenizer class.\n\nbq. Curious what would you plan to index for Thai, words? a grammar for TCC?\n\nYou had mentioned wanting to make a Thai syllabification routine - I was thinking that either you or I would do this.\n\nbq. Also, some of these syllable techniques are probably not very good for search without doing a \"shingle\" later... in some cases it may perform OK like single ideographs or tibetan syllables do with the grammar you have. For others (Khmer, etc) I think the shingling is likely mandatory since they are really only a bit better than indexing grapheme clusters.\n\nI'm thinking of leaving shingling for later, using the conditional branching filter idea (LUCENE-2470) based on token type.\n\nbq. As far as needing punctuation for shingling, the similar problem already exists. For example, after tokenizing, some discarding of information (punctuation) has been lost and its too late to do a nice shingle. practical cheating/workarounds exist for CJK (you could look at the offset or something and cheat, to figure out that they were adjacent), but for something like Tibetan the type of punctuation itself is important: the tsheg being unambiguous syllable separator, but ambiguous word separator, but the shad or whitespace being both.\n\nYou're arguing either for in-tokenizer shingling or passing non-tokenized data out of the tokenizer in addition to the tokens.  Hmm.\n\nbq. Here is the paper I brought up at ehatcher's house recently when we were discussing tibetan, that recommends this syllable bigram technique, where the shingling is dependent on the original punctuation: http://terpconnect.umd.edu/~oard/pdf/iral00b.pdf\n\nInteresting paper. With syllable n-grams (in Tibetan anyway), you trade off (quadrupled) index size for word segmentation, but otherwise, these work equally well.\n\nbq. One alternative for the short term would be to make a tokenfilter that hooks into the ICUTokenizer logic but looks for Complex_Context, or similar. I definitely agree it would be best if standardtokenizer worked the best out of box without doing something like this.\n\nYeah, I'd rather build it into the new StandardTokenizer.\n\nbq. Finally, I think its worth considering a lot of this as a special case of a larger problem that affects even english. For a lot of users, punctuation such as the hyphen in english might have some special meaning and they might want to shingle or something else in that case too. Its a general problem with tokenstreams that the tokenizer often discards this information and the filters are left with only a partial picture. Some ideas to improve it would be to make use of properties like [:Terminal_Punctuation=Yes:] somehow, or to try to integrate Sentence segmentation.\n\nI don't understand how Sentence segmentation could help?\n\nOne other possibility is to return *everything* from the tokenizer, marking the non-tokens with an appropriate type, similar to how the ICU tokenizer works.  This has the unfortunate side effect of *requiring* post-tokenization filtering to discard non-tokens.",
            "date": "2010-06-12T13:56:49.196+0000",
            "id": 70
        },
        {
            "author": "Robert Muir",
            "body": "bq. You had mentioned wanting to make a Thai syllabification routine - I was thinking that either you or I would do this.\n\nOK, this makes sense. \n\nbq. You're arguing either for in-tokenizer shingling or passing non-tokenized data out of the tokenizer in addition to the tokens. Hmm.\n\nOr attributes that mark sentence boundaries. or bumped position increments for sentence boundaries (that also prevent phrase searches across sentences). or maybe other ideas.\n\nbq. Interesting paper. With syllable n-grams (in Tibetan anyway), you trade off (quadrupled) index size for word segmentation, but otherwise, these work equally well.\n\nCareful, the way they did the measurement only tells us that neither one is absolute shit, but i dont think its clear yet they are equal.\neither way, the argument in the paper is for bigrams (n=2)... how is this quadrupled index size? its just like CJKTokenizer...\n\n{quote}\nI don't understand how Sentence segmentation could help?\n\nOne other possibility is to return everything from the tokenizer, marking the non-tokens with an appropriate type, similar to how the ICU tokenizer works. This has the unfortunate side effect of requiring post-tokenization filtering to discard non-tokens.\n{quote}\n\nRight, but it could be attributes or position increments for sentence boundaries too. then you just wouldnt shingle across missing position increments, and phrase queries wouldnt match across sentence boundaries either.\n\nIn my opinion, I think the patch here already solves a lot of problems on its own, and I suggest we explore these ideas later (including thai etc) in a separate issue. With the patch as-is now, people can use the ThaiWordFilter. If they need support for the other languages, they have ICUTokenizer as a workaround. We could think about how to do the more complex stuff in more general ways (sentence seg., conditional branching, etc).\n\nIn general i'd like to think that UAX#29 sentence segmentation, implemented nicely, would be a cool feature that could help with some of these problems, and maybe other problems too. Perhaps it could be re-used by highlighting etc as well.\n\n",
            "date": "2010-06-12T14:15:23.225+0000",
            "id": 71
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nbq. Interesting paper. With syllable n-grams (in Tibetan anyway), you trade off (quadrupled) index size for word segmentation, but otherwise, these work equally well.\n\nCareful, the way they did the measurement only tells us that neither one is absolute shit, but i dont think its clear yet they are equal.\neither way, the argument in the paper is for bigrams (n=2)... \n{quote}\n\nYes, you're right - fine-grained performance comparisons are inappropriate here.  You've said for other language(s?) that unigram/bigram combo works best - too bad they didn't test that here.\n\nbq. how is this quadrupled index size? its just like CJKTokenizer...\n\nFrom the paper:\n\n{quote}\nAs has been observed in other languages [Miller et al., 2000], ngram indexing resulted in explosive growth in the number of terms with increasing n. The index size for word-based indexing was less than one quarter of that of syllable bigrams.\n{quote}\n\nbq. In general i'd like to think that UAX#29 sentence segmentation, implemented nicely, would be a cool feature that could help with some of these problems, and maybe other problems too.\n\nYou mentioned it would be useful to eliminate phrase matches across sentence boundaries - what other problems would it solve?\n",
            "date": "2010-06-12T15:22:31.619+0000",
            "id": 72
        },
        {
            "author": "Robert Muir",
            "body": "bq. Yes, you're right - fine-grained performance comparisons are inappropriate here. You've said for other language(s?) that unigram/bigram combo works best - too bad they didn't test that here.\n\nagreed!\n\nbq. You mentioned it would be useful to eliminate phrase matches across sentence boundaries - what other problems would it solve?\n\nin addition to inhibiting phrase matches, the sentence boundaries themselves (however we would represent them) could be used by later filters: such as inhibiting shingle generation, inhibiting multi-word synonym generation, ... I am sure there are some other ways too that don't immediately come to mind. \n\nat the moment the cleanest way I can think of doing this would be to bump the position increment, but who knows. there doesnt' seem to be a de-facto way of doing this, since nothing in lucene out of box implements or uses sentence boundaries really, which is sad!\n",
            "date": "2010-06-12T15:32:15.991+0000",
            "id": 73
        },
        {
            "author": "Robert Muir",
            "body": "by the way Steven, one alternative idea i had before for this was to have a jflex or rbbi-powered charfilter for sentences.\n\nyou could provide it with string constants in the ctor to replace sentence boundaries, to add position increments just add these to your stopfilter.\n\nthe advantage to this would be that you could use it with other tokenizers by using this special token (i guess just be careful which one you use!).\n\nsorry to stray off topic a bit with this, but i think its sorta a missing piece thats relevant and becomes more important with ComplexContext :)",
            "date": "2010-06-12T15:43:00.777+0000",
            "id": 74
        },
        {
            "author": "Steve Rowe",
            "body": "I'm looking at UAX#29 sentence breaking rules, and this one looks suspicious to me:\n\n{quote}\nBreak after paragraph separators.\nSB4. \tSep | CR | LF \t\u00f7 \t \n{quote}\n\nLots of text I look at includes newlines that don't indicate paragraph boundaries.  In the implementations of sentence breaking that I've done, I always use double newlines for this purpose.  Thoughts?\n\nI'm thinking that it would be difficult to (correctly) incorporate sentence-boundary rules directly into the existing word-boundary rules.  Maybe a two-pass arrangement, where the sentence-boundary detector passes sentences as complete inputs to a word-boundary detector?",
            "date": "2010-06-12T15:53:53.720+0000",
            "id": 75
        },
        {
            "author": "Robert Muir",
            "body": "bq. Lots of text I look at includes newlines that don't indicate paragraph boundaries.\n\nWhat is this text? Some manually-wrapped text? \n\nI mean, i guess the whole point is a reasonable default, yet tailorable with a grammar.\n\nbq. Maybe a two-pass arrangement, where the sentence-boundary detector passes sentences as complete inputs to a word-boundary detector?\n\nWell this is why i liked the charfilter idea. then its separate and optional, and you can do what you want with the sentence boundary indicator strings.\n",
            "date": "2010-06-12T16:01:10.913+0000",
            "id": 76
        },
        {
            "author": "Steve Rowe",
            "body": "bq. by the way Steven, one alternative idea i had before for this was to have a jflex or rbbi-powered charfilter for sentences.\n\nnice idea - composition becomes simpler.\n\n{quote}\nyou could provide it with string constants in the ctor to replace sentence boundaries, to add position increments just add these to your stopfilter.\n\nthe advantage to this would be that you could use it with other tokenizers by using this special token (i guess just be careful which one you use!).\n{quote}\n\nWhy not just insert {{U+2029 PARAGRAPH SEPARATOR (PS)}}?  Then it will also trigger word boundaries, and tokenizers that care about appropriately responding to it can specialize for just this one, instead of having to also be aware of whatever it was that the user specified in the ctor to the charfilter.\n\nbq. sorry to stray off topic a bit with this, but i think its sorta a missing piece thats relevant and becomes more important with ComplexContext\n\nI like where this is going - toward a solid general solution.\n\n{quote}\nbq. Lots of text I look at includes newlines that don't indicate paragraph boundaries.\n\nWhat is this text? Some manually-wrapped text?\n{quote}\n\nEmail.  Source code.  TREC collections (I think - don't have any right here with me).  And yes, manually generated and wrapped text.  Isn't most text manually generated? :)\n",
            "date": "2010-06-12T16:16:41.626+0000",
            "id": 77
        },
        {
            "author": "Robert Muir",
            "body": "bq. Why not just insert U+2029 PARAGRAPH SEPARATOR (PS)?\n\nI would argue because its a sentence boundary, not a paragraph boundary :)\n\nBut i thought it would be best to just allow the user to specify the replacement string (which could be just U+2029 if you want).\nThey could also use \"<boundary/>\" or something entirely different.\n\nbq. and tokenizers that care about appropriately responding to it can specialize for just this one, instead of having to also be aware of whatever it was that the user specified in the ctor to the charfilter.\n\nwell, by default these filters could just work with position increments appropriately, and you add whatever string you use to a stopword filter to create these position increments.\n\nbq. I like where this is going - toward a solid general solution.\n\nGood, if we get some sorta plan we should open a new JIRA issue i think.\n\nbq. Email. Source code. TREC collections (I think - don't have any right here with me). And yes, manually generated and wrapped text. Isn't most text manually generated?\n\nRight, but unicode encodes character :) So things like text wrapping in my opinion belongs in the display component, and not in a character encoding model... most modern text in webpages etc isnt manually wrapped like this.\n\nI think our default implementation should be for unicode text. for the non-unicode text you speak of, you can just tailor the default rules.\n\n\n\n",
            "date": "2010-06-12T16:25:21.670+0000",
            "id": 78
        },
        {
            "author": "Steve Rowe",
            "body": "Ok, so for sentence boundaries, we're talking about a separate composable implementation.\n\nWhat, then, will the replacement for StandardAnalyzer be?  This issue needs to include a substitute definition when replacing StandardTokenizer.\n\n Which of these should be included, in addition to NewStandardTokenizer?:\n\n# SentenceBoundaryCharFilter (clunky name, but descriptive)\n# LowerCaseFilter\n# StopFilter\n",
            "date": "2010-06-12T20:12:54.225+0000",
            "id": 79
        },
        {
            "author": "Robert Muir",
            "body": "bq. Which of these should be included, in addition to NewStandardTokenizer?:\n\nI would say only lowercase + stop.\n\nthe charfilter would just be another optional charfilter, like html-stripping.\nI don't think it should be enabled by standardanalyzer by default, especially for performance reasons.\n",
            "date": "2010-06-12T20:30:29.858+0000",
            "id": 80
        },
        {
            "author": "Robert Muir",
            "body": "OK I created LUCENE-2498 for the sentence boundary charfilter idea. \n\nI think this is really unrelated to standardtokenizer except that its also using the same jflex functionality :)",
            "date": "2010-06-12T20:39:44.913+0000",
            "id": 81
        },
        {
            "author": "Steve Rowe",
            "body": "This is the benchmarking patch brought up-to-date with trunk, and with NewStandardTokenizer added in to the list of tested tokenizers.\n\nHere are the results on my machine (Sun JDK 1.6.0_13; Windows Vista/Cygwin; best of five):\n\n||Operation||recsPerRun||rec/s||elapsedSec||\n|NewStandardTokenizer|1268450|654,852.88|1.94|\n|UAX29Tokenizer|1268451|679,042.31|1.87|\n|StandardTokenizer|1262799|680,021.00|1.86|\n|RBBITokenizer|1268451|575,261.25|2.20|\n|ICUTokenizer|1268451|557,315.88|2.28|\n\nNewStandardTokenizer is consistently slower than UAX29Tokenizer and StandardTokenizer, but still faster than the ICU implementation; it appears that URL and Email tokenization have slowed things down a little bit.  IMHO, recognizing them is worth taking a small hit in throughput.",
            "date": "2010-06-13T04:08:08.272+0000",
            "id": 82
        },
        {
            "author": "Steve Rowe",
            "body": "After a discussion with Robert on #lucene, I think this issue is complete - we can add more stuff later in a separate issue.",
            "date": "2010-06-17T00:00:18.656+0000",
            "id": 83
        },
        {
            "author": "Robert Muir",
            "body": "zip file of my current integration progress.\n\nthe zip file is relevant to modules/analysis/common.\n\nnot all the tests pass as we have to figure a few things out...\nthe first thing to figure out is TestEmails/Urls in TestStandardAnalyzer (currently commented out)\n\nthe problem is how to get the bracketed rules to work without actually including the brackets in the tokens, while using StandardTokenizerInterface.",
            "date": "2010-06-29T16:03:38.906+0000",
            "id": 84
        },
        {
            "author": "Steve Rowe",
            "body": "Robert,\n\nSpecial handling for bracketed URLs makes no sense - that rule can be dropped.\n\nBracketed emails are useful, though, since the domain in the host portion doesn't need to be a registerable domain.  I think this could be handled with two changes to the bracketed email rule.  Here it is in the form you wrote:\n\n{code}\n\"<\" {EMAILaddressLoose} \">\" { return EMAIL_TYPE; }\n{code}\n\nHere is my suggestion:\n\n{code}\n\"<\" {EMAILaddressLoose} / \">\" { ++zzStartRead; return EMAIL_TYPE; }\n{code}\n\nThis combines incrementing the start of the matched region ({{++zzStartRead;}}) and lookahead for the trailing angle bracket ({{/ \">\"}}).  AFAICT, directly modifying {{zzStartRead}} shouldn't cause any problems.  After this rule completes, the trailing angle bracket will be at the beginning of the remaining text to be matched.\n",
            "date": "2010-06-29T19:14:52.867+0000",
            "id": 85
        },
        {
            "author": "Robert Muir",
            "body": "ok here is a patch file. before applying it, you have to run these commands:\n\n{noformat}\n# original grammar -> ClassicTokenizerImpl\nsvn move modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.java modules/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.java\nsvn move modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImplOrig.jflex modules/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizerImpl.jflex\n# this one is not needed, this patch becomes the new grammar\nsvn delete modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.java\nsvn delete modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizerImpl31.jflex\n# expose the old tokenizer, not just via Version, but also as ClassicAnalyzer/Tokenizer/Filter\nsvn copy modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardAnalyzer.java modules/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicAnalyzer.java\nsvn copy modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardTokenizer.java modules/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicTokenizer.java\nsvn copy modules/analysis/common/src/java/org/apache/lucene/analysis/standard/StandardFilter.java modules/analysis/common/src/java/org/apache/lucene/analysis/standard/ClassicFilter.java\nsvn copy modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestStandardAnalyzer.java modules/analysis/common/src/test/org/apache/lucene/analysis/core/TestClassicAnalyzer.java\n# temporarily edit solr/src/java/org/apache/solr/analysis/StandardFilterFactory.java (change the $Id hossman.... to just $Id$)\n# apply the patch.\n{noformat}\n\nif you want to iterate on the patch, make your changes and generate a patch with 'svn --no-diff-deleted'.\n\nsome notes:\n* patch is against 4.0, but i think we can do this in 3.1. all the back compat is preserved, etc. we just gotta figure a few things out. all the tests pass though.\n* The patch is large mainly because of the DFA size. I have some concerns about this... the email/url stuff seems to be the culprit, as the UAX#29 generated class is only 12KB, about the same size as our existing standardtokenizer.\n* I gave backwards compat (you get the old behavior) with Version, but also setup ClassicAnalyzer/Tokenizer/Filter for those that want the...not so international-friendly old version, for its company Identification, etc.\n* I modified token types for icu to be more consistent with this.\n* StandardFilter is currently a no-op for the new grammar. In my opinion this is a place to implement the 'more sophisticated' logic that the standard mentions for certain scripts. We can use token types (IDEOGRAPHIC, SOUTHEAST_ASIAN) to drive this. This way the standardanalyzer is a reasonable tokenizer for most languages.\n\nSo, not completely sure this is the best approach, but it is one... the patch is still rough around the edges but at least now we can iterate more easily on it.\n",
            "date": "2010-06-30T14:21:30.898+0000",
            "id": 86
        },
        {
            "author": "Robert Muir",
            "body": "attached is an updated patch.\n\nSteven and I debugged the large DFA size and reduced it somewhat (.class file drops from 167,945 bytes to 52,399 bytes).\n",
            "date": "2010-07-01T00:02:52.059+0000",
            "id": 87
        },
        {
            "author": "Steve Rowe",
            "body": "Attaching benchmark patch brought up-to-date with Robert's last patch.\n\nHere are the current results on my machine:\n\n||Operation||recsPerRun||rec/s||elapsedSec||\n|ClassicTokenizer|1262799|644,943.31|1.96|\n|ICUTokenizer|1268451|546,040.06|2.32|\n|RBBITokenizer|1268451|570,090.31|2.22|\n|StandardTokenizer|1268450|659,963.56|1.92|\n|UAX29Tokenizer|1268451|643,883.75|1.97|\n",
            "date": "2010-07-11T20:41:25.497+0000",
            "id": 88
        },
        {
            "author": "Robert Muir",
            "body": "Thanks Steven! Looks to me like we have resolved the perf problem?!\n",
            "date": "2010-07-12T12:02:49.750+0000",
            "id": 89
        },
        {
            "author": "Steve Rowe",
            "body": "bq. Thanks Steven! Looks to me like we have resolved the perf problem?! \n\nI don't know... I'll run it a few more times tonight and see if it's consistent.",
            "date": "2010-07-12T13:29:06.824+0000",
            "id": 90
        },
        {
            "author": "Steve Rowe",
            "body": "I ran it three more times, and it appears that the difference between ClassicTokenizer, UAX29Tokenizer, and the new StandardTokenizer is in the noise:\n\n||Operation||recsPerRun||rec/s||elapsedSec||\n|ClassicTokenizer|1262799|665,682.12|1.90|\n|ICUTokenizer|1268451|553,666.94|2.29|\n|RBBITokenizer|1268451|575,261.25|2.20|\n|StandardTokenizer|1268450|658,935.06|1.92|\n|UAX29Tokenizer|1268451|642,579.00|1.97|\n||Operation||recsPerRun||rec/s||elapsedSec||\n|ClassicTokenizer|1262799|668,501.31|1.89|\n|ICUTokenizer|1268451|546,275.19|2.32|\n|RBBITokenizer|1268451|563,255.31|2.25|\n|StandardTokenizer|1268450|651,824.25|1.95|\n|UAX29Tokenizer|1268451|664,806.62|1.91|\n||Operation||recsPerRun||rec/s||elapsedSec||\n|ClassicTokenizer|1262799|674,932.69|1.87|\n|ICUTokenizer|1268451|541,841.50|2.34|\n|RBBITokenizer|1268451|586,431.38|2.16|\n|StandardTokenizer|1268450|635,814.56|2.00|\n|UAX29Tokenizer|1268451|650,487.69|1.95|",
            "date": "2010-07-13T12:54:41.145+0000",
            "id": 91
        },
        {
            "author": "Steve Rowe",
            "body": "I tried increasing the number of documents in the benchmark alg from 10k to 50k, but apparently 50k docs was too much to fit into my OS FS cache, because it thrashed the whole time, and performance was more than an order of magnitude worse.\n\nI increased the number of rounds from 5 to 25, and increased the number of documents from 10k to 20k - below are three runs with these settings:\n\n||Operation||recsPerRun||rec/s||elapsedSec||\n|ClassicTokenizer|2467769|669,134.75|3.69|\n|ICUTokenizer|2481688|548,924.56|4.52|\n|RBBITokenizer|2481688|573,270.50|4.33|\n|StandardTokenizer|2481687|656,704.69|3.78|\n|UAX29Tokenizer|2481688|661,254.44|3.75|\n||Operation||recsPerRun||rec/s||elapsedSec||\n|ClassicTokenizer|2467769|667,867.12|3.69|\n|ICUTokenizer|2481688|546,025.94|4.54|\n|RBBITokenizer|2481688|576,466.44|4.30|\n|StandardTokenizer|2481687|656,878.50|3.78|\n|UAX29Tokenizer|2481688|665,510.31|3.73|\n||Operation||recsPerRun||rec/s||elapsedSec||\n|ClassicTokenizer|2467769|664,092.81|3.72|\n|ICUTokenizer|2481688|551,486.25|4.50|\n|RBBITokenizer|2481688|581,191.56|4.27|\n|StandardTokenizer|2481687|655,317.38|3.79|\n|UAX29Tokenizer|2481688|663,021.12|3.74|\n\nThese are more consistent.  I think the ~3% performance hit for the new StandardTokenizer over ClassicTokenizer is acceptable.",
            "date": "2010-07-14T15:05:33.841+0000",
            "id": 92
        },
        {
            "author": "Robert Muir",
            "body": "Steven, thanks for all these benchmarks.\n\nI think any perf issues are resolved, I also think the DFA size is more manageable from our previous changes, and arguably ok now (ill defer to your judgement on whether we need to attack this more though).\n\nI have a few more questions:\n* Are there still ipv6 issues you wanted to address? I cant remember (lost in the std documents) but I think you found grammar improvements?\n* What about standardfilter with the new scheme? The previous impl does some 'cleanup' on the tokenizer, in the latest patch its a TODO/no-op for Version >= 3.1. Are there any email/url/other things we need to do here? on the unicode side, i think if we want to do anything here, it should be the \"more sophisticated mechanism\" for the SE asian (as then its name Standard would also make sense)... leave as a no-op for now with Version >= 3.1?\n",
            "date": "2010-07-14T15:28:35.628+0000",
            "id": 93
        },
        {
            "author": "Steve Rowe",
            "body": "bq. I think any perf issues are resolved, I also think the DFA size is more manageable from our previous changes, and arguably ok now (ill defer to your judgement on whether we need to attack this more though).\n\nTo address the DFA size I want to try your previous suggestion of a simpler IPv6 regex in the JFlex grammar, then full validation in the action via a java.util.regex NFA.   You've previously said that you thought returning a new type like INVALID_URL would be fine, but I'd prefer not to do that - I want to try backing out and trying an alternate path if this action-based validation fails.\n\nbq. What about standardfilter with the new scheme?\n\nI don't have an opinion on this one, except that it seems a little weird to have a no-op filter in the standard analyzer chain.\n\n",
            "date": "2010-07-14T19:05:09.910+0000",
            "id": 94
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nbq. I think any perf issues are resolved, I also think the DFA size is more manageable from our previous changes, and arguably ok now (ill defer to your judgement on whether we need to attack this more though).\n\nTo address the DFA size I want to try your previous suggestion of a simpler IPv6 regex in the JFlex grammar, then full validation in the action via a java.util.regex NFA. You've previously said that you thought returning a new type like INVALID_URL would be fine, but I'd prefer not to do that - I want to try backing out and trying an alternate path if this action-based validation fails.\n{quote}\n\nThe attached {{StandardTokenizerImpl.jflex}} is the result of my attempt, which appears to be successful - tests all pass.\n\nHowever, the resultant .class file size is even larger than before: 67,947 bytes.\n\nI give up: I think we should go with the full-blown IPv6 regex as part of the DFA.",
            "date": "2010-07-19T05:18:32.699+0000",
            "id": 95
        },
        {
            "author": "Steve Rowe",
            "body": "This patch contains 3 modifications: \n# The {{IPv6Address}} macro in {{StandardTokenizerImpl.jflex}} now makes everything in front of the double colon optional, so that e.g. \"::\" alone is a valid address.\n# The {{EMAILbracketedHost}} macro in {{StandardTokenizerImpl.jflex}} now contains IPv6 and IPv4 addresses, along with a comment about how DFA minimization keeps the size of the resulting DFA in check.\n# Renamed the {{EMAILaddressStrict}} macro to {{EMAIL}} in {{StandardTokenizerImpl.jflex}}.\n# The {{root.zone}} file format has changed (hunh? why? I don't know anything about DNS...), so I modified {{GenerateJflexTLDMacros.java}} to parse the current format in addition to the previous format.\n\nThis version looks roughly the same in terms of performance - below are the numbers for the 25 round, 20k doc benchmark:\n\n||Operation||recsPerRun||rec/s||elapsedSec||\n|ClassicTokenizer|2467769|661,245.69|3.73|\n|ICUTokenizer|2481688|544,827.25|4.55|\n|RBBITokenizer|2481688|571,817.50|4.34|\n|StandardTokenizer|2481687|650,848.94|3.81|\n|UAX29Tokenizer|2481688|655,317.69|3.79|\n\nFor some reason, the size of the .class file for {{StandardTokenizerImpl.jflex}} is smaller: 51,798 bytes.\n",
            "date": "2010-07-19T06:44:29.942+0000",
            "id": 96
        },
        {
            "author": "Steve Rowe",
            "body": "Attached patch includes a Perl script to generate a test based on Unicode.org's WordBreakTest.txt UAX#29 test sequences, along with the java source generated by the Perl script.  Both UAX29Tokenizer and StandardTokenizerImpl are tested, and all Lucene and Solr tests pass.  I added a note to modules/analyzer/NOTICE.txt about the Unicode.org data files used in creating the test class.\n\nThis test suite turned up a problem in both tested grammars: the WORD_TYPE rule could match zero characters, and so was in certain cases involving underscores returning a zero-length token instead of end-of-stream.  I fixed the issue by changing the rule in both grammars to require at least one character for a match to succeed.  All test sequences are now successfully tokenized.\n\nI attempted to also test ICUAnalyzer, but since it downcases, the expected tokens are incorrect in some cases.  I didn't pursue it further.\n\nI ran the best-of-25-rounds/20k docs benchmark, and the grammar change has not noticeably affected the results.\n",
            "date": "2010-07-26T06:18:22.081+0000",
            "id": 97
        },
        {
            "author": "Steve Rowe",
            "body": "Removed unnecessarily re-generated WikipediaTokenizerImpl.java in the previous patch from this patch.",
            "date": "2010-07-26T06:27:42.608+0000",
            "id": 98
        },
        {
            "author": "Simon Willnauer",
            "body": "Last update is a month ago - any idea how far away this is from being committable?",
            "date": "2010-08-31T10:21:01.721+0000",
            "id": 99
        },
        {
            "author": "Steve Rowe",
            "body": "bq. Last update is a month ago - any idea how far away this is from being committable? \n\nTrunk version functionality is complete.  Needs docs and backporting to 3.x branch.  \n\nI'm finishing up LUCENE-2611 (to make backporting a little less painful), and then I'll get back to this issue.\n\nRough completion estimate for this issue: 2010-09-13 @ 02:37 GMT-5.",
            "date": "2010-08-31T12:52:17.648+0000",
            "id": 100
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. Rough completion estimate for this issue: 2010-09-13 @ 02:37 GMT-5.\n\nAwesome!",
            "date": "2010-08-31T15:44:54.672+0000",
            "id": 101
        },
        {
            "author": "Robert Muir",
            "body": "bq. Trunk version functionality is complete. Needs docs and backporting to 3.x branch.\n\nI agree, the testing is now very nice. \nFor example, when we want to bump to Unicode 6.0 we can autogenerate a test class from the 6.0 data files with the perl script.\nGreat work.\n",
            "date": "2010-08-31T23:56:16.506+0000",
            "id": 102
        },
        {
            "author": "Robert Muir",
            "body": "bq. I'm finishing up LUCENE-2611 (to make backporting a little less painful), and then I'll get back to this issue.\n\nBy the way, I dont think you need to produce an explicit 3.x patch?\nwe should be able to svn merge without much trouble I think.",
            "date": "2010-09-01T01:04:30.746+0000",
            "id": 103
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nBy the way, I dont think you need to produce an explicit 3.x patch?\nwe should be able to svn merge without much trouble I think.\n{quote}\n\nGreat, for some reason I thought you had said that backporting would require lots of decisions, so I assumed it would require a separate patch.\n\nThat leaves documentation.  I think I need a MIGRATE.txt entry, some package-level documentation, and notes cross-referencing from ClassicTokenizer/Analyzer to StandardTokenizer/Analyzer and vice-versa.  Anything else?",
            "date": "2010-09-01T15:40:59.022+0000",
            "id": 104
        },
        {
            "author": "Robert Muir",
            "body": "bq. That leaves documentation. I think I need a MIGRATE.txt entry, some package-level documentation, and notes cross-referencing from ClassicTokenizer/Analyzer to StandardTokenizer/Analyzer and vice-versa. Anything else?\n\nAgreed, though the change is completely backwards compatible, so I don't know if we need a MIGRATE.txt entry?\n\n(separately I realize its a big change, but there is no back compat issue)\n",
            "date": "2010-09-01T15:43:53.103+0000",
            "id": 105
        },
        {
            "author": "Steve Rowe",
            "body": "Updated to trunk.  All tests pass.  Documentation improved at package and class level.  modules/analysis/CHANGES.txt entry included.\n\nI think this is ready to commit.",
            "date": "2010-09-15T08:07:54.450+0000",
            "id": 106
        },
        {
            "author": "Robert Muir",
            "body": "bq. I think this is ready to commit.\n\nI think so too, i applied the svn moves and the patch and all tests pass.\n\nOne last question, it might be reasonable to move ClassicTokenizer and friends to .classic package?\nThere is nothing standards-based about them at all and it makes the .standard directory a little confusing.\n\nTo do this i would have to make StandardTokenizerInterface public, but it could marked @lucene.internal.\n",
            "date": "2010-09-15T15:02:24.985+0000",
            "id": 107
        },
        {
            "author": "Robert Muir",
            "body": "bq. One last question, it might be reasonable to move ClassicTokenizer and friends to .classic package?\n\nby the way, if we decide this is best, i would like to open a new issue for it.\nwe don't have to do everything in one step, and currently this patch cleanly applies with the svn move instructions.\n\nso I would like to commit this patch in a few days as-is if they are no objections.\n\nif we want to improve packaging lets open a followup-issue.\n",
            "date": "2010-09-15T15:08:32.090+0000",
            "id": 108
        },
        {
            "author": "Steve Rowe",
            "body": "bq. One last question, it might be reasonable to move ClassicTokenizer and friends to .classic package?\n\nI agree with your arguments about moving to .classic package.  I think new users won't care about what StandardTokenizer/Analyzer used to be.\n\nMy only concern here is existing users' upgrade experience - users should be able to continue using the ClassicTokenizer if they want to keep current behavior.  Right now, they can do that by setting Version to 3.0 in the constructor to StandardTokenizer/Analyzer.  I think this should remain the case until Lucene version 5.0.\n",
            "date": "2010-09-15T15:27:30.958+0000",
            "id": 109
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nMy only concern here is existing users' upgrade experience - users should be able to continue using the ClassicTokenizer if they want to keep current behavior. Right now, they can do that by setting Version to 3.0 in the constructor to StandardTokenizer/Analyzer. I think this should remain the case until Lucene version 5.0.\n{quote}\n\nI agree completely, i think we can do this though with the Classic stuff in a separate package? (like we can have both)",
            "date": "2010-09-15T15:32:51.433+0000",
            "id": 110
        },
        {
            "author": "Steve Rowe",
            "body": "bq. I agree completely, i think we can do this though with the Classic stuff in a separate package? (like we can have both)\n\nRight, I didn't mean to say that moving the Classic stuff out of .standard was antithetical to preserving Classic functionality in StandardTokenizer - I just wanted to make sure that we agreed that that move doesn't mean complete separation (yet).  Sounds like we agree.",
            "date": "2010-09-15T15:39:25.384+0000",
            "id": 111
        },
        {
            "author": "Simon Willnauer",
            "body": "bq.    Assignee: Steven Rowe  (was: Robert Muir)\nYay! :)",
            "date": "2010-09-28T05:21:47.998+0000",
            "id": 112
        },
        {
            "author": "Steve Rowe",
            "body": "Sync'd to trunk (TestThaiAnalyzer.java had conflicts).  All tests pass.  Committing shortly.",
            "date": "2010-09-28T06:06:48.821+0000",
            "id": 113
        },
        {
            "author": "Steve Rowe",
            "body": "Committed to trunk r1002032.\n\nI'll work on merging to the 3.X branch tomorrow.",
            "date": "2010-09-28T06:19:26.662+0000",
            "id": 114
        },
        {
            "author": "Steve Rowe",
            "body": "Backported to 3.x branch revision 1002468",
            "date": "2010-09-29T04:46:52.324+0000",
            "id": 115
        },
        {
            "author": "Robert Muir",
            "body": "I'd like to re-open this issue.\n\nI think that full urls as tokens is not a good default for StandardTokenizer, because i don't think users ever search\non full URLS. its also dangerous, many apps that upgrade will find themselves with huge terms dictionaries, \nand different performance characteristics.\n\ni think it would be better if standardtokenizer just implemented the uax#29 algorithm. the url identification we could\nkeep as a separate tokenizer for people that want that.",
            "date": "2010-11-07T10:48:10.757+0000",
            "id": 116
        },
        {
            "author": "Michael McCandless",
            "body": "+1\n\nWhen I indexed Wikipedia w/ StandardAnalyzer I saw a huge number of full-url tokens, which is just silly as a default.  Inserting WordDelimiterFilter fixed it, but, I don't think StandardTokenizer should produce whole URLs as tokens, to begin with.",
            "date": "2010-11-07T10:53:15.494+0000",
            "id": 117
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nI think that full urls as tokens is not a good default for StandardTokenizer, because i don't think users ever search\non full URLS.\n{quote}\n\nProbably true, but this is a chicken and egg issue, no?  Maybe people never search on full URLs because it doesn't work, because there is no tokenization support for it?\n\nMy preferred solution here, as I [said earlier in this issue|#action_12865759], is to use a decomposing filter, because when people want full URLs, they can't be reassembled after the separator chars are thrown away by the tokenizer.\n\nRobert, when I mentioned the decomposition filter, you [said|#action_12865879] you didn't like that idea.  Do you still feel the same?\n\nI'm really reluctant to drop the ability to recognize full URLs.  I agree, though, that as a default it's not good.\n",
            "date": "2010-11-07T16:08:27.186+0000",
            "id": 118
        },
        {
            "author": "Steve Rowe",
            "body": "bq. I don't think StandardTokenizer should produce whole URLs as tokens, to begin with.\n\nI think Standard *Analyzer* should not by default produce whole URLs as tokens.  But (yay repetition!) if the tokenizer throws away the separator chars, URLs can't be reassembled from their parts.\n\nWould a URL decomposition filter, with full URL emission turned off by default, work here?\n",
            "date": "2010-11-07T16:12:09.204+0000",
            "id": 119
        },
        {
            "author": "Robert Muir",
            "body": "bq.  because when people want full URLs, they can't be reassembled after the separator chars are thrown away by the tokenizer.\n\nWell, i dont much like this argument, because its true about anything.\nIndexing text for search is a lossy thing by definition.\n\nyeah, when you tokenize this stuff, you lose paragraphs, sentences, all kinds of things.\nshould we output whole paragraphs as tokens so its not lost? \n\nbq. Robert, when I mentioned the decomposition filter, you said you didn't like that idea. Do you still feel the same?\n\nWell, i said it was a can of worms, i still feel that it is complicated, yes.\nBut i mean we do have a ghetto decomposition filter (WordDelimiterFilter) already.\nAnd someone can use this with the UAX#29+URLRecognizingTokenizer to index these urls in a variety of ways, including preserving the original full url too.\n\nbq. Would a URL decomposition filter, with full URL emission turned off by default, work here?\n\nIt works in theory, but its confusing that its 'required' to not get absymal tokens.\ni would prefer we switch the situation around: make UAX#29 'standardtokenizer' and give the uax#29+url+email+ip+... a different name.\n\nbecause to me, uax#29 handles urls in nice ways, e.g. my user types 'facebook' and they get back facebook.com\nits certainly simple and won't blow up terms dictionaries...\n\notherwise, creating lots of long, unique tokens (urls) by default is a serious performance trap, particularly for lucene 3.x\n\n",
            "date": "2010-11-07T16:22:38.433+0000",
            "id": 120
        },
        {
            "author": "Steve Rowe",
            "body": "bq. i would prefer we switch the situation around: make UAX#29 'standardtokenizer' and give the uax#29+url+email+ip+... a different name.\n\nUAX29Tokenizer does not have email or hostname recognition.  StandardTokenizer has long had these capabilities (though not standard-based).  Removing them would be bad.",
            "date": "2010-11-07T16:38:55.988+0000",
            "id": 121
        },
        {
            "author": "Michael McCandless",
            "body": "Would it somehow be possible to allow multiple Tokenizers to work together?\n\nToday we only allow one (and then any number of subsequent TokenFilters) in the chain, so if your Tokenizer destroys information (eg erases the . from the host name) it's hard for subsequent TokenFilters to put them back.\n\nBut if, say, we had a Tokenizer that recognizes hostnames/URLs, one that recognizes email addresses, one for proper names/places/date/time, other app dependent stuff like detecting part numbers and what not, then ideally one could simply cascade/compose these tokenizers at will to build up whatever \"initial\" tokenizer you need for you chain?\n\nI think our current lack of composability of the initial tokenizer (\"there can be only one\") makes cases like this hard...",
            "date": "2010-11-07T16:50:46.774+0000",
            "id": 122
        },
        {
            "author": "Robert Muir",
            "body": "bq. UAX29Tokenizer does not have email or hostname recognition. StandardTokenizer has long had these capabilities (though not standard-based). Removing them would be bad.\n\nThats true, so maybe something in the \"middle\" / \"compromise\" is better as a default.\n\nI just tend to really like plain old \"uax#29\" as a default, since its consistent with how \"tokenization\" works elsewhere in people's wordprocessors, browsers, etc\n(e.g. control-F find, that sort of thing), where they dont know specifics of content and want to just have a reasonable default.\n\nbut there might be something else we can do, too.\n",
            "date": "2010-11-07T16:51:28.898+0000",
            "id": 123
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nBut if, say, we had a Tokenizer that recognizes hostnames/URLs, one that recognizes email addresses, one for proper names/places/date/time, other app dependent stuff like detecting part numbers and what not, then ideally one could simply cascade/compose these tokenizers at will to build up whatever \"initial\" tokenizer you need for you chain?\n\nI think our current lack of composability of the initial tokenizer (\"there can be only one\") makes cases like this hard...\n{quote}\n\nI agree that sounds like a \"cool\" idea to have, but at the same time, we should try to not make analysis the \"wonder-do-it-all\" machine.\nI mean some stuff belongs in the app, and i think that includes a lot of things you mentioned... e.g. the app can do \"NER\" and pull\nout proper names/places/dates and put them in separate fields.\n\nI don't think the analysis chain is the easiest or best place to do this, i would prefer if we tried to keep the complexity down and recognize\nthat some things (really a lot of this \"recognizer\" stuff) might be better implemented in the app.\n\n\n",
            "date": "2010-11-07T16:55:58.712+0000",
            "id": 124
        },
        {
            "author": "Steve Rowe",
            "body": "bq. I just tend to really like plain old \"uax#29\" as a default [...] i would prefer if we tried to keep the complexity down\n\nSo we're talking about two separate issues here: a) Lucene's default behavior; and b) Lucene's capabilities. \n\nFor a), you'll have a lot of 'splaining to do if you drop existing functionality (e.g. email and hostname \"recognition\" -- where quotes indicate \"bad\" things, right? \"Cool\"!)\n\nFor b), you appear to agree with Marvin Humphries about keeping the product lean and mean: complexity (a.k.a. functionality beyond the default) is bad because it creates maintenance problems.\n\nbq. we should try to not make analysis the \"wonder-do-it-all\" machine.\n\nWhy not?  Why shouldn't Lucene be a catch-*all* for \"cool\" linguistic stuff?\n",
            "date": "2010-11-07T17:11:39.897+0000",
            "id": 125
        },
        {
            "author": "Robert Muir",
            "body": "bq. So we're talking about two separate issues here: a) Lucene's default behavior; and b) Lucene's capabilities.\n\nagreed!\n\nbq. For a), you'll have a lot of 'splaining to do if you drop existing functionality (e.g. email and hostname \"recognition\" - where quotes indicate \"bad\" things, right? \"Cool\"!)\n\nto me recognizing hostnames is specific to what one application might want.\nif you recognize www.facebook.com but my app wants to find this with a query of 'facebook', it cant.\nyet if just stick to uax#29, if a user queries on www.facebook.com, and they are unsatisfied with the results,\nthat user can always \"refine\" their query by searching on \"www.facebook.com\" and they get a phrasequery.\nI think this is pretty intuitive and users are used to this... again this is just for general defaults...\n\nand again, hostnames are just an example, why do we recognize them and not filenames?\nyet a lot of people are happy being able to do 'partial filename' matching and not the whole path...\nusers that are unhappy with this 'default' behavior can use double quotes to refine their results.\n\nand in both cases, apps that need something more specific can use a custom tokenizer.\n\nbq.  Why not? Why shouldn't Lucene be a catch-all for \"cool\" linguistic stuff?\n\nIn this case I think analysis won't meet their needs anyway. a lot of people wanting to recognize full urls or proper names (mike's example)\nactually want to do this in the 'document build' and dump the extracted entities into a separate field, so they can do things like\nfacet on this field, or find other documents that refer to the same person. This is because they are trying to 'find structure in the unstructured',\nbut it starts to get complicated if we mix this problem with 'feature extraction' which is what i think analysis should be.\n\n\n\n",
            "date": "2010-11-07T17:19:46.778+0000",
            "id": 126
        },
        {
            "author": "Steve Rowe",
            "body": "bq. Would it somehow be possible to allow multiple Tokenizers to work together? \n\nThe only thing I can think of right now is a new kind of component that feeds raw text (or post-char-filter text) to a configurable set of tokenizers/recognizers, then melds their results using some (hopefully configurable) strategy, like \"longest-match-wins\" or \"create-overlapping-tokens\", etc.  This would slow things down, of course, since analysis has to be performed multiple times over the same chunk of input text...",
            "date": "2010-11-07T17:24:27.298+0000",
            "id": 127
        },
        {
            "author": "Steve Rowe",
            "body": "bq. if you recognize www.facebook.com but my app wants to find this with a query of 'facebook', it cant. yet if just stick to uax#29, if a user queries on www.facebook.com, and they are unsatisfied with the results, that  \n\n\"www.facebook.com\" is way non-intuitive.  My guess is the average user would never go there: how is something a phrase, and in need of bounding quotes, if it has no spaces in it?",
            "date": "2010-11-07T17:33:04.539+0000",
            "id": 128
        },
        {
            "author": "Robert Muir",
            "body": "bq. \"www.facebook.com\" is way non-intuitive\n\nwell, i'm just saying that the \"UAX#29\" behavior i describe, people are used to:\n* google and twitter search engines find and highlight say 'cnn' in urls such as 'http:/www.cnn.com/x/y'\n* this is how \"find\" in apps such as browsers, word processors, even windows notepad work.\n* the idea of putting quotes around things to be \"more exact\" is pretty general, e.g. in google i refine queries like \"documents\" with quotes to prevent stemming: try it.\n\nSo i think its just intuitive and becoming rather universal to put quotes around things to get a \"more exact search\".\n\nLike i said, i'm not too picky how we solve the problem, but i think UAX#29 is a great default... its used everywhere else...",
            "date": "2010-11-07T17:44:16.804+0000",
            "id": 129
        },
        {
            "author": "Steve Rowe",
            "body": "bq. So i think its just intuitive and becoming rather universal to put quotes around things to get a \"more exact search\".\n\nYou've convinced me, though I don't think this idea has been around long enough to qualify as intiutive.\n\nbq. hostnames are just an example, why do we recognize them and not filenames?\n\nAlthough following precedent is important (principle of least surprise), we have to be able to revisit these decisions.  My philosophy tends toward kitchen-sinkness, while allowing people to ignore the stuff they don't want (today).  So, yeah, I think we *should* (be able to) recognize filenames, at least as part of a URL-decomposing filter:\n\n{noformat}http://www.example.com/path/file%20name.html?param=value#fragment{noformat}\n=> \n{noformat}http://www.example.com/path/file%20name.html?param=value#fragment{noformat} <URL>\nwww.example.com <HOSTNAME>\nexample.com <HOSTNAME>\nexample <HOSTNAME>\ncom <HOSTNAME>\npath <URL_PATH_ELEMENT>\nfile name.html <URL_FILENAME>\nfile name <URL_FILENAME>\nfile <URL_FILENAME>\nname <URL_FILENAME>\nhtml <URL_FILENAME>\nparam <URL_PARAMETER>\nvalue <URL_PARAMETER_VALUE>\nfragment <URL_FRAGMENT>\n\nOutput of each token type could be optional in a URL decomposition filter.  The URL decomposition filter could serve as a place to handle punycode, too.\n\nbq. i'm not too picky how we solve the problem, but i think UAX#29 is a great default... its used everywhere else...\n\nI think if we remove EMAIL/HOSTNAME recognition, we need to have an alternative that provides the same thing.  So we would have UAX#29 tokenizer as default; a UAX29+EMAIL+HOSTNAME tokenizer as the equivalent to the pre-3.1 StandardTokenizer; and a UAX29+URL+EMAIL tokenizer (current StandardTokenizer).  Or maybe the last two could be combined: a UAX29+URL+EMAIL tokenizer that provides a configurable feature to not output URLs, but instead HOSTNAMEs and URL component tokens?",
            "date": "2010-11-07T18:30:56.474+0000",
            "id": 130
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nYou've convinced me, though I don't think this idea has been around long enough to qualify as intiutive.\n{quote}\n\nWell obviously i dont have hard references to this stuff, but from my interaction with my own users, most of them\ndont even think of double quotes as doing phrases, nor are they technical enough to even know what a phrase\nis or what that means for a search... they just think of it as more exact.\n\n{quote}\nI think if we remove EMAIL/HOSTNAME recognition, we need to have an alternative that provides the same thing. So we would have UAX#29 tokenizer as default; a UAX29+EMAIL+HOSTNAME tokenizer as the equivalent to the pre-3.1 StandardTokenizer; and a UAX29+URL+EMAIL tokenizer (current StandardTokenizer). Or maybe the last two could be combined: a UAX29+URL+EMAIL tokenizer that provides a configurable feature to not output URLs, but instead HOSTNAMEs and URL component tokens?\n{quote}\n\nWell, like i said, i'm not particularly picky, especially since someone can always use ClassicTokenizer to get the old behavior,\nwhich, no one could ever agree on and there was constantly issues about not recognizing my company's name etc etc.\n\nTo some extent, i like UAX#29 because there's someone else making and standardizing the decisions and validating\nits not gonna annoy users of major languages, and making sure it works well by default: like its not gonna be the most \nfull-featured tokenizer but theres little chance it will be really annoying: i think this is great for \"defaults\".\n\nas for all the other \"bonus\" stuff we can always make options, especially if its some pluggable thing somehow (sorry not sure about how this could work in jflex)\nwhere you could have options as to what you want to do.\n\nbut again, i think UAX#29 itself is more than sufficient by default, and even hostname etc is pretty dangerous *by default* \n(again my example of searching partial hostnames being flexible to the end-user and not baked-in, by letting them using quotes).\n",
            "date": "2010-11-07T19:17:20.574+0000",
            "id": 131
        },
        {
            "author": "Earwin Burrfoot",
            "body": "bq. Would it somehow be possible to allow multiple Tokenizers to work together?\nThe fact that Tokenizers now are not TokenFilters bugs me somewhat.\nIn theory, you should just feed initial text as a single monster token from hell into analysis chain, and then you only have TokenFilters, none/one/some of which might split this token.\nIf there are no TokenFilters at all, you get a NOT_ANALYZED case without extra flags, yahoo!\n\nThe only problem here is the need for ability to wrap arbitrary Reader in a TermAttribute :/\n\nbq. But (yay repetition!) if the tokenizer throws away the separator chars, URLs can't be reassembled from their parts.\nWhy don't we teach StandardTokenizer to produce tokens for separator chars?\nA special filter at the end of the chain will drop them, so they won't get into index.\nAnd in the midst of the filter chain you are free to do whatever you want with them - detect emails/urls/sentences/whatever.",
            "date": "2010-11-07T23:00:19.598+0000",
            "id": 132
        },
        {
            "author": "Steve Rowe",
            "body": "bq. Why don't we teach StandardTokenizer to produce tokens for separator chars?\n\nI've been thinking about this - the word break rules in UAX#29 are intended for use in break iterators, and tokenizers take that one step further by discarding stuff between some breaks.\n\nStandardTokenizer is faster, though, since it doesn't have to tokenize the stuff between tokens, so if we go down this route, I think it should go somewhere else: UAX29WordBreakSegmenter or something like that.\n\nI'd like to have (nestable) SentenceSegmenter, ParagraphSegmenter, etc., the output from which could be the input to tokenizers.",
            "date": "2010-11-07T23:25:07.823+0000",
            "id": 133
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nIn theory, you should just feed initial text as a single monster token from hell into analysis chain, and then you only have TokenFilters, none/one/some of which might split this token.\nIf there are no TokenFilters at all, you get a NOT_ANALYZED case without extra flags, yahoo!\n\nThe only problem here is the need for ability to wrap arbitrary Reader in a TermAttribute :/\n{quote}\n\nNo thanks, i dont want to read my entire documents into RAM and have massive gc'ing going on.\nWe don't need to have a mega-tokenizer that solves everyones problems... this is just supposed to be a good \"general-purpose\" tokenizer.\n",
            "date": "2010-11-08T13:03:17.415+0000",
            "id": 134
        },
        {
            "author": "Earwin Burrfoot",
            "body": "bq. No thanks, i dont want to read my entire documents into RAM and have massive gc'ing going on.\nThis is obvious. And that's why I was talking about wrapping Reader in an Attribute, not copying its contents.\nHow to do so, is much less obvious. And that's why I called it a problem.\n\nbq. We don't need to have a mega-tokenizer that solves everyones problems... this is just supposed to be a good \"general-purpose\" tokenizer.\nExactly. That's why I'm thinking of a way to get some composability, instead of having to fully rewrite tokenizer once you want extras.",
            "date": "2010-11-08T15:07:43.046+0000",
            "id": 135
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nThis is obvious. And that's why I was talking about wrapping Reader in an Attribute, not copying its contents.\nHow to do so, is much less obvious. And that's why I called it a problem.\n{quote}\n\nits not a problem, its just not possible. Because you don't know the required context of some downstream \n\"composed\" \"partial-tokenizer\" or whatever, so it must be all read in... \n\nI don't think we need to provide FooAnalyzer or even FooTokenizer that solves everyone's special case problems,\nits domain-dependent and not possible anyway... these are just general ones that solve a majority of use cases, examples really.\n\nThis is why i think a simple UAX#29 standard should be the default... we can certainly have alternatives that do certain common things\nthat people want though, no problem with that.\n",
            "date": "2010-11-08T15:24:03.236+0000",
            "id": 136
        },
        {
            "author": "Steve Rowe",
            "body": "See LUCENE-2763 for swapping UAX29Tokenizer and StandardTokenizer.",
            "date": "2010-11-15T18:27:30.296+0000",
            "id": 137
        }
    ],
    "component": "modules/analysis",
    "description": "It would be really nice for StandardTokenizer to adhere straight to the standard as much as we can with jflex. Then its name would actually make sense.\n\nSuch a transition would involve renaming the old StandardTokenizer to EuropeanTokenizer, as its javadoc claims:\n\nbq. This should be a good tokenizer for most European-language documents\n\nThe new StandardTokenizer could then say\n\nbq. This should be a good tokenizer for most languages.\n\nAll the english/euro-centric stuff like the acronym/company/apostrophe stuff can stay with that EuropeanTokenizer, and it could be used by the european analyzers.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2167",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Implement StandardTokenizer with the UAX#29 Standard",
    "systemSpecification": true,
    "version": "3.1, 4.0-ALPHA"
}