{
    "comments": [
        {
            "author": "Aaron McCurry",
            "body": "All unit tests passed.",
            "date": "2010-01-13T08:29:43.003+0000",
            "id": 0
        },
        {
            "author": "Aaron McCurry",
            "body": "This is the program that used to get performance metrics.  I have also attached the raw output of the test.",
            "date": "2010-01-13T08:41:05.550+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "We've done something very similar to this, on the \"flex\" branch at https://svn.apache.org/repos/asf/lucene/java/branches/flex_1458 (which should land for Lucene 3.1), eg have a look at the oal.index.codecs.standard.SimpleStandardTermsIndexReader, in the inner CoreFieldIndex class, where we use single arrays to track the value for each indexed term.",
            "date": "2010-01-13T10:10:26.324+0000",
            "id": 2
        },
        {
            "author": "Aaron McCurry",
            "body": "I took a look at that class, and it does look somewhat similar.  Has anyone run any numbers against it to see that savings or performance of it?  I'm at work now, but I may try it out in my tests when I get home.",
            "date": "2010-01-13T14:48:34.513+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "I don't think anyone's run specific numbers on the terms dict -- that'd be great if you get a chance to do so!\n\nNote that we plan also to cutover those arrays to packed ints (LUCENE-1990) since eg using a long for fileOffset and blockPointer is very wasteful in general.  Even a short to record the length of each term is wasteful.",
            "date": "2010-01-13T16:56:16.482+0000",
            "id": 4
        },
        {
            "author": "Aaron McCurry",
            "body": "My in memory implementation uses Vints and Vlongs, so they aren't that bad for storing the fileoffsets and blockpointer's.",
            "date": "2010-01-13T17:33:49.455+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "That's very interesting -- I like that approach.  So it  presumably has higher index/enumeration CPU cost but lower RAM cost.  I think this is an OK tradeoff with terms dict, since \"typically\" the cost of the term lookup is dwarfed by the cost of iterating the postings (though, for TermRangeQuery on many low-freq terms, this doesn't hold true).\n\nIn flex you could just implement StandardTermsIndexReader -- the purpose of that abstract class is to allow pluggability of the terms index.  Also, implementing this new approach there would be a good test case [that it's usable, generic enough, etc.].  You should be able to plug it in (temporarily make it the default, for the standard codec), then pass all unit tests...",
            "date": "2010-01-13T18:35:47.523+0000",
            "id": 6
        },
        {
            "author": "Aaron McCurry",
            "body": "Sure I can rework things for that.  Not sure if read through my blog, but it\nseems as if the packing of the bytes as vints and such are actually faster\nthan doing the java reference lookups.  At least for the test cases that I\nhave come up with.  Plus the JVM's GC is much happier without having to\ntraverse the object graph of terms.\n\nOver the next few days I will rework the api to StandardTermsIndexReader and\nrepost a patch.  Thanks.\n\nOn Wed, Jan 13, 2010 at 1:36 PM, Michael McCandless (JIRA)\n\n",
            "date": "2010-01-13T18:53:56.941+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "Another benefit doing this with flex is you can also change the index file format, ie write the vints to disk (so \"build\" is done at index time, not reader startup time), so the init time would be even faster.\n\nHmm... it's surprising you're seeing faster decode time -- it looks like you read a vint per character of each index term compared, during the binary search?  Vs String.compareTo done by trunk.  (Though, if those characters are simple ascii, then the vint is always a single byte read).\n\nActually, couldn't you simply compare the utf8 bytes (plus a \"fixup\", to match UTF16 sort order), which would require no per-character vint decode?  (flex does this, since it holds the term data as utf8 bytes in memory).",
            "date": "2010-01-13T19:57:35.206+0000",
            "id": 8
        },
        {
            "author": "Aaron McCurry",
            "body": "Well to be honest, I spent a lot of time making the uni-code UTF-8/16/32 compare work, and work faster than the default implementation of TermInfosReader.  I thought the same thing, but it didn't seem to work faster.  I think that now that I have a working version as a baseline, I will go back and try some different things in the term.text compare.\n\nAs far as \"Another benefit doing this with flex is you can also change the index file format, ie write the vints to disk (so \"build\" is done at index time, not reader startup time), so the init time would be even faster.\"\n\nI actually have that implemented in our production system to give us an \"instant on\" capability when our huge indexes have to be reloaded.  But I thought I would start simple for my contribution.  :)",
            "date": "2010-01-13T20:42:26.801+0000",
            "id": 9
        },
        {
            "author": "Deepak",
            "body": "Hi Aaron\n\nI would like to test this patch on our environment. It seems patch you have supplied does not contain full version of Term.java\n\nCan you please suppy this class Term.java?\n\nRegards\nD",
            "date": "2010-01-19T01:08:18.719+0000",
            "id": 10
        },
        {
            "author": "Aaron McCurry",
            "body": "Sure, I will post it all in an hour or so.\n\nAaron\n\n\n\n\n",
            "date": "2010-01-19T01:33:55.803+0000",
            "id": 11
        },
        {
            "author": "Deepak",
            "body": "Hi Aaron\n\nI hope you will be able to post the files today\n\nRegards\nD",
            "date": "2010-01-19T23:30:39.105+0000",
            "id": 12
        },
        {
            "author": "Aaron McCurry",
            "body": "The patch as it exists now.  It no longer needs any mods to the Term.java file.",
            "date": "2010-01-20T02:55:05.432+0000",
            "id": 13
        },
        {
            "author": "Aaron McCurry",
            "body": "Here's the last file.  I have also back patched 3.0.0 and 2.9.1 and placed them on my blog incase you want to have a drop in replacement to try out.\n\nhttp://www.nearinfinity.com/blogs/aaron_mccurry/low_memory_patch_for_lucene.html\n",
            "date": "2010-01-20T02:56:44.891+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "Aaron, are you also working on a version for the flex branch?",
            "date": "2010-01-20T10:42:45.209+0000",
            "id": 15
        },
        {
            "author": "Aaron McCurry",
            "body": "Yes\n\nSent from my iPhone\n\nOn Jan 20, 2010, at 5:42 AM, \"Michael McCandless (JIRA)\" <jira@apache.org \n\n",
            "date": "2010-01-20T11:58:56.093+0000",
            "id": 16
        },
        {
            "author": "Doug Cutting",
            "body": "A few comments on the patch:\n - It'd probably be better not to make TermInfosReaderIndex and its subclasses public, to reduce the APIs that must be supported long-term.\n - Could you use BufferedIndexInput directly instead of re-implementing readVInt, readVLong, etc?\n - The code uses tabs for indentation.  Lucene's standard is 2-spaces per level, no tabs. http://wiki.apache.org/lucene-java/HowToContribute\n - It would be good to add some tests, perhaps running some existing set of test searches with a reader  configured to use the new TermInfosReaderIndex implementation.\n\nProbably the \"Fix-version\" of this patch should be 3.5, since it's not fixing a regression.",
            "date": "2011-09-14T22:57:52.526+0000",
            "id": 17
        },
        {
            "author": "Doug Cutting",
            "body": "Also have you tried specifying termInfosIndexDivisor?  I added that feature many years ago to address the memory footprint of the terms index.\n\nhttp://lucene.apache.org/java/3_0_3/api/core/org/apache/lucene/index/IndexReader.html#open(org.apache.lucene.store.Directory, org.apache.lucene.index.IndexDeletionPolicy, boolean, int)\n\nIf this is 2 then the memory use is halved, but the compute cost of looking up each search term is doubled.  It would be interesting to compare the performance of the two approaches, since the approach of this patch probably increases lookup cost somewhat too.",
            "date": "2011-09-14T23:06:24.376+0000",
            "id": 18
        },
        {
            "author": "Aaron McCurry",
            "body": "I will update my patch taking your comments into account and re-submit.\n\nI have tried the termInfosIndexDivisor and it does help with memory consumption but it typically costs access time (if I remember since I last tried changing the values to tune the index).  Since I started running with the patch above I haven't had any memory issues relating to index size, so I can't really comment it's effect.\n",
            "date": "2011-09-14T23:55:17.167+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "Realistically I don't think we should commit such a large change to how the terms dict/index works, on the 3.x (stable) branch.\n\nIn trunk, with flex indexing, we've substantially improved how indexed terms are maintained; we now use an FST (a single byte[]) to hold the prefix trie for all terms.  This is very compact: I recently checked on a large Wikipedia index segment and it was ~0.35 bytes of RAM in the terms index, per term in the terms dict, with the default allowed25 - 48 terms per block.",
            "date": "2011-09-15T10:22:55.873+0000",
            "id": 20
        },
        {
            "author": "Doug Cutting",
            "body": "Michael, thanks for responding.  Great to hear that this is likely greatly improved in 4.0!\n\nThis patch is a minor refactoring to add an extension point and a new implementation of that extension point that's not used by default.  There appear to be both refactorings (LUCENE-3201, LUCENE-3360) and new features scheduled for 3.5.  Do you feel this refactoring is riskier than those?\n\nIt should certainly also be demonstrated that this provides significant advantages over termInfosIndexDivisor before it is committed.",
            "date": "2011-09-15T17:52:03.711+0000",
            "id": 21
        },
        {
            "author": "Michael McCandless",
            "body": "\nOK, thinking more here... the fact that this won't change the index\nformat, and only replaces the low-level representation & methods for\nhow indexed terms are held in RAM and accessed, means that the risk\nhere is actually quite low.\n\nAnd the gains are tremendous (much lower RAM usage; must less GC load;\nfaster IR init time).  Users shouldn't have to wait for 4.0 to get\nthese improvements.\n\nSeek cost will go up, but this likely doesn't often matter (3.x\ndoesn't have any super-seek-intensive queries).  Maybe the primary-key\nlookup case is the worst-case, so we should measure that?\n\nI think we can port back some help from trunk to support this, eg\nByteArrayDataInput (to get readVInt/readVLong/etc. on a byte[]).\n\nI don't think we need to make this switchable with a system prop;\nlet's just do a hard cutover to the new impl?\n\nInstead of writing the term data as vLong, can we just write the UTF8\nbytes?  On seek we can convert incoming term's text to UTF8, and then\nuse trunk's UTF8SortedAsUTF16Comparator to do the compares in the\nbinary search (so we keep 3.x's UTF16 term sort order).\n\nWe should also remember on commit merge this to 4.0's preflex codec...\n\nAaron, on future iterations, could you use \"svn diff\" to produce a\nsingle patch file (instead of separate files as attachments)?  This\nway I (and others) can easily apply it to a local checkout for testing...\n",
            "date": "2011-09-15T19:32:15.720+0000",
            "id": 22
        },
        {
            "author": "Aaron McCurry",
            "body": "I agree with the back porting of ByteArrayDataInput and the UTF8 storing of string data.  I will work getting the changes that have been discussed here reworked and submit a new (single patch).",
            "date": "2011-09-15T19:49:10.028+0000",
            "id": 23
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Aaron, and sorry this process has taken so long!  This is a great improvement.",
            "date": "2011-09-15T19:52:50.471+0000",
            "id": 24
        },
        {
            "author": "Aaron McCurry",
            "body": "I have reimplemented the patch using the UTF8SortedAsUTF16Comparator as well as ByteArrayDataInput.  The patch also contains a unit test and I have run all the current tests of the core plus the contribs and everything passes.  As a plus the code has gotten much simpler.\n\nDuring my functional testing I created a test index with small but very diverse terms.  Roughly 50 terms per document with 50 million documents.  So there are approximately 2.5 billion terms in this index.\n\nThe current 3x branch produces:\n50000000 documents at a heap size of 598902872.\n\nThe patched version produces:\n50000000 documents at a heap size of 282526224.\n\nThe random access performance of this index goes to the patch.  Running 200 passes of a collection of randomly sampled queries (queries changes each time) produces the following:\n\nThe current 3x branch produces:\n4186.0225 avg response time in ms\n\nThe patched version produces:\n2930.1371 avg response time in ms\n\nNOTE: The hard drive I was using is a very slow drive.  While using smaller indexes the patch and the current branch are very close to the same performance.  Depending on the pass the either one was faster.\n",
            "date": "2011-09-21T11:21:26.624+0000",
            "id": 25
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Aaron, I'll have a look at the patch.\n\nIt's interesting that the patch was faster in your testing (I thought it should be somewhat slower, while using much less RAM) -- when you picked randomly sampled queries, was it deterministically random for patch & 3.x?  Ie, patch and 3.x ran the same randomly picked set of queries?\n\nAlso, I would expect the RAM reduction to be even more than you saw in the heap sizes above, since you entirely avoid object overhead (headers, pointers), ints become vInts, etc.",
            "date": "2011-09-21T15:09:15.347+0000",
            "id": 26
        },
        {
            "author": "Aaron McCurry",
            "body": "I would agree on the heap size, I'm will do more analysis on that tonight.  As far the speed, it took a bit of time to get the performance basically the same.  I had to change a few methods inside TermInfosReader to reuse resources.\n\nThe random access test sampled 100,000 terms from the index and stored it in a file.  Then at when I run the test it pulls all of the terms into memory and random selects terms to use in TermQueries.  Then the test times the search in nanotime and averages it.  I will attach my test programs tonight if you want.  While running a MMAPDirectory on a small ~1,000,000 documents the performance is basically the same between the patch and no patch, if there is a difference the current implementation (no patch) is slightly faster, as you would think.",
            "date": "2011-09-21T16:31:49.499+0000",
            "id": 27
        },
        {
            "author": "Aaron McCurry",
            "body": "I found a major bug in my test.  I was using keyword analyzer instead of whitespace or standard, thus it was turning everyone of my sentences that contained 50 randomly generated words into 1 huge token.  This helps to explain why the heap space results are not that stellar, because the fewer terms there are (as well as the larger they are), the less the patch helps reduce space.  I'm retesting now.",
            "date": "2011-09-21T19:02:42.782+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "\nPatch looks great Aaron!  Very much simplified... some comments:\n\n  * Instead of separate build method, could we have\n    TermInfosReaderIndex's ctor take all the args?  Then we can make\n    its private fields final?\n\n  * I think the index and indexLength can be final, in\n    TermInfosReader?\n\n  * Can you put the GrowableByteArrayDataOutput as a separate source\n    file in oal.store?  Seems useful!\n\n  * Hmm should indexToTermsArray be a long[]...?  I wonder how large\n    your index would have to be to overflow 2.1GB of the byte[]\n    format...\n\n  * We could further reduce the RAM usage by using packed ints\n    (oal.util.packed) for the indexToTerms array; this way each\n    indexed term would only use as many bits are actually required to\n    address the byte[] (and, this would solve the int[]/long[] problem\n    since packed ints are logically a long[]).\n\n  * I think we should just always trim?  (Ie we don't need the\n    {{private boolean trim}})\n\n  * Could you add comment \"Just for testing\" to\n    TermInfosReaderIndex.getTerm?\n\n  * For the compareTo methods, can you add to the jdocs that this\n    \"compares term to index term\", ie it returns negative N when term\n    is less than index term?\n\n  * Hmm... I wonder if memory fragmentation will cause problems for\n    the allocating/growing the single byte[].  Also, a single byte[]\n    can \"only\" address 2.1B bytes (the same overflow problem as\n    above).  Maybe we should port back PagedBytes (from trunk\n    oal.util) and use that instead?  If we did that, then we could\n    create a simple DataInput impl that reads from that.\n\n  * Could you please remove the @author tags?  Thanks. It's Apache's\n    policy (or at least discouraged) to not commit author tags...\n",
            "date": "2011-09-21T19:08:23.305+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I was using keyword analyzer instead of whitespace or standard\n\nAha! Good catch :)\n\nI'm also building up a 2B terms index (using Test2BTerms), and then I'll compare patch/3.x on that index.",
            "date": "2011-09-21T19:09:59.821+0000",
            "id": 30
        },
        {
            "author": "Aaron McCurry",
            "body": "Memory fragmentation aside, if an index segment contained 2.1 B terms and a default term index interval of 128, each term would need to about 16M for the byte[] to run out of space.  However due to the growing array the likely hood of it landing on 2.1 exactly is probably not likely.  So it would probably error out sometime before that.\n\nI can back port PagedBytes instead if you think it's really needed.\n\nI'm working on the PackedInts suggestion now, all the others have already been corrected.",
            "date": "2011-09-21T20:02:37.744+0000",
            "id": 31
        },
        {
            "author": "Aaron McCurry",
            "body": "Ok retested things with the same test as before except this time I used a standard analyzer.\n\nDuring my functional testing I created a test index with small but very diverse terms. Roughly 50 terms per document with 5 million documents. So there are approximately 250 million terms in this index.\n\nThe current 3x branch produces:\n5000000 documents at a heap size of 259581864.\n\nThe patched version produces:\n5000000 documents at a heap size of 48991336.\n\nThe random access performance of this index goes to the patch. Running 1000 passes of a collection of randomly sampled queries (queries changes each time) produces the following:\n\nThe current 3x branch produces:\n113.485454 avg response time in ms per collection\n\nThe patched version produces:\n118.926365 avg response time in ms per collection\n\nEach collection is 200 queries.  Also it's seems like the jvm is a slower to hotspot compile the patched version.  In a server implementation this shouldn't be a big concern.",
            "date": "2011-09-21T20:35:15.602+0000",
            "id": 32
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. However due to the growing array the likely hood of it landing on 2.1 exactly is probably not likely. So it would probably error out sometime before that.\n\nActually ArrayUtil.grow is careful about this limit: on that final\ngrow() it'll go right up to Java's max allowed array size.\n\nbq. I'm also building up a 2B terms index (using Test2BTerms), and then I'll compare patch/3.x on that index.\n\nOK this finished -- the test passed with the patch (good news!), and\n3.x (phew!).\n\nWith 3.x, IR.open takes 43.69 seconds and uses 2955 MB of heap.\n\nWith the patch, IR.open takes 9.94 seconds (4.4X faster) and uses 505\nMB of heap (5.9X less): AWESOME!\n\nThe test then does a lookup of a random set of terms.  3.x does this\nin 51.2 sec; patch does it in 48.5 sec, good!  (Same set of terms).\n\nbq. I can back port PagedBytes instead if you think it's really needed.\n\nI think we should cutover to PagedBytes.  Today the number of terms we\ncan support is 2.1B times index interval (default 128), so ~274.9 B\nterms. \n\nBut with the current patch, we can roughly estimate bytes per indexed\nterm:\n\n  * 1 byte for fieldCounter\n\n  * 15 bytes for term UTF8 bytes (non-English content)\n\n  * 1 byte for docFreq (vast majority of terms are < 128 df)\n\n  * 1 byte for skipOffset (vast majority of terms have no skip).\n\n  * 5 bytes for freqOffset\n\n  * 5 bytes for proxOffset\n\n  * 5 bytes for indexOffset\n\n  * 4 bytes for indexToTerms entry\n\nSo total ~37 bytes per indexed term, which means ~58.0 M indexed terms\ncan fit in the 2.1B byte[] limit, or 7.4 B total terms at the default\n128 index interval.  This makes me a little nervous... we've already\nseen have apps that are well over 2.1 B terms.\n\nEven before the 2.1B limit, it makes me nervous relying on the JRE to\nallocate such a large contiguous chunk of RAM.\n\nA couple other random things I noticed:\n\n  * When we estimate the initial size of the byte[] (based on .tii\n    file size), I think we should divide by indexDivisor?\n\n  * We should conditionally write the skipOffset, only when docFreq is\n    >= skipInterval.  Since most terms won't have skip data we can\n    save 1 byte for them...\n",
            "date": "2011-09-22T09:56:45.346+0000",
            "id": 33
        },
        {
            "author": "Michael McCandless",
            "body": "I ported luceneutil's PKLookupTest to 3.x (see\nhttp://code.google.com/a/apache-extras.org/p/luceneutil/source/browse/perf/PKLookupPerfTest3X.java),\nand ran a test w/ 100M docs, spread across 25 segs, doing 100K PK\nlookups.  I temporarily disabled the terms lookup cache.\n\n3.x:\n{noformat}\nReader=ReadOnlyDirectoryReader(segments_1 _d6(3.5):C18018000 _3x(3.5):C18018000 _70(3.5):C18018000 _a3(3.5):C18018000 _bh(3.5):C1801800 _g9(3.5):C18018000 _fp(3.5):C1801800 _gk(3.5):C1801800 _g1(3.5):C180180 _g2(3.5):C180180 _gu(3.5):C1801800 _g7(3.5):C180180 _gd(3.5):C180180 _ge(3.5):C180180 _gj(3.5):C180180 _gl(3.5):C180180 _gp(3.5):C180180 _gv(3.5):C180180 _gw(3.5):C180180 _gx(3.5):C180180 _gy(3.5):C180180 _gz(3.5):C180180 _h0(3.5):C180180 _h1(3.5):C180180 _h2(3.5):C100)\nCycle: warm\n  Lookup...\n  WARM: 10428 msec for 100000 lookups (104.28 us per lookup)\nCycle: test\n  Lookup...\n  10309 msec for 100000 lookups (103.09 us per lookup)\nCycle: test\n  Lookup...\n  10333 msec for 100000 lookups (103.33 us per lookup)\nCycle: test\n  Lookup...\n  10333 msec for 100000 lookups (103.33 us per lookup)\nCycle: test\n  Lookup...\n  10506 msec for 100000 lookups (105.06 us per lookup)\nCycle: test\n  Lookup...\n  10499 msec for 100000 lookups (104.99 us per lookup)\nCycle: test\n  Lookup...\n  10297 msec for 100000 lookups (102.97 us per lookup)\nCycle: test\n  Lookup...\n  10345 msec for 100000 lookups (103.45 us per lookup)\nCycle: test\n  Lookup...\n  10396 msec for 100000 lookups (103.96 us per lookup)\nCycle: test\n  Lookup...\n  10302 msec for 100000 lookups (103.02 us per lookup)\n{noformat}\n\n\nPatch:\n{noformat}\nReader=ReadOnlyDirectoryReader(segments_1 _d6(3.5):C18018000 _3x(3.5):C18018000 _70(3.5):C18018000 _a3(3.5):C18018000 _bh(3.5):C1801800 _g9(3.5):C18018000 _fp(3.5):C1801800 _gk(3.5):C1801800 _g1(3.5):C180180 _g2(3.5):C180180 _gu(3.5):C1801800 _g7(3.5):C180180 _gd(3.5):C180180 _ge(3.5):C180180 _gj(3.5):C180180 _gl(3.5):C180180 _gp(3.5):C180180 _gv(3.5):C180180 _gw(3.5):C180180 _gx(3.5):C180180 _gy(3.5):C180180 _gz(3.5):C180180 _h0(3.5):C180180 _h1(3.5):C180180 _h2(3.5):C100)\nCycle: warm\n  Lookup...\n  WARM: 11164 msec for 100000 lookups (111.64 us per lookup)\nCycle: test\n  Lookup...\n  10838 msec for 100000 lookups (108.38 us per lookup)\nCycle: test\n  Lookup...\n  10882 msec for 100000 lookups (108.82 us per lookup)\nCycle: test\n  Lookup...\n  10873 msec for 100000 lookups (108.73 us per lookup)\nCycle: test\n  Lookup...\n  10871 msec for 100000 lookups (108.71 us per lookup)\nCycle: test\n  Lookup...\n  10870 msec for 100000 lookups (108.7 us per lookup)\nCycle: test\n  Lookup...\n  10896 msec for 100000 lookups (108.96 us per lookup)\nCycle: test\n  Lookup...\n  10840 msec for 100000 lookups (108.4 us per lookup)\nCycle: test\n  Lookup...\n  10860 msec for 100000 lookups (108.6 us per lookup)\nCycle: test\n  Lookup...\n  10847 msec for 100000 lookups (108.47 us per lookup)\n{noformat}\n\n\nSo net/net patch is a bit (~5%) slower, as expected since PKLookup is\nthe worst case here, but I think the enormous gains in RAM reduction /\nstartup time / GC load make this tradeoff acceptable.\n",
            "date": "2011-09-22T10:51:33.703+0000",
            "id": 34
        },
        {
            "author": "Michael McCandless",
            "body": "Aaron, if it would help, I can backport PagedBytes to 3.x?",
            "date": "2011-09-26T17:18:20.698+0000",
            "id": 35
        },
        {
            "author": "Aaron McCurry",
            "body": "I have done the PagedBytes back port already.  It was a simple copy of the class (assuming that's what you want me to do).  As for the oal.util.packed package for the packed ints, I think they should be modified to work against the DataInput and DataOutput instead of the IndexInput and IndexOutput.  If so, should a separate issue in JIRA be opened to modify them in the trunk and then I will back port that version?  What do you think?\n\nAlso I will probably work on this patch again in the next few days and get a final patch (hopefully) at some point this week.\n",
            "date": "2011-09-26T17:39:00.011+0000",
            "id": 36
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I have done the PagedBytes back port already. It was a simple copy of the class (assuming that's what you want me to do).\n\nExcellent, I'm glad it was straightforward.  We also need a DataInput impl that reads from the PagedBytes... I can help on that if you want.\n\nbq. As for the oal.util.packed package for the packed ints, I think they should be modified to work against the DataInput and DataOutput instead of the IndexInput and IndexOutput.\n\nI agree -- I committed this to trunk.",
            "date": "2011-09-27T17:41:06.684+0000",
            "id": 37
        },
        {
            "author": "Aaron McCurry",
            "body": "This patch includes fixes for all the previous comments, however in my small tests it seems to produce more memory overhead then  I would have expected.  I will be on vacation for the next week, so I wanted to attach the current state of the patch incase someone else sees my mistake causing the extra memory to be used.  Also I have not done any performance testing with this patch.",
            "date": "2011-10-01T11:36:41.443+0000",
            "id": 38
        },
        {
            "author": "Michael McCandless",
            "body": "Looking great Aaron!\n\nIt's spooky that PagedBytesDataInput calls fillSlice for every\n.readByte -- can't we have it hold the current block and then only\nswitch to a new block in .readByte() if it's at the end of current\nblock?\n\nSame for PagedBytesDataOutput?\n\nWe can have these DataInput/Output impls be private to PagedBytes (so\nthey can access the pages directly)?\n\nYou should be able to use PackedInts.GrowableWriter, to append the\nints directly, instead of first writing to the indexToTermsArray and\nthen separately to the packed ints?  Saves the added transient RAM\nusage and 2nd pass.\n\nI don't think you need to write the indexToTerms packed ints into a\nPagedBytesDataOutput (if you use GrowableWriter it just uses a byte[]\nunder the hood, and resizes as needed)?  This array will be small\nenough, since it's the packed int byte address of every 128th term, I\nthink (but dataOutput does need to be paged bytes).\n",
            "date": "2011-10-04T21:51:58.956+0000",
            "id": 39
        },
        {
            "author": "Michael McCandless",
            "body": "Hi Aaron, any progress here?  If you want I can iterate from your last patch and do the remaining changes... I think we are close here, and this will be an awesome improvement.",
            "date": "2011-10-20T12:27:35.570+0000",
            "id": 40
        },
        {
            "author": "Aaron McCurry",
            "body": "Sorry that I haven't gotten back to you yet, life has a way of slowing down development.\n\nAs for you comments from a couple of weeks ago.  I agree with the PageBytes comments, I knew there was going to be more work there.  But I may not understand how the GrowableWriter will help.  I understand that it allows me to append more values as I go, but when I start writing them I have no idea what bit size to choose for the packing.  Can you explain?\n\nAlso if you would like to finish this patch that would fine with me.  Let me know if you want me to continue or if you are going to work on it.  Thanks!\n\n",
            "date": "2011-10-20T13:41:34.891+0000",
            "id": 41
        },
        {
            "author": "Michael McCandless",
            "body": "bq. But I may not understand how the GrowableWriter will help. I understand that it allows me to append more values as I go, but when I start writing them I have no idea what bit size to choose for the packing. Can you explain?\n\nIt actually grows in \"both\" dimensions -- it tracks the max value so far and internally will \"upgrade\" to a bigger bits-per-value as needed.  So eg you could start with small bitsPerValue (maybe 4 or something) and then let it grow itself.\n\nbq. Also if you would like to finish this patch that would fine with me. Let me know if you want me to continue or if you are going to work on it. Thanks!\n\nOK thanks Aaron... I'll take a crack at the next iteration.",
            "date": "2011-10-20T18:09:20.917+0000",
            "id": 42
        },
        {
            "author": "Aaron McCurry",
            "body": "Wow GrowableWriter is very cool!  Thanks for explaining!",
            "date": "2011-10-20T19:40:51.135+0000",
            "id": 43
        },
        {
            "author": "Michael McCandless",
            "body": "New patch, iterated from Aaron's last patch.\n\nI moved the DataInput/Output impls into PagedBytes, so they can directly operate on the byte[] blocks.  I also don't write skipOffset unless df >= skipInterval.\n\nI think this is ready!",
            "date": "2011-10-24T22:09:32.549+0000",
            "id": 44
        },
        {
            "author": "Aaron McCurry",
            "body": "Awesome!  Good job!\n\nThank you for working on this with me!",
            "date": "2011-10-25T03:20:25.527+0000",
            "id": 45
        },
        {
            "author": "Michael McCandless",
            "body": "Well thank you Aaron for doing all the hard parts, and, persisting!  Sorry this took so long.\n\nThis will be an enormous improvement for 3.5.0.",
            "date": "2011-10-25T12:35:28.022+0000",
            "id": 46
        },
        {
            "author": "Robert Muir",
            "body": "+1, nice work.\n\njust some tiny tweaks: added a missing license header, randomized the test, and when binary-searching bytesref.grow() versus making new ones in each comparison (this seems to help the pk-lookup perf test on my computer).\n\nWhen committing can we also add the packed ints test to 3.x, and also merge this improvement forward to 4.x's preflex codec?\n",
            "date": "2011-10-27T06:03:43.452+0000",
            "id": 47
        },
        {
            "author": "Michael McCandless",
            "body": "New patch looks great, thanks Robert!\n\nbq. When committing can we also add the packed ints test to 3.x, and also merge this improvement forward to 4.x's preflex codec?\n\nYup I'll do that.  I'll also merge forward the improvements to PagedBytes.\n\nI also ran Test2BTerms (it passed); I think this is ready to go in!",
            "date": "2011-10-27T13:42:49.261+0000",
            "id": 48
        },
        {
            "author": "Michael McCandless",
            "body": "Finally resolved; thanks Aaron!",
            "date": "2011-10-27T20:47:38.294+0000",
            "id": 49
        },
        {
            "author": "Uwe Schindler",
            "body": "Bulk close after release of 3.5",
            "date": "2011-11-27T12:29:23.337+0000",
            "id": 50
        }
    ],
    "component": "core/index",
    "description": "Basically packing those three arrays into a byte array with an int array as an index offset.  \n\nThe performance benefits are stagering on my test index (of size 6.2 GB, with ~1,000,000 documents and ~175,000,000 terms), the memory needed to load the terminfos into memory were reduced to 17% of there original size.  From 291.5 MB to 49.7 MB.  The random access speed has been made better by 1-2%, load time of the segments are ~40% faster as well, and full GC's on my JVM were made 7 times faster.\n\nI have already performed the work and am offering this code as a patch.  Currently all test in the trunk pass with this new code enabled.  I did write a system property switch to allow for the original implementation to be used as well.\n\n-Dorg.apache.lucene.index.TermInfosReader=default or small\n\nI have also written a blog about this patch here is the link.\n\nhttp://www.nearinfinity.com/blogs/aaron_mccurry/my_first_lucene_patch.html\n\n\n\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2205",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Rework of the TermInfosReader class to remove the Terms[], TermInfos[], and the index pointer long[] and create a more memory efficient data structure.",
    "systemSpecification": true,
    "version": ""
}