{
    "comments": [
        {
            "author": "Uwe Schindler",
            "body": "I will now check, if the change of the \"input\" member variable leads to backwards breaks (it was changed from Reader to CharStream)...",
            "date": "2009-09-10T15:42:51.297+0000",
            "id": 0
        },
        {
            "author": "Yonik Seeley",
            "body": "+1, this looks like the best fix.",
            "date": "2009-09-10T15:45:30.477+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "+1, good catch Uwe!",
            "date": "2009-09-10T16:07:28.501+0000",
            "id": 2
        },
        {
            "author": "Mark Miller",
            "body": "Ready Uwe?",
            "date": "2009-09-10T16:26:49.086+0000",
            "id": 3
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. I will now check, if the change of the \"input\" member variable leads to backwards breaks (it was changed from Reader to CharStream)...\n\nWe also have a not-in-CHANGES.txt backwards break. We changed Reader to CharStream. When committing this change, also the backwards-branch was modified to use CharStream. I reverted this change (to get the state of a legacy Tokenizer) and wrote a Test, that called input.read() inside the Tokenizer. The test compiled correct in backwards and also run correct.\n\nIn trunk, the test-tag failed:\n{code}\n    [junit] Testcase: testChangeToCharStream29(org.apache.lucene.analysis.TestTokenizer):       Caused an ERROR\n    [junit] input\n    [junit] java.lang.NoSuchFieldError: input\n    [junit]     at org.apache.lucene.analysis.TestTokenizer$1.next(TestTokenizer.java:43)\n    [junit]     at org.apache.lucene.analysis.TestTokenizer.testChangeToCharStream29(TestTokenizer.java:58)\n    [junit]\n    [junit]\n    [junit] Test org.apache.lucene.analysis.TestTokenizer FAILED\n{code}\n\nSo somebody cannot use his old Tokenizers without recompiling (because Java is not able to respect the type change). After recompiling his classes it works.\n\nIf we want to do this, we should clearly state this in CHANGES at the backwards-breaks.",
            "date": "2009-09-10T16:34:19.523+0000",
            "id": 4
        },
        {
            "author": "Uwe Schindler",
            "body": "Here is the patch for backwards-branch, that fails. It reverts the change from Reader to CharStream and adds a dummy Tokenizer in the test that compiles, but does not run in trunk.",
            "date": "2009-09-10T16:36:49.931+0000",
            "id": 5
        },
        {
            "author": "Uwe Schindler",
            "body": "Sorry, wrong patch, this one is correct. Other one was a even harder test by assigning a value.",
            "date": "2009-09-10T16:40:08.599+0000",
            "id": 6
        },
        {
            "author": "Uwe Schindler",
            "body": "One possibility to prevent this break would be the following:\n- revert the CharStream to a Reader in Tokenizer (restore the old class).\n- add a method correctOffset(int) to Tokenizer that does:\n{code}\nprotected final int correctOffset(int offset) {\n  if (input instanceof CharStream) return ((CharStream) input).correctOffset(offset);\n  return offset;\n}\n{code}\n- Change the usage of correctOffset in all Tokenizers in core/contrib (only a few are affected)\n- revert the backwards-branch changes\n\nIn this case, the input of a Tokenizer stays always a j.io.Reader and the offset correction defaults to just nothing. If a custom Reader extends CharStream, the offset correction is automatically used. This is 100% backwards compatible (only some old Tokenizers not calling correctOffset() before passing to Token/OffsetAttribute would produce invalid offsets if used with CharStreams instead of Readers)",
            "date": "2009-09-10T17:13:07.806+0000",
            "id": 7
        },
        {
            "author": "Mark Miller",
            "body": "What about using an introspection cache again? Kind of nasty depending on how soon we could get rid of it.\n\nPersonally, I think I'd prefer a break to a solution that doesnt solve every case.",
            "date": "2009-09-10T17:27:18.206+0000",
            "id": 8
        },
        {
            "author": "Robert Muir",
            "body": "bq. (only some old Tokenizers not calling correctOffset() before passing to Token/OffsetAttribute would produce invalid offsets if used with CharStreams instead of Readers)\n\nbut don't you have this problem already? I like Uwe's approach.",
            "date": "2009-09-10T17:29:11.286+0000",
            "id": 9
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. What about using an introspection cache again? Kind of nasty depending on how soon we could get rid of it. \n\nA cache for what? I do not understand :-) The problem is the change of the member field.\n\nbq. but don't you have this problem already? I like Uwe's approach.\n\nCorrect, this is always the problem with this change. The offsets are wrong if one legacy Tokenizer does not correct the offset (either through Tokenizer.correctOffset() or CharStream.correctOffset()).",
            "date": "2009-09-10T17:32:17.157+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "bq. We also have a not-in-CHANGES.txt backwards break.\n\nSigh, I forgot to also add an entry to \"Changes in back compat policy\"\nwhen we made this change (in LUCENE-1466).\n\nWe had decided at the time to make an exception to back-compat, ie\nallow reader to become CharStream, but we can definitely revisit\nthat...\n\nbq. One possibility to prevent this break would be the following\n\nAdding an instanceof check in every correctOffset call makes me\nnervous -- what's the performance cost?\n",
            "date": "2009-09-10T17:38:40.157+0000",
            "id": 11
        },
        {
            "author": "Robert Muir",
            "body": "bq. Correct, this is always the problem with this change. The offsets are wrong if one legacy Tokenizer does not correct the offset (either through Tokenizer.correctOffset() or CharStream.correctOffset()).\n\nright, so your proposed fix would correct this issue (the back compat break), but it doesn't introduce any new issue.\n\nif you want to use CharFilter, then your tokenizer must call correctOffset() so the offsets will be correct.\n",
            "date": "2009-09-10T17:39:12.044+0000",
            "id": 12
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Correct, this is always the problem with this change. \n\nRight, we should separately fix this (so that all of our core/contrib tokenizers invoke correctOffset).",
            "date": "2009-09-10T17:41:41.869+0000",
            "id": 13
        },
        {
            "author": "Mark Miller",
            "body": "bq. A cache for what? I do not understand  The problem is the change of the member field.\n\nSorry - I meant to say reflection. I mean check if the instance just overrides reset(Reader), and if so, call it with the CharReader from reset(Reader) (and cache the reflection results) - perhaps it gets more complicated though - I havn't thought on it - just throwing it out. I've glossed over the code involved but I'm still at a pretty high level with it, as I trust there is enough people that already know what they are doing in this area. I was never the kid in a group racing to have the math answer first ;) \n\nbq. Correct, this is always the problem with this change. The offsets are wrong if one legacy Tokenizer does not correct the offset (either through Tokenizer.correctOffset() or CharStream.correctOffset()).\n\nOkay - I took it as - this is 100% backwards compatible, except for this exception where it is not. But if thats always been the case, it is back compat and it sounds fine to me - just got caught up in the wording.\n\nSo if its good, lets ship it! It will take me a bit of time to ensure all the tests and package up, and transfer over again (man does all that maven stuff make transfer take forever - too many little files)",
            "date": "2009-09-10T17:42:08.687+0000",
            "id": 14
        },
        {
            "author": "Uwe Schindler",
            "body": "Here the patch for core. Contrib is unchanged.\n\nIn principle just replace \"input.correctOffset(\" by \"correctOffset(\" everywhere. All core tests pass. The backwards branch must simply revert the commit of LUCENE-1466.",
            "date": "2009-09-10T17:45:00.435+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "I'm still nervous about inserting an instanceof check plus a cast in to every correctOffset call (which is called twice per token).\n\nFor external code that accesses Tokenizer.input, a recompile is all that's required (which 2.9 already requires).  Subclasses that directly assign to input (if any) really should be calling reset instead.  I think breaking back compat here is OK?",
            "date": "2009-09-10T17:58:54.190+0000",
            "id": 16
        },
        {
            "author": "Uwe Schindler",
            "body": "\"instanceof\" is one of the operators directly implemented in the Java Bytecode as a separate instruction. It is fast and used everywhere. See various articles about it.",
            "date": "2009-09-10T18:02:40.857+0000",
            "id": 17
        },
        {
            "author": "Robert Muir",
            "body": "contrib changes.",
            "date": "2009-09-10T18:06:28.317+0000",
            "id": 18
        },
        {
            "author": "Mark Miller",
            "body": "bq.  I think breaking back compat here is OK?\n\nI won't comment from a tech perspective, but in regards to this, that was my initial thought as well - if its just a recompile, we are saying 2.9 needs\nto recompile anyway, else you might blow up - essentially is required as it is.",
            "date": "2009-09-10T18:06:35.697+0000",
            "id": 19
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. \"instanceof\" is one of the operators directly implemented in the Java Bytecode as a separate instruction.\n\nMy computer doesn't run bytecode ;-P\nYes, it's relatively fast, but it's per-token too.\n\nbq. a recompile is all that's required (which 2.9 already requires)\n\nHmmm, I had missed that 2.9 required a recompile.  In that case it doesn't seem like there is any additional back compat breakage and thus the correct fix would be Uwe's first patch?",
            "date": "2009-09-10T18:13:49.930+0000",
            "id": 20
        },
        {
            "author": "Mark Miller",
            "body": "bq. Hmmm, I had missed that 2.9 required a recompile. \n\nBecause it was never officially said one way or another, I was pretty loose about it:\n\nWe recommend that you recompile your application with Lucene 2.9\nrather than attempting to \"drop\" it in. This will alert you to any\nissues you may have to fix if you are affected by one of the backward\ncompatibility breaks. As always, its a really good idea to thoroughly\nread CHANGES.txt before upgrading. Also, remember that this is a\nrelease candidate, and not the final Lucene 2.9 release.\n\nI should probably change it to required. I am saying, recompile or be\nprepared for a random break. Thats got to be a euphemism for recompile required.",
            "date": "2009-09-10T18:17:26.787+0000",
            "id": 21
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Yes, it's relatively fast, but it's per-token too.\n\nIt is once per token. But you do not need to wrap the input Reader using CharReader if you do not want to use CharFilters. If you wrap each call to Reader by CharReader you have a larger overhead (one additional method call per char read, if you tokenize using Reader.read()!).\n\nbq. Hmmm, I had missed that 2.9 required a recompile. In that case it doesn't seem like there is any additional back compat breakage and thus the correct fix would be Uwe's first patch?\n\nA recompile is only needed is rare caces (if you override Scorers and so on). If you do not do any very-special Lucene usages, it works without recompiling. In my opinion, e.g. external language Tokenizer-Packages (as Michael Busch calls them) without source code would not work. This example is always brought by Michael.\n\nIf we accept the break, the first patch is all we need to do (and move the CHANGES entry up to the bw-breaks).",
            "date": "2009-09-10T18:22:59.645+0000",
            "id": 22
        },
        {
            "author": "Mark Miller",
            "body": "bq. In my opinion, e.g. external language Tokenizer-Packages (as Michael Busch calls them) without source code would not work. This example is always brought by Michael.\n\nExcellent point. Hadn't seen it before or didn't remember it.",
            "date": "2009-09-10T18:26:47.982+0000",
            "id": 23
        },
        {
            "author": "Michael McCandless",
            "body": "bq. A recompile is only needed is rare caces (if you override Scorers and so on\n\nOr implement Searchable (an interface that we've added methods to), or implemented Weight (an interface that we changed to an abstract class), or your own MergePolicy (IndexWriter instance now required to ctor).  I agree these ones are way-expert things.\n\n{quote}\nbq. In my opinion, e.g. external language Tokenizer-Packages (as Michael Busch calls them) without source code would not work. This example is always brought by Michael.\n\nExcellent point. Hadn't seen it before or didn't remember it.\n{quote}\n\nOK I agree, this does make me nervous, too.\n\nOK you've convinced me Uwe: I think we should in fact restore input to be a Reader not a CharStream.  I think the potential performance hit is the lesser evil here.\n\nMaybe for 3.0 we can declare that this will become a CharStream?",
            "date": "2009-09-10T18:45:39.771+0000",
            "id": 24
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Maybe for 3.0 we can declare that this will become a CharStream?\n\nI think the instanceof check is less evil than:\n\n{quote}\nbq. Yes, it's relatively fast, but it's per-token too.\n\nIt is once per token. But you do not need to wrap the input Reader using CharReader if you do not want to use CharFilters. If you wrap each call to Reader by CharReader you have a larger overhead (one additional method call per char read, if you tokenize using Reader.read()!).\n{quote}\n\nI think we should wait a while and think one night about it. Lets move RC4 to tomorrow morning.\n\nWe have both possibilities, let's collect arguments +/-\n\nbq. Excellent point. Hadn't seen it before or didn't remember it.\n\nHe brought this up several times in the TokenStream discussion. This is why we mad this very fancy backwards layer that works with many special usages of TokenStreams like subclassing Token and so on (see extra BW Test). Hard stuff :-) And this because of this argument.",
            "date": "2009-09-10T18:52:58.754+0000",
            "id": 25
        },
        {
            "author": "Uwe Schindler",
            "body": "Here the updated patches for trunk (some missing occurences of CharReader-wrapping and so on) and backwards-branch (revert changes).\n\nThe changes also show one posiible problem with current trunk: StandardTokenizer did not call super.reset(stream) which maybe user-provided tokenizers could also fail. Changing of public/protected member fields is a bad idea (and one reason to not have non-final public fields and use get/set methods instead).\n\nAll test pass. Any other thoughts about this? Yonik? Mark? Robert? Mike? :-)",
            "date": "2009-09-10T20:47:54.567+0000",
            "id": 26
        },
        {
            "author": "Robert Muir",
            "body": "uwe, i like your patch.\n\nwhat was that StandardTokenizer setInput (that did not call super.reset) supposed to do ?\n",
            "date": "2009-09-10T21:35:50.050+0000",
            "id": 27
        },
        {
            "author": "Uwe Schindler",
            "body": "It was never used and seems to be a relict from a time when the jflex file created the tokenizer directly (???). It is package private and used nowhere, so no problem with deleting it. At least it should call reset(Reader) if we leave it there.",
            "date": "2009-09-10T21:42:04.318+0000",
            "id": 28
        },
        {
            "author": "Mark Miller",
            "body": "I say we go with it - 'instance of' will have no practical effect from what I can tell with any micro benches. That, plus what I've read has me not too worried.\n\nMy vote is to go with with you have. We might as well wait till the morning at this point no matter what really - and that will give anyone else an opportunity to chime in.",
            "date": "2009-09-10T22:49:04.941+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good Uwe!",
            "date": "2009-09-10T22:53:46.227+0000",
            "id": 30
        },
        {
            "author": "Koji Sekiguchi",
            "body": "+1, patch looks good, thanks Uwe!",
            "date": "2009-09-10T23:56:35.940+0000",
            "id": 31
        },
        {
            "author": "Uwe Schindler",
            "body": "Some tweaks in JavaDocs. I committed this patch.",
            "date": "2009-09-11T06:13:21.963+0000",
            "id": 32
        },
        {
            "author": "Uwe Schindler",
            "body": "Committed revision: 813671\n\nThere may be some changes needed in Solr (correctOffset in Tokenizer / CharStream->Reader), should I open an issue?",
            "date": "2009-09-11T06:15:33.077+0000",
            "id": 33
        },
        {
            "author": "Koji Sekiguchi",
            "body": "bq. There may be some changes needed in Solr (correctOffset in Tokenizer / CharStream->Reader), should I open an issue?\n\nYes, please.",
            "date": "2009-09-11T11:53:39.032+0000",
            "id": 34
        },
        {
            "author": "Uwe Schindler",
            "body": "It seems that Solr compiles with the new JAR files (there is only one compile error not related to this in PatternTokenizerFactory). Tests seem to work. Not tested thoroughly.\n\nThere seem to be no custom analyzers using correctOffset at all in Solr.",
            "date": "2009-09-11T12:01:42.390+0000",
            "id": 35
        },
        {
            "author": "Uwe Schindler",
            "body": "I created an issue (SOLR-1423). Maybe there is nothing to do, I am not very familar with Solr. But it should be checked, that all Tokenizers correct their offsets (what some of them do not seem to do).",
            "date": "2009-09-11T12:12:30.898+0000",
            "id": 36
        }
    ],
    "component": "modules/analysis",
    "description": "When reviewing the new CharStream code added to Tokenizers, I found a\nserious problem with backwards compatibility and other Tokenizers, that do\nnot override reset(CharStream).\n\nThe problem is, that e.g. CharTokenizer only overrides reset(Reader):\n\n{code}\n  public void reset(Reader input) throws IOException {\n    super.reset(input);\n    bufferIndex = 0;\n    offset = 0;\n    dataLen = 0;\n  }\n{code}\n\nIf you reset such a Tokenizer with another CharStream (not a Reader), this\nmethod will never be called and breaking the whole Tokenizer.\n\nAs CharStream extends Reader, I propose to remove this reset(CharStream\nmethod) and simply do an instanceof check to detect if the supplied Reader\nis no CharStream and wrap it. We could also remove the extra ctor (because\nmost Tokenizers have no support for passing CharStreams). If the ctor also\nchecks with instanceof and warps as needed the code is backwards compatible\nand we do not need to add additional ctors in subclasses.\n\nAs this instanceof check is always done in CharReader.get() why not remove\nctor(CharStream) and reset(CharStream) completely?\n\nAny thoughts?\n\nI would like to fix this somehow before RC4, I'm, sorry :(\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1906",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Blocker",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Backwards problems with CharStream and Tokenizers with custom reset(Reader) method",
    "systemSpecification": true,
    "version": "2.9"
}