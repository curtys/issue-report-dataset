{
    "comments": [
        {
            "author": "Robert Haschart",
            "body": "Source code for UnicodeNormalizationFilter",
            "date": "2008-07-22T19:33:01.940+0000",
            "id": 0
        },
        {
            "author": "Robert Haschart",
            "body": "Java 6 contains a class named java.text.Normalizer that is able to perform Unicode normalization, earlier versions of java do not have that class, and therefore need the code in this jar (which is a subset of the icu4j library) to be able to perform Unicode normalization.    The UnicodeNormalizationFilter can work with either the java 6 class java.text.Normalizer or the class com.ibm.icu.text.Normalizer in the jar here.",
            "date": "2008-07-22T19:41:32.377+0000",
            "id": 1
        },
        {
            "author": "Hoss Man",
            "body": "Random related comment (just because this issue seemed like a good place to put it)\n\nPeople may also want to consider constructing a Filter based on the substitution tables from the perl Text::Unidecode module...\n\nhttp://search.cpan.org/~sburke/Text-Unidecode/\nhttp://interglacial.com/~sburke/tpj/as_html/tpj22.html\n\n...i have no idea how it's behavior compares to the UnicodeNormalizationFilter, just that it seems to have similar goals.",
            "date": "2008-07-22T20:13:48.709+0000",
            "id": 2
        },
        {
            "author": "Steve Rowe",
            "body": "Hi Robert,\n\nMy comments below assume you're intrestested in having this code hosted in the Lucene source repository - please disregard if that's not the case.\n\nHave you seen the [HowToContribute page on the Lucene wiki|http://wiki.apache.org/lucene-java/HowToContribute]?  It outlines some of the basics concerning code submissions.\n\nA couple of things I noticed that need to be addressed before the code will be accepted:\n\n# Tab characters should be converted to spaces\n# Indentation increment should be two spaces\n# Test(s) should be moved from the UnicodeNormalizationFilterFactory.main() method into standalone class(es) that extend LuceneTestCase\n# More/more explicit javadocs - for example, you should describe the set of provided transformations (e.g. Cyrillic diacritic stripping is included).\n# Solr is a separate code base, so the UnicodeNormalizationFilterFactory should be moved to a Solr JIRA issue\n# Because it has a dependency on the ICU jar, this contribution will have to live in the contrib/ area -- the Java packages name should be adjusted accordingly.\n# The submission should be repackaged as a patch (instructions available on the above-linked wiki page).\n",
            "date": "2008-07-22T20:20:39.096+0000",
            "id": 3
        },
        {
            "author": "Lance Norskog",
            "body": "Some languages like Cyrillic have a standard latin-1 transliteration, and deserve their own filters. \n\nCyrillic is one case of this. It is based on three alphabets: 1/3 latin, 1/3 greek, and 1/3 new characters for 'ya/ye', 'ts', 'sh', 'ch', 'zh', and 'sh-ch' (fiSH CHips!).\n\nUnit tests are the best way to document the many ways this thing can work.\n\n\n\n",
            "date": "2008-08-13T20:31:18.318+0000",
            "id": 4
        },
        {
            "author": "Ken Krugler",
            "body": "Hi Robert,\n\nFWIW, the issues being discussed here are very similar to those covered by the [Unicode Security Considerations|http://www.unicode.org/reports/tr36/] technical report #36, and associated data found in the [Unicode Security Mechanisms|http://www.unicode.org/reports/tr39/] technical report #39.\n\nThe fundamental issue for int'l domain name spoofing is detecting when two sequences of Unicode code points will render as similar glyphs...which is basically the same issue you're trying to address here, so that when you search for something you'll find all terms that \"look\" similar.\n\nSo for a more complete (though undoubtedly slower & bigger) solution, I'd suggest using ICU4J to do a NFKD normalization, then toss any combining/spacing marks, lower-case the result, and finally apply mappings using the data tables found in the technical report #39 referenced above.\n\n-- Ken",
            "date": "2008-08-14T03:49:15.798+0000",
            "id": 5
        },
        {
            "author": "Erik Hatcher",
            "body": "{quote}\nUnit tests are the best way to document the many ways this thing can work.\n{quote}\n\ngets a judges score of 11 from me.  Gold for Lance for Quote of the Day.",
            "date": "2008-08-14T08:42:31.617+0000",
            "id": 6
        },
        {
            "author": "Robert Haschart",
            "body": "The UnicodeNormalizationFilter does use the decompose normalization \nportion of the icu4j library as a starting point.  However even with \nthat there are several instances where the normalizer code does not \ndecompose a character into an unaccented character and a accent mark, a \nnotable one being   ( \u0141 -> L )  so the UnicodeNormalizationFilter start \nwith the approach you outlined, perform a decompose normalization \nfollowed by discarding all non-spacing modifier characters, and then can \ngo on from there to further normalize the data by folding the additional \ncharacters that aren't handled by the decompose normalization onto their \nLatin1 lookalikes.\n\n-Robert\n\n\n\n\n",
            "date": "2008-08-14T17:10:45.098+0000",
            "id": 7
        },
        {
            "author": "Ken Krugler",
            "body": "Hi Robert,\n\nSo given that you and the Unicode consortium seem to be working on the same problem (normalizing visually similar characters), how similar are your tables to the ones that have been developed to deter spoofing of int'l domain names?\n\n-- Ken",
            "date": "2008-08-14T23:41:00.961+0000",
            "id": 8
        },
        {
            "author": "Mark Miller",
            "body": "Mr Muir, can you take a look at this? Offer anything over the ASCIIFoldingFilter? If not, we should close, if so, what do you recommend?",
            "date": "2009-12-06T20:09:19.132+0000",
            "id": 9
        },
        {
            "author": "Robert Muir",
            "body": "The big picture here and all these other duplicated normalization issues across jira is related to the outdated unicode support in the JDK. \n\nThis issue speaks of removing diacritical marks / NSM's, but the underlying issue is missing unicode normalization, duplicated here (incorrectly named): LUCENE-1215 and also here: LUCENE-1488 (disclaimer: my impl)\n\nSpeaking for the accent removal: In truth I do not think we should be simply removing NSMs because in most cases, they are there for a reason. For example, they are diacritics in a lot of european languages, but for many eastern languages they are the actual vowels. (i.e. all the indic scripts)\n\nWe need to separate the issue of missing unicode normalization (which is clearly something lucene needs), from the issue of removing diacritics (which is language-specific and doing it based on unicode properties is inappropriate).\n\nFinally just normalizing unicode in Lucene by itself is not very useful, because there is a careful interaction with other processes and attention needs to be paid to the order in which filters are run. For example, its interaction with case folding can be a bit tricky. If you are interested in this issue I urge you to read the javadocs writeup I placed in the ICUNormalizationFilter in LUCENE-1488.\n",
            "date": "2009-12-06T20:54:49.136+0000",
            "id": 10
        },
        {
            "author": "Ken Krugler",
            "body": "Just to make sure this point doesn't get lost in the discussion over normalization - the issue of \"visual normalization\" is one that I think ISOLatin1AccentFilter originally was trying to address. Specifically how to fold together forms of letters that a user, when typing, might consider equivalent.\n\nThis is indeed language specific, and re-implementing support that's already in ICU4J is clearly a Bad Idea.\n\nI think there's value in a general normalizer that implements the Unicode Consortium's algorithm/data for normalization of int'l domain names, as this is intended to avoid visual spoofing of domain names.\n\nDon't know/haven't tracked if or when this is going into ICU4J. But (similar to ICU generic sorting) it provides a useful locale-agnostic approach that would work well-enough for most Lucene use cases.",
            "date": "2009-12-06T21:44:23.423+0000",
            "id": 11
        },
        {
            "author": "Robert Muir",
            "body": "Hi Ken, such functionality does exist, although it is new and I think still changing (you are talking about StringPrep/IDN/etc?).\n\nIf a filter for this is desired, we can do it with ICU, though I think its relatively new (probably not optimized, only works on String, etc etc)\n\nI still think even this is stupid, because unicode encodes characters, not glyphs.",
            "date": "2009-12-06T22:24:26.451+0000",
            "id": 12
        },
        {
            "author": "DM Smith",
            "body": "I also am dubious about a general purpose folding filter that maps letters to their ASCII look-alike and agree that folding is language dependent.\n\nMay Americans are illiterate when it comes to text with diacritics and NSM. Personally I'm nearly illiterate. I think having prominent folding filters without adequate explanation about their pitfalls or usefulness may lead illiterates into a false sense of sufficiency.\n\nIf it makes sense to have a filter for TR39 I think that should be a separate issue. If that's what this issue is all about then it's description should be modified.\n\nI think this should otherwise be closed as a bad idea.\n\nRobert Muir, Would it make sense to have a Greek filter that strips diacritics? My thought is that if the letter is Greek then the diacritics would be removed, but otherwise it would not.\n\nSimilar question for Hebrew, I see value in two filters: one would strip cantillation and the other, vowel points. Or would it be better to have one that can do both depending on flags?",
            "date": "2009-12-07T15:20:14.268+0000",
            "id": 13
        },
        {
            "author": "Robert Muir",
            "body": "bq. Robert Muir, Would it make sense to have a Greek filter that strips diacritics? My thought is that if the letter is Greek then the diacritics would be removed, but otherwise it would not.\n\nThe GreekLowerCaseFilter (incorrectly named) does this also, somewhat. it removes tone marks... but this might not be what you \"want\" (depending on what that is), if you are dealing with polytonic Greek (sorry for my ignorance of the biblical test you are looking at, but I think it is ancient Greek?)\n\nbq. Similar question for Hebrew, I see value in two filters: one would strip cantillation and the other, vowel points. Or would it be better to have one that can do both depending on flags?\n\nThis depends on your use case, and then you have dagesh,shin dot, too... These are all NSMs. But this is going to depend on the user, and I think every person will need their own, they can use CharFilter or other ways of defining these tables.\n",
            "date": "2009-12-07T15:29:45.002+0000",
            "id": 14
        },
        {
            "author": "DM Smith",
            "body": "{quote}\n bq.   Robert Muir, Would it make sense to have a Greek filter that strips diacritics? My thought is that if the letter is Greek then the diacritics would be removed, but otherwise it would not.\n\nThe GreekLowerCaseFilter (incorrectly named) does this also, somewhat. it removes tone marks... but this might not be what you \"want\" (depending on what that is), if you are dealing with polytonic Greek (sorry for my ignorance of the biblical test you are looking at, but I think it is ancient Greek?)\n{quote}\n\nYes, I'm referring to ancient Greek (grc, not el) and they are tone and breathing marks. Most ancient texts did not have these marks but modern do. Even some modern representations of the ancient. While I have several semesters of koine Greek under my belt and might be wrong, there may be ambiguities where two words have the same letters but differ on marks, but they are infrequent (I don't know of any).\n\nThe GreekLowerCaseFilter appears to only do some of the work and only works on composed characters.\n\nMy question is not whether I'd find the filter useful, but whether it'd be a useful addition to Lucene.\n\n{quote}\nbq.   Similar question for Hebrew, I see value in two filters: one would strip cantillation and the other, vowel points. Or would it be better to have one that can do both depending on flags?\n\nThis depends on your use case, and then you have dagesh,shin dot, too... These are all NSMs.\n{quote}\nI have a terrible habit of not being exact or using the proper terms. Shame on me. I meant that the latter strip all other marks.\n\nbq. But this is going to depend on the user, and I think every person will need their own, they can use CharFilter or other ways of defining these tables.\n\nIf there is no general purpose contribution, then it should not be part of Lucene and I'll have my own.\n\nWhen I do work them up, I'll create an issue or two and attach the results. If they are deemed useful then they can be added to Lucene, otherwise ignored.",
            "date": "2009-12-07T16:07:47.097+0000",
            "id": 15
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nYes, I'm referring to ancient Greek (grc, not el) and they are tone and breathing marks. Most ancient texts did not have these marks but modern do. Even some modern representations of the ancient. While I have several semesters of koine Greek under my belt and might be wrong, there may be ambiguities where two words have the same letters but differ on marks, but they are infrequent (I don't know of any).\n{quote}\n\nI guess I brought this up because this is where you have several situations where case folding and normalization interact, eg. applying FC_NFKC set when case folding so that later NFK[CD] normalization will be closed, I know this is supposed to solve various ways the YPOGEGRAMMENI can be implemented but I forget the details...\n\nThis is why I think, the general purpose contribution should be case folding, normalization, and the stuff like this (FC_NFKC set) to make sure they work together...\n\nIf you later want to apply something more specialized like StringPrep, you need this logic anyway, see http://www.ietf.org/rfc/rfc3454.txt (especially section 3.2) \n",
            "date": "2009-12-07T16:18:45.201+0000",
            "id": 16
        },
        {
            "author": "Robert Muir",
            "body": "OK! I think we have a good solution here!.\n\nWe can use ICU's Normalizer2 to implement this, by simply creating a custom normalization mapping.\nThis way we can meet multiple use-cases, e.g. someone wants to remove diacritics, someone else doesn't.\n\nAnd we get solid unicode behavior and high performance to boot.\n\nSo I will keep this issue open, I think the best solution is to take the accent-folding mappings here (or use the ones in AsciiFoldingFilter?) and create a .txt file of mappings, passing it to gennorm2 along with NFKC case fold mappings.\n\nThis way we can implement this on top of LUCENE-2399, all compiled to an efficient binary form with no code.\nI'll take a shot at this once LUCENE-2399 is resolved.",
            "date": "2010-04-17T14:34:18.653+0000",
            "id": 17
        },
        {
            "author": "Robert Muir",
            "body": "Attached is a patch that implements UTR#30 as a tailored unicode normalization form.\n\nEssentially it acts as a combined \"Internationalized AsciiFoldingFilter\" + NFKC_CaseFold (Unicode Case Folding, Default Ignorable removal, and NFKC normalization).\n\nThis is a nice alternative to just using ICUNormalizer2Filter in the case that you want \"fuzzy matching\" (e.g. ignore diacritical marks). \n\nThe patch is large because it contains all the source data files necessary for gennorm2 to regenerate the 41KB binary trie file... the java implementation is trivial.\n",
            "date": "2010-04-19T08:35:48.122+0000",
            "id": 18
        },
        {
            "author": "Robert Muir",
            "body": "attached is the binary file that goes in the resources/ directory.\n\nAlthough I provide the ant logic to regenerate this, its kind of a pain because\n* you must download/compile ICU4c (version 4.4), there is no java gennorm2\n* you must run this on a big-endian machine.\n",
            "date": "2010-04-19T08:37:36.004+0000",
            "id": 19
        },
        {
            "author": "Robert Muir",
            "body": "By the way, I have been running this with the ASCIIFoldingFilter tests and ensuring its a superset (e.g. we have at least all their mappings).\n\nBut there are some bugs in ASCIIFoldingFilter that should be fixed:\n\nFor example, U+1E9B (LATIN SMALL LETTER LONG S WITH DOT ABOVE)\nBut in unicode. this is canonically equivalent to U+017F (LONG S) U+0307 (COMBINING DOT ABOVE)\nAsciiFoldingFilter folds U+1E9B (LONG S WITH DOT) to an F\nbut it folds U+017F (LONG S) to an S\n\nUnicode defines this character as a compatibility equivalent to S anyway, but its worse that ASCIIFoldingFilter is canonically inconsistent with itself.\n",
            "date": "2010-04-20T13:51:22.208+0000",
            "id": 20
        },
        {
            "author": "Robert Muir",
            "body": "attached is a modified patch (i will upload the new datafile too).\n* applied ICU or Unicode copyright headers to any datafiles where I sourced from their data, and added a mention to NOTICE.txt to that effect.\n* added some additional punctuation mappings to ensure it contains all ASCIIFoldingFilter foldings\n\nAs noted previously, there are 5 places where this disagrees with ASCIIFoldingFilter:\nU+1E9B: LATIN SMALL LETTER LONG S WITH DOT ABOVE (should be s)\nU+2033: DOUBLE PRIME (should be two single quotes)\nU+2036: REVERSED DOUBLE PRIME (same as above)\nU+2038: CARET (folds to CIRCUMFLEX ACCENT, which should be deleted as its [:Diacritic:]\nU+FF3E: FULLWIDTH CIRCUMFLEX ACCENT (same as above)\n\nI plan to commit in a few days if no one objects.\n",
            "date": "2010-04-20T14:35:15.102+0000",
            "id": 21
        },
        {
            "author": "Robert Muir",
            "body": "updated datafile.",
            "date": "2010-04-20T14:36:06.534+0000",
            "id": 22
        },
        {
            "author": "Robert Muir",
            "body": "Committed revision 936657.",
            "date": "2010-04-22T08:48:46.762+0000",
            "id": 23
        },
        {
            "author": "Jamie",
            "body": "Very useful for unicode normalization/folding.  But after trying this package in the nightly build I looked back at the patch and realized that it has a dependency on IBM ICU.\n\nimport com.ibm.icu.text.Normalizer2;\n\nIs this intentional?  Will it remain dependent?",
            "date": "2010-04-27T07:17:39.051+0000",
            "id": 24
        },
        {
            "author": "Uwe Schindler",
            "body": "Yes, as this contrib package is called \"ICU\". If you dont want to use ICU, dont use this contrib. You can alway use ASCIIFoldingFilter, it will not get removed.",
            "date": "2010-04-27T07:47:31.571+0000",
            "id": 25
        },
        {
            "author": "Robert Muir",
            "body": "backported to 3x, revision 941694.",
            "date": "2010-05-06T12:28:45.243+0000",
            "id": 26
        },
        {
            "author": "Grant Ingersoll",
            "body": "Bulk close for 3.1",
            "date": "2011-03-30T15:49:53.360+0000",
            "id": 27
        }
    ],
    "component": "modules/analysis",
    "description": "The ISOLatin1AccentFilter takes Unicode characters that have diacritical marks and replaces them with a version of that character with the diacritical mark removed.  For example \u00e9 becomes e.  However another equally valid way of representing an accented character in Unicode is to have the unaccented character followed by a non-spacing modifier character (like this:  \u00e9  )    The ISOLatin1AccentFilter doesn't handle the accents in decomposed unicode characters at all.    Additionally there are some instances where a word will contain what looks like an accented character, that is actually considered to be a separate unaccented character  such as  \u0141  but which to make searching easier you want to fold onto the latin1  lookalike  version   L  .   \n\nThe UnicodeNormalizationFilter can filter out accents and diacritical marks whether they occur as composed characters or decomposed characters, it can also handle cases where as described above characters that look like they have diacritics (but don't) are to be folded onto the letter that they look like ( \u0141  -> L )",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1343",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "A replacement for AsciiFoldingFilter that does a more thorough job of removing diacritical marks or non-spacing modifiers.",
    "systemSpecification": true,
    "version": "3.1"
}