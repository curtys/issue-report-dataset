{
    "comments": [
        {
            "author": "Robert Muir",
            "body": "Tim, if you have modified benchmark to work with various formats of older TREC collections, that would be really nice.",
            "date": "2010-09-24T00:50:58.382+0000",
            "id": 0
        },
        {
            "author": "Doron Cohen",
            "body": "Indeed TrecContentSource is inadequate for the Trec-Disks-4+5-minus-CR collection (FBIS, FR94, FT, LATimes) so I am writing something to process this collection, in which, interestingly, each sub-collection's format slightly differs. (Will use this with the robust 2004 queries.) If there are ready to use building blocks for this that would be helpful. \n\nI think of writing separate content source implementations for each format - current one being gov2 format, and at the method openNextFile() identify the correct trec format according to the file path - i.e. if it is under LATimes will use that appropriate content source. The default will remain as today, for backcompat, and will be used if the path does not match any of the defined patterns.Also should be possible to specify - perhaps in a property - the default trec format.",
            "date": "2011-01-11T09:52:19.050+0000",
            "id": 1
        },
        {
            "author": "Shai Erera",
            "body": "Perhaps instead of separate ContentSource implementations, we can have TrecContentSource use a TrecDocParser (new class) or something, for parsing different formats. We can then have Gov2Parser, LATimesParser etc. for parsing the different formats, and TrecContentSource would use the appropriate parser per the path detected, as you suggest.\n\nIn addition, we can have it use a specific format through a configuration parameter, in which case it will not attempt to auto-detect the right format, but always use the specified parser. Through Benchmark (as well as all other contrib / modules) does not need to maintain back-compat, I think that if we go with this approach, it can default to using the Gov2Parser, and thus you achieve backwards support.",
            "date": "2011-01-11T12:17:19.298+0000",
            "id": 2
        },
        {
            "author": "Doron Cohen",
            "body": "Initial patch - against 3.x - not ready to commit - refactors parsing of trec text from TrecContentSource into interface TrecDocParser, currently with single impl - TrecGov2Parser. \n\nThe interaction between TCS and TDP is less clean than I hoped, for two reasons:\n# trying to keep the synchronization pattern added while ago to that class, in which the reading of data from the file is synced but the parsing can go in parallel. For this reason there are two methods in that interface.\n# allowing the TDP impls to use whatever is in TCS caused required to expose some of its methods, and also to pass TCS as param to TDP. \n\nWith this patch:\n# TDP was cleaned to use ContentSource's method getInputStream() - this also supporting .gz, .bz2, and plain text (before the patch it supports only .gz).\n# should be easy to add parsers for other formats.\n\nI removed the retry logic for opening the stream - I don't remember why it was added in the first place and it seems strange - if opening failed in first trial why would the next trial succeed?\n\nRemaining to do:\n- add parsers for the other formats\n- add tests for the other formats and also for bz2, plain text.\n- allow a single run to ingest file of different formats (needed for the disks 4+5 track).\n- fix some documemtation.\n- allow to specify the TDP to use in a property.\n- changes.txt.\n- port to trunk, so as to first commit in trunk and then backport to 3.x.",
            "date": "2011-01-13T23:03:56.707+0000",
            "id": 3
        },
        {
            "author": "Shai Erera",
            "body": "Patch looks good !\n\nCan you make TrecContentSource.read() public and not package-private? That way people can use it outside benchmark's package as well, supporting other/newer/older TREC formats.",
            "date": "2011-01-15T05:27:06.509+0000",
            "id": 4
        },
        {
            "author": "Doron Cohen",
            "body": "Hi Shai, thanks for reviewing!\nI agree about making read() public, and same for parseDate().\nAs we discussed offline the interface with TrecParser is not ideal - I looked at the option we discussed to have the TrecContentSource just read everything between <Doc> and </Doc> and then let the TrecDocParser do everything - in one call to it - but as this would mean going through the data 3 times (comparing to 2 times today) this is not appealing either and I rather stay with the two methods interface for now.",
            "date": "2011-01-23T21:20:53.236+0000",
            "id": 5
        },
        {
            "author": "Shai Erera",
            "body": "Ok though I really think the 3 vs 2 times is negligible. The extra time we add is very simple - it's the only one that does IO, and even then, it reads lines and compares them to <DOC> or </DOC> (which is a very simple comparison). From then on, it parses the actual TREC document in-memory.\n\nThis is something I think could have even improved the current multi-threading support in TrecContentSource - today the threads sync on each one reading the TREC document, which means parsing its structure, and the only thing that's done in parallel is parsing the Html content. It'd be interesting to benchmark the 3-passes method, where each thread would sync on reading the section from <DOC> to </DOC> and then proceed to actually parse the structure.\n\nIt sounds like TrecContentSource could have acted like a SAX parser, reading TrecDoc objects and emitting them to a BlockingQueue, while threads would read from it and proceed on their own.\n\nWhat I do agree on is that 3-passes is unnecessarily more expensive for single-threaded benchmarks.",
            "date": "2011-01-24T04:20:26.219+0000",
            "id": 6
        },
        {
            "author": "Doron Cohen",
            "body": "Updated patch:\n* first the entire doc is read into docBuf, then it is parsed by trecParser\n* added trec parser impls for LATimes, FT, FBIS, FR94 - so covering all of Trec-Disks-4+5-minus-CR collection.\n* added a parser by path - it selects which parser to use according to the path of the input file.\n\nStill not ready to commit but almost there.\n\nWith this patch the following alg would index all the 4 dirs, each with its own trec-parser:\n\n{code}\n# ----- properties\ncontent.source=org.apache.lucene.benchmark.byTask.feeds.TrecContentSource\ncontent.source.verbose=false\ncontent.source.excludeIteration=true\ndoc.maker.forever=false\ndocs.dir=<my-in-dir>\ntrec.doc.parser=org.apache.lucene.benchmark.byTask.feeds.TrecParserByPath\ncontent.source.log.step=2500\ndoc.term.vector=false\ncontent.source.forever=false\ndirectory=FSDirectory\nwork.dir=<my-result-index-dir>\ndoc.stored=true\ndoc.body.stored=false\ndoc.tokenized=true\n# ----- alg\nResetSystemErase\nCreateIndex\n{ AddDoc > : *\nCloseIndex\nRepAll\n{code}\n\nI am thinking of making TrecDocParser an abstract class, and move to it some of the functionality currently in TrecContentSource / TrecParserByPath.\n\nAlso thinking of serving each input file to a separate thread - I think this would allow better performance when someone indexes in multiple threads - as with current synchronization (we sync on reading from the file) I got fastest indexing when running sequential, compared again 2,3,4 threads - likely in a separate issue.",
            "date": "2011-01-31T22:02:25.722+0000",
            "id": 7
        },
        {
            "author": "Shai Erera",
            "body": "Patch looks good !\n\nFew comments:\n\nIn TrecFTParser.parse(), I think you can extract the logic which finds the date and title into a common method which receives the strings to look for as parameters (e.g. find(String str, String start, int startlen, String end))? Then the code can be simplified to:\n{code}\nDate date = trecSrc.parseDate(find(dobBuf, DATE, DATE_LENGTH, DATE_END));\nString title = find(docBuf, HEADLINE, HEADLINE_LENGTH, HEADLINE_END);\n{code}\n\nI believe this method will be useful for other parsers as well, so might be good to pull it up to the abstract TrecDocParser (and +1 for making it abstract and moving logic from TCS to it).\n\nIn TrecContentSource you changed rawDocSize from int to int[], however it's an array that's always allocated at size 1 and never resized. I think it can be an int?\n\nAlso, TCS.cleanTags has two versions, one taking a String and one a StringBuilder (took me a minute to notice the difference) -- do you think the performance gain (of not allocating a String in the SB variant) is worth the code dup? I didn't understand what does cleanTags do - does it strip tags off of the HTML content?\n\nI would also make all those static methods public (and move them to TrecDocParser) in case someone wants to impl his own parser.\n\nThanks for adding support for GZIP in ContentSource - I had this on my TODO list for a long time :). Two things:\n# I think the try-catch can be extracted to wrap the 'switch' because it is now needed by both BZIP and GZIP.\n# Is it possible to add support for ZIP as well? If it's not trivial, then let's resolve it in a different issue.",
            "date": "2011-02-01T04:32:52.658+0000",
            "id": 8
        },
        {
            "author": "Doron Cohen",
            "body": "Thanks for the review Shai!\nAll comments accepted.\nGood catch with the int[] - added that sometimes and forgot to cleanup.\nI think ZIP can wait for another day - let's get this one in.\nNote that we are using [CompressorInputStream.createCompressorInputStream()|http://commons.apache.org/compress/apidocs/org/apache/commons/compress/compressors/CompressorStreamFactory.html#createCompressorInputStream(java.lang.String,%20java.io.InputStream)] which at version 1.1 only supports BZIP2 and GZIP. But the docs for Compress specify ZIP as well - so I guess this is possible, just needs a deeper dig into, in another issue, which, I guess, should also upgrade benchmark/compress from 1.0 to 1.1 (solr already uses 1.1).\n\nbq. do you think the performance gain (of not allocating a String in the SB variant) is worth the code dup?\nI think so, it is a rather short method, will add a javadoc clarification.",
            "date": "2011-02-01T07:46:44.975+0000",
            "id": 9
        },
        {
            "author": "Doron Cohen",
            "body": "updated patch for 3x.\n\nTo apply this also copy attached trecdocs.zip under lucene/contrib/benchmark/src/test/org/apache/lucene/benchmark/byTask/feeds\n\nA test case was added which reads all 5 trec formats with mix of txt/bz2/gz file formats.\n\nI moved unzip() from backcompat test to LuceneTestCase and fixed it to also open directory hierarchy. \n\nTrecDocParser is now abstract class as discussed and also the other suggestions by Shai were followed.\n\nPlanning to commit tomorrow if there are no reservations.",
            "date": "2011-02-01T23:42:05.946+0000",
            "id": 10
        },
        {
            "author": "Shai Erera",
            "body": "We're very close indeed !\n\n* Maybe instead of moving the unzip method to LuceneTestCase, you can put it as a static method in _TestUtil? LTC is crowded enough to be added more functionality :). Also, _TestUtil already has a rmDir method, I think we should use it? I would also do the same for fullTempDir.\n\n* The method pathType(File f) in TrecDocParser -- maybe instead of walking up the path elements you can obtain its full absolute path (which is a String) and then do indexOf() checks for the 4 types? It will simplify matters IMO.\n\n* stripTags:\n** Typo in TDP: unmodofied --> unmodified.\n** Maybe we can use String.replaceAll() which takes a regex? This is not critical ...\n** Does stripTags strips off tags of the HTML content? Or is it used for the TREC types other than GOV2?\n\n* In TrecContentSource, can you replace TrecParserByPath.pathType to TrecDocParser.pathType?\n* Also, do we still need TrecParserByPath? I don't see that it's used.",
            "date": "2011-02-02T04:18:19.005+0000",
            "id": 11
        },
        {
            "author": "Doron Cohen",
            "body": "Thanks for reviewing Shai!\n\nbq. Maybe instead of moving the unzip method to LuceneTestCase, you can put it as a static method in _TestUtil? Also, _TestUtil already has a rmDir method, I think we should use it? I would also do the same for fullTempDir.\nGood point, will do.\n\nbq. The method pathType(File f) in TrecDocParser \u2013 maybe instead of walking up the path elements you can obtain its full absolute path (which is a String) and then do indexOf() checks for the 4 types? It will simplify matters IMO.\nNot sure yet if I like better this file separator sensitive approach, I'll take a look.\n\nbq. Typo in TDP: unmodofied --> unmodified.\nWill fix.\n\nbq. Maybe we can use String.replaceAll() which takes a regex? This is not critical ...\nRight, much simpler this way, will do!\n\nbq. Does stripTags strips off tags of the HTML content? Or is it used for the TREC types other than GOV2?\nIt strips any tags, but it is used by parsers which are not using the HTML parser, that is, the Gov2 one does not use it.\n\nbq. In TrecContentSource, can you replace TrecParserByPath.pathType to TrecDocParser.pathType?\nGood catch, this is part of older code, will do.\n\nbq. Also, do we still need TrecParserByPath? I don't see that it's used.\nYes we do, this is an important addition of this patch - allowing you to index trec docs of several formats. It is used, but dynamically, through the algorithm in TrecContentSourceTest.testTrecFeedDirAllTypes(). So removing it will not break compilation but will fail the tests.",
            "date": "2011-02-02T09:00:14.053+0000",
            "id": 12
        },
        {
            "author": "Doron Cohen",
            "body": "Updated patch, plan to commit later today or tomorrow.",
            "date": "2011-02-02T17:58:57.019+0000",
            "id": 13
        },
        {
            "author": "Shai Erera",
            "body": "I see that we both missed the CHANGES entry? :)\n\nOther than that, patch looks good. +1 to commit !",
            "date": "2011-02-02T19:47:27.052+0000",
            "id": 14
        },
        {
            "author": "Doron Cohen",
            "body": "Committed:\nr1066771 - 3x\nr1067359 - trunk,\n\nA comment about the merging and the fixing of mergeinfo's. \nThe great [wiki page about svn-merge|http://wiki.apache.org/lucene-java/SvnMerge] was very helpful, just that I merged from 3x to trunk, while there it is recommended the other way around. I think the two are equivalent, but had to go carefully with it. \n\nEventually these are the commands I ran:\n\n{noformat}\ncd trunk/lucene/src\nsvn merge -c 1066771 https://svn.apache.org/repos/asf/lucene/dev/branches/branch_3x/lucene/src\ncd trunk/modules/benchmark\nsvn merge -c 1066771 https://svn.apache.org/repos/asf/lucene/dev/branches/branch_3x/lucene/contrib/benchmark\ncd trunk\nsvn merge --record-only -c 1066771 https://svn.apache.org/repos/asf/lucene/dev/branches/branch_3x\nsvn revert  --depth empty modules/benchmark\nsvn revert solr\n{noformat}\n\nThe record-only merge discarded, by itself, the (new) mergeinfo prop from trunk/lucene/src, and updated the ones in trunk and trunk/src.\nNote the use of *revert --depth empty* for reverting (all) property changes.\n\nI'll keep this issue open for a day in case any problem is revealed with this merge process which I am doing for the first time.",
            "date": "2011-02-05T00:54:02.171+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "I think this commit has caused a failure on at least 3.x?\n{noformat}\n[junit] Testcase: testTrecFeedDirAllTypes(org.apache.lucene.benchmark.byTask.feeds.TrecContentSourceTest):\tCaused an ERROR\n    [junit] expected:<TEST-00[0]> but was:<TEST-00[1]>\n    [junit] \tat org.apache.lucene.benchmark.byTask.feeds.TrecContentSourceTest.assertDocData(TrecContentSourceTest.java:70)\n    [junit] \tat org.apache.lucene.benchmark.byTask.feeds.TrecContentSourceTest.testTrecFeedDirAllTypes(TrecContentSourceTest.java:369)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1045)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:977)\n    [junit] \n    [junit] \n    [junit] Tests run: 6, Failures: 0, Errors: 1, Time elapsed: 0.488 sec\n    [junit] \n    [junit] ------------- Standard Error -----------------\n    [junit] WARNING: test method: 'testBadDate' left thread running: Thread[Thread-6,5,main]\n    [junit] RESOURCE LEAK: test method: 'testBadDate' left 1 thread(s) running\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TrecContentSourceTest -Dtestmethod=testBadDate -Dtests.seed=-1485993969467368126:6510043524258948665 -Dtests.multiplier=5\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TrecContentSourceTest -Dtestmethod=testTrecFeedDirAllTypes -Dtests.seed=-1485993969467368126:-9055415333820766139 -Dtests.multiplier=5\n    [junit] NOTE: test params are: locale=tr_TR, timezone=Europe/Zagreb\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TrecContentSourceTest]\n    [junit] NOTE: FreeBSD 8.2-RC2 amd64/Sun Microsystems Inc. 1.6.0 (64-bit)/cpus=16,threads=1,free=66439840,total=86376448\n    [junit] ------------- ---------------- ---------------\n{noformat}",
            "date": "2011-02-06T12:18:54.577+0000",
            "id": 16
        },
        {
            "author": "Doron Cohen",
            "body": "I am able to reproduce this on Linux.\nThe test fails with *locale tr_TR* because TrecDocParser was upper-casing the file names for deciding which parser to apply.\nProblem with this is that toUpperCase is locale sensitive, and so the file name no longer matched the enum name.\nFixed by adding a lower case dirName member to the enums.\nAlso recreated the test files zip with '-UN u' for UTF8 handling of file names in the zip.\n\ncommitted at r1067699 for 3x.\n\nIn trunk the test passes with same args also in Linux, but fails if you pass the locale that was randomly selected in 3x, i.e. like this: \nant test -Dtestcase=TrecContentSourceTest -Dtestmethod=testTrecFeedDirAllTypes -Dtests.locale=tr_TR\n\nWill merge the fix to trunk shortly.",
            "date": "2011-02-06T17:09:50.901+0000",
            "id": 17
        },
        {
            "author": "Robert Muir",
            "body": "Hi Doron, about the test random seeds:\n\nIt is complicated (though maybe we could fix this!) for the same random seed in trunk to work just like 3.x\n\nBut for the locales: the way it picks a random locale is from the available system locales. This changes from jre to jre,\nso unfortunately we cannot guarantee that the same seed chooses the same locale randomly... Its the same with \ntimezones too... and these even change in minor jdk updates! \n\nI wish we knew of a good solution, because I hate it when things aren't completely reproducible everywhere.\n",
            "date": "2011-02-06T17:19:11.774+0000",
            "id": 18
        },
        {
            "author": "Doron Cohen",
            "body": "Fix for the locale issue merged to trunk at r1076605.\nKeeping open for a day or so to make sure there are no more failures.",
            "date": "2011-02-06T17:22:31.396+0000",
            "id": 19
        },
        {
            "author": "Doron Cohen",
            "body": "bq. I wish we knew of a good solution, because I hate it when things aren't completely reproducible everywhere.\n\nThanks Robert, I am actually very pleased with this array of testing with various parameters like locale and others randomly selected - it is very powreful, and since the failure printed all the parameters used and even the ant line to reproduce(\\!)  - it was possible to reproduce in 3x, and, once understanding what the problem was also possible to reproduce in trunk - to me this is testing's heaven...",
            "date": "2011-02-06T20:14:25.459+0000",
            "id": 20
        },
        {
            "author": "Doron Cohen",
            "body": "Following suggestions by Robert, brought back case insensitivity of path names by upper casing with Locale.ENGLISH as suggested in [toUpperCase()|http://download.oracle.com/javase/6/docs/api/java/lang/String.html#toUpperCase%28%29]. \nCommitted:\n- r1067764 - 3x\n- r1067772 - trunk",
            "date": "2011-02-06T21:30:07.354+0000",
            "id": 21
        },
        {
            "author": "Doron Cohen",
            "body": "ok no new failures, closing as fixed, Thanks Shai and Robert for your help here!",
            "date": "2011-02-07T07:15:15.218+0000",
            "id": 22
        },
        {
            "author": "Robert Muir",
            "body": "Hi guys, this is just a feature request (we can open a new issue if anyone is up for it).\n\nI was wondering if we could do a simple write-up and put it in the website notes for 3.1, 3.2,\nwhenever we can get to it, with some basic instructions on how to use this functionality.\n\nI noticed with more research-oriented search engines, there are simple instructions for\nhow to index trec collections, run relevance experiments, and get trec_eval results... I feel\nlike if we had these it would be really beneficial towards getting new folks involved with Lucene.\n\nSome examples (some are simpler than others, but at least they all describe how to build the index):\n* Terrier: http://terrier.org/docs/current/trec_examples.html\n* Zettair: http://www.seg.rmit.edu.au/zettair/quick_start_trec.html\n* MG4J: http://mg4j.dsi.unimi.it/man/manual/ch01s04.html#id2769812\n* Indri: http://ciir.cs.umass.edu/~strohman/indri/\n\n",
            "date": "2011-02-17T16:01:26.447+0000",
            "id": 23
        },
        {
            "author": "Doron Cohen",
            "body": "I agree, this would be helpful. \nLet's have a new issue on this, I'll take it.\nDoron",
            "date": "2011-02-17T20:00:26.272+0000",
            "id": 24
        },
        {
            "author": "Grant Ingersoll",
            "body": "Bulk close for 3.1",
            "date": "2011-03-30T15:50:04.627+0000",
            "id": 25
        }
    ],
    "component": "modules/benchmark",
    "description": "The benchmarking utilities for  TREC test collections (http://trec.nist.gov) are quite limited and do not support some of the variations in format of older TREC collections.  \n\nI have been doing some benchmarking work with Lucene and have had to modify the package to support:\n* Older TREC document formats, which the current parser fails on due to missing document headers.\n* Variations in query format - newlines after <title> tag causing the query parser to get confused.\n* Ability to detect and read in uncompressed text collections\n* Storage of document numbers by default without storing full text.\n\nI can submit a patch if there is interest, although I will probably want to write unit tests for the new functionality first.\n\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1540",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Improvements to contrib.benchmark for TREC collections",
    "systemSpecification": true,
    "version": ""
}