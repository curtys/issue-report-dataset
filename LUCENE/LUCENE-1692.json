{
    "comments": [
        {
            "author": "Robert Muir",
            "body": "first I looked at BrazilianAnalyzer... out of curiousity can someone explain to me how the behavior of BrazilianStemmer differs from the Portuguese snowball analyzer... because it looks to be the same algorithm to me!\n",
            "date": "2009-06-15T21:48:36.397+0000",
            "id": 0
        },
        {
            "author": "Robert Muir",
            "body": "answered my own question, here's tests for brazilian as a start.\n",
            "date": "2009-06-15T22:51:12.677+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "add tests for dutchanalyzer.\n\nthis analyzer claims to implement snowball, although tests reveal some differences. it also has about 1MB of text files that don't appear to be in use at all...",
            "date": "2009-06-16T05:30:05.327+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "These are much needed... thanks Robert.  Let me know when you're done iterating (and/or when we need to wrap up 2.9) and we'll get these in.",
            "date": "2009-06-16T14:17:40.737+0000",
            "id": 3
        },
        {
            "author": "Robert Muir",
            "body": "Michael: LUCENE-973 would save me from having to create tests for the CJKAnalyzer.\n\nIt would also fix a bug.",
            "date": "2009-06-16T14:33:03.878+0000",
            "id": 4
        },
        {
            "author": "Robert Muir",
            "body": "thanks, i'll upload some more tests hopefully soon. I think most have rudimentary tests.\n\nbut some are not sufficient to ensure any api conversion is really working.\n\nfor example ThaiAnalyzer does not have any offset tests, but if that broke then highlighting would break.",
            "date": "2009-06-16T22:02:47.100+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "Robert, you should probably also hold up on API conversion, since the API itself is now changing (LUCENE-1693).",
            "date": "2009-06-17T09:01:03.392+0000",
            "id": 6
        },
        {
            "author": "Robert Muir",
            "body": "michael, ok. I know additional tests here (against the old api) might be more code to convert, but I think it will actually make the process easier, whenever that is or whatever is involved.\n\ni have some time this evening to try to improve the coverage here (against the old api).\n",
            "date": "2009-06-17T14:54:59.278+0000",
            "id": 7
        },
        {
            "author": "Robert Muir",
            "body": "adds tests for thaianalyzer token offsets and types, both of which have bugs!\ntests for correct behavior are included but commented out.\n",
            "date": "2009-06-18T06:51:59.255+0000",
            "id": 8
        },
        {
            "author": "Robert Muir",
            "body": "added tests for czech.\nadded additional tests for smartchineseanalyzer, there is a bug very similar to the recent CJK one here... generating empty tokens.",
            "date": "2009-06-18T18:58:47.649+0000",
            "id": 9
        },
        {
            "author": "Robert Muir",
            "body": "michael: I'm think I'm done here.\n\nif you consider any of the bugs important just let me know, can try to help get them fixed.\n",
            "date": "2009-06-18T19:00:48.892+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "bq. michael: I'm think I'm done here.\n\nOK I'll review.  Thanks!!\n\nbq. if you consider any of the bugs important just let me know, can try to help get them fixed.\n\nLikely I won't be able to judge the severity of these bugs... so please chime in if you think they should be fixed...",
            "date": "2009-06-18T20:16:16.597+0000",
            "id": 11
        },
        {
            "author": "Robert Muir",
            "body": "Michael, I think it would be nice to fix the Thai offset bug, so highlighter will work. this is a safe one-line fix and its an obvious error.\n\nThe SmartChineseAnalyzer empty token bug is pretty serious, i think indexing empty tokens for every piece of punctuation could really hurt similarity computation (am i wrong, never tried?)\n\nThe Thai .type() bug is something that could be fixed later, i don't think the token type being ALPHANUM versus NUM is really hurting anyone.\n\nThe issue where DutchAnalyzer doesnt do what it claims, i think thats not really hurting anyone, and they can use the snowball version if they want accurate snowball behavior.\nI do think the huge files in DutchAnalyzer that aren't being used can be removed if you want to save 1MB, but I'm not sure how important that is.\n\nLet me know your thoughts. ",
            "date": "2009-06-18T20:24:14.285+0000",
            "id": 12
        },
        {
            "author": "Michael McCandless",
            "body": "I'm seeing this test failure:\n{code}\n    [junit] Testcase: testBuggyPunctuation(org.apache.lucene.analysis.cn.TestSmartChineseAnalyzer):\tCaused an ERROR\n    [junit] null\n    [junit] java.lang.AssertionError\n    [junit] \tat org.apache.lucene.analysis.StopFilter.next(StopFilter.java:240)\n    [junit] \tat org.apache.lucene.analysis.cn.TestSmartChineseAnalyzer.testBuggyPunctuation(TestSmartChineseAnalyzer.java:51)\n{code}\n\nIt's because null is being passed to ts.next in the final assertTrue line:\n\n{code}\n    nt = ts.next(nt);\n    while (nt != null) {\n      assertEquals(result[i], nt.term());\n      i++;\n      nt = ts.next(nt);\n    }\n    assertTrue(ts.next(nt) == null);\n{code}",
            "date": "2009-06-18T20:26:54.362+0000",
            "id": 13
        },
        {
            "author": "Mark Miller",
            "body": "heh -\n\n+1 on fixing them all. Including reclaiming that 1 mb of space if we can ...",
            "date": "2009-06-18T20:29:02.403+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "Me too :)  Robert can you cons up a patch?  Which files can be safely removed from the DutchAnalyzer?  (stems/words.txt?)",
            "date": "2009-06-18T20:31:43.653+0000",
            "id": 15
        },
        {
            "author": "Robert Muir",
            "body": "michael, i guess junit from my eclipse != junit from ant, because it passes in eclipse...annoying\n\nI will fix the test so it runs correctly from ant.",
            "date": "2009-06-18T20:32:17.286+0000",
            "id": 16
        },
        {
            "author": "Robert Muir",
            "body": "michael: yes the stems/words.txt\n\nfor stems.txt/words.txt: I am scratching my head trying to figure out what they were originally intended to do. If its to support dictionary stemming with wordlistloader, then it really needs to be one tab-separated file, not two files.",
            "date": "2009-06-18T20:33:54.425+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "Probably eclipse isn't running with asserts?",
            "date": "2009-06-18T20:38:08.799+0000",
            "id": 18
        },
        {
            "author": "Robert Muir",
            "body": "probably, fixed it and testing with ant now. ill upload it at least so you can verify the behavior i've discovered.\n\ndo you want me to include patch with the two bugfixes (chinese empty token and thai offsets), or give you something separate for those?\n\nfor the other 2 bugs:\nfixing the Thai tokentype bug, well its really a bug in the standardtokenizer grammar. i wasn't sure you wanted to change that at this moment, but if you want it fixed let me know!\nin my opinion: fix for DutchAnalyzer is to deprecate/remove the contrib completely, since it claims to do snowball stemming, why shouldnt someone just use the Dutch snowball stemmer from the contrib/snowball package!\n\n  \n",
            "date": "2009-06-18T20:46:32.300+0000",
            "id": 19
        },
        {
            "author": "Robert Muir",
            "body": "Having trouble figuring this one out",
            "date": "2009-06-18T21:11:47.644+0000",
            "id": 20
        },
        {
            "author": "Robert Muir",
            "body": "michael: here is an updated patch.\n\ni removed that chinese test, there's something strange going on here [see my screenshot] but i can't seem to create a test case to show it!\n",
            "date": "2009-06-18T21:16:31.716+0000",
            "id": 21
        },
        {
            "author": "Robert Muir",
            "body": "ok got it,\n\nthe IDEOGRAPHIC FULL STOP is being converted into a comma token by the tokenizer.\nif you use the default constructor: SmartChineseAnalyzer(), it won't load the default stopwords list, such as from my Luke screenshot.\nif you instead instantiate it like this: SmartChineseAnalyzer(true), then it loads the default stopwords list.\nthe default stopwords list includes things like comma, so it ends out getting removed.\n\nmaybe its not a bug, but this is really non-obvious behavior...!\n",
            "date": "2009-06-18T21:30:48.451+0000",
            "id": 22
        },
        {
            "author": "Robert Muir",
            "body": "patch with new testcase demonstrating the chinese behavior.",
            "date": "2009-06-18T21:33:23.718+0000",
            "id": 23
        },
        {
            "author": "Robert Muir",
            "body": "later tonight i can workup a patch to address the thai offset issue and at least javadoc'ing the chinese behavior.\n\nif you think the addt'l 2 issues [thai tokentype, dutchanalyzer behavior/huge files] should be fixed or documented in some way, please let me know.\n",
            "date": "2009-06-18T21:43:45.557+0000",
            "id": 24
        },
        {
            "author": "Robert Muir",
            "body": "patch with the two one-line fixes:\n1. fix offsets for thai analyzer so highlighting, etc will work.\n2. use stopwords list by default for smartchineseanalyzer so punctuation isn't indexed in a strange way.\n\ni updated the testcases to reflect these.\n\n\n",
            "date": "2009-06-19T03:43:02.207+0000",
            "id": 25
        },
        {
            "author": "Michael McCandless",
            "body": "Latest patch looks good Robert, thanks!\n\nDeprecating DutchAnalyzer (in favor of Snowball) makes sense to me -- any objections out there?\n\n(And I'll \"svn rm\" the two large & unused files).\n\nRobert, could you open a new issue for the Thai token type bug (that requires a change to StandardTokenizer's grammar)?  We seem to be accumulating a number of these \"fix StandardTokeninizer's grammar\" but we don't have a good way to do this back-compatibly... matchVersion is a good way for the user to express compatibility requirement, but we don't know how to [cleanly] switch on that to different grammar variants.\n\nIs that the only issue not addressed by the latest patch?",
            "date": "2009-06-19T09:41:21.495+0000",
            "id": 26
        },
        {
            "author": "Robert Muir",
            "body": "michael, yes the only issue... i'll open another issue for the thai token type.\n",
            "date": "2009-06-19T14:13:07.665+0000",
            "id": 27
        },
        {
            "author": "Michael McCandless",
            "body": "OK I will commit this soon.  Thanks Robert!",
            "date": "2009-06-19T15:38:50.302+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Robert!",
            "date": "2009-06-19T15:52:45.620+0000",
            "id": 29
        },
        {
            "author": "Robert Muir",
            "body": "michael, I updated my svn and I think you might have missed some of the tests.\n\nthere are tests in the patch for BrazilianAnalyzer, CzechAnalyzer, and DutchAnalyzer... (these are new directories, maybe that is why?)",
            "date": "2009-06-19T17:22:20.434+0000",
            "id": 30
        },
        {
            "author": "Michael McCandless",
            "body": "Duh, I forgot to svn add them!  Sorry.  I'm glad you caught that.  I'm really wanting \"svn patch\"....",
            "date": "2009-06-19T17:56:20.611+0000",
            "id": 31
        },
        {
            "author": "Michael McCandless",
            "body": "OK I committed them.  Thanks for catching this Robert!",
            "date": "2009-06-19T18:03:03.528+0000",
            "id": 32
        },
        {
            "author": "Robert Muir",
            "body": "patch with a couple addtl tests for contrib/analysis, with some javadocs cleanup and wording.\nthere is also fix to the synonyms test to actually test its reset() ...\nno code changes though.\n",
            "date": "2009-08-17T05:26:13.132+0000",
            "id": 33
        },
        {
            "author": "Robert Muir",
            "body": "if possible, i think these might be good to add for the release.",
            "date": "2009-08-17T05:26:49.579+0000",
            "id": 34
        },
        {
            "author": "Robert Muir",
            "body": "correct missing cjk test. \n\nif no one objects i would like to commit these javadocs and tests tomorrow.",
            "date": "2009-08-17T11:45:04.419+0000",
            "id": 35
        },
        {
            "author": "Robert Muir",
            "body": "Committed revision 805400.",
            "date": "2009-08-18T13:01:23.420+0000",
            "id": 36
        }
    ],
    "component": "modules/analysis",
    "description": "The analyzers in contrib need tests, preferably ones that test the behavior of all the Token 'attributes' involved (offsets, type, etc) and not just what they do with token text.\n\nThis way, they can be converted to the new api without breakage.",
    "hasPatch": false,
    "hasScreenshot": true,
    "id": "LUCENE-1692",
    "issuetypeClassified": "OTHER",
    "issuetypeTracker": "OTHER",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Contrib analyzers need tests",
    "systemSpecification": true,
    "version": ""
}