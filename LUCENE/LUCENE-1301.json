{
    "comments": [
        {
            "author": "Michael Busch",
            "body": "Mike, I think the ArrayUtil class is missing in your patch?",
            "date": "2008-06-16T02:34:50.423+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "Woops, sorry, I forgot to svn add that.  I'm attaching my current\nstate, with that file added.  Does this one work?  (You may need to\nforcefully remove DocumentsWriterFieldData.java if applying the patch\ndoesn't do so).\n\n",
            "date": "2008-06-16T10:17:06.835+0000",
            "id": 1
        },
        {
            "author": "Michael Busch",
            "body": "Just a quick update, Mike: \nWith your latest patch it's compiling fine now. Thanks!\nI'm seeing NullPointerExceptions in TestStressIndexing2 though,\nbut I guess this patch is not final yet.\n\nI haven't read the patch yet, hope I'll find some time soon.",
            "date": "2008-06-18T01:35:02.818+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "Attached new rev of the patch.\n\nbq. I'm seeing NullPointerExceptions in TestStressIndexing2 though,\n\nI believe this patch fixes that.  All tests should now pass.",
            "date": "2008-06-19T10:50:01.331+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "New rev of the patch attached.  I've fixed all nocommits.  All tests\npass.  I believe this version is ready to commit!\n\nI'll wait a few more days before committing...\n\nI ran some indexing throughput tests, indexing Wikipedia docs from a\nline file using StandardAnalyzer.  Each result is best of 4.  Here's\nthe alg:\n\n{code}\nanalyzer=org.apache.lucene.analysis.standard.StandardAnalyzer\n\ndoc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n\ndocs.file=/Volumes/External/lucene/wiki.txt\ndoc.stored = true\ndoc.term.vector = true\ndoc.add.log.step=2000\n\ndirectory=FSDirectory\nautocommit=false\ncompound=false\n\nwork.dir=/lucene/work\nram.flush.mb=64\n\n{ \"Rounds\"\n  ResetSystemErase\n  { \"BuildIndex\"\n    - CreateIndex\n     { \"AddDocs\" AddDoc > : 200000\n    - CloseIndex\n  }\n  NewRound\n} : 4\n\nRepSumByPrefRound BuildIndex\n{code}\n\nGives these results with term vectors & stored fields:\n{code}\npatch\n  BuildIndex -  - 1 -  -   1 -  -  200000 -  -   900.4 -  - 222.12 - 410,938,688  1,029,046,272\n\ntrunk\n  BuildIndex -  - 1 -  -   1 -  -  200000 -  -   969.0 -  - 206.39 - 400,372,256  1,029,046,272\n\n2.3\n  BuildIndex      2        1       200002        905.4      220.89   391,630,016  1,029,046,272\n{code}\n\n\nAnd without term vectors & stored fields:\n\n{code}\npatch\n  BuildIndex -  - 3 -  -   1 -  -  200000 -  - 1,297.5 -  - 154.15 - 399,966,592  1,029,046,272\n\ntrunk\n  BuildIndex -  - 1 -  -   1 -  -  200000 -  - 1,372.5 -  - 145.72 - 390,581,376  1,029,046,272\n\n2.3\n  BuildIndex -  - 1 -  -   1 -  -  200002 -  - 1,308.5 -  - 152.85 - 389,224,640  1,029,046,272\n{code}\n\nSo, the bad news is the refactoring had made things a bit (~5-7%)\nslower than the current trunk.  But the good news is trunk was already\n6-7% faster than 2.4, so they nearly cancel out.\n\nIf I repeat these tests using tiny docs (~100 bytes per body) instead,\nindexing the first 10 million docs, the slowdown is worse (~13-15% vs\ntrunk, ~11-13% vs 2.3)... I think it's because the additional method calls\nwith the refactoring become a bigger part of the time.\n\nWith term vectors & stored fields:\n\n{code}\npatch\n  BuildIndex -  - 3 -  -   1 -   10000000 -   38,320.1 -  - 260.96 - 313,980,832  1,029,046,272\n\ntrunk\n  BuildIndex      2        1     10000000     45,194.1      221.27   414,987,072  1,029,046,272\n\n2.3\n  BuildIndex -  - 1 -  -   1 -   10000002 -   42,861.4 -  - 233.31 - 182,957,440  1,029,046,272\n{code}\n\nWithout term vectors & stored fields:\n\n{code}\npatch\n  BuildIndex -  - 1 -  -   1 -   10000000 -   60,778.4 -  - 164.53 - 341,611,456  1,029,046,272\n\ntrunk\n  BuildIndex      2        1     10000000     68,387.8      146.23   405,388,960  1,029,046,272\n\n2.3\n  BuildIndex      0        1     10000002     68,052.7      146.95   330,334,912  1,029,046,272\n{code}\n\nI think these small slowdowns are worth the improvement in code\nclarity.\n",
            "date": "2008-07-11T17:59:43.757+0000",
            "id": 4
        }
    ],
    "component": "core/index",
    "description": "I've been working on refactoring DocumentsWriter to make it more\nmodular, so that adding new indexing functionality (like column-stride\nstored fields, LUCENE-1231) is just a matter of adding a plugin into\nthe indexing chain.\n\nThis is an initial step towards flexible indexing (but there is still\nalot more to do!).\n\nAnd it's very much still a work in progress -- there are intemittant\nthread safety issues, I need to add tests cases and test/iterate on\nperformance, many \"nocommits\", etc.  This is a snapshot of my current\nstate...\n\nThe approach introduces \"consumers\" (abstract classes defining the\ninterface) at different levels during indexing.  EG DocConsumer\nconsumes the whole document.  DocFieldConsumer consumes separate\nfields, one at a time.  InvertedDocConsumer consumes tokens produced\nby running each field through the analyzer.  TermsHashConsumer writes\nits own bytes into in-memory posting lists stored in byte slices,\nindexed by term, etc.\n\nDocumentsWriter*.java is then much simpler: it only interacts with a\nDocConsumer and has no idea what that consumer is doing.  Under that\nDocConsumer there is a whole \"indexing chain\" that does the real work:\n\n  * NormsWriter holds norms in memory and then flushes them to _X.nrm.\n\n  * FreqProxTermsWriter holds postings data in memory and then flushes\n    to _X.frq/prx.\n\n  * StoredFieldsWriter flushes immediately to _X.fdx/fdt\n\n  * TermVectorsTermsWriter flushes immediately to _X.tvx/tvf/tvd\n\nDocumentsWriter still manages things like flushing a segment, closing\ndoc stores, buffering & applying deletes, freeing memory, aborting\nwhen necesary, etc.\n\nIn this first step, everything is package-private, and, the indexing\nchain is hardwired (instantiated in DocumentsWriter) to the chain\ncurrently matching Lucene trunk.  Over time we can open this up.\n\nThere are no changes to the index file format.\n\nFor the most part this is just a [large] refactoring, except for these\ntwo small actual changes:\n\n  * Improved concurrency with mixed large/small docs: previously the\n    thread state would be tied up when docs finished indexing\n    out-of-order.  Now, it's not: instead I use a separate class to\n    hold any pending state to flush to the doc stores, and immediately\n    free up the thread state to index other docs.\n\n  * Buffered norms in memory now remain sparse, until flushed to the\n    _X.nrm file.  Previously we would \"fill holes\" in norms in memory,\n    as we go, which could easily use way too much memory.  Really this\n    isn't a solution to the problem of sparse norms (LUCENE-830); it\n    just delays that issue from causing memory blowup during indexing;\n    memory use will still blowup during searching.\n\nI expect performance (indexing throughput) will be worse with this\nchange.  I'll profile & iterate to minimize this, but I think we can\naccept some loss.  I also plan to measure benefit of manually\nre-cycling RawPostingList instances from our own pool, vs letting GC\nrecycle them.\n\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1301",
    "issuetypeClassified": "REFACTORING",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Refactor DocumentsWriter",
    "systemSpecification": true,
    "version": "2.3, 2.3.1, 2.3.2, 2.4"
}