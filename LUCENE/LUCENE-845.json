{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "This bug is actually rather serious.\n\nIf you set maxBufferedDocs to a very large number (on the expectation\nthat it's not used since you will manually flush by RAM usage) then\nthe merge policy will always merge the index down to 1 segment as soon\nas it hits mergeFactor segments.\n\nThis will be an O(N^2) slowdown.  EG if based on RAM you are\nflushing every 100 docs, then at 1000 docs you will merge to 1\nsegment.  Then at 1900 docs, you merge to 1 segment again.  At 2800,\n3700, 4600, ... (every 900 docs) you keep merging to 1 segment.  Your\nindexing process will get very slow because every 900 documents the\nentire index is effectively being optimized.\n\nWith LUCENE-843 I'm thinking we should deprecate maxBufferedDocs\nentirely and switch to flushing by RAM usage instead (you can always\nmanually flush every N documents in your app if for some reason you\nneed that).  But obviously we need to resolve this bug first.\n",
            "date": "2007-03-23T15:27:08.228+0000",
            "id": 0
        },
        {
            "author": "Yonik Seeley",
            "body": "> With LUCENE-843 I'm thinking we should deprecate maxBufferedDocs entirely\n\nThat might present a problem for users of ParallelReader.  Right now, it's possible to construct two indicies with corresponding docids.... switching to flush-by-ram would makesegment merging unpredictable and destroy the docid matching.",
            "date": "2007-03-26T16:22:44.921+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": ">> With LUCENE-843 I'm thinking we should deprecate maxBufferedDocs entirely\n>\n> That might present a problem for users of ParallelReader. Right now,\n> it's possible to construct two indicies with corresponding\n> docids.... switching to flush-by-ram would makesegment merging\n> unpredictable and destroy the docid matching.\n\nAhhh, this is a very good point.  OK I won't deprecate \"flushing by\ndoc count\" and instead will allow either \"flush by RAM usage\" (default\nto this?) or \"flush by doc count\".\n",
            "date": "2007-03-26T18:13:37.978+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "Just recapping some following discussion from java-dev ...\n\nThe current merge policy can be thought of logically as two different\nsteps:\n\n  1. How to determine the \"level\" of each segment in the index.\n\n  2. How & when to pick which level N segments into a level N+1\n     segment.\n\nThe current policy determines a segment's level by looking at the doc\ncount in the segment as well as the current maxBufferedDocs, which is\nvery problematic when you \"flush by RAM usage\" instead.  This Jira\nissue, then, is proposing to instead look at overall byte size of a\nsegment for determining its level, while keeping step 2. above.\n\nHowever, I would propose we also fix LUCENE-854 (which addresses step\n2 above and not step 1) at the same time, as a single merge policy,\nand maybe at some point in the future make this new merge policy the\ndefault merge policy.\n",
            "date": "2007-03-31T12:21:49.559+0000",
            "id": 3
        },
        {
            "author": "Steven Parkes",
            "body": "Following up on this, it's basically the idea that segments ought to be created/merged both either by-segment-size or by-doc-count but not by a mixture? That wouldn't be suprising ...\n\nIt does impact the APIs, though. It's easy enough to imagine, with factored merge policies, both by-doc-count and by-segment-size policies. But the initial segment creation is going to be handled by IndexWriter, so you have to manually make sure you don't set that algorithm and the merge policy in conflict. Not great, but I don't have any great ideas. Could put in an API handshake, but I'm not sure if it's worth the mess?\n\nAlso, it sounds like, so far, there's no good way of managing parallel-reader setups w/by-segment-size algorithms, since the algorithm for creating/merging segments has to be globally consistent, not just per index, right?\n\nIf that is right, what does that say about making by-segment-size the default? It's gonna break (as in bad results) people relying on that behavior that don't change their code. Is there a community consensus on this? It's not really an API change that would cause a compile/class-load failure, but in some ways, it's worse ...",
            "date": "2007-04-30T23:24:38.438+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "> Following up on this, it's basically the idea that segments ought to be created/merged both either by-segment-size or by-doc-count but not by a mixture? That wouldn't be suprising ...\n\nRight, but we need the refactored merge policy framework in place\nfirst.  I'll mark this issue dependent on LUCENE-847.\n\n> It does impact the APIs, though. It's easy enough to imagine, with factored merge policies, both by-doc-count and by-segment-size policies. But the initial segment creation is going to be handled by IndexWriter, so you have to manually make sure you don't set that algorithm and the merge policy in conflict. Not great, but I don't have any great ideas. Could put in an API handshake, but I'm not sure if it's worth the mess?\n\nGood question.  I think it's OK (at least for our first go at this --\nprogress not perfection!) to expect the developer to choose a merge\npolicy and then to use IndexWriter in a way that's \"consistent\" with\nthat merge policy?  I think it's going to get too complex if we try to\nformally couple \"when to flush/commit\" with the merge policy?\n\nBut, I think the default merge policy needs to be resilient to people\ndoing things like changing maxBuffereDocs/mergeFactor partway through\nan index, calling flush() whenever they want, etc.  The merge policy\ntoday is not resilient to these \"normal\" usages of IndexWriter.  So I\nthink we need to do something here even without the pressure from\nLUCENE-843.\n\n> Also, it sounds like, so far, there's no good way of managing parallel-reader setups w/by-segment-size algorithms, since the algorithm for creating/merging segments has to be globally consistent, not just per index, right?\n\nRight.  We clearly need to keep the current \"by doc\" merge policy\neasily available for this use case.\n\n> If that is right, what does that say about making by-segment-size the default? It's gonna break (as in bad results) people relying on that behavior that don't change their code. Is there a community consensus on this? It's not really an API change that would cause a compile/class-load failure, but in some ways, it's worse ...\n\nI think there are actually two questions here:\n\n  1) What exactly makes for a good default merge policy?\n\n     I think the merge policy we have today has some limitations:\n\n       - It's not resilient to \"normal\" usage of the public APIs in\n         IndexWriter.  If you call flush() yourself, if you change\n         maxBufferedDocs (and maybe mergeFactor?) partway through an\n         index, etc, you can cause disastrous amounts of over-merging\n         (that's this issue).\n\n \t I think the default policy should be entirely resilient to\n\t any usage of the public IndexWriter APIs.\n\n       - Default merge policy should strive to minimize net cost\n         (amortized over time) of merging, but the current one\n         doesn't:\n\n         - When docs differ in size (frequently the case) it will be\n           too costly in CPU/IO consumption because small segments are\n           merged with large ones.\n\n         - It does too much work in advance (too much \"pay it\n           forward\").  I don't think a merge policy should\n           \"inadvertently optimize\" (I opened LUCENE-854 to describe\n           this).\n\n       - It blocks LUCENE-843 (flushing by RAM usage).\n\n         I think Lucene \"out of the box\" should give you good indexing\n         performance.  You should not have to do extra tuning to get\n         substantially better performance.  The best way to get that\n         is to \"flush by RAM\" (with LUCENE-843).  But current merge\n         policy prevents this (due to this issue).\n\n  2) Can we change the default merge policy?\n\n     I sure hope so, given the issues above.\n\n     I think the majority of Lucene users do the simple \"create a\n     writer, add/delete docs, close writer, while reader(s) use the\n     same index\" type of usage and so would benefit by the gained\n     performance of LUCENE-843 and LUCENE-854.\n\n     I think (but may be wrong!) it's a minority who use\n     ParallelReader and therefore have a reliance on the specific\n     merge policy we use today?\n\nIdeally we first commit the \"decouple merge policy from IndexWriter\"\n(LUCENE-847), then we would make a new merge policy that resolves this\nissue and LUCENE-854, and make it the default policy.  I think this\npolicy would look at size (in bytes) of each segment (perhaps\nproportionally reducing # bytes according to pending deletes against\nthat segment), and would merge any adjacent segments (not just\nrightmost ones) that are \"the most similar\" in size.  I think it would\nmerge N (configurable) at a time and at no time would inadvertently\noptimize.\n\nThis would mean users of ParallelReader on upgrading to this would\nneed to change their merge policy to the legacy \"merge by doc count\"\npolicy.",
            "date": "2007-05-02T09:58:39.364+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "\nFirst cut patch.  You have to first apply the most recent patch from\nLUCENE-847:\n\n  https://issues.apache.org/jira/secure/attachment/12363880/LUCENE-847.patch.txt\n\nand then apply this patch over it.\n\nThis patch has two merge policies:\n\n  LogDocMergePolicy\n\n    This is \"backwards compatible\" to current merge policy, yet,\n    resolve this \"over-merge issue\" by not using the current setting\n    of \"maxBufferedDocs\" when computing levels.  I think it should\n    replace the current LogDocMergePolicy from LUCENE-847.\n\n  LogByteSizeMergePolicy\n\n    Chooses merges according to net size in bytes of all files for a\n    segment.  I think we should make this one the default merge\n    policy, and also change IndexWriter to flush by RAM by default.\n\nThey both subclass from abstract base LogMergePolicy and differ only\nin the \"size\" method which defines how you measure a segment's size (#\ndocs in that segment or net size in bytes of that segment).\n\nThe gist of the approach is the same as the current merge policy: you\ngenerally try to merge segments that are \"roughly\" the same size\n(where size can be doc count or byte size), mergeFactor at a time.\n\nThe big difference is instead of starting from maxBufferedDocs and\n\"going up\" to determine level, I start from the max segment size (of\nall segments in the index) and \"go down\" to determine level.  This\nresolves the bug because levels are \"self-defined\" by the segments,\nrather than by the current value of maxBufferedDocs on IndexWriter.\n\nI then pick merges exactly the same as the current merge policy: if\nany level has >= mergeFactor segments, we merge them.\n\nAll tests pass, except:\n\n  * One assert in testAddIndexesNoOptimize which was relying on the\n    specific invariants of the current merge policy (it's the same\n    assert that LUCENE-847 had changed; this assert is testing\n    particular corner cases of the current merge policy).  Changing\n    the assertEquals to \"4\" instead of \"3\" fixes it.\n\n  * TestLogDocMergePolicy (added in LUCENE-847) doesn't compile\n    against the new version above because it's using methods that\n    don't exist in the new one.\n",
            "date": "2007-08-15T23:57:42.625+0000",
            "id": 6
        },
        {
            "author": "Steven Parkes",
            "body": "This increases file descriptor usage in some cases, right? In the old scheme, if you set mergeFactor to 10 and maxBufferedDocs to 1000, you'd only get 10 segments with size <= 1000. But with this code, you can't bound that anymore. If I create single doc segments (perhaps by flushing based on latency), I can get 30 of them?\n\nOf course, if what we're trying to do is manage the number of file descriptors, we should just do that, rather than using using maxBufferedDocs as a proxy (with all it's nasty overmerging behavior).",
            "date": "2007-08-16T01:10:07.778+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "\n> This increases file descriptor usage in some cases, right? In the\n> old scheme, if you set mergeFactor to 10 and maxBufferedDocs to\n> 1000, you'd only get 10 segments with size <= 1000. But with this\n> code, you can't bound that anymore. If I create single doc segments\n> (perhaps by flushing based on latency), I can get 30 of them?\n\nRight, the # segments allowed in the index will be more than it is w/\nthe current merge policy if you consistently flush with [far] fewer\ndocs than maxBufferedDocs is set to.\n\nBut, this is actually the essense of the bug.  The case we're trying\nto fix is where you set maxBufferedDocs to something really large (say\n1,000,000) to avoid flushing by doc count, and you setRamBufferSizeMB\nto something like 32 MB.  In this case, the current merge policy would\njust keep merging any set of 10 segments with < 1,000,000 docs each,\nsuch that eventually all your indexing time is being spent doing\nhighly sub-optimal merges.\n\n",
            "date": "2007-08-16T09:17:57.893+0000",
            "id": 8
        },
        {
            "author": "Steven Parkes",
            "body": "I understand the merge problem but I'm still concerned about the increased number of file descriptors. Is this a concern?\n\nIt seems like there are ways of approaching this, that might be able to fix both problems?\n\nFor example, right now (pre-fix), if you have maxBufferedDocs set to 1000, mergeFactor set to 10, and add (for the sake of obvious example) 10 single doc segments, it's going to do a merge to one segment of size 1010, which is not great.\n\nOne solution to this would be in cases like this to merge the small segments to one but not include the big segments. So you get [1000 10] where the last segment keeps growing until it reaches 1000. This does more copies than the current case, but always on small segments, with the advantage of a lower bound on the number of file descriptors?\n\nOf course, if no one's worried about this \"moderate\" (not exactly large, not exactly small) change in file descriptor usage, then it's not a big deal. It doesn't impact my work but I'm not sure about the greater community.",
            "date": "2007-08-16T15:03:09.664+0000",
            "id": 9
        },
        {
            "author": "Yonik Seeley",
            "body": "Is there a change in filedescriptor use if you don't use setRamBufferSizeMB?\n",
            "date": "2007-08-16T15:12:16.516+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "\n> Is there a change in filedescriptor use if you don't use setRamBufferSizeMB?\n\nYes.  EG, if you set maxBufferedDocs to 1000 but then flush after\nevery added doc, and you add 1000 docs, with the current merge policy,\nevery 10 flushes you will merge all segments together.  Ie, first\nsegment has 10 docs, then 20, 30, 40, 50, ..., 1000.  This is where\nO(N^2) cost on merging comes from.  But, you will never have more than\n10 segments in your index.\n\nWhereas the new merge policy will make levels (segments of size 100,\n10, 1) and merge only segments from the same level together.  So merge\ncost will be much less (not O(N^2)), but, you will have more max segments\nin the index (up to 1 + (mergeFactor-1) * log_mergeFactor(numDocs)),\nor 28 segments in this example (I think).\n\nBasically the new merge policy tries to make levels \"all the way\ndown\" rather than forcefully stopping when the levels get smaller than\nmaxBufferedDocs, to avoid the O(N^2) merge cost.\n\n> One solution to this would be in cases like this to merge the small\n> segments to one but not include the big segments. So you get [1000\n> 10] where the last segment keeps growing until it reaches 1000. This\n> does more copies than the current case, but always on small\n> segments, with the advantage of a lower bound on the number of file\n> descriptors?\n\nI'm not sure that helps?  Because that \"small segment\" will have to\ngrow bit by bit up to 1000 (causing the O(N^2) cost).\n\nNote that the goal here is to be able to switch to flushing by RAM\nbuffer size instead of docCount (and also merge by byte-size of\nsegments not doc count), by default, in IndexWriter.  But, even once\nwe do that, if you always flush tiny segments the new merge policy\nwill still build levels \"all the way down\".\n\nHere's an idea: maybe we can accept the O(N^2) merge cost, when the\nsegments are \"small\"?  Ie, maybe doing 100 sub-optimal merges (in the\nexample above) does not amount to that much actual cost in practice.\n(After all nobody has complained about this :).\n\nI will run some tests.  Clearly at some point the O(N^2) cost will\ndominate your indexing time, but maybe we can set a \"rough\" docCount\nbelow which all segments are counted as a single level and not take\ntoo much of a indexing performance hit.\n",
            "date": "2007-08-16T18:28:27.282+0000",
            "id": 11
        },
        {
            "author": "Yonik Seeley",
            "body": "You may avoid the cost of a bunch of small merges, but then you pay the price in searching performance.  I'm not sure that's the right tradeoff because if someone wanted to optimize for indexing performance, they would do more in batches.\n\nHow does this work when flushing by MB?  If you set setRamBufferSizeMB(32), are you guaranteed that you never have more than 10 segments less than 32MB (ignoring LEVEL_LOG_SPAN for now) if mergeFactor is 10?\n\nAlmost seems like we need a minSegmentSize parameter too - using setRamBufferSizeMB confuses two different but related issues.",
            "date": "2007-08-16T18:53:06.221+0000",
            "id": 12
        },
        {
            "author": "Steven Parkes",
            "body": " \tHere's an idea: maybe we can accept the O(N^2) merge cost, when the\n\tsegments are \"small\"?\n\nThat's basically the underlying idea I was trying to get at.",
            "date": "2007-08-16T19:05:03.508+0000",
            "id": 13
        },
        {
            "author": "Michael McCandless",
            "body": "\n> You may avoid the cost of a bunch of small merges, but then you pay\n> the price in searching performance. I'm not sure that's the right\n> tradeoff because if someone wanted to optimize for indexing\n> performance, they would do more in batches.\n\nAgreed.\n\nIt's like we would want to run \"partial optimize\" (ie, merge the tail\nof \"small\" segments) on demand, only when a reader is about to\nrefresh.\n\nOr here's another random idea: maybe IndexReaders should load the tail\nof \"small segments\" into a RAMDirectory, for each one.  Ie, an\nIndexReader is given a RAM buffer \"budget\" and it spends it on any\nnumerous small segments in the index....?\n\n> How does this work when flushing by MB? If you set\n> setRamBufferSizeMB(32), are you guaranteed that you never have more\n> than 10 segments less than 32MB (ignoring LEVEL_LOG_SPAN for now) if\n> mergeFactor is 10?\n\nNo, we have the same challenge of avoiding O(N^2) merge cost.  When\nmerging by \"byte size\" of the segments, I don't look at the current\nRAM buffer size of the writer.\n\nI feel like there should be a strong separation of \"flush params\" from\n\"merge params\".\n\n> Almost seems like we need a minSegmentSize parameter too - using\n> setRamBufferSizeMB confuses two different but related issues.\n\nExactly!  I'm thinking that I add \"minSegmentSize\" to the\nLogMergePolicy, which is separate from \"maxBufferedDocs\" and\n\"ramBufferSizeMB\".  And, we default it to values that seem like an\n\"acceptable\" tradeoff of the cost of O(N^2) merging (based on tests I\nwill run) vs the cost of slowdown to readers...\n\nI'll run some perf tests.  O(N^2) should be acceptable for certain\nsegment sizes....\n",
            "date": "2007-08-16T19:42:31.564+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "> > Here's an idea: maybe we can accept the O(N^2) merge cost, when\n> > the segments are \"small\"?\n>\n> That's basically the underlying idea I was trying to get at.\n\nAhh, good!  We agree :)",
            "date": "2007-08-16T19:43:11.934+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "> Or here's another random idea: maybe IndexReaders should load the\n> tail of \"small segments\" into a RAMDirectory, for each one. Ie, an\n> IndexReader is given a RAM buffer \"budget\" and it spends it on any\n> numerous small segments in the index....?\n\nFollowing up on this ... I think IndexReader could load \"small tail\nsegments\" into RAMDirectory and then do a merge on them to make\nsearch even faster.  It should typically be extremely fast if we set the\ndefaults right, and RAM usage should be quite low since merging\nsmall segments usually gives great compression in net bytes used.\n\nThis would allow us to avoid (or, minimize) the O(N^2) cost on merging\nto ensure that an index is \"at all instants\" ready for a reader to\ndirectly load it.  This basically gives us our \"merge tail segments on\ndemand when a reader refreshes\".\n\nWe can do a combination of these two approaches, whereby the\nIndexWriter is free to make use a \"long tail\" of segments so it\ndoesn't have O(N^2) slowdown on merge cost, yet a reader pays very\nsmall (one-time) cost for such segments.\n\nI think the combination of these two changes should give a net/net\nsizable improvement on \"low latency\" apps.... because IndexWriter is\nfree to make miniscule segments (document by document even) and\nIndexReader (especially combined with LUCENE-743) can quickly\nre-open and do a \"mini-optimize\" on the tail segments and have\ngreat performance.\n",
            "date": "2007-08-16T20:01:53.878+0000",
            "id": 16
        },
        {
            "author": "Steven Parkes",
            "body": "\tI think the combination of these two changes should give a net/net\n\tsizable improvement on \"low latency\" apps....\n\nI think this would be great. It's always been a pet peeve of mine that even in low pressure/activity environments, there is often a delay from write to read.\n\nSounds like this would help take most of the work/risk off the developer.",
            "date": "2007-08-16T20:33:04.490+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "> I think this would be great. It's always been a pet peeve of mine\n> that even in low pressure/activity environments, there is often a\n> delay from write to read.\n\nI'll open a new issue.\n\n> Sounds like this would help take most of the work/risk off the\n> developer.\n\nPrecisely!  Out of the box we can have very low latency from\nIndexWriter flushing single doc segments, and not having to pay the\nO(N^2) merge cost of merging down such segments to be \"at all moments\"\nready for an IndexReader to open the index, while IndexReader can load\nsuch an index (or re-open by loading only the \"new\" segments) and very\nquickly reduce the # segments so that searching is still fast.\n\n",
            "date": "2007-08-16T21:32:21.077+0000",
            "id": 18
        },
        {
            "author": "Yonik Seeley",
            "body": "Merging small segments in the reader seems like a cool idea on it's own.\nBut if it's an acceptable hit to merge in the reader, why is it not in the writer?\n\nThink about a writer flushing 10 small segments and a new reader opened each time:\nThe reader would do ~10*10/2 merges if it just merged the small segments.\nIf the writer were to do the merging instead, it would need to merge ~10 segments.\n\nThinking about it anotherway... if there were no separation between reader and writer, and small segments were merged on an open, why not just write out the result so it wouldn't have to be done again?  Now move \"merge on an open\" to \"merge on the close\" and that's what IndexWriter currently does.  Why is it OK for a reader to pay the price but not the writer?\n\nAlso, would this tail merging on an open be able to reduce the peak number of file descriptors?\nIt seems like to do so, the tail would have to be merged *before* other index files were opened, further complicating matters.\n",
            "date": "2007-08-17T17:34:56.979+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "> Merging small segments in the reader seems like a cool idea on it's\n> own.  But if it's an acceptable hit to merge in the reader, why is\n> it not in the writer?\n\nGood point.  I think it comes down to how often we expect readers to\nrefresh vs writers flushing.\n\nIf indeed it's 1 to 1 (as the truest \"low latency\" app would in fact\nbe, or a \"single writer + reader with no separation\"), then the writer\nshould merge them because although it's paying an O(N^2) cost to keep\nthe tail \"short\", merging on open would pay even more cost.\n\nBut if writer flushes frequently and reader re-opens less frequently\nthen it's better to merge on open.\n\nOf course, if the O(N^2) cost for IndexWriter to keep a short tail is\nin practice not too costly then we should just leave this in\nIndexWriter.  I still need to run that test for LUCENE-845.\n\n> Also, would this tail merging on an open be able to reduce the peak\n> number of file descriptors?  It seems like to do so, the tail would\n> have to be merged *before* other index files were opened, further\n> complicating matters.\n\nRight I think to keep peak descriptor usage capped we must merge the\ntail, first, then open the remaining segments, which definitely\ncomplicate things...\n",
            "date": "2007-08-17T19:33:56.094+0000",
            "id": 20
        },
        {
            "author": "Yonik Seeley",
            "body": "> But if writer flushes frequently and reader re-opens less frequently\n> then it's better to merge on open.\n\nSeems like an odd case though, because if readers aren't opened frequently, then it's a wast to flush small segments so often (and much slower for the writer than not doing so).",
            "date": "2007-08-17T20:12:58.764+0000",
            "id": 21
        },
        {
            "author": "Michael McCandless",
            "body": "Agreed.  OK, I think this is a dead end: it adds complexity and won't\nhelp in \"typical\" uses of Lucene.\n\nSo ... my plan of action is to assess the \"actual\" O(N^2) cost for\nIndexWriter to keep the tail short, add a parameter to LogMergePolicy\nso that it \"floors\" the level and always merges segments less than\nthis floor together, despite the O(N^2) cost.  And then pick a\nreasonable default for this floor.\n",
            "date": "2007-08-17T20:44:35.578+0000",
            "id": 22
        },
        {
            "author": "Michael McCandless",
            "body": "In the latest patch on LUCENE-847 I've added methods to\nLogDocMergePolicy (setMinMergeDocs) and LogByteSizeMergePolicy\n(setMinMergeMB) to set a floor on the segment levels such that all\nsegments below this min size are aggressively merged as if they were in\none level.  This effectively \"truncates\" what would otherwise be a\nlong tail of segment sizes, when you are flushing many tiny segments\ninto your index.\n\nIn order to pick reasonable defaults for the min segment size, I ran\nsome benchmarks to measure the indexing cost of truncating the tail.\n\nI processed Wiki content into ~4 KB plain text documents and then\nindexed the first 10,000 docs using this alg:\n\n  analyzer=org.apache.lucene.analysis.SimpleAnalyzer\n  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n  directory=FSDirectory\n  docs.file=/lucene/wiki4K.txt\n  max.buffered = 500\n\n  ResetSystemErase\n  CreateIndex\n  {AddDoc >: 10000\n  CloseIndex\n\n  RepSumByName\n\nI'm using the SerialMergeScheduler.\n\nI modified contrib/benchmark to always flush a new segment after each\nadded document: this simulates the \"worst case\" of tiny segments, ie,\nlowest latency indexing where every added doc must then be visible to\nsearchers.\n\nEach time is best of 2 runs.  This is run on Linux (2.6.22.1) Core II\nDuo 2.4 Ghz machine with 4 GB RAM, RAID 5 IO system using Java 1.5\n-server.\n\n    maxBufferedDocs   seconds    slowdown\n    10                40         1.0\n    100               50         1.3\n    200               59         1.5\n    300               64         1.6\n    400               72         1.8\n    500               80         2.0\n    750               97         2.4\n   1000              114         2.9\n   1500              138         3.5\n   2000              169         4.2\n   3000              205         5.1\n   4000              264         6.6\n   5000              320         8.0\n   7500              404        10.1\n  10000              645        16.1\n\nHere's my thinking:\n\n  * If you are flushing zillions of such tiny segments I think it's OK\n    to accept a net/net sizable slowdown of your overall indexing\n    speed.  I'll choose a 4X slowdown \"tolerance\" to choose default\n    values.  This corresponds roughly to the \"2000\" line above.\n    However, because I tested on a fairly fast CPU & IO system I'll\n    multiply the numbers by 0.5.\n\n  * Given this, I propose we default the minMergeMB\n    (LogByteSizeMergePolicy) to 1.6 MB (avg size of real segments at\n    the 2000 point above was 3.2 MB) and default minMergeDocs\n    (LogDocMergePolicy) to 1000.\n\n  * Note that when you are flushing large segments (larger than these\n    min size settings) then there is no slowdown at all because the\n    flushed segments are already above the minimum size.\n\nThese are just defaults, so a given application can always change\ntheir \"min merge size\" as needed.\n",
            "date": "2007-09-11T10:16:43.014+0000",
            "id": 23
        },
        {
            "author": "Yonik Seeley",
            "body": "Thanks for adding minMergeMB, the default seems fine.\nShound minMergeDocs default to maxBufferedDocs (that should yield the old behavior)?\nAlthough 1000 isn't bad... much better to slow indexing a little in the odd app than to break it by running it out of descriptors.",
            "date": "2007-09-11T11:57:18.351+0000",
            "id": 24
        },
        {
            "author": "Michael McCandless",
            "body": "> Shound minMergeDocs default to maxBufferedDocs (that should yield\n> the old behavior)?\n\nGood idea -- I think we could do this dynamically so that whenever\nIndexWriter is flushing by doc count and the merge policy is\nLogDocMergePolicy we \"write through\" any changes to maxBufferedDocs\n--> LogDocMergePolicy.setMinMergeDocs?  I'll take that approach to\nkeep backwards compatibility.\n\n> Although 1000 isn't bad... much better to slow indexing a little in\n> the odd app than to break it by running it out of descriptors.\n\nAgreed, that's the right direction to \"err\" here.\n",
            "date": "2007-09-11T14:00:35.444+0000",
            "id": 25
        }
    ],
    "component": "core/index",
    "description": "I think a good way to maximize performance of Lucene's indexing for a\ngiven amount of RAM is to flush (writer.flush()) the added documents\nwhenever the RAM usage (writer.ramSizeInBytes()) has crossed the max\nRAM you can afford.\n\nBut, this can confuse the merge policy and cause over-merging, unless\nyou set maxBufferedDocs properly.\n\nThis is because the merge policy looks at the current maxBufferedDocs\nto figure out which segments are level 0 (first flushed) or level 1\n(merged from <mergeFactor> level 0 segments).\n\nI'm not sure how to fix this.  Maybe we can look at net size (bytes)\nof a segment and \"infer\" level from this?  Still we would have to be\nresilient to the application suddenly increasing the RAM allowed.\n\nThe good news is to workaround this bug I think you just need to\nensure that your maxBufferedDocs is less than mergeFactor *\ntypical-number-of-docs-flushed.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-845",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "If you \"flush by RAM usage\" then IndexWriter may over-merge",
    "systemSpecification": true,
    "version": "2.1"
}