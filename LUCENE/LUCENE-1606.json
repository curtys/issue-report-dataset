{
    "comments": [
        {
            "author": "Robert Muir",
            "body": "patch",
            "date": "2009-04-16T08:48:08.020+0000",
            "id": 0
        },
        {
            "author": "Robert Muir",
            "body": "Here is an updated patch with AutomatonWildCardQuery.\n\nThis implements standard Lucene Wildcard query with AutomatonFilter.\n\nThis accelerates quite a few wildcard situations, such as ??(a|b)?cd*ef\nSorry, provides no help for leading *, but definitely for leading ?.\n\nAll wildcard tests pass.",
            "date": "2009-04-16T11:24:14.100+0000",
            "id": 1
        },
        {
            "author": "Mark Miller",
            "body": "Very nice Robert. This looks like it would make a very nice addition to our regex support.\n\nFound the benchmarks here quite interesting: http://tusker.org/regex/regex_benchmark.html (though it sounds like your \"special\" enumeration technique makes this regex imp even faster for our uses?)",
            "date": "2009-04-16T11:42:02.578+0000",
            "id": 2
        },
        {
            "author": "Robert Muir",
            "body": "oops I did say in javadocs score is constant / boost only so when Wildcard has no wildcards and rewrites to termquery, wrap it with ConstantScoreQuery(QueryWrapperFilter)) to ensure this.\n\n",
            "date": "2009-04-16T11:43:26.915+0000",
            "id": 3
        },
        {
            "author": "Robert Muir",
            "body": "mark yeah, the enumeration helps a lot, it means a lot less comparisons, plus brics is *FAST*.\n\ninside the AutomatonFilter i describe how it could possibly be done better, but I was afraid I would mess it up.\nits affected somewhat by the size of the alphabet so if you were using it against lots of CJK text, it might be worth it to instead use the State/Transition objects in the package. Transitions are described by min and max character intervals and you can access intervals in sorted order...\n\nits all so nice but I figure this is a start.",
            "date": "2009-04-16T11:46:14.680+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "Can this do everything that RegexQuery currently does?  (Ie we'd deprecate RegexQuery)?",
            "date": "2009-04-16T11:48:17.780+0000",
            "id": 5
        },
        {
            "author": "Robert Muir",
            "body": "Mike the thing it cant do is stuff that cannot be determinized. However I think you only need an NFA for capturing group related things:\n\nhttp://oreilly.com/catalog/regex/chapter/ch04.html\n\nOne thing is that the brics syntax is a bit different. i.e. ^ and $ are implied and I think some things need to be escaped. \nSo I think it can do everything RegexQuery does, but maybe different syntax is required.\n",
            "date": "2009-04-16T11:56:44.495+0000",
            "id": 6
        },
        {
            "author": "Uwe Schindler",
            "body": "I looked into the patch, looks good. Maybe it would be good to make the new AutomatonRegExQuey als a subclass of MultiTermQuery. As you also seek/exchange the TermEnum, the needed FilteredTermEnum may be a little bit complicated. But you may do it in the same way like I commit soon for TrieRange (LUCENE-1602).\nThe latest changes from LUCENE-1603 make it possible to write a FilteredTermEnum, that handles over to different positioned TermEnums like you do.\nWith MultiTermQuery you get all for free: ConstantScore, Boolean rewrite and optionally the Filter (which is not needed here, I think). And: You could also overwrite difference in FilteredTermEnum to rank the hits.\nA note: The FilteredTermEnum created by TrieRange is not for sure really ordered correctly according Term.compareTo(), but this is not really needed for MultiTermQuery.",
            "date": "2009-04-16T12:26:11.642+0000",
            "id": 7
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, I agree with you, with one caveat: for this functionality to work the Enum must be ordered correctly according to Term.compareTo().\n\nOtherwise it will not work correctly...",
            "date": "2009-04-16T12:30:00.792+0000",
            "id": 8
        },
        {
            "author": "Uwe Schindler",
            "body": "It will work, that was what I said. For MultiTermQuery, it must *not* be ordered, the ordering is irrelevant for it, MultTermQuery only enumerates the terms. TrieRange is an example of that, the order of terms is not for sure ordered correctly (it is at the moment because of the internal implementation of splitLongRange(), but I tested it with the inverse order and it still worked). If you want to use the enum for something other, it will fail.\nThe filters inside MultiTermQuery and the BooleanQuery do not need to have the terms ordered.",
            "date": "2009-04-16T12:38:46.307+0000",
            "id": 9
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, i'll look and see how you do it for TrieRange.\n\nif it can make the code for this simpler that will be fantastic. maybe by then I will have also figured out some way to cleanly and non-recursively use min/max character intervals in the state machine to decrease the amount of seeks and optimize a little bit.\n\n ",
            "date": "2009-04-16T12:54:00.386+0000",
            "id": 10
        },
        {
            "author": "Uwe Schindler",
            "body": "I committed TrieRange revision 765618. You can see the impl here:\nhttp://svn.apache.org/viewvc/lucene/java/trunk/contrib/queries/src/java/org/apache/lucene/search/trie/TrieRangeTermEnum.java?view=markup",
            "date": "2009-04-16T13:01:50.861+0000",
            "id": 11
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, thanks. I'll think on this and on other improvements. \nI'm not really confident in my ability to make the code much cleaner at the end of the day, but more efficient and get some things for free as you say.\nFor now it is working much better than a linear scan, and the improvements wont change the order, but might help a bit.\n\nThink i should try to correct this issue or create a separate issue?\n",
            "date": "2009-04-16T13:19:14.919+0000",
            "id": 12
        },
        {
            "author": "Uwe Schindler",
            "body": "Let's stay with this issue!",
            "date": "2009-04-16T13:23:11.081+0000",
            "id": 13
        },
        {
            "author": "Robert Muir",
            "body": "ok I refactored this to use FilteredTermEnum/MultiTermQuery as Uwe suggested.\n\non my big index its actually faster without setting the constant score rewrite (maybe creating the huge bitset is expensive?)\n\nI also changed the term enumeration to be a bit smarter, so it will work well on a large alphabet like CJK now.\n",
            "date": "2009-04-18T00:24:27.263+0000",
            "id": 14
        },
        {
            "author": "Mark Miller",
            "body": "{quote}on my big index its actually faster without setting the constant score rewrite (maybe creating the huge bitset is expensive?){quote}\nThats surprising, because I have seen people state the opposite on a couple occasions. Perhaps it has to do with how many terms are being enumerated?",
            "date": "2009-04-18T00:31:44.780+0000",
            "id": 15
        },
        {
            "author": "Robert Muir",
            "body": "its ~700ms if i .setConstantScoreRewrite(true)\nits ~150ms otherwise...\n ",
            "date": "2009-04-18T00:42:10.984+0000",
            "id": 16
        },
        {
            "author": "Mark Miller",
            "body": "How many terms are being enumerated for the test? My guess is that for queries that turn into very large BooleanQueries, it can be much faster to build the filter, but for a smaller BooleanQuery or TermQuery, filter construction dominates?",
            "date": "2009-04-18T12:52:11.475+0000",
            "id": 17
        },
        {
            "author": "Robert Muir",
            "body": "~ 116,000,000 terms.\n\nI've seen the same behavior with other lucene queries on this index, where I do not care about score and thought filter would be best, but queries still have the edge.\n",
            "date": "2009-04-18T13:03:58.492+0000",
            "id": 18
        },
        {
            "author": "Robert Muir",
            "body": "my test queries are ones that match like 50-100 out of those 116,000,000... so maybe this helps paint the picture.\n\ni can profile each one if you are curious?",
            "date": "2009-04-18T13:06:05.843+0000",
            "id": 19
        },
        {
            "author": "Robert Muir",
            "body": "well here it is just for the record:\n\nin the query case (fast), time is dominated by AutomatonTermEnum.next(). This is what I expect.\nin the filter case (slower), time is instead dominated by OpenBitSetIterator.next().\n\nI've seen this with simpler (non-MultiTermQuery) queries before as well.\n\nFor this functionality I still like the constant score rewrite option because there is no risk of hitting the boolean clause limit.\n\n",
            "date": "2009-04-18T13:30:17.308+0000",
            "id": 20
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. For this functionality I still like the constant score rewrite option because there is no risk of hitting the boolean clause limit.\n\nI thought about that, too. Maybe there will be a possibility to do an auto-switch in MultiTermQuery. If a TooManyBooleanClauses exception is catched during the rewrite() method, it could fall back to returning the ConstantScore variant. The problem: The time for iterating the terms until the Exception thrown is lost... Maybe we could store the iterated terms for reuse (if FilteredTermEnum or a wrapper like BufferedTermEnum has something like the known mark() option from BufferedInputStreams).\n\nThis is just an idea, but has nothing to do with this query, it affects all MultiTermQueries.",
            "date": "2009-04-18T15:33:15.402+0000",
            "id": 21
        },
        {
            "author": "Robert Muir",
            "body": "Uwe: yes I tried to think of some heuristics for this query to guess which would be the best method.\n\nFor example, if the language of the automaton is infinite (for example, built from a regular expression/wildcard with a * operator), it seems best to set constant score rewrite=true.\n\nI didn't do any of this because I wasn't sure if this constant score rewrite option is something that should be entirely left to the user, or not.",
            "date": "2009-04-18T15:48:54.993+0000",
            "id": 22
        },
        {
            "author": "Robert Muir",
            "body": "yes, I just verified and can easily and quickly detect if the FSM can accept more than BooleanQuery.getMaxClauseCount() Strings.\n\n !Automaton.isFinite() || Automaton.getFiniteStrings(BooleanQuery.getMaxClauseCount()) == null\n\nIf you think its ok, I could set constant score rewrite=true in this case.",
            "date": "2009-04-18T16:08:14.808+0000",
            "id": 23
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. I didn't do any of this because I wasn't sure if this constant score rewrite option is something that should be entirely left to the user, or not.\n\nYes, it should be normally be left to the user. And the slower filter on large indexes with only sparingly filled bitsets is related to LUCENE-1536.\n\nE.g. I did some comparisions for TrieRangeQuery on a 5 mio doc index, integer field, 8 bit precision step (so about 400 terms per query), the filter is about double as fast. But the ranges were random and hit about 1/3 of all documents in average per query, so the bitset is not so sparse.\nTrieRangeQuery is a typical example of a MultiTermQuery, that also works well with Boolean rewrite, because the upper term count is limited by the precision step (for ints and 8 bit the theoretical, but never reached, maximum is about 1700 terms, for lower precisionSteps even less).",
            "date": "2009-04-18T16:09:43.504+0000",
            "id": 24
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, ok based on your tests I tried some of my own... on my index when the query matches like less than 10-20% of the docs Query method is faster.\n\nwhen it matches something like over 20%, the Filter method starts to win.\n\n",
            "date": "2009-04-18T16:26:10.625+0000",
            "id": 25
        },
        {
            "author": "Mark Miller",
            "body": "When refactoring multitermquery I tried just computing the bit set iterator on the fly. It did not appear to work out, but I wonder if there are cases where it would be a better option.\n\nbq.For example, if the language of the automaton is infinite (for example, built from a regular expression/wildcard with a * operator), it seems best to set constant score rewrite=true.\n\nOkay, that starts to make more sense then. I think the reports that it was faster on some large indexes was based on wildcard queries I think (hard to remember 100%).\n\n",
            "date": "2009-04-18T17:29:49.073+0000",
            "id": 26
        },
        {
            "author": "Mark Miller",
            "body": "bq. If you think its ok, I could set constant score rewrite=true in this case.\n\nI agree that it should just be left up to the user. Its probably not a good idea to change the scoring for what to a user could appear to be arbitrary queries.",
            "date": "2009-04-18T17:50:35.258+0000",
            "id": 27
        },
        {
            "author": "Robert Muir",
            "body": "updated with smarter enumeration. I think this is mathematically the best you can get with a DFA.\n\nfor example if the regexp is (a|b)cdefg it knows to position at acdefg, then bcdefg, etc\nif the regexp is (a|b)cd*efg it can only position at acd, etc.\n\nnextString() is now cpu-friendly, and instead walks the state transition character intervals in sorted order instead of brute-forcing characters.\n",
            "date": "2009-04-19T05:43:54.913+0000",
            "id": 28
        },
        {
            "author": "Robert Muir",
            "body": "this includes an alternative for another slow linear query, fuzzy query.\n\nautomatonfuzzyquery creates a DFA that accepts all strings within an edit distance of 1.\n\non my 100M term index this works pretty well:\nfuzzy: 251,219 ms\nautomatonfuzzy: 172 ms\n\nwhile its true its limited to edit distance of one, on the other hand it supports transposition and is fast.\n",
            "date": "2009-04-19T21:36:43.427+0000",
            "id": 29
        },
        {
            "author": "Robert Muir",
            "body": "found this interesting article applicable to this query: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652\n\n\"We show how to compute, for any fixed bound n and any input word W, a deterministic Levenshtein-automaton of degree n for W in time linear in the length of W.\"\n",
            "date": "2009-04-21T20:48:00.412+0000",
            "id": 30
        },
        {
            "author": "Eks Dev",
            "body": "Robert, \nin order for Lev. Automata to work, you need to have the complete dictionary as DFA. Once you have dictionary as DFA (or any sort of trie), computing simple regex-s or simple fixed or weighted Levenshtein distance becomes a snap. Levenshtein-Automata is particularity fast at it, much simpler and only slightly slower method (one pager code)   \"K.Oflazer\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.136.3862\n\nAs said, you cannot really walk  current term dictionary as automata/trie (or you have an idea on how to do that?). I guess there is enough application where stoing complete Term dictionary into RAM-DFA is not a problem. Even making some smart (heavily cached) persistent trie/DFA  should not be all that complex.\n\nOr you intended just to iterate all terms, and compute distance faster  \"break LD Matrix computation as soon as you see you hit the boundary\"? But this requires iteration over all terms?  \n\n\nI have done something similar, in memory, but unfortunately someone else paid me for this and is not willing to share... \n",
            "date": "2009-04-21T21:42:59.039+0000",
            "id": 31
        },
        {
            "author": "Robert Muir",
            "body": "eks:\n\nthe AutomatonTermEnumerator in this patch does walk the term dictionary according to the transitions present in the DFA. Thats what this JIRA issue is all about to me, not iterating all the terms! So you do not need the complete dictionary as a DFA.\n\nfor example: a regexp query of (a|b)cdefg with this patch seeks to 'acdefg', then 'bcdefg', as opposed to the current regex support which exhaustively enumerates all terms.\n\nslightly more complex example, query of (a|b)cd*efg first seeks to 'acd' (because of kleen star operator). suppose it then encounters term 'acda', it will next seek to 'acdd', etc. if it encounters 'acdf', then next it seeks to 'bcd'.\n\nthis patch implements regex, wildcard, and fuzzy with n=1 in terms of this enumeration. what it doesnt do is fuzzy with arbitrary n!. \n\nI used the simplistic quadratic method to compute a DFA for fuzzy with n=1 for the FuzzyAutomatonQuery present in this patch, the paper has a more complicate but linear method to compute the DFA.\n",
            "date": "2009-04-21T22:00:59.790+0000",
            "id": 32
        },
        {
            "author": "Eks Dev",
            "body": "hmmm, sounds like good idea, but I am still not convinced it would work for Fuzzy\n\ntake simple dictionary:\none\ntwo\nthree\nfour \n\nquery Term is, e.g. \"ana\", right? and n=1, means your DFA would be: {.na, a.a, an., an, na, ana, .ana, ana., a.na, an.a, ana.} where dot represents any character in you alphabet.\n\nFor the first element in DFA (in expanded form) you need to visit all terms, no matter how you walk DFA... or am I missing something?\n\nWhere you could save time is actual calculation of LD Matrix for terms that do not pass automata\n\n",
            "date": "2009-04-21T22:39:19.094+0000",
            "id": 33
        },
        {
            "author": "Robert Muir",
            "body": "eks, well it does work well for fuzzy n=1 (I have tested against my huge\nindex).\n\nfor your simple dictionary it will do 3 comparisons instead of 4.\nthis is because your simple dictionary is sorted in the index as such:\nfour\none\nthree\ntwo\n\nwhen it encounters 'three' it will next ask for a TermEnum(\"una\") which will\nreturn null.\n\ngive it a try on a big dictionary, you might be surprised :)\n\n\n\n\n\n-- \nRobert Muir\nrcmuir@gmail.com\n",
            "date": "2009-04-21T22:48:50.458+0000",
            "id": 34
        },
        {
            "author": "Robert Muir",
            "body": "eks in your example it does three comparisons instead of four (not much of a gain for this example, but a big gain on a real index)\n\nthis is because it doesnt need to compare 'two', after encountering 'three' it requests TermEnum(\"uana\"), which returns null.\n\ni hope you can see how this helps for a large index... (or i can try to construct a more realistic example)\n\n",
            "date": "2009-04-21T22:49:55.532+0000",
            "id": 35
        },
        {
            "author": "Robert Muir",
            "body": "eks in case this makes it a little better explanation for your example, \nassume a huge term dictionary where words start with a-zA-Z for simplicity.\n\nfor each character in that alphabet it will look for 'Xana' and 'Xna' in the worst case.\nthats 110 comparisons to check all the words that don't start with 'a'.\n(the enumeration thru all the words that start with 'a' is a little more complex).\n\nif you have say, 1M unique terms you can see how doing something like 100-200 comparisons is a lot better than 1M.",
            "date": "2009-04-21T23:13:33.290+0000",
            "id": 36
        },
        {
            "author": "Robert Muir",
            "body": "removed use of multitermquery's getTerm()\n\nequals/hashcode are defined based upon the field and the language accepted by the FSM, i.e. regex query of AB.*C equals() wildcard query of AB*C because they are the same.\n",
            "date": "2009-04-28T18:55:46.839+0000",
            "id": 37
        },
        {
            "author": "Mark Miller",
            "body": "This is a cool issue, but it hasn't found an assignee yet. We may have to push it to 3.1.\n\nAny interest Uwe?",
            "date": "2009-06-15T13:52:31.698+0000",
            "id": 38
        },
        {
            "author": "Uwe Schindler",
            "body": "I take it, I think it is almost finished. The only problems at the moment are bundling the external library in contrib, which is BSD licensed, are there any problems?\n\nIf not, I can manage the inclusion into the regex contrib.",
            "date": "2009-06-15T14:09:41.468+0000",
            "id": 39
        },
        {
            "author": "Mark Miller",
            "body": "I don't think there is a problem with BSD. I know Grant has committed a BSD licensed stop word list in the past.\n\nI've asked explicitly about it before, but got no response.\n\nI'll try and dig a little, but Grant is the PMC head and he did it, so we wouldnt be following bad company...",
            "date": "2009-06-15T14:13:05.497+0000",
            "id": 40
        },
        {
            "author": "Uwe Schindler",
            "body": "Robert: I applied the patch locally, one test was still using @Override, fixed that. I did only download automaton.jar not the source package.\n\nDo you know, if automaton.jar is compiled using -source 1.4 -target 1.4  (it was compiled using ant 1.7 and Java 1.6). If not sure, I will try to build it again from source and use the correct compiler switches. The regex contrib module is Java 1.4 until now. If automaton only works with 1.5, we should wait until 3.0 to release it.",
            "date": "2009-06-15T15:41:07.257+0000",
            "id": 41
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, you are correct, I just took a glance at the automaton source code and saw StringBuilder, so I think it is safe to say it only works with 1.5...",
            "date": "2009-06-15T15:47:30.019+0000",
            "id": 42
        },
        {
            "author": "Uwe Schindler",
            "body": "Doesn't seem to work, I will check the sources:\n\n{code}\ncompile-core:\n    [javac] Compiling 12 source files to C:\\Projects\\lucene\\trunk\\build\\contrib\\regex\\classes\\java\n    [javac] C:\\Projects\\lucene\\trunk\\contrib\\regex\\src\\java\\org\\apache\\lucene\\search\\regex\\AutomatonFuzzyQuery.java:11: cannot access dk.brics.automaton.Automaton\n    [javac] bad class file: C:\\Projects\\lucene\\trunk\\contrib\\regex\\lib\\automaton\n.jar(dk/brics/automaton/Automaton.class)\n    [javac] class file has wrong version 49.0, should be 48.0\n    [javac] Please remove or make sure it appears in the correct subdirectory of\n the classpath.\n    [javac] import dk.brics.automaton.Automaton;\n    [javac]                           ^\n    [javac] 1 error\n{code}",
            "date": "2009-06-15T15:47:37.228+0000",
            "id": 43
        },
        {
            "author": "Uwe Schindler",
            "body": "So I tend to move this to 3.0 or 3.1, because of missing support in regex contrib.",
            "date": "2009-06-15T15:49:08.307+0000",
            "id": 44
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, sorry about this.\n\nI did just verify automaton.jar can be compiled for Java 5 (at least it does not have java 1.6 dependencies), so perhaps this can be integrated for a later release.",
            "date": "2009-06-15T16:05:17.002+0000",
            "id": 45
        },
        {
            "author": "Uwe Schindler",
            "body": "I move this to 3.0 (and not 3.1), because it can be released together with 3.0 (contrib modules do not need to wait until 3.1).\n\nRobert: you could supply a patch with StringBuilder toString() variants and all those @Override uncommented-in. And it works correct with 1.5 (I am working with 1.5 here locally - I hate 1.6...).",
            "date": "2009-06-15T16:09:31.951+0000",
            "id": 46
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, ok.\n\nNot to try to complicate things, but related to LUCENE-1689 and java 1.5, I could easily modify the Wildcard functionality here to work correctly with suppl. characters\n\nThis could be an alternative to fixing the WildcardQuery ? operator in core.\n",
            "date": "2009-06-15T16:19:08.054+0000",
            "id": 47
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Regarding the license - I think we already have BRICS in one of Nutch's plugins, so we should be OK with the BSD licensed jar in our repo.\n\n./urlfilter-automaton/lib/automaton.jar\n",
            "date": "2009-06-17T03:32:40.101+0000",
            "id": 48
        },
        {
            "author": "Uwe Schindler",
            "body": "Robert: Do you want to take this again? It's your's and contrib :-)",
            "date": "2009-10-01T20:40:45.457+0000",
            "id": 49
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, sure. I will bring this patch up to speed (java 5, etc)",
            "date": "2009-10-01T20:50:10.105+0000",
            "id": 50
        },
        {
            "author": "Robert Muir",
            "body": "updated patch to trunk:\n* add support for optional regex features\n* remove recursion\n* improve performance for worst-case regexp/wildcard/FSM\n* improved docs & test \n* remove the fuzzy impl, NFA->DFA too slow for this, maybe a later addition.\n",
            "date": "2009-10-13T18:17:17.541+0000",
            "id": 51
        },
        {
            "author": "Robert Muir",
            "body": "if anyone can spare a sec to take a glance/review before 3.0, i think its ok...",
            "date": "2009-10-16T03:56:43.712+0000",
            "id": 52
        },
        {
            "author": "Robert Muir",
            "body": "if no one objects, i'd like to commit this in a few days. Can someone help out and commit the update to NOTICE?",
            "date": "2009-10-20T21:57:53.531+0000",
            "id": 53
        },
        {
            "author": "Uwe Schindler",
            "body": "No prob! I will help you, I am on heavy committing :-)",
            "date": "2009-10-20T22:00:37.216+0000",
            "id": 54
        },
        {
            "author": "Grant Ingersoll",
            "body": "Why are new features going into 3.0?  I was under the impression that 3.0 was just supposed to be cleanup plus Java 1.5",
            "date": "2009-10-22T14:50:30.869+0000",
            "id": 55
        },
        {
            "author": "Robert Muir",
            "body": "Grant, I thought it was ok from Uwe's comment:\n\nbq. I move this to 3.0 (and not 3.1), because it can be released together with 3.0 (contrib modules do not need to wait until 3.1). \n\nI guess now I am a little confused about what should happen for 3.0 with contrib in general? \nNo problem moving this to 3.1, let me know!\n",
            "date": "2009-10-22T15:01:06.603+0000",
            "id": 56
        },
        {
            "author": "Uwe Schindler",
            "body": "3.0 is just the switch to 1.5 and generics. So this is a typical java 1.5 issue and can go into 3.0 even if it is a new feature. Contrib is not core and may have own rules.\n\nIn my opinion, this would be a nice addition to the regex contrib and should also have been in 2.9, but the underlying library is Java 5 only, so we had to wait until 3.0.",
            "date": "2009-10-22T16:20:27.308+0000",
            "id": 57
        },
        {
            "author": "Mark Miller",
            "body": "So Robert - what do you think about paring down the automaton lib, and shoving all this in core? I want it, I want, I want it :)\n\nYou should also post the info about that Fuzzy possibility you were mentioning - perhaps a math head will come along and take care of that for us with the proper setup.",
            "date": "2009-11-20T04:35:47.435+0000",
            "id": 58
        },
        {
            "author": "Robert Muir",
            "body": "bq. So Robert - what do you think about paring down the automaton lib, and shoving all this in core? I want it, I want, I want it \n\nMark, some notes on size. jarring up the full source code (no paring) is 81KB.\nin practice, the jar file is larger because it contains some 'precompiled DFAs' for certain things like Unicode blocks, XML types... are these really needed?\n\nsee here for a list of what I mean: http://www.brics.dk/automaton/doc/dk/brics/automaton/Datatypes.html\nI enabled these in the patch (they could be easily disabled): an example of how they are used in a regexp is like this: <Arabic>* (match 0 or more arabic characters)\n\nif a user really wanted them, they can load them themselves, you can also create custom ones and use a DataTypesAutomatonProvider to register them for some name:\nExample: your users want to be able to use <make> or <model> inside their regexps, you can register <make> and <model> to match to some DFA you make yourself.\nits really a nice mechanism, but I don't think we need all the precompiled ones? \n",
            "date": "2009-11-20T04:51:28.686+0000",
            "id": 59
        },
        {
            "author": "Robert Muir",
            "body": "bq. You should also post the info about that Fuzzy possibility you were mentioning - perhaps a math head will come along and take care of that for us with the proper setup.\n\nRight, i created a FuzzyQuery that builds in the 'naive' method. The problem is that for large strings this exponential-time naive mechanism creates a rather large NFA, and the NFA->DFA conversion is very slow.\nOnce the DFA is built, actually running it on a term dictionary is fast :)\nSo the slow part has nothing to do with lucene at all.\n\nSo we just need to build these DFAs in an efficient way:\nWe show how to compute, for any fixed bound n and any input word W , a deterministic Levenshtein-automaton of degree n for W in time linear in the length of W\nhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652\n",
            "date": "2009-11-20T04:55:16.271+0000",
            "id": 60
        },
        {
            "author": "Robert Muir",
            "body": "By the way Mark, in case you are interested, the TermEnum here still has problems with 'kleene star' as I have mentioned many times.\nSo wildcard of ?abacadaba is fast, wildcard of *abacadaba is still slow\nin the same manner, regex of .abacadaba is fast, wildcard of .*abacadaba is still slow.\n\nbut there are algorithms to reverse an entire dfa, so you could use ReverseStringFilter and support wildcards AND regexps with leading *\nI didnt implement this here though yet.\n",
            "date": "2009-11-20T05:04:04.528+0000",
            "id": 61
        },
        {
            "author": "Mark Miller",
            "body": "{quote}\nBy the way Mark, in case you are interested, the TermEnum here still has problems with 'kleene star' as I have mentioned many times.\nSo wildcard of ?abacadaba is fast, wildcard of *abacadaba is still slow\nin the same manner, regex of .abacadaba is fast, wildcard of .*abacadaba is still slow.\n{quote}\n\nNo problem in my mind - nothing the current WildcardQuery doesn't face. Any reason we wouldn't want to replace the current WCQ that with this?\n\n{quote}\nbut there are algorithms to reverse an entire dfa, so you could use ReverseStringFilter and support wildcards AND regexps with leading *\nI didnt implement this here though yet.\n{quote}\n\nNow that sounds interesting - now sure I fully understand you though - are you saying we can do a prefix match, but without having to index terms reversed in the index? That would be very cool.",
            "date": "2009-11-20T05:09:36.123+0000",
            "id": 62
        },
        {
            "author": "Robert Muir",
            "body": "bq. No problem in my mind - nothing the current WildcardQuery doesn't face. Any reason we wouldn't want to replace the current WCQ that with this?\n\nI don't think there is any issue. by implementing WildcardQuery with the DFA, leading ? is no longer a problem, \ni mean depending on your term dictionary if you do something stupid like ???????abacadaba it probably wont be that fast.\n\nI spent a lot of time with the worst-case regex, wildcards to ensure performance is at least as good as the other alternatives.\nThere is only one exception, the leading * wildcard is a bit slower with a DFA than if you ran it with actual WildcardQuery (less than 5% in my tests)\nBecause of this, currently this patch rewrites this very special case to a standard WildcardQuery.\n\nbq. Now that sounds interesting - now sure I fully understand you though - are you saying we can do a prefix match, but without having to index terms reversed in the index? That would be very cool.\n\nNo, what I am saying is that you still have to index the terms in reversed order for the leading *  or .* case, except then this reversing buys you faster wildcard AND regex queries :)\n",
            "date": "2009-11-20T05:16:08.085+0000",
            "id": 63
        },
        {
            "author": "Mark Miller",
            "body": "Okay - still not an issue I don't think - leading wildcards are already an issue - 5% is worth the other speedups I think - though you've taken care of that anyway - so sounds like gold to me. I didn't expect this to solve leading wildcard issues, so no loss to me.\n\nbq. No, what I am saying is that you still have to index the terms in reversed order for the leading * or .* case, except then this reversing buys you faster wildcard AND regex queries \n\nbummer :) Does it make sense to implement here though? Isn't the ReverseStringFilter enough if a user wants to go this route? Solr's support for this is fairly good, but I don't think it needs to be as 'built in' for Lucene?",
            "date": "2009-11-20T05:20:41.774+0000",
            "id": 64
        },
        {
            "author": "Robert Muir",
            "body": "bq. Does it make sense to implement here though?\n\nI do not think so. I tested another solution where users wanted leading * wildcards on 100M+ term dictionary.\nI found out what was acceptable (clarification: to these specific users/system) was for *  to actually match .{0,3} (between 0 and 3 of anything), and rewrote it to an equivalent regex like this.\nThis performed very well, because it can still avoid comparing many terms.",
            "date": "2009-11-20T05:23:51.770+0000",
            "id": 65
        },
        {
            "author": "Mark Miller",
            "body": "That is a cool tradeoff to be able to make.",
            "date": "2009-11-20T05:36:59.001+0000",
            "id": 66
        },
        {
            "author": "Robert Muir",
            "body": "bq. That is a cool tradeoff to be able to make. \n\nMark, yes. I guess someone could implement the DFA-reversing if they wanted to, to enable leading .* regex support with ReverseStringFilter.\nyou can still use this Wildcard impl with ReverseStringFilter just like the core Wildcard impl, because its just so easy to reverse a wildcard string.\n\nbut you don't want to try to reverse a regular expression! that would be hairy. easier to reverse a DFA.\n\nbut even without this, there are tons of workarounds, like the tradeoff i mentioned earlier.\nalso, another one that might not be apparent is that its only the leading .* that is a problem, depending on corpus of course.\n\n[a-z].*abacadaba will avoid visiting terms that start with 1,2,3 or are in chinese, etc, which might be a nice improvement.\nof course if all your terms start with a-z, then its gonna be the same as entering .*abacadaba, and be bad.\n\nall depends on how selective the regular expression is wrt your terms.\n",
            "date": "2009-11-20T06:43:41.524+0000",
            "id": 67
        },
        {
            "author": "Robert Muir",
            "body": "bq. So Robert - what do you think about paring down the automaton lib, and shoving all this in core? I want it, I want, I want it \n\nI think trying this out around in contrib (after 3.0 is released) would be best in the short term?\n\nSeparately, my quickly 'pared' automaton library is now 53KB jar (14 java files, some just simple POJO)\nDo you have a target size I should shoot for?\n",
            "date": "2009-11-20T14:44:41.009+0000",
            "id": 68
        },
        {
            "author": "Mark Miller",
            "body": "bq. I think trying this out around in contrib (after 3.0 is released) would be best in the short term?\n\nWhat are your concerns? If it passes the current wildcard tests and survives in trunk for a dev cycle, isn't that likely enough?\n\nbq. Do you have a target size I should shoot for?\n\nAs small as possible ;) But I don't personally have any issue adding 53k to the core jar for this goodness. Guess we will have to see what others say - but its a low percentage of the current 1.1 MB, and pretty sweet functionality.",
            "date": "2009-11-20T15:05:24.768+0000",
            "id": 69
        },
        {
            "author": "Robert Muir",
            "body": "bq. What are your concerns? If it passes the current wildcard tests and survives in trunk for a dev cycle, isn't that likely enough?\n\ni don't really have any, except that I don't necessarily trust the current wildcard tests. Shouldn't they have detected 2.9.0 scorer bug? :)\n\nbq. As small as possible \n\nok, i will work at this some more. obviously i could pare it down to just what we are using, but i am trying to preserve 'reasonable' functionality that might be handy elsewhere.\n",
            "date": "2009-11-20T15:15:35.206+0000",
            "id": 70
        },
        {
            "author": "Mark Miller",
            "body": "bq. i don't really have any, except that I don't necessarily trust the current wildcard tests. Shouldn't they have detected 2.9.0 scorer bug? \n\nIf they caught that, they wouldn't catch another :) How do you want to improve them? I'll help test and write tests - we can make something much more intensive if you'd like, and then just put a flag to tone it down for normal test running.\n\nbq. ok, i will work at this some more. obviously i could pare it down to just what we are using, but i am trying to preserve 'reasonable' functionality that might be handy elsewhere.\n\nRight - don't go further than makes sense - even 53k -> 20k - I don't think it really matters that much. So really I meant, as small as makes reasonable sense.",
            "date": "2009-11-20T15:19:55.963+0000",
            "id": 71
        },
        {
            "author": "Robert Muir",
            "body": "bq. How do you want to improve them?\n\nwell for one, i test all the rewrite methods and boosts here.\nok these are also now fixed as of 3.0 in core wildcard tests also (LUCENE-1951), but those were two 'buglets', just an example.\n",
            "date": "2009-11-20T15:26:52.890+0000",
            "id": 72
        },
        {
            "author": "Mark Miller",
            "body": "Point taken - the tests are not perfect. They never are. But it doesn't stop us chugging along :) We can always write more tests and trunk tends to get quite a work out if you put changes in towards the beginning of a dev cycle. Bugs are inevitable, in trunk or contrib. But I don't think the wildcard impl will get much exposure in contrib anyway - its not wired into the queryparser, and it won't come with a sign saying check this out. Users will still use the standard wildcardquery - and I want to see it improved. We can work out the patch, work out the tests, and then decided its not good enough - or perhaps another committer will look and decided that. I'd still love to put it in core myself.",
            "date": "2009-11-20T15:34:33.421+0000",
            "id": 73
        },
        {
            "author": "Robert Muir",
            "body": "Mark, ok. I will supply a new patch with no lib dependency, instead it includes the pared Automaton code in one pkg.\nthis compiles to about a 48KB jar right now. Reducing it more would involve sacrificing readability or useful stuff.\n\nExample: keeping the \"Matcher\" would be useful if you want to use this for a really fast 'PatternTokenizer', but not needed here.\n",
            "date": "2009-11-20T15:38:00.692+0000",
            "id": 74
        },
        {
            "author": "Robert Muir",
            "body": "attached is an alternate patch with no library dependency (LUCENE-1606_nodep.patch)\ninstead it imports 'pared-down' automaton source code (compiles to 48KB jar)\nit is still setup in contrib regex because...\n\nMark: some practical questions, I'd like to create a patch that integrates it nicely into core, just so we can see what it would look like.\nThoughts on class names and pkg names?\n* I assume we should nuke the old WildcardQuery, rename AutomatonWildcardQuery to WildcardQuery?\n* but then what should AutomatonRegexQuery be called, we already have RegexQuery :)\n\nThoughts on the automaton src code? Should I reformat to our style... (I did not do this).\nshould we rename the pkg? \n\nsorry the patch is monster, if it makes it any easier i could split the automaton library itself away from the lucene integration (queries, etc)?\nalso, i did not remove any tests, for example, TestWildcardQuery already exists, so the test here is just duplication, i just might add a test or 2 to the existing TestWildcardQuery\n",
            "date": "2009-11-20T16:18:26.221+0000",
            "id": 75
        },
        {
            "author": "Mark Miller",
            "body": "bq. I assume we should nuke the old WildcardQuery, rename AutomatonWildcardQuery to WildcardQuery?\n\nYes - I think so - but how to handle the fact that you fall back to it? We might either rename it or incorporate it into the new WildcardQuery?\n\nbq. but then what should AutomatonRegexQuery be called, we already have RegexQuery?\n\nShouldn't this one eventually make the old obsolete? I say we name it RegexQuery.\n\nbq. Thoughts on the automaton src code? Should I reformat to our style... (I did not do this).\n\nYup - I think we should reformat and drop the author tags. We can mention that type of info in the NOTICE file.\n\nbq. should we rename the pkg?\n\nI think so - perhaps util.brics?  No need for dk I don't think.\n\nbq. sorry the patch is monster, if it makes it any easier i could split the automaton library itself away from the lucene integration (queries, etc)?\n\nOne large patch is fine with me - my IDE will make short work of groking it :)",
            "date": "2009-11-20T16:28:27.774+0000",
            "id": 76
        },
        {
            "author": "Robert Muir",
            "body": "bq. Yes - I think so - but how to handle the fact that you fall back to it? We might either rename it or incorporate it into the new WildcardQuery?\nWe could just remove the .rewrite(). it is only that very special case, for leading *, where the existing WildcardQuery logic is slightly faster (< 5%).\nI was actually surprised the wildcardquery logic beats a DFA, i guess something to be said for that hairy logic :)\n\nbq. Shouldn't this one eventually make the old obsolete? I say we name it RegexQuery.\nI do not know, all regex is not created equal. This one has different syntax and stuff from the other impl's.\nAny other ideas? Obviously the name RegexpQuery, with a p,  is available\n\nbq. Yup - I think we should reformat and drop the author tags. We can mention that type of info in the NOTICE file.\nok, this is easy, i already have NOTICE in the patch. i was sure all files from brics have their license header also.\n\nbq. I think so - perhaps util.brics? No need for dk I don't think.\no.a.l.util.brics?\n",
            "date": "2009-11-20T16:34:27.655+0000",
            "id": 77
        },
        {
            "author": "Mark Miller",
            "body": "bq. We could just remove the .rewrite(). it is only that very special case, for leading *, where the existing WildcardQuery logic is slightly faster (< 5%).\n\nAgreed - not worth the extra code for speeding up such a horrible case by 5%.\n\nbq. Any other ideas?\n\nI'd rather change the contrib names and let core have the good name :) We can start with RegexpQuery I think.\n\nbq. o.a.l.util.brics?\n\nThats my best thought at the moment.",
            "date": "2009-11-20T16:50:33.083+0000",
            "id": 78
        },
        {
            "author": "Robert Muir",
            "body": "OK we have the start of a plan, only one final nit I am worried about.\n\nI pared away the 'built-in named automata':\n* example <Lu> (uppercase letter, from Unicode)\n* example <QName> (from XML)\n\nif we keep the original pkg name, a user can have these just by adding brics.jar into their path.\nThey would just pass new DatatypesAutomatonProvider() to the constructor of RegexpQuery, done.\n\nif we rename the pkg, this will not work because the DataTypesAutomatonProvider from the jar file implements dk.brics.automaton.AutomatonProvider,\nnot o.a.l.util.brics.AutomatonProvider.\n\nalternatively, we could rename the pkg, but I could restore perhaps a subset of these datatypes, maybe without all the xml ones, just the basic unicode categories and stuff?\nThis would cost a little space though. Here is the list:\nhttp://www.brics.dk/automaton/doc/dk/brics/automaton/Datatypes.html#get%28java.lang.String%29\n\ni ask this question because personally i don't use any of these built-ins, but users might want them? what do you think?\n",
            "date": "2009-11-20T17:03:36.426+0000",
            "id": 79
        },
        {
            "author": "Mark Miller",
            "body": "On the way hand I'd say, well lets not rename the package then - its not that important. But these things could get out of sync anyway, so I'm not sure its worth it to try and maintain some sort of compatibility. If these features are useful enough, we could end up adding them later. Your call though. Personally, I'd think we start just by adding the essentials and build from there as makes sense.",
            "date": "2009-11-20T17:08:19.665+0000",
            "id": 80
        },
        {
            "author": "Robert Muir",
            "body": "Mark, ok. In that case I will not include these, and rename the pkg as you suggest. \nThese default named automata are not enabled by default in the library anyway if you use the RegExp() default constructor.\nthe (renamed and pared) api is still extensible, if you want to create named automata to use in your regular expressions, you just implement the very simple interface DatatypesAutomatonProvider.\nthen you pass this to the constructor of RegexpQuery\n",
            "date": "2009-11-20T17:13:03.829+0000",
            "id": 81
        },
        {
            "author": "Robert Muir",
            "body": "bq. Okay - still not an issue I don't think - leading wildcards are already an issue - 5% is worth the other speedups I think\n\nMark, the old WildcardTermEnum is public, so we must keep it around for a while anyway.\nI can use it for this case, so we don't lose this 5% in the special case :)\n\nMight be worth deprecating this old WildcardTermEnum still though, just because its code to be maintained, hardly used except for this purpose.",
            "date": "2009-11-20T18:12:14.426+0000",
            "id": 82
        },
        {
            "author": "Robert Muir",
            "body": "Mark, I think this patch is ok, all tests pass etc.\nCan you take a look and let me know your thoughts?",
            "date": "2009-11-20T19:20:02.097+0000",
            "id": 83
        },
        {
            "author": "Mark Miller",
            "body": "Nice! Resulting jar is still just 1.0 MB. Looks great on a quick look through. I'll go over more thoroughly when I get a chance.\n\nAs far as testing, one of the simple things we can try is generating random wildcard strings against a large corpus and auto comparing the results of the old and the new.\n\n+1 on the automaton name for the util package.\n\nI'd almost still prefer RegexQuery - its contrib vs core and different packages - I hate to lose out on the better name. Though thats a bit subjective ;)",
            "date": "2009-11-20T19:59:28.644+0000",
            "id": 84
        },
        {
            "author": "Robert Muir",
            "body": "Mark, thanks, let me know if you have the chance to look more thoroughly.\n\nI agree, lets consider some ideas for testing wildcards, yours sounds good.\nOne problem I had is trying to figure out: \"what is the average/common case\" for wildcards/regex :)\nIts important also when considering some additional optimizations i havent yet implemented.\n\nalso, I think i might have some additional wildcards tests from the contrib patch.\nI left TestWildCard completely as-is for now tho, b/c i thought it would be nice to show it passes unchanged.\n",
            "date": "2009-11-20T20:07:06.047+0000",
            "id": 85
        },
        {
            "author": "Uwe Schindler",
            "body": "I like it, too, some thoughts:\n\n- Maybe make AutomatonTermEnum public instead package private (if it maybe extended and of usage for own sub classes like a future FuzzyQuery to return a difference())\n- The code in WildcardTermEnum is deprecated but still there and teherefor duplicated functionality. Maybe we could make this class subclass of AutomatonTermEnum, but it initializes to be a simple WildCard. The TermEnum has no longer a test (the deprecated one), so we maybe must add a test.",
            "date": "2009-11-20T20:09:14.757+0000",
            "id": 86
        },
        {
            "author": "Robert Muir",
            "body": "bq. As far as testing, one of the simple things we can try is generating random wildcard strings against a large corpus and auto comparing the results of the old and the new.\n\nAn idea i have here is ORP-2 corpus, it has approx 417K unique terms and 160K docs.\nand its open, so anyone could participate. :)\n",
            "date": "2009-11-20T20:11:11.514+0000",
            "id": 87
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, both your ideas are great. thank you for looking.\nI will take a stab at those.",
            "date": "2009-11-20T20:12:29.536+0000",
            "id": 88
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, i looked at the WildcardTermEnum and it was easy to make it a subclass, with no logic, just a ctor.\n\n{code}\n  public WildcardTermEnum(IndexReader reader, Term term) throws IOException {\n    super(WildcardQuery.toAutomaton(term), term, reader);\n  }\n{code}\n\nThe problem is that this hardly removes any duplicated code, because we must keep this available:\n{code}\n public static final boolean wildcardEquals(String pattern, int patternIdx,\n    String string, int stringIdx)\n  {\n{code}\n\nThis is where all the logic really is anyway.\nSo I think i would rather leave this one alone? But I will add a test for it, to ensure it doesn't break since we are not using it.\nWhat do you think?\n",
            "date": "2009-11-20T22:37:16.929+0000",
            "id": 89
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Uwe, i looked at the WildcardTermEnum and it was easy to make it a subclass, with no logic, just a ctor. \n\nThat was my idea!\n\nbq. This is where all the logic really is anyway.\n\nWe should simply add a test for this method and everything else is the WildCardEnum. The good thing of subclassing it is, that one has a more performat class if it uses common prefixes and so on than the version we currently have. The wildcardEquals method must stay, but it is not used, so explicitely mark it as \"dead code\".\n\nThe good thing: the method is final (this is what I see from yor fragment) - so nobody was able to override it to change the behaviour of the enum, so nothing can break.\n\nI would go this way.",
            "date": "2009-11-20T22:43:12.537+0000",
            "id": 90
        },
        {
            "author": "Robert Muir",
            "body": "bq. We should simply add a test for this method and everything else is the WildCardEnum. The good thing of subclassing it is, that one has a more performat class if it uses common prefixes and so on than the version we currently have. The wildcardEquals method must stay, but it is not used, so explicitely mark it as \"dead code\".\n\nif we do this, there are lots of cases where it will perform better, yes (virtually anything involving ? operator)\nbut if we do this, there are also some cases where it won't perform quite as well, really bad wildcards where it is better to just do linear scan than skip around many many times. \nThis is why i have detection for these cases, in the getEnum() instead I return \"DumbTermEnum\" aka LinearTermEnum in AutomatonQuery.\nif you think this is no problem, we can subclass it anyway. excerpt below:\n\n{code}\n    /*\n     * If the DFA has a leading kleene star, or something similar, it will\n     * need to run against the entire term dictionary. In this case its much\n     * better to do just that than to use fancy enumeration.\n     * \n     * this heuristic looks for an initial loop, with a range of at least 1/3\n     * of the unicode BMP.\n     */\n    State state = automaton.getInitialState();\n    for (Transition transition : state.getTransitions())\n      if (transition.getDest() == state\n          && (transition.getMax() - transition.getMin()) > (Character.MAX_VALUE / 3))\n        return new LinearTermEnum(reader);\n\n    return new AutomatonTermEnum(automaton, term, reader);\n{code}\n",
            "date": "2009-11-20T22:51:03.292+0000",
            "id": 91
        },
        {
            "author": "Robert Muir",
            "body": "by the way Uwe, I do not particularly like how this AutomatonQuery \"decides to use smart or dumb termenum\" in getEnum() works.\nI wish instead the AutomatonTermEnum would always be fast, instead of relying on the query to decide.\nI think this would be cleaner, and make subclassing easier.\n\nbut on the other hand, having these two separate, it makes things easy to understand, as the two methods work in two completely different ways.\ni wonder if you have any ideas on this.",
            "date": "2009-11-20T23:11:51.417+0000",
            "id": 92
        },
        {
            "author": "Uwe Schindler",
            "body": "see LUCENE-2075, why it is not so fast (the setEnum calls use seeking and this is not optimized by the TermCache). Yonik has poited us to that.\n\nIf the dumb enumeration would be included inside AutomatonTermEnum, one could use it without thinking. I would like to move your posted code into AutomatonTermEnum and have two modi dumb and intelligent. This would need an if switch on each next() call and a delegation to super.next(). That would make the enum ugly... But would work. So just fold the LinearTermEnum into it and make  a switch: if (linearMode) return super.next();\nBut you have to remove the assert inside endEnum() and change it. In the intelligent case, the endEnum method is never called (because super.next() is never called). So the assert must be assert linearMode;\ntermCompare looks identical in both enums, for the indelligent case the comonPrefix is \"\".",
            "date": "2009-11-20T23:28:16.867+0000",
            "id": 93
        },
        {
            "author": "Robert Muir",
            "body": "bq. That would make the enum ugly... But would work.\n\nThis is why i did not do it (i tried and it was ugly), i did not want to make a complicated enum ugly!\nI'll try to think of how this can be done without it being so ugly.\n\nedit, by the way Uwe, if you are bored and want to take a stab at this :) You know your way around multitermquery better than me.",
            "date": "2009-11-20T23:31:40.559+0000",
            "id": 94
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Robert,\n\nhere is my patch. The WildCard and RegExp test querys still pass. I also added a test for the deprec TermEnum (just a simple MTQ that returns it is used and should produce same results as WildcardQuery).\n\nThe AutomatonTermEnum now switches between smart(R) and non-smart mode using your detection algorithm. termCompare now handles both cases.\nnext() just calls super in the linear case (so it behaves like a normal FilteredTermEnum) and uses the smart(R) code in all other cases.\n\nI will go to bed now, tell me if you like it.",
            "date": "2009-11-21T00:55:43.500+0000",
            "id": 95
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, thank you. This is much nicer!\n\nI think now it will be easier for some subclass to extend this enum, for example to override difference() or whatever for fuzzy.\n",
            "date": "2009-11-21T01:07:06.445+0000",
            "id": 96
        },
        {
            "author": "Michael McCandless",
            "body": "I'm not following this very closely, but, it looks really really cool!",
            "date": "2009-11-21T09:58:46.435+0000",
            "id": 97
        },
        {
            "author": "Uwe Schindler",
            "body": "Some cleanups and a more consistent endEnum handling. Also added Javadocs explaining smart and linear mode.",
            "date": "2009-11-21T12:25:20.665+0000",
            "id": 98
        },
        {
            "author": "Uwe Schindler",
            "body": "Again some updates, moved the '*' and '?' constants also to WildcardQuery and use them in switch. Also added better deprecation messeg to WildcardTermEnum.",
            "date": "2009-11-21T12:38:41.935+0000",
            "id": 99
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, lookout for that \u00f8 in the NOTICE.txt... I will fix it, thanks for your cleanups :)",
            "date": "2009-11-21T14:48:19.833+0000",
            "id": 100
        },
        {
            "author": "Robert Muir",
            "body": "fix the \u00f8 in NOTICE, cleanup some unused imports, etc.\n\nnow that Uwe fixed a performance bug in the dumb enum (it would never set endEnum=true but instead false), \nI will dig up my old performance tests and see how we are looking.\n",
            "date": "2009-11-21T15:22:20.673+0000",
            "id": 101
        },
        {
            "author": "Robert Muir",
            "body": "I reran my tests, Uwe's fix removes this '5%' problem I mentioned before for leading *.\nNow wildcardquery is always faster than before (before it was comparing terms from another field due to the endEnum bug)\n\nThis makes sense, because RunAutomaton.run() is just array access, instead of all the conditional/branching in the old wildcardEquals.\nBut i could not figure out before for the life of me, why this was slower before!\n\nI will create a better benchmark now that generates lots of random numeric wildcards with lots of patterns, and post the results and code.\n",
            "date": "2009-11-21T16:05:47.839+0000",
            "id": 102
        },
        {
            "author": "Robert Muir",
            "body": "this patch fixes a bug i introduced when i removed recursion.\nthe wildcard tests do not detect it... told you i didnt trust them :)\n\nI will add a test for this, although it was an obvious mistake on my part.",
            "date": "2009-11-21T17:41:40.295+0000",
            "id": 103
        },
        {
            "author": "Robert Muir",
            "body": "attached is benchmark, which generates random wildcard queries.\nit builds an index of 10million docs, each with a term from 0-10million.\nit will fill a pattern such as N?N?N?N? with random digits, substituting a random digit for N.\n\n||Pattern||Iter||AvgHits||AvgMS (old)||AvgMS (new)||\n|N?N?N?N|10|1000.0|288.6|38.5|\n|?NNNNNN|10|10.0|2453.1|6.4|\n|??NNNNN|10|100.0|2484.2|10.1|\n|???NNNN|10|1000.0|2821.3|47.8|\n|????NNN|10|10000.0|2346.9|299.8|\n|NN??NNN|10|100.0|34.8|6.3|\n|NN?N*|10|10000.0|26.5|9.4|\n|?NN*|10|100000.0|2009.0|73.5|\n|*N|10|1000000.0|6837.4|6087.9|\n|NNNNN??|10|100.0|1.9|2.3|\n\ni would like to incorporate part of this logic into the junit tests, on maybe a smaller index, because its how i found the recursion bug.\n",
            "date": "2009-11-21T18:20:15.484+0000",
            "id": 104
        },
        {
            "author": "Michael McCandless",
            "body": "Those are impressive gains!",
            "date": "2009-11-21T19:00:21.071+0000",
            "id": 105
        },
        {
            "author": "Robert Muir",
            "body": "Thanks Mike, it is not that impressive really, until you look at regex performance :)\n\nThe current regexp implementations will scan entire term dictionary for an expression like \"[dl]og?\", because there is no 'constant prefix'\nThe idea here, is that lucene should be smart enough to look for do, dog, lo, and log.",
            "date": "2009-11-21T19:13:08.972+0000",
            "id": 106
        },
        {
            "author": "Mark Miller",
            "body": "You are the man Robert. This is going to be great.\n\nNext on my wish list is getting the scalable fuzzy done :) We should start a new issue for that, seeding it with the info you have here. If you don't get to it, I'll be happy to.\n\nStill on my list to help with review on this patch too. Thanks Uwe as well! Love seeing this stuff make its way into core.",
            "date": "2009-11-21T19:16:42.164+0000",
            "id": 107
        },
        {
            "author": "Robert Muir",
            "body": "Mark, yeah lets create a separate issue for fuzzy. \nI found someone implemented that algorithm in python or some other language, we should look/contact them to see what they did.\n\ncurrently, I am trying to check this LUCENE-2075 issue, to see if this caching will help cases where the enum must seek a lot, for example the pattern ????NNN\nit is still better than the current wildcard case, but you see it gets a lot worse when you have a lot more seeks.\n\ni think though, this means i have to cut over to the new FilteredTermsEnum api for the flex branch... which looks interesting btw but this is a complicated enum.\n",
            "date": "2009-11-21T19:28:30.607+0000",
            "id": 108
        },
        {
            "author": "Robert Muir",
            "body": "Mark, one last comment. I want to mention this impl is largely unoptimized (from a code perspective, but the algorithm is better)\nI think you see that from the NNNNN?? being 2.3ms on average versus 1.9, not that I am sure that isnt just a random hiccup.\n\nSo I want to incorporate the logic of some of this benchmark into the tests, so that we can improve the actual code impl. to speed up cases like that.\nWhile i focus on the scalability, i know a lot of people have small indexes and maybe lots of qps and I don't want to slow them down.\n\nSome of this is easy, for example we make State.getSortedTransitionArray public, so we don't have to convert from arrays to lists to arrays and such, for no good reason.\n",
            "date": "2009-11-21T19:46:52.703+0000",
            "id": 109
        },
        {
            "author": "Michael McCandless",
            "body": "Are we going to deprecate contrib/regex with this?",
            "date": "2009-11-22T14:54:09.015+0000",
            "id": 110
        },
        {
            "author": "Robert Muir",
            "body": "bq. Are we going to deprecate contrib/regex with this? \n\nI would argue against that, only because the other regex impl's have different features and syntax, even if they are slow.\n",
            "date": "2009-11-22T15:01:17.036+0000",
            "id": 111
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nbq. Are we going to deprecate contrib/regex with this?\n\nI would argue against that, only because the other regex impl's have different features and syntax, even if they are slow.\n{quote}\n\nAhh OK, I agree then.  I didn't realize this new query doesn't subsume contrib's.  Would be good to call out what's different in the javadocs somewhere...",
            "date": "2009-11-22T16:35:35.727+0000",
            "id": 112
        },
        {
            "author": "Robert Muir",
            "body": "bq. Would be good to call out what's different in the javadocs somewhere...\n\ngood idea, let me know if you have some suggested wording.\nits really a general issue, the supported features and syntax of even the existing regex implementations in contrib are different I think?\n(i.e. they are not compatible: you cannot just swap impls around, without testing that it supports the syntax and features you are using)\n",
            "date": "2009-11-22T16:39:13.852+0000",
            "id": 113
        },
        {
            "author": "Michael McCandless",
            "body": "I don't have any wording -- I don't really know the differences :)\n\nIf it's \"only\" that the syntax is different, that's one thing... but if eg certain functionality isn't possible w/ new or old, that's another.",
            "date": "2009-11-22T16:43:50.143+0000",
            "id": 114
        },
        {
            "author": "Robert Muir",
            "body": "bq. If it's \"only\" that the syntax is different, that's one thing... but if eg certain functionality isn't possible w/ new or old, that's another.\n\nfrom a glance, it appears to me that both the syntax and functionality of our two contrib impls (java.util and jakarta), are very different.\n\nhere is one example. Java.util supports reluctant {m,n} closures, jakarta does not, it says this right in the javadocs.\nhttp://jakarta.apache.org/regexp/apidocs/org/apache/regexp/RE.html\n\nShould RE support reluctant {m,n} closures (does anyone care)?\nBut it supports reluctant versus greedy for other operators.\n\nin automaton, this concept of reluctance versus greedy, does not even exist, as spelled out on their page:\nThe * operator is mathematically the Kleene star operator (i.e. we don't have greedy/reluctant/possesive variants). \nhttp://www.brics.dk/automaton/faq.html\n\nthis is an example, where all 3 are different... i guess i kinda assumed everyone was aware that all these regex packages are very different.",
            "date": "2009-11-22T16:49:20.061+0000",
            "id": 115
        },
        {
            "author": "Robert Muir",
            "body": "we call this out nicely in the current RegexQuery,\nThe expressions supported depend on the regular expression implementation used by way of the RegexCapabilities interface. \n\nwhat should I say for the automaton implementation? it already has a javadoc link to the precise syntax supported,\nso in my opinion its actually less ambiguous than contrib RegexQuery.\n\nbut maybe improve this, instead of\n{code}\nThe supported syntax is documented in the {@link RegExp} class.\n{code}\n\nmaybe:\n{code}\nThe supported syntax is documented in the {@link RegExp} class.\nwarning: this might not be the syntax you are used to!\n{code}\n",
            "date": "2009-11-22T17:28:10.481+0000",
            "id": 116
        },
        {
            "author": "Michael McCandless",
            "body": "OK that warning seems good.  Maybe also reference contrib/regex, as another alternative, nothing that syntax/capabilities are different?",
            "date": "2009-11-22T18:06:16.758+0000",
            "id": 117
        },
        {
            "author": "Michael McCandless",
            "body": "First cut @ cutting over to flex API attached -- note that this\napplies to the flex branch, not trunk!\n\nI made some small changes to the benchmarker: use constant score\nfilter mode, and print the min (not avg) time (less noise).\n\nAlso, I ported the AutomatonTermEnum to the flex API, so this is now a\nbetter measure (\"flex on flex\") of what future perf will be.  It's\npossible there's a bug here, though TestWildcard passes.\n\nI still need to investigate why \"non-flex on non-flex\" and \"non-flex\non flex\" perform worse.\n\nI ran like this:\n\n  java -server -Xmx1g -Xms1g BenchWildcard\n\njava is 1.6.0_14 64 bit, on OpenSolaris.\n\nResults (msec is min of 10 runs each);\n\n||Pattern||ITrunk (min msec)||(Flex (min msec)||\n|N?N?N?N0.0|13|18|\n|?NNNNNN|1|3|\n|??NNNNN|4|6|\n|???NNNN|23|28|\n|????NNN|210|170|\n|NN??NNN|3|3|\n|NN?N*|7|4|\n|?NN*|62|30|\n|*N|4332|2576|\n|NNNNN??|1|1|\n\nLooks like flex API is faster for the slow queries.  Once I fix caching on trunk we should retest...\n",
            "date": "2009-11-22T18:17:23.618+0000",
            "id": 118
        },
        {
            "author": "Robert Muir",
            "body": "bq. Looks like flex API is faster for the slow queries. Once I fix caching on trunk we should retest...\n\nMike, this is cool. I like the results. it appears tentatively, that flex api is faster for both \"dumb\" (brute force linear reading) and \"fast\" (lots of seeking) modes.\nat least looking at ????NNN, and *N, which are the worst cases of both here. So it would seem its faster in every case.\n\nI'll look at what you did to port this to the TermsEnum api!\n",
            "date": "2009-11-22T18:22:43.317+0000",
            "id": 119
        },
        {
            "author": "Robert Muir",
            "body": "Mike, I think your port to TermsEnum is correct, and its definitely faster here.\n\nOne question, is it possible to speed this up further, by using UnicodeUtil/char[] conversion from TermRef instead of String?\nBecause its trivial to use char[] with the Automaton api (even tho that is not exposed, its no problem)\n\nI use only string because of the old TermEnum api.\n",
            "date": "2009-11-22T18:42:17.927+0000",
            "id": 120
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nOne question, is it possible to speed this up further, by using UnicodeUtil/char[] conversion from TermRef instead of String?\nBecause its trivial to use char[] with the Automaton api (even tho that is not exposed, its no problem)\n\nI use only string because of the old TermEnum api.\n{quote}\n\nOh, that'd be great!  It would be faster.  I was bummed at how many new String()'s I was doing...",
            "date": "2009-11-22T18:52:04.225+0000",
            "id": 121
        },
        {
            "author": "Robert Muir",
            "body": "bq. Oh, that'd be great! It would be faster. I was bummed at how many new String()'s I was doing...\n\nit would be nice I think if TermRef provided a helper method to make the char[] available? \ni.e. I don't think i should do unicode conversion in a multitermquery?\n",
            "date": "2009-11-22T19:01:42.418+0000",
            "id": 122
        },
        {
            "author": "Michael McCandless",
            "body": "bq. it would be nice I think if TermRef provided a helper method to make the char[] available? \n\nI agree... though, this requires state (UnicodeUtil.UTF16Result).  We could lazily set such state on the TermRef, but, that's making TermRef kinda heavy (it's nice and light weight now).  Hmmm.",
            "date": "2009-11-22T20:52:36.758+0000",
            "id": 123
        },
        {
            "author": "Robert Muir",
            "body": "bq. I agree... though, this requires state (UnicodeUtil.UTF16Result). We could lazily set such state on the TermRef, but, that's making TermRef kinda heavy (it's nice and light weight now). Hmmm.\n\ni guess the state could be in the TermsEnum, but that doesnt make for general use of TermRef.\nwhat else uses TermRef?\n",
            "date": "2009-11-22T20:55:59.405+0000",
            "id": 124
        },
        {
            "author": "Michael McCandless",
            "body": "Besides TermsEnum.. TermRef is used by the terms dict, when doing the binary search + scan to find a term.  And also by TermsConsumer (implemented by the codec, and used when writing a segment to the index).\n\nMaybe MTQ holds the state, or FilteredTermsEnum?  Other consumers of TermsEnum don't need to convert to char[].\n\nWe can discuss this under the new [separately] \"optimization\" issue for MTQs?\n\nAlso, remember that the current API is doing not only new String() but also new Term() when it enums the terms, so having to do new String() for MTQs on flex API is OK for starters.",
            "date": "2009-11-22T21:23:32.718+0000",
            "id": 125
        },
        {
            "author": "Robert Muir",
            "body": "bq. We can discuss this under the new [separately] \"optimization\" issue for MTQs?\n\nis there a jira issue for this??\n\nbq. Also, remember that the current API is doing not only new String() but also new Term() when it enums the terms, so having to do new String() for MTQs on flex API is OK for starters.\n\noh yeah, its clear the flex api is already better, from benchmarks.\nI am just trying to think of ways to make it both faster, at the same time easy too.",
            "date": "2009-11-22T21:25:26.225+0000",
            "id": 126
        },
        {
            "author": "Michael McCandless",
            "body": "bq. is there a jira issue for this??\n\nI thought you were about to open one!\n\nbq. I am just trying to think of ways to make it both faster, at the same time easy too.\n\nWhich is great: keep it up!\n\nActually... wouldn't we need to convert to int[] (for Unicode 4) not char[], to be most convenient for \"higher up\" APIs like automaton?  If we did char[] you'd still have to handle surrogates process (and then it's not unlike doing byte[]).",
            "date": "2009-11-22T21:29:51.820+0000",
            "id": 127
        },
        {
            "author": "Robert Muir",
            "body": "bq. I thought you were about to open one!\n\nI opened one for Automaton specifically, should i change it to be all MTQ?\n\nbq. Actually... wouldn't we need to convert to int[] (for Unicode 4) not char[], to be most convenient for \"higher up\" APIs like automaton? If we did char[] you'd still have to handle surrogates process (and then it's not unlike doing byte[]).\n\nnope. because unicode and java are optimized for UTF-16, not UTF-32. so we should use char[], but use the codePoint apis, which are designed such that you can process text in UTF-16 (char[]) efficiently, yet also handle the rare case of supp. characters.\nchar[] is correct, its just that we have to be careful to use the right apis for processing it.\nWith String() a lot of the apis such as String.toLowerCase do this automatically for you, so most applications have no issues.\n",
            "date": "2009-11-22T21:35:23.491+0000",
            "id": 128
        },
        {
            "author": "Robert Muir",
            "body": "bq. Actually... wouldn't we need to convert to int[] (for Unicode 4) not char[], to be most convenient for \"higher up\" APIs like automaton? If we did char[] you'd still have to handle surrogates process (and then it's not unlike doing byte[]).\n\nI wanted to make another comment here, I agree that this somewhat like byte[].\nBut there are some major differences:\n# the java API provides mechanisms in Character, etc for processing text this way.\n# lots of stuff is unaffected. for example .startsWith() is not broken for supp characters.\n it does not have to use codepoint anywhere, can just compare chars, which are surrogates, but this is ok.\n so lots of char[]-based processing is already compatible, and completely unaware of this issue. this is not true for byte[]\n# it will perform the best overall, its only needed in very few places and we can be very careful where we add these checks, so we don't slow anything down or increase RAM usage, etc.",
            "date": "2009-11-22T21:45:53.528+0000",
            "id": 129
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I opened one for Automaton specifically, should i change it to be all MTQ?\n\nOh, sorry no, just automaton.\n\nbq. nope. because unicode and java are optimized for UTF-16, not UTF-32.\n\nOK char[] it is!",
            "date": "2009-11-22T21:47:49.157+0000",
            "id": 130
        },
        {
            "author": "Robert Muir",
            "body": "Mike, here is an update to your flex patch. I restored back two tests that disappeared (TestRegexp, etc)\nAlso, I converted the enum to use char[] as an experiment. i posted the results on LUCENE-2090.\nthis is just a hack, it stores the UTF16Result in AutomatonEnum\nI figured i would pass it back in case, just in the case you wanted to play more.\n",
            "date": "2009-11-22T23:37:46.101+0000",
            "id": 131
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Mike, here is an update to your flex patch. I restored back two tests that disappeared (TestRegexp, etc)\n\nWoops, thanks!  Need svn patch, badly...\n\n{quote}\nAlso, I converted the enum to use char[] as an experiment. i posted the results on LUCENE-2090.\nthis is just a hack, it stores the UTF16Result in AutomatonEnum\nI figured i would pass it back in case, just in the case you wanted to play more.\n{quote}\n\nWow, not \"new String()\"ing all over gave a sizable gain on the full linear scan query...",
            "date": "2009-11-23T09:26:27.711+0000",
            "id": 132
        },
        {
            "author": "Robert Muir",
            "body": "Michael, the problem is this code (automaton itself), like many other code, is unaware of supplementary characters.\nIt uses a symbolic interval range of 'char' for state transitions.\nBut this is ok! When matching an input string with suppl. characters, things work just fine.\n\nThis is one reason why i am concerned about the change to byte[] in flex branch.\nI would have to rewrite this DFA library for this enum to work!",
            "date": "2009-11-23T11:47:03.455+0000",
            "id": 133
        },
        {
            "author": "Robert Muir",
            "body": "Mike, just one comment here.\n\nI am definitely willing to do refactoring here to support this byte[] scheme if necessary, I don't want to give the wrong impression. I think i already have an issue here related to UTF-16 binary order vs UTF-8 binary order that I need to fix, although I think this is just writing a Comparator.\n\nedit: pretty sure this exists. If someone has say, both data from Arabic Presentation forms and Chinese text outside the BMP in the index, the \"smart\" enumerator will unknowningly skip right past the Arabic Presentation forms block, because it sorts after the lead surrogate in UTF-16 order, but before the entire codepoint in UTF-8/UTF-32 order. I have not experienced this in practice, because i normalize my text so i don't have stuff in Arabic Presentation forms block :) I can fix this, but i would like to see what the approach is for the flex branch, as its sufficiently compex that I would rather not fix it twice.\n\nI am just concerned about other similar applications outside of lucene, or some already in lucene core itself!\n",
            "date": "2009-11-23T15:27:57.534+0000",
            "id": 134
        },
        {
            "author": "Robert Muir",
            "body": "I think i have a workaround for this enum that will not hurt performance as well.\nThere are two problems, one exists with the existing api, one will become a problem with the flex API if we move to byte[] TermRef, which, from performance numbers, it seems we almost certainly should.\n\n* problem 1 is the fact that this enum depends upon 'sorted transitions' where each transition is an interval of UTF-16 characters. \nTo fix this, two things are required:\n# splitting intervals that overlap between the BMP and surrogate range into separate intervals\n# sorting intervals in codepoint order, which means ordering the surrogate range intervals after any BMP intervals.\n\n* problem 2 is the fact the enum works on UTF-16 characters again, and we can try to seek in the enumerator to a place ending with a lead surrogate.\n# In this case, we should just tack on U+DC00 (the lowest of trail surrogates), which is functionally equivalent. \n# for the \"common prefix\" we just remove any trail surrogates, although this common prefix is currently not even used at all, so we should get rid of it, if the first state is not a loop we are in smart mode anyway!\n\nI'll fix these problems, by providing a new \"codepoint-order\" comparator for transitions behind the scenes in automaton, along with a getSortedTransitionsCodepointOrder() or something similar to make the whole thing work.\n\nit might seem at a glance that using 'int' (UTF-32 intervals) instead is a better fix, but this is not true, because it would cause a RunAutomaton to use 1MB memory where it currently uses 64KB, only for these stupid rare cases.\n",
            "date": "2009-11-23T19:50:29.748+0000",
            "id": 135
        },
        {
            "author": "Robert Muir",
            "body": "I spent a while with this, thinking I would be slick and create a version of Automaton that works with both trunk and flex branch correctly.\nFinally, i figured it out, this is not possible.\n\nThere is no bug with the current version, because in trunk, IndexReader.terms() uses UTF-16 binary order.\nIn the flex branch, it uses UTF-8 binary order.\n\nI can emulate UTF-8 binary order in the enum, but then it won't work correctly on trunk, but will work on flex branch!\n\nThis enum is sensitive to the order of terms coming in...\n\ndoh!\n",
            "date": "2009-11-24T01:04:53.756+0000",
            "id": 136
        },
        {
            "author": "Yonik Seeley",
            "body": "geeze... maybe we should have just stuck with CESU-8 ;-)",
            "date": "2009-11-24T01:16:45.502+0000",
            "id": 137
        },
        {
            "author": "Robert Muir",
            "body": "Yonik, maybe we can use this trick?\n\nUTF-8 in UTF-16 Order\nThe following comparison function for UTF-8 yields the same results as UTF-16 binary\ncomparison. In the code, notice that it is necessary to do extra work only once per string,\nnot once per byte. That work can consist of simply remapping through a small array; there\nare no extra conditional branches that could slow down the processing.\n{code}\nint strcmp8like16(unsigned char* a, unsigned char* b) {\n  while (true) {\n  int ac = *a++;\n  int bc = *b++;\n  if (ac != bc) return rotate[ac] - rotate[bc];\n  if (ac == 0) return 0;\n  }\n}\n\nstatic char rotate[256] =\n{0x00, ..., 0x0F,\n0x10, ..., 0x1F,\n. .\n. .\n. .\n0xD0, ..., 0xDF,\n0xE0, ..., 0xED, 0xF0, 0xF1,\n0xF2, 0xF3, 0xF4, 0xEE, 0xEF, 0xF5, ..., 0xFF};\n{code}\n\nThe rotate array is formed by taking an array of 256 bytes from 0x00 to 0xFF, and rotating\n0xEE and 0xEF to a position after the bytes 0xF0..0xF4. These rotated values are shown in\nboldface. When this rotation is performed on the initial bytes of UTF-8, it has the effect of\nmaking code points U+10000..U+10FFFF sort below U+E000..U+FFFF, thus mimicking\nthe ordering of UTF-16.",
            "date": "2009-11-24T02:15:22.576+0000",
            "id": 138
        },
        {
            "author": "Robert Muir",
            "body": "updated patch:\n* don't seek to high surrogates, instead tack on \\uDC00. this still works for trunk, but also with flex branch.\n* don't use a high surrogate prefix, instead truncate. this isn't being used at all, i would rather use 'constant suffix'\n* add tests that will break if lucene's sort order is not UTF-16 (or if automaton is not adjusted to the new sort order)\n* add another enum constructor, where you can specify smart or dumb mode yourself\n* regexp javadoc note\n* add wordage to LICENSE, not just NOTICE\n",
            "date": "2009-11-24T12:22:06.784+0000",
            "id": 139
        },
        {
            "author": "Robert Muir",
            "body": "sorry, my ide added a @author tag. i need to look to see where to turn this @author generation off for eclipse.",
            "date": "2009-11-24T12:26:47.576+0000",
            "id": 140
        },
        {
            "author": "Uwe Schindler",
            "body": "what is UTF-38? :-) I think you mean UTF-32, if such exists.\n\nElse it looks good!",
            "date": "2009-11-24T13:25:45.226+0000",
            "id": 141
        },
        {
            "author": "Robert Muir",
            "body": "i think there is one last problem with this for flex branch, where you have abacadaba\\uFFFC, abacadaba\\uFFFD and abacadaba\\uFFFE  in the term dictionary, but a regex the matches say abacadaba[\\uFFFC\\uFFFF]. in this case, the match on abacadaba\\uFFFD will fail, it will try to seek to the \"next\" string, which is abacadaba\\uFFFF, but the FFFF will get replaced by FFFD by the byte conversion, and we will loop.\n\nmike i don't think this should be any back compat concern, unlike the high surrogate case which i think many CJK applications are probably doing to...\n",
            "date": "2009-11-24T13:28:55.987+0000",
            "id": 142
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, where do you see UTF-38 :)",
            "date": "2009-11-24T13:32:13.281+0000",
            "id": 143
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Uwe, where do you see UTF-38  \nPatch line 6025.",
            "date": "2009-11-24T13:35:52.835+0000",
            "id": 144
        },
        {
            "author": "Uwe Schindler",
            "body": "about the cleanupPrefix method: it is only used in the linear case to initially set the termenum. What happens if the nextString() method returs such a string ussed to seek the next enum?",
            "date": "2009-11-24T13:40:11.725+0000",
            "id": 145
        },
        {
            "author": "Robert Muir",
            "body": "bq. about the cleanupPrefix method: it is only used in the linear case to initially set the termenum. What happens if the nextString() method returs such a string ussed to seek the next enum? \n\nlook at the code to nextString() itself. \nit uses cleanupPosition() which works differently.\n\nwhen seeking, we can append \\uDC00 to achieve the same thing as seeking to a high surrogate.\nwhen using a prefix, we have to truncate the high surrogate, because we cannot use it with TermRef.startsWith() etc, it cannot be converted into UTF-8 bytes. (and we can't use the \\uDC00 trick, obviously, or startsWith() will return false when it should not)",
            "date": "2009-11-24T13:41:58.096+0000",
            "id": 146
        },
        {
            "author": "Robert Muir",
            "body": "bq. Patch line 6025.\n\nThanks for reviewing the patch and catching this. I'm working on trying to finalize this.\nIt already works fine for trunk, but I don't want it to suddenly break with the flex branch, so I'm adding a lot of tests and improvements in that regard.\nThe current wildcard tests aren't sufficient anyway to tell if its really working.\nAlso, when Mike ported it to the flex branch, he reorganized some code some in a way that I think is better, so I want to tie that in too.\n",
            "date": "2009-11-24T16:57:02.247+0000",
            "id": 147
        },
        {
            "author": "Uwe Schindler",
            "body": "Did he changed the FilteredTermEnum.next() loops? if yes, maybe the better approach also works for NRQ. I am just interested, but had no time to thoroughly look into the latest changes.\n\nI am still thinking about an extension of FilteredTermEnum that works with these repositioning out of the box. But I have no good idea. The work in FilteredTerm*s*Enum is a good start, but may be extended, to also support something like a return value \"JUMP_TO_NEXT_ENUM\" and a mabstract method \"nextEnum()\" that returns null per default (no further enum).",
            "date": "2009-11-24T17:04:35.679+0000",
            "id": 148
        },
        {
            "author": "Robert Muir",
            "body": "No, the main thing he did here that i like better, is that instead of caching the last comparison in termCompare(), he uses a boolean 'first'\n\nThis still gives the optimization of 'don't seek in the term dictionary unless you get a mismatch, as long as you have matches, read sequentially'\nBut in my opinion, its cleaner.\n",
            "date": "2009-11-24T17:10:20.775+0000",
            "id": 149
        },
        {
            "author": "Uwe Schindler",
            "body": "OK, so doesn't affect NRQ, as it uses a different algo",
            "date": "2009-11-24T17:14:25.016+0000",
            "id": 150
        },
        {
            "author": "Robert Muir",
            "body": "Yeah, but in general I think I already agree that FilteredTerm*s*Enum is easier for stuff like this.\n\nEither way its still tricky to make enums like this, so I am glad you are looking into it.\n",
            "date": "2009-11-24T17:17:53.888+0000",
            "id": 151
        },
        {
            "author": "Uwe Schindler",
            "body": "I think the approach with nextEnum() would work for Automaton and NRQ, because both use this iteration approach. You have nextString() for repositioning, and I have a LinkedList (a stack) of pre-sorted range bounds.",
            "date": "2009-11-24T17:23:48.608+0000",
            "id": 152
        },
        {
            "author": "Robert Muir",
            "body": "And I could still use this with \"dumb mode\"?, just one enum, right?",
            "date": "2009-11-24T17:25:16.492+0000",
            "id": 153
        },
        {
            "author": "Uwe Schindler",
            "body": "yes.",
            "date": "2009-11-24T17:40:26.065+0000",
            "id": 154
        },
        {
            "author": "Robert Muir",
            "body": "this patch removes constant prefix, as its only used in dumb mode, and in dumb mode there is no constant prefix.\ninstead its replaced with constant suffix, which speeds up comparisons.\n\nconstant suffix is implemented naively as reversing the DFA, taking its constant prefix, then reversing that.\n",
            "date": "2009-11-24T18:45:26.661+0000",
            "id": 155
        },
        {
            "author": "Michael McCandless",
            "body": "Responding from LUCENE-2075...\n\n{quote}\nbq. So I guess if there's a non-empty common suffix you should just always seek?\n\nthe suffix is just for quick comparison, not used at all in seeking.\n{quote}\n\nI think I'm confused -- if the query is ???1234, the common suffix is\n1234, and so shouldn't the DFA tell us the next XXX1234 term to try to\nseek to (and we should never use next() on the enum)?\n",
            "date": "2009-11-24T20:38:27.027+0000",
            "id": 156
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nI think I'm confused - if the query is ???1234, the common suffix is\n1234, and so shouldn't the DFA tell us the next XXX1234 term to try to\nseek to (and we should never use next() on the enum)?\n{quote}\n\nnot really the suffix, but more general, the structure of the dfa itself tells us that for ???1234, if you are evaluating 5551234, that the next term should really be 5561234\nyou can tell this by basically 'walking the graph'\n\nbut this knowledge is not always available, sometimes we only have 'partial' knowledge of where to go next.\nThe culprit here is the * operator, because it eats up anything :)\nSo when you walk the graph, the * operator is this giant monster in your way. \n\nso sometimes, depending on the dfa (which depends on the wildcard or regex used to construct it), \nautomaton knows enough to seek to all the exact locations, if its finite (like ???1234)\n\nsometimes, it only knows partial information. when a loop is encountered walking the graph (the giant monster), it has to stop and only use the path information it knows so far.\nfor example a wildcard of abcd*1234, the best it can do is seek to abcd.\n\nyour description of ping-pong is right, this is how these situations are handled.\n\nright now, the way the enum works, is i don't even bother seeking until i hit a mismatch.\nyou can see this in the comments 'as long as there is a match, keep reading. this is an optimization when many terms are matched sequentially like ab*'\n\ni tested this along time ago, perhaps we should re-test to see if its appropriate?\n",
            "date": "2009-11-24T20:56:32.244+0000",
            "id": 157
        },
        {
            "author": "Robert Muir",
            "body": "mike, here is a more complex example of the ping-pong:\n\na wildcard of abcd*12?4\n\nit has to seek to abcd first because of the * (the loop stops me)\nthe ping-pong from the term dictionary, returns say abcdk1000\nit moves me past the giant monster.\nnow i know enough to seek to abcdk12\\u00004\n\nin this case its nice, the TermEnum moved me past it in one seek.\nsometimes its not so nice, if the TermEnum gave me abcda, i'm not past the monster.\n\nall i can do is generate the next possible term that won't put me into a DFA reject state, which is abcda\\u0000... forcing the enum to move me forwards.\nmaybe i seek to abcda\\u0000, and it gives me abcdaa back, ill do the same thing again.\n\nthe reason is, somewhere down the line there could be abcdaaaaaaaaaaaaaaaaaaaaaaaaaa1234 :)\n",
            "date": "2009-11-24T21:11:45.738+0000",
            "id": 158
        },
        {
            "author": "Michael McCandless",
            "body": "But, take the abcd*1234 case -- you first seek to abcd, the terms enum finds (say) abcdX1234 -- don't you (DFA) know at this point that next possible candidate is abcdY1234?  Ie, you should seek to that term?  (Doing next() at that point is most likely a waste, and anyway the enum will turn your seek into a next if it's \"close\").\n\nThat said, seeking on trunk is alot more costly than seeking on flex, because trunk has to make a new [cloned] SegmentTermEnum for each seek.",
            "date": "2009-11-24T21:12:18.092+0000",
            "id": 159
        },
        {
            "author": "Robert Muir",
            "body": "bq. But, take the abcd*1234 case - you first seek to abcd, the terms enum finds (say) abcdX1234 - don't you (DFA) know at this point that next possible candidate is abcdY1234? Ie, you should seek to that term? (Doing next() at that point is most likely a waste, and anyway the enum will turn your seek into a next if it's \"close\"). \n\npretty sure I know this information, but in general i only seek on mismatches, I think for the reason that there can be a lot of seeks for AB* (say 1 million terms).\nso instead i wait for a mismatch until i seek again, I think tests showed this due to what you mentioned below?\n\nbq. That said, seeking on trunk is alot more costly than seeking on flex, because trunk has to make a new [cloned] SegmentTermEnum for each seek.\n\nI think that might be whats killing me, when i ran tests on lucene 2.9 or whatever.\nWe should retest performance on flex.\n\nThis is why i said, significant rework of this maybe should take place in flex (although I still think this is an improvement for trunk already), to fully take advantage of it.\n",
            "date": "2009-11-24T21:18:38.859+0000",
            "id": 160
        },
        {
            "author": "Robert Muir",
            "body": "I guess here is the big question Mike, pretend ab* isn't rewritten to a prefixquery (it is, but there are more complex examples like this that cannot be)\n\nis it faster to seek 1M times and get the 1M terms, or just read them sequentially?\nfurthermore to \"seek\" is not just lucene seek, I have to walk the transitions and compute the next place to go... (and create a few objects along the way)\n",
            "date": "2009-11-24T21:27:11.122+0000",
            "id": 161
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\npretend ab* isn't rewritten to a prefixquery (it is, but there are more complex examples like this that cannot be)\nis it faster to seek 1M times and get the 1M terms, or just read them sequentially?\n{quote}\n\nThis case should definitely be done sequentially.\n\nBut the fixed trailing prefix case I think should be all seeks.\n\n{quote}\nbq. That said, seeking on trunk is alot more costly than seeking on flex, because trunk has to make a new [cloned] SegmentTermEnum for each seek.\n\nI think that might be whats killing me, when i ran tests on lucene 2.9 or whatever.\nWe should retest performance on flex.\n{quote}\n\nWell, the seeks need to be done anyway... so you can't work around that.  The only question is if a wasted next() was done before each, I guess...",
            "date": "2009-11-24T21:32:22.977+0000",
            "id": 162
        },
        {
            "author": "Robert Muir",
            "body": "bq. But the fixed trailing prefix case I think should be all seeks.\n\nI'll use regular expressions here, just to elaborate on this.\n\nwhat if its ab.*[ab]? but the ab.*[a-z]? ... where do you draw the line :)\n\nits also worth mentioning that the automaton \"seek\", nextString() is a lot more heavyweight right now than its \"compare\", which is extremely fast. its the DFA in tableized (array) form, just as an array lookup.\nThis is why it beats even the hairy wildcard code we had before, after Uwe fixed my bug of course :)\n \nI think there are heuristics like you say we can do, and there's a lot of knowledge in the DFA we can use to implement these for optimal behavior.\nI think we can also improve the code itself, so that \"seek\", the nextString() method itself, is more lightweight.\n\non the other hand the big unknown is the distribution of the term dictionary itself.\n\nI did a very basic implementation here, I'm hoping we can come up with better ideas that work well on average.\nOne problem is, what is an \"average\" regular expression or wildcard query :)\n",
            "date": "2009-11-24T21:40:21.999+0000",
            "id": 163
        },
        {
            "author": "Robert Muir",
            "body": "benchmark results from mike's idea. I don't use any heuristic, just remove the extra 'next' to show the tradeoffs.\n\ndisclaimer: against trunk with LUCENE-2075\n\n||Pattern||Iter||AvgHits||AvgMS||AvgMS (noNext)||\n|N?N?N?N|10|1000.0|37.5|28.4|\n|?NNNNNN|10|10.0|6.4|6.1|\n|??NNNNN|10|100.0|9.6|9.2|\n|???NNNN|10|1000.0|52.7|40.9|\n|????NNN|10|10000.0|300.7|262.3|\n|NN??NNN|10|100.0|4.9|4.1|\n|NN?N*|10|10000.0|9.6|28.9|\n|?NN*|10|100000.0|80.4|235.4|\n|*N|10|1000000.0|3811.6|3747.5|\n|*NNNNNN|10|10.0|2098.3|2221.9|\n|NNNNN??|10|100.0|2.2|2.4|\n\nMike my gut feeling, which will require a lot more testing, is that if the automaton accepts a finite language (in the wildcard case, no *), we should not do the next() call.\nbut more benchmarking is needed, with more patterns, especially on flex branch to determine if this heuristic is best.\n",
            "date": "2009-11-24T22:02:34.585+0000",
            "id": 164
        },
        {
            "author": "Robert Muir",
            "body": "in this patch, if the automaton is finite, always seek.\nif its infinite, keep reading terms sequentially until a term fails (then seek)\n\nit seems to be the best of both worlds, and makes perfect sense to me.\n\nonly thing that has me nervous is that SpecialOperations.isFinite() is defined recursively... will have to look into maybe trying to write this method iteratively, in case someone builds some monster automaton from a 2 page regexp or something like that.\n",
            "date": "2009-11-24T22:19:17.105+0000",
            "id": 165
        },
        {
            "author": "Robert Muir",
            "body": "sorry, wrong file. getting lost in iterations of this patch.\n",
            "date": "2009-11-24T22:20:28.307+0000",
            "id": 166
        },
        {
            "author": "Michael McCandless",
            "body": "bq. (Doing next() at that point is most likely a waste, and anyway the enum will turn your seek into a next if it's \"close\")\n\nActually -- I just remembered -- flex branch is failing to do this optimization (there's already a nocommit reminding us to do it).  Ie, it's always doing the binary search through the indexed terms... and not doing a scan when it determines the term you're seeking to is within the same index block.\n\nBut I don't think this'll impact your tests with a large suffix since each seek will jump way ahead to a new index block.",
            "date": "2009-11-25T12:59:01.036+0000",
            "id": 167
        },
        {
            "author": "Robert Muir",
            "body": "bq. But I don't think this'll impact your tests with a large suffix since each seek will jump way ahead to a new index block.\n\nMike, if you add that optimization, that takes care of lucene itself, its smart enough to turn a seek into a read when it should, so I you might say I should simplify my code and just always seek.\nBut if I were to do this, then that would kill the TermRef comparison speedup, because then no matter how much i optimize \"my seek\" nextString(), it needs to do the unicode conversion, which we have seen is expensive across many terms.\n\n",
            "date": "2009-11-25T13:09:26.948+0000",
            "id": 168
        },
        {
            "author": "Michael McCandless",
            "body": "bq. But if I were to do this, then that would kill the TermRef comparison speedup, because then no matter how much i optimize \"my seek\" nextString(), it needs to do the unicode conversion, which we have seen is expensive across many terms.\n\nRight, I think ATE must still pick & choose when to seek itself vs seek Lucene, based on how costly nextString() is...",
            "date": "2009-11-25T13:14:25.731+0000",
            "id": 169
        },
        {
            "author": "Michael McCandless",
            "body": "Make that \"when to seek itself vs next() Lucene\"",
            "date": "2009-11-25T13:14:51.611+0000",
            "id": 170
        },
        {
            "author": "Robert Muir",
            "body": "Mike by the way, I profiled the seeking on trunk, right at the top with 20% in hprof is the SegmentTermEnum clone... this is why at least for now, and on flex for different reasons, I think we should keep this stupid heuristic.\n\nBut its improved now, because at least its using some knowledge of the DFA (whether or not it contains loops) to make this determination, thanks for the idea!\n",
            "date": "2009-11-25T13:18:41.678+0000",
            "id": 171
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Mike by the way, I profiled the seeking on trunk, right at the top with 20% in hprof is the SegmentTermEnum clone... this is why at least for now, and on flex for different reasons, I think we should keep this stupid heuristic.\n\nOK let's keep it for now?  But somehow we need to remember when this gets onto flex branch to put it back...",
            "date": "2009-11-25T16:07:26.462+0000",
            "id": 172
        },
        {
            "author": "Robert Muir",
            "body": "bq. OK let's keep it for now? But somehow we need to remember when this gets onto flex branch to put it back...\n\nI guess what I am saying is I think the latest patch, which uses the isFinite() property of the DFA to determine whether or not to seek itself versus trying next(), is the best for both trunk and flex, but for different reasons?\n\nedit:\nthe different reasons being: seek is expensive in trunk, because of the SegmentTermEnum clone()\nseek is \"expensive\" in flex, because doing a seek when we are in a loop entails unicode conversion, but next() avoids this with TermRef comparison.\n\nIt gives the best of all the scores from https://issues.apache.org/jira/browse/LUCENE-1606?focusedCommentId=12782198&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12782198\n\nIn the future this could be refined, such that whether to try the extra next() or go ahead and seek() could instead be driven by whether or not we are 'ping-ponging against a loop', i.e. actually pingponging against a wildcard *, rather than computed up-front for the entire query. this can be determined from the state/transitions of the path being evaluated, but its not a one-liner!\n",
            "date": "2009-11-25T16:16:19.135+0000",
            "id": 173
        },
        {
            "author": "Robert Muir",
            "body": "Mike,\n\nThis is ok for the trunk, but I have a question about \\uFFFF in flex (I guess we do not need to figure it out now, just think about it).\nMy understanding is that now \\uFFFF can be in the index, and I can seek to it (it won't get replaced with \\uFFFD).\nFrom your comment this seems undefined at the moment, but for this enum I need to know, otherwise it will either skip \\uFFFF terms, or go into a loop.\n\n",
            "date": "2009-11-30T13:49:57.652+0000",
            "id": 174
        },
        {
            "author": "Michael McCandless",
            "body": "bq. My understanding is that now \\uFFFF can be in the index, and I can seek to it (it won't get replaced with \\uFFFD).\n\nYes, \\uFFFF should be untouched now (though I haven't verified  -- actually I'll go add it to the test we already have for \\uFFFF).",
            "date": "2009-11-30T15:36:29.924+0000",
            "id": 175
        },
        {
            "author": "Robert Muir",
            "body": "Thanks Mike, I will change the enum to reflect this.\nCurrently I cheat and take advantage of this property (in trunk) to make the code simpler.",
            "date": "2009-11-30T15:40:38.433+0000",
            "id": 176
        },
        {
            "author": "Robert Muir",
            "body": "here is an updated patch. In my opinion all that is needed is to add the random testing, and code reorganization (but no algorithmic/feature changes), maybe better docs too.\n\nThis patch has:\n* more tests, especially surrogate handling\n* some more automaton paring\n* the seek positions and common suffix are always valid UTF-8 strings\n* backtracking is aware of U+FFFF, so terms can have it.\n\nThis patch works correctly on both trunk and flex, although I don't have an included test for U+FFFF since I can't put it in a trunk index.\n\nI'm not too terribly happy about the way the unicode handling works here, its correct but could be coded better.\nThe code I wrote is not the easiest to read and suggestions welcome :)\nBut it is all localized to one method, cleanupPosition(). This is defined as\n\n{code}\n* if the seek position cannot be converted to valid UTF-8,\n* then return the next valid String (in UTF-16 sort order) that\n* can be converted to valid UTF-8.\n{code}\n\nif you have ideas on how to make this nicer I am happy to hear them.\n\n",
            "date": "2009-12-02T22:20:28.498+0000",
            "id": 177
        },
        {
            "author": "Robert Muir",
            "body": "edit: edit out my chinese chars and replaced with <chineseCharOutsideBMP> as there are some problems indexing this comment.\n\nbtw the unicode complexity i mention here is not just me being anal, its an impedence mismatch between the automaton library using UTF-16 code unit representation and the new flex api requiring valid UTF-8. \n\nI am not trying to introduce complexity for no reason, here is an example:\n\n{code}\nRegExp re = new RegExp(\"(<chineseCharOutsideBMP>|<chineseCharOutsideBMP>)\");\nSystem.out.println(re.toAutomaton());\n{code}\n{noformat}\ninitial state: 0\nstate 0 [reject]:\n  \\ud866 -> 2\nstate 1 [accept]:\nstate 2 [reject]:\n  \\udf05-\\udf06 -> 1\n{noformat}\n\nas you can see, the automaton library handles these characters correctly, but as code units.\nso its important not to seek to invalid locations when walking thru the DFA, because these will be replaced by U+FFFD, \nand terms could be skipped, or we go backwards, creating a loop.\nThats why i spent so much time on this.\n\n",
            "date": "2009-12-02T22:43:50.205+0000",
            "id": 178
        },
        {
            "author": "Robert Muir",
            "body": "I added random testing for wildcards and regexps. \nDon't know what else needs to be done here, please review if you can.",
            "date": "2009-12-04T14:51:33.629+0000",
            "id": 179
        },
        {
            "author": "Mark Miller",
            "body": "I'll play with it some for one. Fantastic commenting man - this whole patch is pretty darn thorough.",
            "date": "2009-12-04T15:16:35.871+0000",
            "id": 180
        },
        {
            "author": "Robert Muir",
            "body": "attached is a port of the latest trunk patch to flex branch, for experimenting or whatever.\n",
            "date": "2009-12-04T18:12:00.607+0000",
            "id": 181
        },
        {
            "author": "Robert Muir",
            "body": "btw this patch is a bit different than the last port to flex in one way.\nLike the trunk patch the commonSuffix is only computed for \"linear mode\" aka slow queries.\nbut computing this for flex will be a win even for faster queries in \"smart mode\", because it can dodge more unicode conversion with TermRef byte[] comparison.\n\nthe problem is my implementation of getCommonSuffix() is a little crappy, reverse the entire automaton, redeterminize, take its common prefix, then reverse that.\ninstead, improving reverse() so that it keeps determinism when its already a DFA (DFA->DFA) will allow this commonSuffix to be used in both modes without any concern that it will ever hurt performance.\n",
            "date": "2009-12-04T18:19:18.491+0000",
            "id": 182
        },
        {
            "author": "Uwe Schindler",
            "body": "Here a flex patch for automaton. It contains LUCENE-2110, as soon as 2110 is committed I will upload a new patch. But its hard to differentiate between all modified files.\n\nRobert: Can you do performance tests with the old and new flex patch, I do not want to commit 2110 before.",
            "date": "2009-12-05T15:06:03.222+0000",
            "id": 183
        },
        {
            "author": "Robert Muir",
            "body": "bq. Robert: Can you do performance tests with the old and new flex patch, I do not want to commit 2110 before.\n\nUwe I will run a benchmark on both versions!",
            "date": "2009-12-05T15:10:47.537+0000",
            "id": 184
        },
        {
            "author": "Uwe Schindler",
            "body": "New patch, there was a lost private field. Also changed the nextSeekTerm method to be more straigtForward.\n\nRobert: Sorry, it would be better to test this one *g*",
            "date": "2009-12-05T15:15:55.524+0000",
            "id": 185
        },
        {
            "author": "Robert Muir",
            "body": "Hi Uwe, I ran my benchmarks, and with your patch the performance is the same.\n\nBut the code is much simpler and easier to read... great work.",
            "date": "2009-12-05T15:44:21.452+0000",
            "id": 186
        },
        {
            "author": "Uwe Schindler",
            "body": "An update with the changed nextSeekTerm() semantics from LUCENE-2110.\n\nRobert: Can you test performance again and compare with old?",
            "date": "2009-12-05T19:52:02.258+0000",
            "id": 187
        },
        {
            "author": "Uwe Schindler",
            "body": "There was a bug in the patch before, sorry. I will finish work for today, I am exhausted like the enums.",
            "date": "2009-12-05T20:02:34.025+0000",
            "id": 188
        },
        {
            "author": "Uwe Schindler",
            "body": "Stop everything I get a collaps!!!!! Again wrong patch.",
            "date": "2009-12-05T20:07:07.427+0000",
            "id": 189
        },
        {
            "author": "Uwe Schindler",
            "body": "Now the final one.\n\nI somehow need a test enum which does very strange things like seeking forward and backwards and returning all strange stati.\n\nWill think about one tomorrow.",
            "date": "2009-12-05T20:19:52.738+0000",
            "id": 190
        },
        {
            "author": "Mark Miller",
            "body": "The new WildcardQuery is holding up very well under random testing -\n\nI'm comparing the results of the old WildcardQuery impl with the new WildcardQuery impl.\n\nI'm using a 2 million doc english and 2 million doc french index. (wikipedia dumps)\n\nGenerating random queries - both random short strings built up from random unicode chars mixed with some random wildcards, and random english/french words from dictionaries, randomly chopped or not, with random wildcards injected. A whole lot of crazy randomness.\n\nThey have always produced the same number of results so far (a few hours of running).\n\nThe new impl is generally either a bit faster in these cases, or about the same - at worst (in general), I've seen it about .01s  slower. When its faster, its offten > .1s faster (or more when a few '?' are involved).\n\nOn avg, I'd say the perf is about the same - where the new impl shines appears to be when '?' is used (as I think Robert has mentioned).\n\nSo far I haven't seen any anomalies in time taken or anything of that nature.",
            "date": "2009-12-05T20:39:40.245+0000",
            "id": 191
        },
        {
            "author": "Robert Muir",
            "body": "Mark, thanks for testing!\n\nYes, the new wildcard should really only help for ? with trunk (especially leading ?)\nWith flex it should help a lot more, even leading * gets the benefit of \"common suffix\" and byte[] comparison and things like that.\nThis code is in the trunk patch but does not really help yet because trunk enum works on String.\n\nbtw how many uniq terms is the field you are testing... this is where it starts to help with ?, when you have a ton of unique terms.\nBut I am glad you are testing with hopefully a smaller # of uniq terms, this is probably more common.\n",
            "date": "2009-12-05T20:47:57.954+0000",
            "id": 192
        },
        {
            "author": "Uwe Schindler",
            "body": "Here is the patch with the getEnum/getTermsEnum changes instead of rewrite but with reverted LUCENE-2110, which was stupid.",
            "date": "2009-12-05T20:50:07.959+0000",
            "id": 193
        },
        {
            "author": "Mark Miller",
            "body": "bq. how many uniq terms is the field you are testing\n\nI'm not sure at the moment - but its wikipedia dumps, so I'd guess its rather high actually. It is hitting the standard analyzer going in (mainly because I didn't think about changing it on building the indexes). And the queries are getting hit with the lowercase filter (stole the code anyway).",
            "date": "2009-12-05T20:55:32.236+0000",
            "id": 194
        },
        {
            "author": "Uwe Schindler",
            "body": "again - krr to the hell with the AM/PM bug in JIRA! It is *****xxx***",
            "date": "2009-12-05T20:59:23.138+0000",
            "id": 195
        },
        {
            "author": "Robert Muir",
            "body": "bq. I'm not sure at the moment - but its wikipedia dumps, so I'd guess its rather high actually. \n\nSee the description, I created this for working mainly regexp on indexes with 100M+ unique terms.\nWildcard doesn't get as much benefit, except ? operator and the comparisons being faster (array-based DFA)\n\nI'm pleased to hear its doing so well on such a \"small\" index as wikipedia, as I would think automata overhead would make it slower (although this can probably be optimized away)\n",
            "date": "2009-12-05T21:10:14.886+0000",
            "id": 196
        },
        {
            "author": "Robert Muir",
            "body": "bq. I'm not sure at the moment - but its wikipedia dumps, so I'd guess its rather high actually.\n\nI looked at the wikipedia dump in benchmark (when indexed with standardanalyzer), body only has 65k terms... I think thats pretty small :)\nI do not think automaton will help much with such a small number of terms, its definitely a worst case benchmark you are performing.\nI think very little time is probably spent here in term enumeration so scalability does not matter for that corpus.\n\nMore interesting to see the benefits would be something like indexing geonames data (lots of terms), or even that (much smaller) persian corpus i mentioned with nearly 500k terms... \n",
            "date": "2009-12-05T22:58:55.393+0000",
            "id": 197
        },
        {
            "author": "Mark Miller",
            "body": "bq.  I think thats pretty small\n\nOkay, fair enough ;) Guess it depends on your idea of small - though I would have guess (wrongly it appears), that it would be more. One diff is that I think the bechmark uses a 200mb (zipped) or so dump by default? I'm using a 5 gig dump - though that prob doesn't add too many more in the scheme of things.\n\nbq. More interesting to see the benefits...\n\nRight, but I'm not really testing for benefits - more for correctness and no loss of performance (on a fairly standard corpus). I think the benches you have already done are probably plenty good for benefits testing.",
            "date": "2009-12-05T23:05:55.385+0000",
            "id": 198
        },
        {
            "author": "Robert Muir",
            "body": "bq. Right, but I'm not really testing for benefits - more for correctness and no loss of performance (on a fairly standard corpus). I think the benches you have already done are probably plenty good for benefits testing.\n\noh ok, I didnt know. Because my benchmark as Mike said, is definitely very \"contrived\". \n\nBut its kind of realistic, there are situations where the number of terms compared to the number of docs is much higher (maybe even 1-1 for unique product ids and things like that). \n\nI am glad you did this test, because I was concerned about the \"small index\" case too. And definitely correctness....\n\nI think you are right about the partial dump. I am indexing the full dump now (at least I think). I will look at it too, at least for curiousity sake.\n",
            "date": "2009-12-05T23:10:37.188+0000",
            "id": 199
        },
        {
            "author": "Mark Miller",
            "body": "bq. And definitely correctness....\n\nRight - thats my main motivation - comparing the results of the old wildcardquery with the new - I actually put the timings in there as an after thought - just because I was curious.\n\nI really just wanted to make sure every random query acts the same with both impls and that no random input can somehow screw things up (Im using commons lang to pump in random unicode strings, along with turning the dict entires into wildcards that more likely to get many hits).\n\nDidn't expect to find anything, but it will make me feel better about +1ing the commit ;)\n\nAlso going over the code, but thats going to take more time.",
            "date": "2009-12-05T23:15:43.035+0000",
            "id": 200
        },
        {
            "author": "Robert Muir",
            "body": "Mark oh ok, well thanks for spending so much time here testing and reviewing.\n\nbq. I really just wanted to make sure every random query acts the same with both impls and that no random input can somehow screw things up (Im using commons lang to pump in random unicode strings, along with turning the dict entires into wildcards that more likely to get many hits).\n\nYeah I tried to do some of this in a very quick way if you look at the tests... I generate some random wildcard/regexp queries (mainly to prevent bugs from being introduced).\n\nThe unicode tests (TestAutomatonUnicode) took me quite some time, they are definitely contrived but I think cover the bases for any unicode problems.\nOne problem is that none of this unicode stuff is ever a problem on trunk!\n\nIf you save this test setup, maybe in the future I can trick you into running your tests on flex, where the unicode handling matters (as TermRef must be valid UTF-8 there).\n",
            "date": "2009-12-05T23:25:04.039+0000",
            "id": 201
        },
        {
            "author": "Mark Miller",
            "body": "bq. Yeah I tried to do some of this in a very quick way if you look at the tests... I generate some random wildcard/regexp queries (mainly to prevent bugs from being introduced).\n\nYeah, I think the tests are pretty solid (from the briefs looks I've had thus far) - this is mainly just a precaution - so that we are not surprised by a more realistic corpus. And to have the opportunity to compare with the old WildcardQuery - I'd rather not keep it around for tests - once we are confident its the same (and I am at this point), I'm happy to see it fade into the night. Replacing such a core piece though, I want to be absolutely sure everything is on the level.\n\nbq, they are definitely contrived but I think cover the bases for any unicode problems.\n\nRight - in terms of unit tests, I think you've done great based on what I've seen. This is just throwing more variety at a larger more realistic corpus. More of a one time deal than something that should be incorporated into the tests. Ensures there are no surprises for me - since I didn't write any of this code (and I'm not yet super familiar with it), it helps with my comfort level :)\n\nbq. One problem is that none of this unicode stuff is ever a problem on trunk!\n\nYeah - I assumed not. But as I'm not that familiar with the automaton stuff yet, I wanted to be sure there wasn't going to be any input that somehow confused it. I realize that your familiarity level probably tells you thats not possible - but mine puts me in the position of testing anyway - else I'll look like a moron when I +1 this thing ;)\n\nbq. If you save this test setup, \n\nI'll save it for sure.",
            "date": "2009-12-05T23:48:16.618+0000",
            "id": 202
        },
        {
            "author": "Robert Muir",
            "body": "bq. Also going over the code, but thats going to take more time.\n\nBtw, I will accept any criticism here. I am not happy with the complexity of the enum in the trunk patch, personally.\nBut here are the three main issues that I think make it complex: (not to try to place blame elsewhere)\n\n* This trie<->DFA intersection is inherently something i would want to define recursively, but this would be obviously bad.\n* The DFA library uses UTF-16 whereas TermRef requires UTF-8. Changing automaton to use 'int' would fix this, but then would destroy performance. The reason brics is the fastest java regex library is that it tableizes the DFA into a 64k UTF-16 char[]. See RunAutomaton for the impl. I think making this require 1MB for the corner cases is bad.\n* MultiTermQuerys that seek around are pretty complex in trunk. In my opinion this enum is a lot easier to understand with the improvements Uwe is working on for FilteredTermsEnum (see his branch patch, I think its easier there).\n\nif you have ideas how we can simplify any of this in trunk for easier readability (instead of just adding absurd amounts of comments as I did), I'd be very interested.",
            "date": "2009-12-05T23:49:15.419+0000",
            "id": 203
        },
        {
            "author": "Mark Miller",
            "body": "Sorry - haven't been paying a lot of attention to all of the Unicode issues/talk lately.\n\nCould you briefly explain cleanupPosition? Whats the case where a seek position cannot be converted to UTF-8?\n\n*edit*\n\nBecause of next string guesses that might not be valid UTF-8?",
            "date": "2009-12-06T00:58:27.960+0000",
            "id": 204
        },
        {
            "author": "Robert Muir",
            "body": "yes, Mark you have it right. This is not an issue for trunk, only flex, but I fixed it ahead of time to prevent problems.\n\nso i have a chinese example here: https://issues.apache.org/jira/browse/LUCENE-1606?focusedCommentId=12785034&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12785034\n\nbut at the same time, this kind of thing can happen with a simple wildcard *, or ?\nBecause a wildcard ? will be converted into a UTF-16 DFA transition of \\u0000-\\uFFFF, when enumerating this transition (depending on your term dictionary), we cannot create a nextString for example that ends with say, \\uD800, else it will get replaced with \\uFFFD, and we will miss terms.\n\nThe enumeration in nextString is greedy, so we can't just look for these in final position either. For example, if you have a regexp of [lm]ar[kl], this enum first tries to seek to lark (it will continue to walk transitions until an accept state or a loop).",
            "date": "2009-12-06T01:38:57.871+0000",
            "id": 205
        },
        {
            "author": "Robert Muir",
            "body": "here is an explanation of the cleanupPosition, and the cases it handles\n\nSymbols:\nA = \\u0000 - \\uD7FFF (lower BMP)\nH = \\uD800 - \\uDBFFF (high/lead surrogates)\nL = \\uDC00 - \\uDFFFF (low/trail surrogates)\nZ = \\uE000 - \\uFFFF (upper BMP)\n\ncase 1: \n// an illegal combination, where we have not yet enumerated into the supp. plane\n// so we increment to H + \\uDC00 (the lowest possible trail surrogate)\nHA -> H \\uDC00\nHH -> H \\uDC00\ncase 2:\n// an illegal combination where we have already enumerated the supp. plane\n// we must replace both H and Z with \\uE000 (the lowest possible \"upper BMP\") \nHZ -> \\uE000\ncase 3:\n// an illegal combination where we have a final lead surrogate.\n// we have not yet enumerated the supp plane, so append \\uDC00 (lowest possible trail surrogate)\n// this is just like case 1, except in final position.\nH$ -> H \\uDC00\ncase 4:\n// an unpaired trail surrogate. this is invalid when not preceded by lead surrogate\n// (and if there was one, the above rules would have dealt with it already)\n// in this case we have to bump to \\uE000 (the lowest possible \"upper BMP\")\nunpaired L -> \\uE000\ncase 5:\n// this is just like case 4, its obviously illegal because the term starts with a trail surrogate.\n// (because it is in initial position)\n^L -> \\uE000\n\nedit: sorry for the many edits :)\n",
            "date": "2009-12-06T01:58:40.569+0000",
            "id": 206
        },
        {
            "author": "Robert Muir",
            "body": "in this patch, i take some commented out code in UnicodeUtil (validUTF16String), and pervert it slightly into nextValidUTF16String.\n\nall the tests pass using this on trunk and flex, and I think it reads much easier.\n",
            "date": "2009-12-06T02:43:50.202+0000",
            "id": 207
        },
        {
            "author": "Robert Muir",
            "body": "here is an update to the last one, using UnicodeUtil constants, etc.\n\nSo I think I do not absolutely hate the unicode handling code in this enum anymore.",
            "date": "2009-12-06T03:02:32.008+0000",
            "id": 208
        },
        {
            "author": "Robert Muir",
            "body": "btw one thing we could do is put this nextValidUTF16String in UnicodeUtil and also use it in SegmentReader.LegacyTermEnum to replace the \"hack\", just in case someone else wrote an enum like mine.\n\nthis would provide better backwards compatibility, as they would receive the 'next Term' in IndexReader.terms() just as they did before, even if the term is completely jacked...\n\nbelow is the existing hack:\n\n{code}\n   // this is a hack only for backwards compatibility.\n   // previously you could supply a term ending with a lead surrogate,\n   // and it would return the next Term.\n   // if someone does this, tack on the lowest possible trail surrogate.\n   // this emulates the old behavior, and forms \"valid UTF-8\" unicode.\n   if (text.length() > 0 \n      && Character.isHighSurrogate(text.charAt(text.length() - 1)))\n        tr = new TermRef(t.text() + \"\\uDC00\");\n   else\n        tr = new TermRef(t.text());\n{code}\n\ninstead it could read something like tr = new TermRef(UnicodeUtil.nextValidUTF16String(t.text()));\n",
            "date": "2009-12-06T03:22:35.713+0000",
            "id": 209
        },
        {
            "author": "Michael McCandless",
            "body": "bq. one thing we could do is put this nextValidUTF16String in UnicodeUtil and also use it in SegmentReader.LegacyTermEnum to replace the \"hack\", just in case someone else wrote an enum like mine.\n\n+1",
            "date": "2009-12-06T09:03:40.192+0000",
            "id": 210
        },
        {
            "author": "Robert Muir",
            "body": "Mike I created LUCENE-2121 for this.\nIf you get a chance to review it, I can create a new version of the flex branch patch for this issue... this would resolve one of my \"big 3 complaints\" about complexity of the code.\n",
            "date": "2009-12-06T13:34:16.601+0000",
            "id": 211
        },
        {
            "author": "Robert Muir",
            "body": "Setting this issue as depending on LUCENE-2111\n\nThis is because I do not feel comfortable committing this code in trunk, it is too complicated there.\nInstead I would like simply work it against the flex branch where we can make it nice",
            "date": "2009-12-06T17:17:38.246+0000",
            "id": 212
        },
        {
            "author": "Robert Muir",
            "body": "latest flex patch, after LUCENE-2110 and LUCENE-2121",
            "date": "2009-12-06T22:29:03.232+0000",
            "id": 213
        },
        {
            "author": "Robert Muir",
            "body": "the current wildcardquery has getTerm(), this is needed for bw compat. I added it.",
            "date": "2009-12-06T22:47:26.617+0000",
            "id": 214
        },
        {
            "author": "Robert Muir",
            "body": "this is an update to improve performance for lots of seeking (wildcards like ?????NN, crazy regular expressions, fuzzy)\n\n* expose State.getNumber() as an expert method. this returns the consecutive integer of the state in the automaton.\n* (re)use a bitset for tracking 'visited' instead of creating a hashset for each seek\n* use an array indexed by state number for caching transitions, instead of a hashmap.\n",
            "date": "2009-12-07T14:34:29.413+0000",
            "id": 215
        },
        {
            "author": "Robert Muir",
            "body": "this builds off the last improvement, and uses RunAutomaton (the tablelized DFA) for nextString.\n\nthis means parts of nextString are now O(n) where n is number of states, instead of transitions.\ndoesn't mean much for wildcard as these DFAs typically do not have many transitions per state, but for more complex DFAs such as regexp/wildcard this helps... and it simplifies code.\n",
            "date": "2009-12-07T18:42:26.146+0000",
            "id": 216
        },
        {
            "author": "Robert Muir",
            "body": "here i removed the use of String in the enum.\nthis seems to help a bit when seeking.\n\ninstead a char[] is reused, and nextString() etc returns boolean if more solutions exist.\nI think its actually more readable in a way, need to reorganize a bit more but I need a break from this enum.\n",
            "date": "2009-12-07T22:20:41.855+0000",
            "id": 217
        },
        {
            "author": "Uwe Schindler",
            "body": "I updated the patch because of my last commit.\nYour's looks good, my change was only adding the method param and removing the access to the noew private tenum.",
            "date": "2009-12-08T10:50:51.441+0000",
            "id": 218
        },
        {
            "author": "Robert Muir",
            "body": "So what do you guys think? I am pretty satisfied with how this enum looks in flex branch myself.\n\nI think it would be nice to start looking at committing this to flex so we do not have to work with huge patches?\n\nIf there are reservations please speak up, I think the automaton code imported here is solid, and if you run clover you will see our testcases exercise the important bits (not .toString or .toDot <graphviz> or other things, but those work too).\n\nIf you have concerns or think it is confusing, i will do my best to try to figure out ways to simplify or improve it from here.\n",
            "date": "2009-12-08T18:00:26.998+0000",
            "id": 219
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I think it would be nice to start looking at committing this to flex so we do not have to work with huge patches?\n\n+1",
            "date": "2009-12-08T18:57:01.144+0000",
            "id": 220
        },
        {
            "author": "Robert Muir",
            "body": "Ok, if no one objects I will (heavy) commit this to the flex branch tomorrow.\n\nThe only differences from Uwe's patch will be:\n* ensure the barred-O (\u00f8) is corrrect in Anders name for the NOTICE.txt\n* remove the unused instance variable in the enum, as it is unused and irrelevant for FilteredTermsEnum\n",
            "date": "2009-12-08T19:28:37.142+0000",
            "id": 221
        },
        {
            "author": "Robert Muir",
            "body": "Committed revision 888891.",
            "date": "2009-12-09T17:46:34.645+0000",
            "id": 222
        },
        {
            "author": "Robert Muir",
            "body": "btw, Thanks to Uwe, Mike, Mark for all the help here!\n",
            "date": "2009-12-09T17:53:49.143+0000",
            "id": 223
        }
    ],
    "component": "core/search",
    "description": "Attached is a patch for an AutomatonQuery/Filter (name can change if its not suitable).\n\nWhereas the out-of-box contrib RegexQuery is nice, I have some very large indexes (100M+ unique tokens) where queries are quite slow, 2 minutes, etc. Additionally all of the existing RegexQuery implementations in Lucene are really slow if there is no constant prefix. This implementation does not depend upon constant prefix, and runs the same query in 640ms.\n\nSome use cases I envision:\n 1. lexicography/etc on large text corpora\n 2. looking for things such as urls where the prefix is not constant (http:// or ftp://)\n\nThe Filter uses the BRICS package (http://www.brics.dk/automaton/) to convert regular expressions into a DFA. Then, the filter \"enumerates\" terms in a special way, by using the underlying state machine. Here is my short description from the comments:\n\n     The algorithm here is pretty basic. Enumerate terms but instead of a binary accept/reject do:\n      \n     1. Look at the portion that is OK (did not enter a reject state in the DFA)\n     2. Generate the next possible String and seek to that.\n\nthe Query simply wraps the filter with ConstantScoreQuery.\n\nI did not include the automaton.jar inside the patch but it can be downloaded from http://www.brics.dk/automaton/ and is BSD-licensed.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1606",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Automaton Query/Filter (scalable regex)",
    "systemSpecification": true,
    "version": ""
}