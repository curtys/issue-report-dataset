{
    "comments": [
        {
            "author": "Robert Muir",
            "body": "Trivial test: if you add computeCurrentState() unconditionally as the first line of AttributeSource.clearAttributes, the test will pass.\n\n{noformat}\npublic class SimpleTest extends BaseTokenStreamTestCase {\n  public void testSimple() throws IOException {\n    String testString = \"t\";\n    \n    Analyzer analyzer = new MockAnalyzer(random);\n    TokenStream stream = analyzer.reusableTokenStream(\"dummy\", new StringReader(testString));\n    stream.reset();\n    while (stream.incrementToken()) {\n      // consume\n    }\n    \n    assertAnalyzesToReuse(analyzer, testString, new String[] { \"t\" });\n  }\n}\n{noformat}",
            "date": "2011-04-22T19:31:09.886+0000",
            "id": 0
        },
        {
            "author": "Uwe Schindler",
            "body": "Now I am totally confused, this is impossible to my knowledge. :(\n\nI will look into it.",
            "date": "2011-04-22T19:35:01.272+0000",
            "id": 1
        },
        {
            "author": "Uwe Schindler",
            "body": "I found the bug, its totally confusing but is a design error in the state caching originated by Michael Busch:\n- Tokenizer and all Filters share the same Map instances for the attributes\n- BUT: Tokenizer and every filter have its own cached state\n- if you call clearAttributes on the filter, it clears its own state, but not the one of its parent tokenizer\n- if tokenizer calls clearAttributes, nothing is done\n\nFix is to make the AttributeSourcesnot only share the maps, but also the state",
            "date": "2011-04-22T20:37:14.484+0000",
            "id": 2
        },
        {
            "author": "Uwe Schindler",
            "body": "Here a quick patch using a reference (1-ele array)  to the state that is shared between all AttributeSources",
            "date": "2011-04-22T20:50:46.428+0000",
            "id": 3
        },
        {
            "author": "Uwe Schindler",
            "body": "New patch, that makes the code simplier to read:\n\n- I replaced the computeCurrentState by a  getter method that does the check and returns the state.\n- This makes it easier to use and removes lots of checks even for hasAttributes()\n- Now, most methods using State now only consist of the for Loop.",
            "date": "2011-04-22T21:13:20.147+0000",
            "id": 4
        },
        {
            "author": "Uwe Schindler",
            "body": "Repost patch, first had logic error.",
            "date": "2011-04-22T21:20:26.727+0000",
            "id": 5
        },
        {
            "author": "Uwe Schindler",
            "body": "Just to conclude:\nThis bug is not so serious as it appears (else someone would have noticed before), as it would never happen on 0-8-15 TokenStreams, when used like IndexWriter does.\nThis bug only appears if you have TokenFilters and you add Attributes on the top level Filter later (after using the TokenStream for first time). Using the TokenStream means that you calculate the states and so every Filter/Tokenizer got his own cached state. Adding them a new Attribute on the last filter will never invalidate the cache of the Tokenizer.\n\nThis bug could affect:\n- Analyzers that reuse TokenStreams partly and plug filters on top in the reuseableTokenStream() method, reusing the partially cached tokenstream. Like those, that always add a non-cacheable TokenFilter on top of a base TS.\n- TokenStreams that add attributes on the-fly in one of their filters.\n\nWe should backport this patch to 3.x, 3.1.1 and maybe even 2.9.x and 3.0.x branches (if somebody wants to patch 3.0). In general this is a serious issue of the new TokenStream API since 2.9.\n",
            "date": "2011-04-22T21:44:36.937+0000",
            "id": 6
        },
        {
            "author": "Uwe Schindler",
            "body": "Committed trunk revision: 1096073, 1096077\nCommitted 3.x revision: 1096078\n\nPlease reopen when 3.1.1 or 2.9.5 should be released.",
            "date": "2011-04-22T23:08:10.686+0000",
            "id": 7
        },
        {
            "author": "Uwe Schindler",
            "body": "I thought about it:\nThis bug is so serious it should be fixed in all branches, too (even if never released anymore). This is important for e.g. 2.9 users whcih are stuck with that version.\n\nCommitted 3.1 branch revision: 1096127\nCommitted 3.0 branch revision: 1096128\nCommitted 2.9 branch revision: 1096129",
            "date": "2011-04-23T10:05:32.310+0000",
            "id": 8
        },
        {
            "author": "Robert Muir",
            "body": "Bulk closing for 3.2",
            "date": "2011-06-03T16:37:15.078+0000",
            "id": 9
        }
    ],
    "component": "modules/analysis",
    "description": "If you work a tokenstream, consume it, then reuse it and add an attribute to it, the computed state is wrong.\nthus for example, clearAttributes() will not actually clear the attribute added.\n\nSo in some situations, addAttribute is not actually clearing the computed state when it should.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-3042",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Critical",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "AttributeSource can have an invalid computed state",
    "systemSpecification": true,
    "version": "2.9.4, 3.0.3, 3.1, 3.2, 4.0-ALPHA"
}