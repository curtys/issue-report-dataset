{
    "comments": [
        {
            "author": "DM Smith",
            "body": "The following has been addressed in this patch:\n1. JavaDoc is improved (as always, there is still room for improvement. For example, it says the field type is interned, but it is not.)\n\n2. Deprecated the Token constructors taking a String.\n\n3. Changed the allocation policy to be less aggressive.\n\n5. Optimized the growing of the internal termBuffer immediately followed by storing a new term. In doing this added, setTermBuffer(String) and setTermBuffer(String, int, int). Setting from a string is roughly the same cost as setting from a char[].\n\n6. TokenStream has next() has been deprecated. The javadoc has been updated to recommend next(Token) over next().\n\n7. Rather than modifying Term to take a Token, public String term() has been added. With termText() still deprecated, this gives upgraders a clean choice to use term() or termBuffer(), with the knowledge of the performance differences.\n\nI also updated TestToken to test all the changes.\n\nLeft to do: (I'd like to get a nod of whether I need to make further changes to the supplied patch before doing #4)\n4. Changing of the remainder of core and contrib to remove calls to deprecated Token and TokenStream methods, i.e. to use the reuse API.\n\n",
            "date": "2008-07-14T21:28:26.367+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "\nThis patch looks good; thanks DM!\n\nI made a few small changes & attached a new rev of the patch:\n\n  * Fixed Token.setTermLength to throw IllegalArgumentException if you\n    pass in a length > termBuffer.length\n\n  * Changed Token.growTermBuffer to use oal.util.ArrayUtil (it has\n    that same growth logic)\n\n  * Changed if statements in Token.growTermBuffer to first handle the\n    [I think most frequent] case where termBuffer is already\n    allocated.\n\n  * Javadoc/whitespace\n",
            "date": "2008-07-30T15:30:04.850+0000",
            "id": 1
        },
        {
            "author": "DM Smith",
            "body": "Better comments based upon migrating all of core and contrib. This patch replaces the prior two.\n\nMarked fields as deprecated, with documentation that they should be made private and setters/getters should be used instead.",
            "date": "2008-08-04T20:01:43.652+0000",
            "id": 2
        },
        {
            "author": "DM Smith",
            "body": "I've broken up the changes to core and contrib into small patches. My reasoning for this is that it is a lot easier for me to keep up with other people's changes and re-issue a small patch.\n\nApply LUCENE-1333.patch before any of the following. I don't know if there is an order dependency for any of the following.\n\nThis one is for core in o.a.l.analysis but not the files contained in LUCENE-1333.patch",
            "date": "2008-08-04T20:07:22.471+0000",
            "id": 3
        },
        {
            "author": "DM Smith",
            "body": "This patch covers the rest of core Lucene.",
            "date": "2008-08-04T20:07:58.107+0000",
            "id": 4
        },
        {
            "author": "DM Smith",
            "body": "This is for contrib analyzers.",
            "date": "2008-08-04T20:08:42.300+0000",
            "id": 5
        },
        {
            "author": "DM Smith",
            "body": "for contrib snowball",
            "date": "2008-08-04T20:09:19.425+0000",
            "id": 6
        },
        {
            "author": "DM Smith",
            "body": "for contrib highlighter",
            "date": "2008-08-04T20:09:51.492+0000",
            "id": 7
        },
        {
            "author": "DM Smith",
            "body": "For contrib instantiated",
            "date": "2008-08-04T20:10:26.802+0000",
            "id": 8
        },
        {
            "author": "DM Smith",
            "body": "for contrib lucli",
            "date": "2008-08-04T20:10:54.361+0000",
            "id": 9
        },
        {
            "author": "DM Smith",
            "body": "for contrib memory",
            "date": "2008-08-04T20:11:14.659+0000",
            "id": 10
        },
        {
            "author": "DM Smith",
            "body": "for contrib misc",
            "date": "2008-08-04T20:11:43.349+0000",
            "id": 11
        },
        {
            "author": "DM Smith",
            "body": "for contrib queries",
            "date": "2008-08-04T20:12:03.392+0000",
            "id": 12
        },
        {
            "author": "DM Smith",
            "body": "for contrib wikipedia",
            "date": "2008-08-04T20:12:24.151+0000",
            "id": 13
        },
        {
            "author": "DM Smith",
            "body": "for contrib wordnet",
            "date": "2008-08-04T20:12:43.517+0000",
            "id": 14
        },
        {
            "author": "DM Smith",
            "body": "for contrib xml-query-parser",
            "date": "2008-08-04T20:13:10.946+0000",
            "id": 15
        },
        {
            "author": "DM Smith",
            "body": "All the code has been migrated to use the reuse interface. I have run all the tests and they pass. (There is a weird dependency. One test depends upon demo. I hacked that to work.) I did not test to see if the code were any slower or faster. I'm not sure how one would do that. I think it should be faster since it doesn't bounce back and forth with termText and termBuffer.\n\nI did not improve the code to use char[] instead of String. The places that can be improved call Token.setTermBuffer(String). I don't think these are necessary to this issue being resolved.\n\nA couple of other minor opportunities for char[] manipulation via o.a.l.util.ArrayUtil.\n\nHere and there, there is a need to remove leading and trailing whitespace from input before (sub-)tokenizing it. Currently this is done with String.trim(), even when working with char[]. It is sub-optimal as marshalling it into a String involves allocation and copying. Likewise, so does getting it out. It would be better to have int trim(char[]) which shifts leading spaces off the front and returns the length of the string without trailing spaces.\n\nThere is a \"randomize\" routine that shuffles an array. While this is only used in one place, it appears to be general purpose array manipulation.",
            "date": "2008-08-04T20:45:38.042+0000",
            "id": 16
        },
        {
            "author": "DM Smith",
            "body": "This issue supersedes LUCENE-1350, incorporating all the changes in it. This either needs to go in after it or instead of it.",
            "date": "2008-08-06T21:48:27.247+0000",
            "id": 17
        },
        {
            "author": "DM Smith",
            "body": "This patch includes all the previous ones.\n\nNote: It includes the functionality solving  LUCENE-1350. If this patch is applied before LUCENE-1350, then that issue is resolved. If it is done after then the patch will need to be rebuilt.\n\nI did not do the \"reuse\" API mentioned in LUCENE-1350.",
            "date": "2008-08-08T21:03:21.829+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "OK since you pulled it all together under this issue, I think we should commit this one instead of LUCENE-1350.  I'll review the [massive] patch -- thanks DM!",
            "date": "2008-08-08T23:59:58.077+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "DM, one pattern that makes me nervous is this, from QueryTermVector.java:\n\n{code}\n          for (Token next = stream.next(new Token()); next != null; next = stream.next(next)) {\n{code}\n\nI don't think you should be \"recycling\" that next and passing it back in the next time you call stream.next, because a TokenStream is not *required* to use the Token you had passed in and so suddenly you are potentially asking it to re-use a token it had previously returned, which it may not expect.  Likely it won't matter but I think this is still safer:\n\n{code}\n          final Token result = new Token();\n          for (Token next = stream.next(result); next != null; next = stream.next(result)) {\n{code}",
            "date": "2008-08-09T10:12:02.390+0000",
            "id": 20
        },
        {
            "author": "DM Smith",
            "body": "I'll give my analysis here. Feel free to make the change or kick it back to me to make it, if you think your pattern is best. (If I do it, it will be after this weekend.)\n\nI've tried to be consistent here. The prior pattern was inconsistent and often was:\n{code}\n Token token = null;\n  while ((token = input.next()) != null) {\n{code}\n\nThere were other variants including \"forever loops\". As you noticed, I replace\nThere are two basic implementations of Token next(Token):\n1) Producer: These create tokens from input. Their pattern is to take their argument and call clear on it and then set startOffset, endOffset and type appropriately. Their assumption is that they have to start with a pristine token and that other than space, there is nothing about the token that is passed in that can be reused.\n\n2) Consumer: These \"filter\" their argument. Their only assumption is that in the call chain that there was a producer that created the token that they need to reuse. In this case, they typically will preserve startOffset and endOffset because those are to represent the position of the token in the input. They may refine type, flags and payload, but otherwise have to preserve them. Most typically, they will set the termBuffer. There are a few types of consumers. Here are some:\na) Transformational Filters: They take their argument and transform it's termBuffer.\nb) Splitting Filters: They take their argument and split the token into several. Sometimes they will return the original; other times just the parts. When creating these tokens, calling clone() on the prototype will preserve flags, payloads, start and end offsets and type. These clones are sometimes stored in a buffer, but sometimes are incrementally computed with each call to next(Token). With the latter, they will typically cache a clone of the passed in token. I think that, when possible, incremental computation is preferable, but at the cost of a less obvious implementation.\nc) Caching Filter: If their buffer is empty, they repeatedly call result = input.next(token), clone and buffer cache their result in some collection. Once full, they will return their buffer's content. If, the caching filter is resettable, they must return clones of their content. Otherwise, down stream consumers may change their arguments, disastrously.\n\nCallers of Token next(Token) have the responsibility of never calling with a null token. (I think producer tokens probably should check and create a token if it is so. But I don't think that is what they do now.)\n\nThe upshot of all of this, Producers don't care which token they reuse. If it was from the original loop, or from the result of the last call to token = stream.next(token), both are equally good. The token pre-existed and needs to be fully reset. Consumers presume that the token was produced (or at least appropriately re-initialized and filled in) by a producer.\n\nYour form of the loop is very advisable in a few places. Most typically with a loop within a loop, with the inner looping over all the tokens in a stream. In this case, the final Token would be created outside the outer loop. Using your pattern, there would encourage maximal reuse. Using mine, the programmer would have to figure out when it was appropriate to do one or the other.\n\nThe other value to your pattern is that next(Token) is always called with a non-null Token.\n\nI think that calling the token \"result\" is not the best. It is a bit confusing as it is not the result of calling next(Token). Perhaps, to make reuse acutely obvious:\n{code}\n final Token reusableToken = new Token();\n for (Token token = stream.next(reusableToken); token != null; token = stream.next(reusableToken)) {\n{code}\n\n",
            "date": "2008-08-09T12:16:29.814+0000",
            "id": 21
        },
        {
            "author": "Doron Cohen",
            "body": "This 'final' pattern is indeed more clear about reuse.\n\nBut still would like to clarify on what can the TokenStream assume. I think \nTokenStream cannot assume anything about the token it gets as input, and, \nonce it returned a token, it cannot assume anything about how that token \nis used.  So why should it not expect being passed the token it just returned?\n",
            "date": "2008-08-09T20:22:56.791+0000",
            "id": 22
        },
        {
            "author": "Doron Cohen",
            "body": "The new patch applies cleanly.\nA few tests are failing though - TestChineseTokenizer for one.\nI'm looking into it, might be that a problem is in the test.\n",
            "date": "2008-08-10T08:56:54.692+0000",
            "id": 23
        },
        {
            "author": "Doron Cohen",
            "body": "Seems start/end offset were lost in ChineseTokenizer.\nAdding this in flush(Token) will fix it.\n{code}\ntoken.setStartOffset(start);\ntoken.setEndOffset(start+length);\n{code}\nPossibly true for other places where new Token() that takes start/end offsets was replaced by clear() and setTermBuffer().",
            "date": "2008-08-10T10:14:59.744+0000",
            "id": 24
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nBut still would like to clarify on what can the TokenStream assume. I think\nTokenStream cannot assume anything about the token it gets as input, and,\nonce it returned a token, it cannot assume anything about how that token\nis used. So why should it not expect being passed the token it just returned?\n{quote}\n\n{quote}\nThe upshot of all of this, Producers don't care which token they reuse.\n{quote}\n\nI agree -- technically speaking, whenever a Token is returned from a source/filter's next(Token) method, *anything* is allowed to happen do it (including any & all changes, and subsequent reuse in future calls to next(Token)) and so the current pattern will run correctly if all sources & filters are implemented correctly. This is the contract in the reuse API.\n\nIt's just that it looks spooky, when you are consuming tokens, not to create & re-use your own reusable token.  I think it's also possible (but not sure) that the JRE can compile/run the \"single reusable token\" pattern more efficienctly, since you are making many method calls with a constant (for the life time of the for loop) single argument, but this is pure speculation on my part...\n\nI think from a code-smell standpoint I'd still like to use the \"single re-use\" pattern when applicable.  DM I'll make this change & post a new patch.",
            "date": "2008-08-10T10:30:40.235+0000",
            "id": 25
        },
        {
            "author": "Michael McCandless",
            "body": "I'm also seeing the test failures.",
            "date": "2008-08-10T10:31:21.815+0000",
            "id": 26
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Seems start/end offset were lost in ChineseTokenizer.\n\nOK, I'll fold this into my changes to the patch.  Thanks Doron!",
            "date": "2008-08-10T11:54:39.496+0000",
            "id": 27
        },
        {
            "author": "Michael McCandless",
            "body": "bq. (I think producer tokens probably should check and create a token if it is so. But I don't think that is what they do now.)\n\nActually I think one should never pass null to next(Token) API -- ie a source token stream need not check for null.",
            "date": "2008-08-10T12:02:53.726+0000",
            "id": 28
        },
        {
            "author": "Doron Cohen",
            "body": "Thanks Mike!\n\nOne more...\n\nWhile looking into the failure of TestSingleTokenTokenFilter I saw that if these lines are executed:\n{code}\nToken t1 = new Token();\nToken t2 = t1.clone();\nboolean isEqual = t1.equals(t2)\n{code}\nSurprisingly,   *isEqual = false*\n\nDo you see this too?\n\n",
            "date": "2008-08-10T12:06:25.262+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Do you see this too?\n\nHmm, indeed I do see this too -- but this is because Token has never overridden \"equals\" right? ",
            "date": "2008-08-10T12:13:53.479+0000",
            "id": 30
        },
        {
            "author": "Doron Cohen",
            "body": "{quote}\nActually I think one should never pass null to next(Token) API - ie a source token stream need not check for null.\n{quote}\nFunny I was about to comment on the same thing... \n\nIn fact when the reuse API was introduced I believe null was suppose to mean - \"nothing to be reused, please just create your own\". \n\nIn case of a producer that cannot reuse - say it creates its own implementation of Token - then there is no point in creating tokens by the consumer that will never be reused. \n\nBut this also meant that in all the common cases, all tokenizers would need an additional if() to verify that the reusable token is not null. Not so nice. \n\nSo yes, I agree with you, just need to clarify this in TokenStream.nextToken(Token)'s javadocs.",
            "date": "2008-08-10T12:16:26.083+0000",
            "id": 31
        },
        {
            "author": "Michael McCandless",
            "body": "bq. just need to clarify this in TokenStream.nextToken(Token)'s javadocs.\n\nI agree; I'll do this in next patch iteration...",
            "date": "2008-08-10T12:22:26.220+0000",
            "id": 32
        },
        {
            "author": "Doron Cohen",
            "body": "{quote}\nHmm, indeed I do see this too - but this is because Token has never overridden \"equals\" right?\n{quote}\n\nYes you're right. I was under the impression for a moment Object's equals() works like clone() and goes in one layer only... that's stupid of course, it just compares the object references (or nulls).  I wonder how this test ever passed before... Oh I see it now, - trunk's of SingleTokenTokenStream never called Token.clone() while patched version calls it twice. So definitely, Token should implement equals().",
            "date": "2008-08-10T12:25:53.543+0000",
            "id": 33
        },
        {
            "author": "Michael McCandless",
            "body": "bq. So definitely, Token should implement equals().\n\nI agree.  This is technically a break in backwards compatibility, but I think it's OK?",
            "date": "2008-08-10T12:43:46.979+0000",
            "id": 34
        },
        {
            "author": "Doron Cohen",
            "body": "{quote}\nThis is technically a break in backwards compatibility, but I think it's OK?\n{quote}\nI think so. \nI feel good about this whole change, it will make reuse chain more clear, especially once all deprecations can be removed.",
            "date": "2008-08-10T13:00:07.088+0000",
            "id": 35
        },
        {
            "author": "Doron Cohen",
            "body": "I couldn't leave this without seeing that test passing... so here is the code for Token.equals() in case you didn't write that part yet -\n{code}\n  @Override\n  public boolean equals(Object obj) {\n    if (this == obj)\n      return true;\n   \n    Token other = (Token) obj;\n    \n    return\n      termLength == other.termLength &&\n      startOffset == other.startOffset &&  \n      endOffset == other.endOffset &&\n      flags == other.flags &&\n      positionIncrement == other.positionIncrement && \n      subEqual(termBuffer, other.termBuffer) &&\n      subEqual(type, other.type) &&\n      subEqual(payload, other.payload);\n  }\n\n  private boolean subEqual(Object o1, Object o2) {\n    if (o1==null)\n      return o2==null; \n    return o1.equals(o2);\n  }\n{code}",
            "date": "2008-08-10T13:07:57.927+0000",
            "id": 36
        },
        {
            "author": "Michael McCandless",
            "body": "I'm unable to get javacc to run cleanly on PrecedenceQueryParser.jj.  It produces this:\n\n{code}\nbash-3.2$ javacc ./src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj\nJava Compiler Compiler Version 4.0 (Parser Generator)\n(type \"javacc\" with no arguments for help)\nReading from file ./src/java/org/apache/lucene/queryParser/precedence/PrecedenceQueryParser.jj . . .\norg.javacc.parser.ParseException: Encountered \"<<\" at line 658, column 3.\nWas expecting one of:\n    <STRING_LITERAL> ...\n    \"<\" ...\n    \nDetected 1 errors and 0 warnings.\n{code}\n\nDoes anyone else hit this?",
            "date": "2008-08-10T14:29:48.901+0000",
            "id": 37
        },
        {
            "author": "Michael McCandless",
            "body": "bq. here is the code for Token.equals() in case you didn't write that part yet\n\nThanks Doron; I'll merge into my version :)  Both your version and my version had bugs, so it's great you posted yours!  And I hope the merged result has no bugs :)",
            "date": "2008-08-10T14:36:23.959+0000",
            "id": 38
        },
        {
            "author": "Doron Cohen",
            "body": "ok now I'm curious cause I still can't see that bug... :-)\n\nFor PrecedenceQueryParser, Is there a way to use ant to run javacc on contrib/misc? ",
            "date": "2008-08-10T14:47:41.685+0000",
            "id": 39
        },
        {
            "author": "Michael McCandless",
            "body": "\nOK new patch attached:\n\n  * Created Token.equals and Payloads.equals -- this fixed\n    TestSingleTokenTokenFilter.\n\n  * Switched to the \"final reusableToken\" pattern.\n\n  * Added to TokenStream.next javadoc stating that parameter should\n    never be null\n\n  * Fixed test failures (all tests should pass now)\n",
            "date": "2008-08-10T14:50:01.595+0000",
            "id": 40
        },
        {
            "author": "Michael McCandless",
            "body": "bq. For PrecedenceQueryParser, Is there a way to use ant to run javacc on contrib/misc? \n\nI couldn't find a way... there is a TODO comment in it's build.xml saying something about it though!\n\nbq. ok now I'm curious cause I still can't see that bug...\n\nOK I'll let you diff :)  And please look closely to see if there are any other bugs!  These equals() methods are tricky!",
            "date": "2008-08-10T14:51:14.435+0000",
            "id": 41
        },
        {
            "author": "Doron Cohen",
            "body": "{quote}\nFor PrecedenceQueryParser... there is a TODO comment in it's build.xml saying something about it though!\n{quote}\nYes I saw that, but since I don't have a javacc executable.\nMaybe I'll look at that build.xml later.\n\n{quote}\nOK I'll let you diff\n{quote}\nOK I can see them now... you were right as usual :-)\n\nPatch applies cleanly and I'm running the test now, so far all looks good but my PC is not that fast and it will take some more time to complete. I hope to review later tonight or tomorrow morning.\n\n\n",
            "date": "2008-08-10T15:13:12.756+0000",
            "id": 42
        },
        {
            "author": "DM Smith",
            "body": "Back from a trip. Would have jumped in to help, but it looks like you've found and fixed a couple of my mistakes/oversights. Thanks.\n\nRegarding the equals implementation. Perhaps it would also be good to finish the canonical implementation and provide hashcode? That would allow for storage in sets and maps. (Not terribly sure how that would be useful. But I can imagine sorting on startOffsets to organize the contents.)\n\nRegarding the changes to the jj files. I also made the corresponding changes in their generate java files. While it would be good to fix the generation problem, you can compare the jj and java pairs to see that they match.\n\nI agree that Token next(Token) should assume a non-null. Perhaps an assert in a producer is a good idea. I find it easier to debug a failed assert than a null pointer exception.",
            "date": "2008-08-10T23:16:22.244+0000",
            "id": 43
        },
        {
            "author": "Doron Cohen",
            "body": "All tests pass here. \n131 files were modified - I reviewed core and part of contrib, with minor comments only:\n\n* CompoundWordTokenFilterBase - createToken() calls clone() which deep \n   copies the char array, and then calls also setTermBuffer() which iterates the chars again.\n   This can come to play in some analyzers. Usually I would ignore things like this, but this \n   issue is so much about reusing and avoiding unneeded reallocation that I decided to \n   bring this up. Actually there is no reallocation, just re-copying, so maybe it is not too bad?\n** Similarly in NgramTokenFilter - seems termBuffer would be copied twice for each \"secondary\" token.\n\n* Cloning: behavior regarding cloning is modified in few places. \n**   In SingleTokenTokenStream - adding cloning - I think it is correct.\n**   In TokenTypeSinkTokenizer.add(Token) cloning was removed because it is\n      taken care of in super.add(). I first thought it a bug but no, patch is correct.\n\n* I find the comment in TokenFilter about implementing next() vs. next(Token) confusing.\n  Perhaps use here the same comment as in Tokenizer.\n",
            "date": "2008-08-11T09:07:28.397+0000",
            "id": 44
        },
        {
            "author": "Doron Cohen",
            "body": "I think that LUCENE-1350's failing test can be included here?\n\nTestSnowball.testFilterTokens() then changes to:\n\n{code}\n  public void testFilterTokens() throws Exception {\n    final Token tok = new Token(2, 7, \"wrd\");\n    tok.setTermBuffer(\"accents\");\n    tok.setPositionIncrement(3);\n    Payload tokPayload = new Payload(new byte[]{0,1,2,3});\n    tok.setPayload(tokPayload);\n    int tokFlags = 77;\n    tok.setFlags(tokFlags);\n\n    SnowballFilter filter = new SnowballFilter(\n        new TokenStream() {\n          public Token next(Token token) {\n            return tok;\n          }\n        },\n        \"English\"\n    );\n\n    Token newtok = filter.next(new Token());\n\n    assertEquals(\"accent\", newtok.term());\n    assertEquals(2, newtok.startOffset());\n    assertEquals(7, newtok.endOffset());\n    assertEquals(\"wrd\", newtok.type());\n    assertEquals(3, newtok.getPositionIncrement());\n    assertEquals(tokFlags, newtok.getFlags());\n    assertEquals(tokPayload, newtok.getPayload());\n  }\n{code}\n",
            "date": "2008-08-11T09:39:49.927+0000",
            "id": 45
        },
        {
            "author": "Michael McCandless",
            "body": "\nI forgot to say (in last patch): I also added Token.reinit() methods.\n\nbq. I think that LUCENE-1350's failing test can be included here? \n\nOK I'll update this test case.\n\n{quote}\nI find the comment in TokenFilter about implementing next() vs. next(Token) confusing.\nPerhaps use here the same comment as in Tokenizer.\n{quote}\n\nOK I'll update comment in TokenFilter.\n\nbq. I also made the corresponding changes in their generate java files. While it would be good to fix the generation problem, you can compare the jj and java pairs to see that they match.\n\nI ended up doing the same.\n\nbq. Perhaps it would also be good to finish the canonical implementation and provide hashcode?\n\nOK I'll add.\n\nbq. Perhaps an assert in a producer is a good idea. I find it easier to debug a failed assert than a null pointer exception.\n\nAgreed, I'll add.\n",
            "date": "2008-08-11T11:05:29.574+0000",
            "id": 46
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nCompoundWordTokenFilterBase - createToken() calls clone() which deep\ncopies the char array, and then calls also setTermBuffer() which iterates the chars again.\n{quote}\nMaybe we need a variant of clone that takes a new term buffer & start/end offsets, and creates a new token but with the new term buffer & start/end offsets you've passed in?  Analagous to reinit, but first making a cloned token.",
            "date": "2008-08-11T11:08:51.019+0000",
            "id": 47
        },
        {
            "author": "Michael McCandless",
            "body": "Attached new patch w/ above changes.",
            "date": "2008-08-11T11:44:13.817+0000",
            "id": 48
        },
        {
            "author": "DM Smith",
            "body": "Caching flies in the face of reuse. I think that a comment needs to be some where to that effect.\nPutting Tokens into a collection requires that the reusable token be copied. I.e. via new or clone. One cannot directly store the reusable Token, i.e. the argument from Token next(Token), nor the value to be returned from it.\n\nIf a caching TokenStream is also resettable, then that means that the tokens coming from it need to be protected from being changed. This means that they need to return a copy. (Perhaps comment reset() to that effect?)\n\nThe only value I see in caching is if the computation of the token stream is so expensive that re-using it has a significant savings.\n\n(The current code does not take such care and is a bug. This patch fixes it. It would have become obvious if the cache were used in the context of reuse.)\n\nSome TokenStreams cache Tokens internally (as a detail of their implementation) and then return them incrementally. Many of these can be rewritten to compute the next Token when next(Token) is called. This would improve both time and space usage.\n\n(I felt that such changes were outside the scope of this patch.)\n\nAll this leads to my response to the NGram filter.\n\nThe NGram filters could be improved in this fashion. This would eliminate the clone() problem noted above.\n\nBut failing that, a variant of clone to solve the intermediate problem would work. So would using new Token(...). The problem with using new Token() is that it requires manual propagation of flags, payloads, offsets and types and is not resilient to future fields.",
            "date": "2008-08-11T12:02:17.584+0000",
            "id": 49
        },
        {
            "author": "Doron Cohen",
            "body": "{quote}\nI also made the corresponding changes in their generate java files. While it would be good to fix the generation problem, you can compare the jj and java pairs to see that they match.\n{quote}\n\n{quote}\nI ended up doing the same.\n{quote}\n\nThere's a patch for this in LUCENE-1353 ...",
            "date": "2008-08-12T07:09:27.440+0000",
            "id": 50
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I'm unable to get javacc to run cleanly on PrecedenceQueryParser.jj.\n\nI figured this out: you have to use javacc 3.2 (not 4.0 or beyond) for contrib/misc, then \"ant javacc\" runs fine (with the patch from LUCENE-1353).",
            "date": "2008-08-12T10:25:02.323+0000",
            "id": 51
        },
        {
            "author": "Michael McCandless",
            "body": "Attached new patch:\n\n  * Fixed comments about caching & cloning\n\n  * Added partial clone method, that clones all except replaces termBuffer & start/end offset; fixed NGramTokenFilter and CompoundWordTokenFilterBase to use this method\n\n  * Fixed bug in PrecedenceQueryParser.jj that became obvious once I ran javacc\n\nI think we are close!",
            "date": "2008-08-12T11:15:03.733+0000",
            "id": 52
        },
        {
            "author": "Doron Cohen",
            "body": "I diffed current patch and previous one and all seems correct to me. \n\nOne tiny thing in Token - in two locations there's this code:\n{code}\nsetTermBuffer(newTermBuffer, newTermOffset, newTermLength);\nsetTermLength(newTermLength);\n{code}\nSecond call is redundant since first call already sets termLength.\n\n",
            "date": "2008-08-12T12:54:28.467+0000",
            "id": 53
        },
        {
            "author": "Michael McCandless",
            "body": "Woops, you're right -- new patch attached.",
            "date": "2008-08-12T13:44:18.605+0000",
            "id": 54
        },
        {
            "author": "Doron Cohen",
            "body": "Still with previous patch - all core tests passed, but then contrib compilation failed. \nI tried ant clean contrib-test (to not repeat core test) and still compilation errors:\n{noformat}\n    [javac] Compiling 27 source files to build\\contrib\\analyzers\\classes\\test\n    [javac] contrib\\analyzers\\src\\test\\org\\apache\\lucene\\analysis\\miscellaneous\\TestSingleTokenTokenFilter.java:23: cannot find symbol\n    [javac] symbol  : class LuceneTestCase\n    [javac] location: package org.apache.lucene.util\n    [javac] import org.apache.lucene.util.LuceneTestCase;\n    [javac]                              ^\n{noformat}\nIn yesterday's version all tests passed.\nLet me check recent patch.",
            "date": "2008-08-12T13:48:52.684+0000",
            "id": 55
        },
        {
            "author": "Doron Cohen",
            "body": "Wait... \n\"ant  clean  test-comtrib\" was a bad idea, because there's a missing dependency of contrib-test in test-compile.\nSo \"ant clean compile-test  test-contrib\"  doesn't have the compile errors above.\nThe errors that got me started on this were in contrib/analyzers but with the new patch and after clean, all contrib tests passed!\n\n",
            "date": "2008-08-12T14:03:04.792+0000",
            "id": 56
        },
        {
            "author": "Grant Ingersoll",
            "body": "The SinkTokenizer changes don't seem right to me.   The token is already cloned in the add() method, no need to clone again in the next() call.\n\nAt least, that's my understanding based on trying to piece together the changes being proposed here.",
            "date": "2008-08-12T19:32:31.020+0000",
            "id": 57
        },
        {
            "author": "DM Smith",
            "body": "{quote}\nThe SinkTokenizer changes don't seem right to me. The token is already cloned in the add() method, no need to clone again in the next() call.\n\nAt least, that's my understanding based on trying to piece together the changes being proposed here.\n{quote}\n\nIt is because SinkTokenizer implements reset(). SinkTokenizer needs to ensure that the subsequent times it returns the same token that the token is actually the same.\n\nA token is not immutable, so SinkTokenizer needs to ensure that the tokens in its list are immutable.\n\nThe token that is added to the list can be a reusable Token. If it is added without cloning, some other user of the token might change it and thus the list changes.\n\nWhen the token is removed from the list and returned from next, it is presumed to be a reusable token. (This is true whether next() or next(Token) is called.) If it does not return a clone then some other user of the token might change it and thus the internal list changes. This is only a problem when reset is implemented. This is because a single token can be returned more than once from the TokenStream.\n\nIf SinkTokenizer did not implement reset, then next() would not need to create a clone.",
            "date": "2008-08-12T20:50:13.259+0000",
            "id": 58
        },
        {
            "author": "Grant Ingersoll",
            "body": "My point is it is already cloned when it is added to the list, so now it is being cloned twice, and I think the second clone is extraneous.  In other words, it already is returning a clone from next.  The only way that Token could be changed is by doing it via the getTokens() method, which would have to be done outside of the Analysis process, which I would argue does not violate the reusability process.\n\nCloning Tokens is not cheap, as I recall.  In fact, I seem to recall testing that it was cheaper to do a new.  Now, maybe that is fixed in this issue, but I am not sure.  ",
            "date": "2008-08-13T11:13:04.643+0000",
            "id": 59
        },
        {
            "author": "Doron Cohen",
            "body": "But how do you cope with reset()?\n\nConsider this:\n* step 1: tokens added to the sink. They are cloned, so sink \"owns\" them.\n* step 2: sink is used, until exhausted (until its next(Token) returns null).\n* step 3: sink.reset() is invoked.\n* step 4: sink is used again, until exhaustion.\n\nPoint is that if step 2 did not return clones, the consumer that invoked step 2 might have overridden the tokens it got from step 2. Now in step 4 sink \"thinks\" it returns clones of original tokens, unaware that the consumer of step 2 modified them.\n",
            "date": "2008-08-13T11:22:50.516+0000",
            "id": 60
        },
        {
            "author": "Grant Ingersoll",
            "body": "Sorry, you guys are right.  My bad.  Does look like we improved some of the cloning costs, though.",
            "date": "2008-08-13T12:27:50.147+0000",
            "id": 61
        },
        {
            "author": "DM Smith",
            "body": "{quote}\nCloning Tokens is not cheap, as I recall. In fact, I seem to recall testing that it was cheaper to do a new. Now, maybe that is fixed in this issue, but I am not sure.\n{quote}\n\nI was going on hearsay when I uniformly used clone() rather than new when dealing with creating a deep copy of an existing token. I was under the impression that clone was faster than new to do equivalent work.\n\nThe test is rather simple and worthy of doing before accepting this issue. I don't think I have time to do it today.\n\nThe equivalent of clone is (done from memory, so this is close):\nToken token = new Token(oldToken.startOffset(), oldToken.endOffset(), oldToken.getFlags(), oldToken.type());\ntoken.setPositionIncrement(oldToken.positionIncrement());\nif (oldToken.getPayload() != null) {\n Payload p = new Payload(....); // Create a new Payload with a deep copy of the payload\n}\n\nWhile this might be faster, there are two flaws with this that clone avoids, clone has direct access to the parts and avoids method calls and also is future proof. If a new field is added to Token, it will automatically be carried forward.\n\nThere are a couple of places in the code where:\npublic Token(Token prototype) // only if new is faster\nand\npublic void copyFrom(Token prototype)\nwould beneficially solve these maintenance issues.\n\n{quote}\nBut how do you cope with reset()?\n{quote}\nThis problem is a bug in the existing code. Today, one can create a chain of TokenFilters, each of which calls input.next() or input.next(token), and any one of which modifies the return value. It does not matter which is invoked. If the token returned is held in a cache then the cache is corrupted. Every cache of Tokens needs to ensure that it's cache is immutable on creation. It also needs to ensure that it is immutable on usage if the tokens can be served more than once.\n\nTwo personal opinions:\n* Caches that don't implement reset should return cache.remove(0) [or equivalent] so it is clear that the cache can only be used once.\n* Caches should not be used except when it gives a clear performance advantage.",
            "date": "2008-08-13T12:54:17.524+0000",
            "id": 62
        },
        {
            "author": "Doron Cohen",
            "body": "I'm not using sink tokenizer (yet) so not sure if the following worths it, but...\n\nOne simple possibility to avoid those extra clones is to add a way to allow notifying the sink that reset() will not be called anymore. \n\nI.e. that for each token to be consumed from now on, this is the last time it s consumed. \n\nThis can be added in the constructor, or a special setter can be added for that, such as: disableRest(). \nThere would not be an enableReset().\nWhen disabled, next() would not clone, and reset() will throw an exception.\n",
            "date": "2008-08-13T12:59:47.415+0000",
            "id": 63
        },
        {
            "author": "Grant Ingersoll",
            "body": "I think the main performance issue with clone in the old Token was that it had to do the initTermBuffer before, even though clone already knows what size the buffer should be, etc.  It looks like this is fixed now, so it may be a non-issue.",
            "date": "2008-08-13T13:06:41.383+0000",
            "id": 64
        },
        {
            "author": "DM Smith",
            "body": "Regarding the implementation of hashCode:\nYou are using the following:\n{code}\n  private static int hashCode(int i) {\n    return new Integer(i).hashCode();\n  }\n{code}\n\nThis is rather expensive. Integer.hashCode() merely returns its value. Constructing a new Integer is unnecessary.\n\nWhile adding Token's integer values in Token's hashCode is perfectly fine, it is not quite optimal. And may cause unnecessary collisions.\n\nIt might be better to pretend that Token's integer values are also in an array (using the ArrayUtil algorithm, this could be):\n{code}\n  public int hashCode() {\n    initTermBuffer();\n    int code = termLength;\n   code = code * 31 + startOffset;\n   code = code * 31 + endOffset;\n   code = code * 31 + flags;\n   code = code * 31 + positionIncrement;\n   code = code * 31 + type.hashCode();\n   code = (payload == null ? code : code * 31 + payload.hashCode());\n   code = code * 31 + ArrayUtil.hashCode(termBuffer, 0, termLength);\n   return code;\n  }\n{code}\n\nAlso, are the reinit methods used? If not, I'd like to work up a patch that uses them. (And I'll include the above in it.)\n(never mind. I see that they are! super! But I'm working up a patch for this and a couple of minor optimizations that affect Token)\n\nI'll probably add copyFrom(Token) as a means to initialize one token to have the same content as another. There are a couple of places that this is appropriate.\n",
            "date": "2008-08-13T15:00:37.477+0000",
            "id": 65
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. This is rather expensive. Integer.hashCode() merely returns its value. Constructing a new Integer is unnecessary.\n\nDuh, I didn't realize that's the hashCode function for Integer.  I like you're new hashCode.\n\nbq. I'll probably add copyFrom(Token) as a means to initialize one token to have the same content as another. There are a couple of places that this is appropriate.\n\nI like this, but maybe name it reinit(Token)?\n\nbq. Also, are the reinit methods used?\n\nI think we could use it in further places (I didn't search exhaustively).\n\nDM if you can pull together a patch w/ these fixes that'd be great!",
            "date": "2008-08-13T22:14:41.891+0000",
            "id": 66
        },
        {
            "author": "DM Smith",
            "body": "* Added reinit(Token ...) methods to initialize one token from another.\n* Improved hashCode.\n* Made Token next(Token) have a final argument everywhere it is implemented to clarify the \"best practice\" for reuse and did the same for helper methods in various classes. As part of this, I renamed the token retrieved by next(Token) to be nextToken.\n\nThe typical pattern I used is for TokenFilters:\n{code}\npublic Token next(final Token reusableToken) {\n  assert reusableToken != null;\n  Token nextToken = input.next(reusableToken);\n   if (nextToken != null)\n       return null;\n   .... Do something with nextToken ....\n   return nextToken;\n}\n{code}\nand for other TokenStreams:\n{code}\npublic Token next(final Token reusableToken) {\n  assert reusableToken != null;\n   .... Do something with reusableToken ....\n   return reusableToken;\n}\n{code}\n\nand for looping over a TokenStream:\n{code}\nfinal Token reusableToken = new Token();\nfor (Token nextToken = stream.next(reusableToken); nextToken != null; nextToken = stream.next(reusableToken)) {\n    .... Do something with nextToken ....\n}\n{code}\n* Improved Payload.clone() to avoid new if possible.\n* Removed last remaining calls to termText() in a *.jj file.\n\n",
            "date": "2008-08-18T15:04:37.294+0000",
            "id": 67
        },
        {
            "author": "Michael McCandless",
            "body": "Patch & changes look good, thanks DM.\n\nI attached new patch with tiny changes:\n\n  * Fixed javadoc warnings\n\n  * Removed extra unnecessary comparison in ArrayUtil.getShrinkSize\n\nI plan to commit in a day or two!",
            "date": "2008-08-18T16:12:42.504+0000",
            "id": 68
        },
        {
            "author": "Michael McCandless",
            "body": "I just committed this.  Thanks DM!  This was one humongous patch -- almost 10K lines.  I sure hope we didn't break anything :)",
            "date": "2008-08-20T14:40:41.553+0000",
            "id": 69
        }
    ],
    "component": "modules/analysis",
    "description": "This was discussed in the thread (not sure which place is best to reference so here are two):\nhttp://mail-archives.apache.org/mod_mbox/lucene-java-dev/200805.mbox/%3C21F67CC2-EBB4-48A0-894E-FBA4AECC0D50@gmail.com%3E\nor to see it all at once:\nhttp://www.gossamer-threads.com/lists/lucene/java-dev/62851\n\nIssues:\n1. JavaDoc is insufficient, leading one to read the code to figure out how to use the class.\n2. Deprecations are incomplete. The constructors that take String as an argument and the methods that take and/or return String should *all* be deprecated.\n3. The allocation policy is too aggressive. With large tokens the resulting buffer can be over-allocated. A less aggressive algorithm would be better. In the thread, the Python example is good as it is computationally simple.\n4. The parts of the code that currently use Token's deprecated methods can be upgraded now rather than waiting for 3.0. As it stands, filter chains that alternate between char[] and String are sub-optimal. Currently, it is used in core by Query classes. The rest are in contrib, mostly in analyzers.\n5. Some internal optimizations can be done with regard to char[] allocation.\n6. TokenStream has next() and next(Token), next() should be deprecated, so that reuse is maximized and descendant classes should be rewritten to over-ride next(Token)\n7. Tokens are often stored as a String in a Term. It would be good to add constructors that took a Token. This would simplify the use of the two together.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1333",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Token implementation needs improvements",
    "systemSpecification": false,
    "version": "2.3.1"
}