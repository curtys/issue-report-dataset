{
    "comments": [
        {
            "author": "Jason Rutherglen",
            "body": "It looks like DirectoryIndexReader needs to have an IndexWriter mode\nas unfortunately subclassing won't work. In this context overriding a\nmethod implies the IW mode is being used.\n\nI assume we'll share the segmentInfos object from IW rather than\nshare a clone with the IR?\n\n* DirectoryIndexReader.doCommit needs to be overridden \n* IndexReader uses the IndexDeletionPolicy from IndexWriter \n* DirectoryIndexReader.acquireWriteLock is overridden and synchronizes\n  on the write lock of IndexWriter, other details need to be worked out\n* Method requiring synchronization in IW: optimize, expungeDeletes,\n  prepareCommit, flush, addIndexes, methods that modify segmentInfos.\n  Do we synchronize on IW or writeLock? \n* addDocument, updateDocument, deleteDocument do not seem to require \n  synchronization \n* IW.getReader returns a cloned reader. IW keeps it's own reference. \n  This is to allow IW to perform deletes using the internal reader rather than \n  open a new reader and in general not affect IW's reader while still sharing\n  resources.\n",
            "date": "2009-01-10T02:06:46.950+0000",
            "id": 0
        },
        {
            "author": "Grant Ingersoll",
            "body": "It still seems to me that we should think about what objects need to be shared and how they can be produced/instantiated appropriately instead of adding a Reader onto the Writer, which, IMO, pollutes the Writer API.  I know it complicates things, but I think it will be less confusing to users of the API.",
            "date": "2009-01-11T17:41:25.415+0000",
            "id": 1
        },
        {
            "author": "Jason Rutherglen",
            "body": "Patch includes the latest patch from LUCENE-1314 and is unfortunately\nnot readable. Is there a way to create a patch minus another patch?\n\nThe IW internal reader is always the results of the latest external\nIR.reopen. In this way deletes made in IW and from the external IR\nremain in sync. The latest IR always has the write lock anyways\naccording to IR.clone semantics. \n\nDocumentsWriter.applyDeletes reuses the reader in IW rather than call\nSegmentReader.get/.deleteDocument\n\nIn IW.doFlush the internal reader is reopened with the new\nsegmentinfos\n\nIncluded a very basic test case \n",
            "date": "2009-01-14T23:50:13.922+0000",
            "id": 2
        },
        {
            "author": "Jason Rutherglen",
            "body": "This is an extremely early patch and has a bunch of System.outs for debugging\n\n- TestIndexWriterReader.testMerge tries to use IW.addIndexes and fails\nbecause the lookup of the SegmentReader from the IW internal IR\nreturns the incorrect reader (I spent a lot of time on trying to\nfigure out why and could not). \n- TestIndexWriterReader.testReader passes \n",
            "date": "2009-02-10T01:09:07.926+0000",
            "id": 3
        },
        {
            "author": "Jason Rutherglen",
            "body": "Some terminology for this patch, an internal reader is the\nIndexWriter's reader. An external reader given via the\nIW.reopenReader method. \n\nIf DirectoryIndexReader has a writer, no lock is acquired on updates.\nIR.clone normally passes the writeLock to the new reader, however the\nexternal IR and the IW internal IR both need to hold the write lock.\nFor this reason the user must be careful when flushing to insure the\nproper instance of the IR's deletes are merged with the writer. \n\nThe external IR.flush does not flush the deletes to disk, instead it\nmerges with the IW's internal IR which is in RAM. IW.flush causes\ndeletes and new segments to be flushed to the directory.\n\nThe test cases from TestIndexWriterReader testIndexWriterDeletes and\ntestIndexWriterReopenSegment fail when the IW is opened again after\ncommit and close. The index files are being deleted during IW.commit.\nI traced this to IW.finishCommit -> deleter.checkpoint ->\ndeleter.deletePendingFiles. \n\n\n\n",
            "date": "2009-02-17T18:43:56.009+0000",
            "id": 4
        },
        {
            "author": "Jason Rutherglen",
            "body": "Added TestIndexWriterReader.testIndexWriterDeletes,\ntestIndexWriterDeletesOptimize, testIndexWriterReopenSegment,\ntestIndexWriterReopenSegmentOptimize. Where the optimize methods\nfail, the non optimize ones work. The optimize methods delete _0.fdt\nand _0.fdx and so fail when the writer is created again because it\ncannot find those files. It could be a segment infos merging problem\nor something else.",
            "date": "2009-02-18T00:34:03.877+0000",
            "id": 5
        },
        {
            "author": "Jason Rutherglen",
            "body": "The previously mentioned issue in the test case is fixed. Perhaps\nIW.reopenInternalReader() should be called in IW.checkpoint? ",
            "date": "2009-02-18T01:03:50.463+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "\nLooks good, Jason.  This is big change, and I expect to go through a\nnumber of iterations before settling... plus we still need to figure\nout how the API is exposed.  Comments:\n\n  * All this logic needs to be conditional (this also depends on what\n    API we actually settle on to expose this...): right now you always\n    open a reader whenever IW is created.\n\n  * We should assume we do not need to support autoCommit=true in this\n    patch (since this will land after 3.0).  This simplifies things.\n\n  * IW.reopenInternalReader only does a clone not a reopen; how does\n    it cover the newly flushed segment?\n\n  * After a merge commits you don't seem to reopen the reader?  This\n    is actually tricky to do right, for realtime search: we somehow\n    need to allow for warming of the newly created (merged) segment,\n    in such a way that we do not block the flushing of further\n    segments and reopen of readers against those new segments.  I\n    think what may be best is to subclass IW, and override a newly\n    added \"postMerge\" method that's invoked on the new segment before\n    the merge is committed into the SegmentInfos.  This is cleaner\n    than allowing the change into the SegmentInfos and then having to\n    make a custom deletion policy & track history of each segment.\n\n  * It seems like reader.reopen() (where reader was obtained with\n    IW.getReader()) doesn't do the right thing?  (ie it's looking for\n    the most recent segments_N in the Directory, but it should be\n    looking for it @ IW.segmentInfos).\n\n  * I think we should decouple \"materializing deletes down to docIDs\"\n    from \"flushing deletes to disk\".  IW does both as the same\n    operation now (because it doesn't want to hold SR open for a long\n    time), but once we have persistent open SegmentReaders we should\n    separate these.  It's not necessary for IW to write new .del files\n    when it materializes deletes.\n\n  * Instead of having to merge readers, I think we should have a\n    single source to obtain an SR from.  This way, when IW needs to\n    materialize deletes, it will grab the same instance of SR for a\n    given segment that the currently open MSR is using.  Also, when\n    merging kicks off, it'll grab the SR from the same source (this\n    way deletes in RAM will be correctly merged away).  Also, I think\n    we should not use MSR for doing deletions (and still go segment by\n    segment): it's quite a bit slower since every invocation must do\n    the binary search again.\n\n  * Likewise, you have to fix the commitMergedDeletes to decouple\n    computing the new BitVector from writing the .del file to disk.\n    That method should only create a new BitVector, for the newly\n    merged segment.  It must be synchronized to prevent any new\n    deletions against the segments that were just merged.  In fact,\n    this is a real danger: after a merge finishes, if one continues to\n    use an older reader to do deletions you get into trouble.\n\n  * I still don't really like having both the IR and IW able to do\n    deletions, with slightly different semantics.  As it stands now,\n    since you can't predict when IW materializes deletes, your reader\n    will suddenly see a bunch of deletes appear.  I think it's better\n    if no deletes appear, ever, until you reopen your reader.  Maybe\n    we simply prevent deletion through the IR?\n\n  * We need some serious unit tests here!\n",
            "date": "2009-02-18T15:44:40.779+0000",
            "id": 7
        },
        {
            "author": "Jason Rutherglen",
            "body": "Mike, good points... \n\n{quote} since you can't predict when IW materializes deletes, your reader\nwill suddenly see a bunch of deletes appear.{quote}\n\nThe reader would need to be reopened to see the deletes. Isn't that\nexpected behavior?\n\n{quote} Instead of having to merge readers, I think we need a single\nsource to obtain an SR from {quote}\n\nI like this however how would IR.clone work? I like having the\ninternal reader separate from the external reader. The main reason to\nexpose IR from IW is to allow delete by doc id and norms updates\n(eventually column stride fields updates). I don't see how we can\ngrab a reader during a merge, and block realtime deletes occurring on\nthe external reader. However it is difficult to rectify deletes to an\nexternal SR that's been merged away. \n\nIt seems like we're getting closer to using a unique long UID for\neach doc that is carried over between merges. I was going to\nimplement this above LUCENE-1516 however we may want to make UIDs a\npart of LUCENE-1516 to implement the behavior we're discussing. \n\nIf the updates to SR are queued, then it seems like the only way to\nachieve this is a doc UID. This way merges can happen in the\nbackground, the IR has a mechanism for mapping it's queue to the\nnewly merged segments when flushed. Hopefully we aren't wreaking\nhavoc with the IndexReader API?\n\nThe scenario I think we're missing is if there's multiple cloned SRs\nout there. With the IW checkout an SR model how do we allow cloning?\nA clone's updates will be placed into a central original SR queue?\nThe queue is drained automatically on a merge or IW.flush? What\nhappens when we want the IR deletes to be searchable without flushing\nto disk? Do a reopen/clone? \n\nbq. number of iterations before settling\n\nAgreed, if it were simple it wouldn't be fun. \u263a\n\n{quote} It's not necessary for IW to write new .del files when it\nmaterializes deletes.{quote}\n\nGood point, DocumentsWriter.applyDeletes shouldn't be flushing to\ndisk and this sounds like a test case to add to TestIndexWriterReader.\n\n{quote} IW.reopenInternalReader only does a clone not a reopen; however\ndoes it cover the newly flushed segment? {quote}\n\nThe segmentinfos is obtained from the Writer. In the test case\ntestIndexWriterReopenSegment it looks like using clone reopens the\nnew segments.\n\n{quote} I think it's better if no deletes appear, ever, until you reopen\nyour reader. Maybe we simply prevent deletion through the IR? {quote}\n\nPreventing deletion through the IR would seem to defeat the purpose\nof the patch unless there's some alternative mechanism for deleting\nby doc id? \n\n{quote} commitMergedDeletes to decouple computing the new BitVector from\nwriting the .del file to disk.{quote}\n\nA hidden method I never noticed. I'll keep it in mind.\n\n{quote} It seems like reader.reopen() (where reader was obtained with\nIW.getReader()) doesn't do the right thing? (ie it's looking for the\nmost recent segments_N in the Directory, but it should be looking for\nit @ IW.segmentInfos).{quote}\n\nUsing the reopen method implementation for a Reader with IW does not\nseem necessary. It seems like it could call clone underneath?\n\n",
            "date": "2009-02-19T18:14:12.648+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\n> since you can't predict when IW materializes deletes, your reader\n> will suddenly see a bunch of deletes appear.\n\nThe reader would need to be reopened to see the deletes. Isn't that\nexpected behavior?\n{quote}\n\nAhh right, so long as we keep internal (private) clone, materializing\nthe deletes won't affect the external reader.\n\n{quote}\n> Instead of having to merge readers, I think we need a single\n> source to obtain an SR from \n\nI like this however how would IR.clone work?\n{quote}\n\nIt should work fine?  The single source would only be used internally\nby IW (for merging, for materializing deletes, for the internal\nreader).\n\nbq. I like having the internal reader separate from the external reader.\n\nI think we should keep that separation.\n\n{quote}\nThe main reason to\nexpose IR from IW is to allow delete by doc id and norms updates\n(eventually column stride fields updates). I don't see how we can\ngrab a reader during a merge, and block realtime deletes occurring on\nthe external reader. However it is difficult to rectify deletes to an\nexternal SR that's been merged away.\n\nIt seems like we're getting closer to using a unique long UID for\neach doc that is carried over between merges. I was going to\nimplement this above LUCENE-1516 however we may want to make UIDs a\npart of LUCENE-1516 to implement the behavior we're discussing.\n\nIf the updates to SR are queued, then it seems like the only way to\nachieve this is a doc UID. This way merges can happen in the\nbackground, the IR has a mechanism for mapping it's queue to the\nnewly merged segments when flushed. Hopefully we aren't wreaking\nhavoc with the IndexReader API?\n{quote}\n\nBut... do we need delete by docID once we have realtime search?  I\nthink the last compelling reason to keep IR's delete by docID was\nimmediacy, but realtime search can give us that, from IW, even when\ndeleting by Term or Query?\n\n(Your app can always add that long UID if it doesn't already have\nsomething usable).\n\ndocIDs are free to changing inside IW.  I don't see how we can hand\nout a reader, allow deletes by docID to it, and merge those deletes\nback in at a later time, unless we track the genealogy of the\nsegments?\n\n{quote}\nThe scenario I think we're missing is if there's multiple cloned SRs\nout there. With the IW checkout an SR model how do we allow cloning?\nA clone's updates will be placed into a central original SR queue?\nThe queue is drained automatically on a merge or IW.flush? What\nhappens when we want the IR deletes to be searchable without flushing\nto disk? Do a reopen/clone?\n{quote}\n\nThis is why I think all changes must be done through IW if you've\nopened a reader from it.  In fact, with the addition of realtime\nsearch to Lucene, if we also add updating norms/column-stride fields\nto IW, can't we move away from allowing any changes via IR?  (Ie\ndeprecate deleteDocuments/setNorms/etc.)\n\n{quote}\n> It's not necessary for IW to write new .del files when it\n> materializes deletes.\n\nGood point, DocumentsWriter.applyDeletes shouldn't be flushing to\ndisk and this sounds like a test case to add to TestIndexWriterReader.\n{quote}\n\nWell, if IW has no persistent reader to hold the deletes, it must keep\ndoing what it does now (flush immediately to disk)?\n\n{quote}\n> IW.reopenInternalReader only does a clone not a reopen; however\n> does it cover the newly flushed segment? \n\nThe segmentinfos is obtained from the Writer. In the test case\ntestIndexWriterReopenSegment it looks like using clone reopens the\nnew segments.\n{quote}\n\nWait, where is this test?  Maybe you need to svn add it?\n\nAnd, clone should not be reopening segments...?\n\n{quote}\n> I think it's better if no deletes appear, ever, until you reopen\n> your reader. Maybe we simply prevent deletion through the IR? \n\nPreventing deletion through the IR would seem to defeat the purpose\nof the patch unless there's some alternative mechanism for deleting\nby doc id?\n{quote}\n\nSee above.\n\n{quote}\n> commitMergedDeletes to decouple computing the new BitVector from\n> writing the .del file to disk.\n\nA hidden method I never noticed. I'll keep it in mind.\n{quote}\n\nIt's actually very important.  This is how IW allows deletes to\nmaterialize to docIDs, while a merge is running -- any newly\nmaterialized deletes against the just-merged segments are coalesced\nand carried over to the newly created segment.  Any further deletes\nmust be done against the docIDs in the new segment (which is why I\ndon't see how we can allow deletes by docID to happen against a\nchecked out reader).\n\n{quote}\n> It seems like reader.reopen() (where reader was obtained with\n> IW.getReader()) doesn't do the right thing? (ie it's looking for the\n> most recent segments_N in the Directory, but it should be looking for\n> it @ IW.segmentInfos).\n\nUsing the reopen method implementation for a Reader with IW does not\nseem necessary. It seems like it could call clone underneath?\n{quote}\n\nWell, clone should be very different from reopen.  It seems like\ncalling reader.reopen() (on reader obtained from writer) should\nbasically do the same thing as calling writer.getReader().  Ie they\nare nearly synonyms?  (Except for small difference in ref counting --\nI think writer.getReader() should always incRef, but reopen only\nincRefs if it returns a new reader).\n",
            "date": "2009-02-19T23:26:03.648+0000",
            "id": 9
        },
        {
            "author": "Jason Rutherglen",
            "body": "Added the test case to the patch.  ",
            "date": "2009-02-20T00:14:17.384+0000",
            "id": 10
        },
        {
            "author": "Jason Rutherglen",
            "body": "The path forward seems to be exposing a cloned readonly reader\nfrom IW.getReader. This would be easier than doing hula hoops to do\nsegment genealogy (at least for now \u263a)\n\n{quote}can't we move away from allowing any changes via IR? (Ie\ndeprecate deleteDocuments/setNorms/etc.){quote}\n\nThis would simplify things however as a thought experiment how would\nthe setNorms work if it were a part of IndexWriter? \n\n{quote} And, clone should not be reopening segments...? {quote}\n\nDirectoryIndexReader.clone(boolean openReadonly) calls\ndoReopen(SegmentInfos infos, boolean doClone, boolean openReadOnly)\nwhich is an abstract method that in SegmentReader and\nMultiSegmentReader reopens the segments? The segment infos for a\nReaderIW is obtained from IW, which is how it knows about the new\nsegments. Perhaps not desired behavior?\n\n{quote} do we need delete by docID once we have realtime search? I\nthink the last compelling reason to keep IR's delete by docID was\nimmediacy, but realtime search can give us that, from IW, even when\ndeleting by Term or Query? {quote}\n\nGood point! I think we may want to support it but for now it's\nshouldn't be necessary. I'm thinking of the case where someone is\nusing the field cache (or some variant), performs some sort of query\non it and then needs to delete based on doc id. What do they do?\nWould we expose a callback mechanism where a deleteFrom(IndexReader\nir) method is exposed and deletes occur at the time of the IW's\nchoosing?\n\n{quote} It seems like calling reader.reopen() (on reader obtained\nfrom writer) should basically do the same thing as calling\nwriter.getReader(). Ie they are nearly synonyms? (Except for small\ndifference in ref counting - I think writer.getReader() should always\nincRef, but reopen only incRefs if it returns a new reader). {quote}\n\nPerhaps ReaderIW.reopen will call IW.getReader underneath instead of\nusing IR's usual mechanism.\n\n\n",
            "date": "2009-02-20T01:18:02.745+0000",
            "id": 11
        },
        {
            "author": "Grant Ingersoll",
            "body": "This latest patch doesn't seem to apply.\n\n",
            "date": "2009-02-20T11:25:06.814+0000",
            "id": 12
        },
        {
            "author": "Grant Ingersoll",
            "body": "Is there ever a need for the normal IR construction anymore?  Or do we always just ask for it from the IW (or wherever we choose to expose this, as I still don't think it belongs on the IW API wise, but that isn't a big deal right now) every time?  I suppose if I know I'm not going to be changing my index, I can still just get a read-only IR, right?\n\nAPI wise, I think we could do something like (with obvious other variations):\n{code}\nIndexAccessor{\n\n  IndexWriter getWriter(Directory);\n\n  //returns read-only reader\n  IndexReader getReader(Directory);\n\n  //returns the external IR described above\n  IndexReader.getReader(IndexWriter);\n}\n{code}\n\nThen, everyone has a single point of entry for both writers and readers and all of this stuff can just be done through package private methods on the IW and it allows us to change things if we decide otherwise and it means that the IW is not coupled with the IR publicly.",
            "date": "2009-02-20T11:42:49.595+0000",
            "id": 13
        },
        {
            "author": "Michael McCandless",
            "body": "Jason, I think you need to \"svn up\".  Or, tell us which revision you're on and we can downgrade to that revision before applying the patch.  (We need \"svn patch\"!).",
            "date": "2009-02-20T12:14:15.118+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nThe path forward seems to be exposing a cloned readonly reader\nfrom IW.getReader.\n{quote}\n\n+1\n\n{quote}\n> can't we move away from allowing any changes via IR? (Ie\n> deprecate deleteDocuments/setNorms/etc.)\n\nThis would simplify things however as a thought experiment how would\nthe setNorms work if it were a part of IndexWriter?\n{quote}\n\nI think it'd look like this?\n{code}\nIndexWriter.setNorm(Term term, String field, byte norm)\n{code}\n\nIe the Term IDs the doc(s) you want to set the norm for.\n\n{quote}\n\n> And, clone should not be reopening segments...? \n\nDirectoryIndexReader.clone(boolean openReadonly) calls\ndoReopen(SegmentInfos infos, boolean doClone, boolean openReadOnly)\nwhich is an abstract method that in SegmentReader and\nMultiSegmentReader reopens the segments? The segment infos for a\nReaderIW is obtained from IW, which is how it knows about the new\nsegments. Perhaps not desired behavior?\n{quote}\n\nOK, I think it does not reopen *existing* segments.  Meaning, if a\nsegment is in common w/ old and new, it truly clones it (does not\nreopen norms nor del).  But if there is a new segment that did not\nexist in old, it opens a whole new segment reader?  I'll commit an\nassert that this doesn't happen -- if caller passes in \"doClone=true\"\nthen caller should not have passed in a segmentInfos with changes?\nElse the reader is on thin ice (mismatch what's in RAM vs what\nSegmentInfo says).\n\n{quote}\n> do we need delete by docID once we have realtime search? I\n> think the last compelling reason to keep IR's delete by docID was\n> immediacy, but realtime search can give us that, from IW, even when\n> deleting by Term or Query? \n\nGood point! I think we may want to support it but for now it's\nshouldn't be necessary. I'm thinking of the case where someone is\nusing the field cache (or some variant), performs some sort of query\non it and then needs to delete based on doc id. What do they do?\nWould we expose a callback mechanism where a deleteFrom(IndexReader\nir) method is exposed and deletes occur at the time of the IW's\nchoosing?\n{quote}\n\nWouldn't delete-by-Query cover this?  Ie one could always make a\nFilter implementing the \"look @ field cache, do some logic, provide\ndocIDs to delete\", wrap as Query, then delete-by-Query?\n\n{quote}\n> It seems like calling reader.reopen() (on reader obtained\n> from writer) should basically do the same thing as calling\n> writer.getReader(). Ie they are nearly synonyms? (Except for small\n> difference in ref counting - I think writer.getReader() should always\n> incRef, but reopen only incRefs if it returns a new reader). \n\nPerhaps ReaderIW.reopen will call IW.getReader underneath instead of\nusing IR's usual mechanism.\n{quote}\n\nRight, that's what I'm thinking.  Once you've obtained reader coupled\nto a writer, you can then simply reopen it whenever you want to see\n(materialize) changes done by the writer.\n\nWe still need a solution for the \"warm the just merged\nsegment\"... else we will not be realtime, especially when big merge\nfinishes.  It seems like after merge finishes, it should immediately\n1) open a SegmentReader on the new segment, 2) invoke the method you\npassed in (or you subclassed -- not sure which), 3) carry over deletes\nthat materialized during the merge, 4) commit the merge (replace old\nsegments w/ new one).\n",
            "date": "2009-02-20T12:19:00.118+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. I suppose if I know I'm not going to be changing my index, I can still just get a read-only IR, right?\n\nRight, I think we still want to allow opening a standalone (uncoupled)\nreader.\n\nbq. Then, everyone has a single point of entry for both writers and readers and all of this stuff can just be done through package private methods on the IW and it allows us to change things if we decide otherwise and it means that the IW is not coupled with the IR publicly.\n\nI'm torn... the IndexAccessor would need to expose many variants to\ncarry over all the options we now have (String or File or Directory,\nIndexCommit or not, IndexDeletionPolicy or not, create or not).  It\nwill end up exposing a number of new methods...  and, would it try to\nbe \"smart\" (like IndexModifier, and the LuceneIndexAccessor class in\nLUCENE-390), keeping track of references to the readers it's handed\nout, etc.?  Or is it simply a pass-through to the underlying\nopen/ctors we have today?\n\nThe alternative (as of right now, unless we are missing something\nfurther with these changes) is adding one method to IndexWriter,\ngetReader, that returns a readOnly IndexReader, \"coupled\" to the\nwriter you got it from in that it's able to search un-committed\nchanges and if you reopen it, writer will materialize all changes and\nmake them visible to the reopened reader.\n\nI guess so far I don't really see why this small (one method) API\nchange merits a switch to a whole new accessor API for creating\nreaders & writers on an index?  Maybe there is a\nstraw-that-breaks-the-camel's-back argument that I'm missing...\n",
            "date": "2009-02-20T12:43:17.975+0000",
            "id": 16
        },
        {
            "author": "Grant Ingersoll",
            "body": "Good points, MIke, but maybe we don't need all those variants?  String, File and Directory are all easily enough collapsed down to just Directory.\n{code}\nnew IndexWriter(new Directory(indexFile));\n{code}\n\nAdditionally, there are no more variants than there already are on the IW and IR, right?   \n\nAs for pass-through or not, I think it would just pass-through, at least initially, but it certainly leaves open the possibility for reference counting in the future if someone wants to implement that.\n\nAs someone who teaches people these APIs on a regular basis, I feel pretty confident in saying that adding an IR to the IW as a public API is going to confuse a good chunk of people just as the delete stuff on the IR currently does now.  You wouldn't ask FileWriter for a FileReader, would you?  I don't see why it would be good to ask a IW for an IR, API-wise (I get why we are doing this, it makes sense).\n\nLikewise, isn't it just as logical to ask for an IW from an IR?  If I have an IR already and I want it to be aware of the writes I want to do, why wouldn't we then add IR.getIW()?  And then we can have total circular dependency. \n\n",
            "date": "2009-02-20T13:06:03.191+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nmaybe we don't need all those variants? String, File and Directory are all easily enough collapsed down to just Directory.\n{code}\nnew IndexWriter(new Directory(indexFile));\n{code}\n{quote}\n\n(You'd presumably need to close that Directory).  But, yeah, we may be\nable to drop some of them, although I do think they are convenient for\nnew users of Lucene.  And forcing users to switch to a totally new yet\npass through API on ugprade, but not giving them one to one carryover,\nis not very nice.\n\nbq. Additionally, there are no more variants than there already are on the IW and IR, right?\n\nRight, I'm just saying IndexAccessor will have many methods.  And then\nyou're asking every app to make this switch, on upgrade.  It's alot of\nAPI swapping/noise vs a single added expert method to IW.\n\n{quote}\nAs for pass-through or not, I think it would just pass-through, at\nleast initially, but it certainly leaves open the possibility for\nreference counting in the future if someone wants to implement that.\n{quote}\n\nIf we think it'll be more than just pass through, we should try to\nhash out, somewhat, what it will & won't do up front (changing it\nlater is a big change)?  And we should start from LUCENE-390.\n\n{quote}\nAs someone who teaches people these APIs on a regular basis, I feel\npretty confident in saying that adding an IR to the IW as a public API\nis going to confuse a good chunk of people just as the delete stuff on\nthe IR currently does now.\n{quote}\n\nBut this will be an expert/advanced API, a single added method to IW.\nI wouldn't expect users to be confused: on upgrade I think most users\nwill not even notice its existence!\n\nbq. You wouldn't ask FileWriter for a FileReader, would you?\n\nI'm not sure that's the right comparison -- Lucene's IW does far more\nthan a FileWriter.  And the fact that Lucene allows \"point in time\"\nsearching (which is very useful and rather unique) is a very big\ndifference vs FileReader/Writer.\n\n{quote}\nLikewise, isn't it just as logical to ask for an IW from an IR?\n{quote}\n\nI don't think so: the functionality is not symmetric, because Lucene\nallows only one writer open at a time, but many readers (eg on\ndifferent commits).  Since a writer is the one making changes, it\nmakes sense that you'd ask it, right now, to give you a reader\nreflecting all changes up to that point.  And call it again later to\nget a reader seeing changes after that, etc.",
            "date": "2009-02-20T16:37:46.102+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "Or, here's an idea: can we do both?  Put IndexAccessor as an optional\n\"convenience\" layer that simplifies the ctors and expert methods of IW\n& IR, but leave public direct access to the ctros & expert methods?\nThis way on upgrade nobody is forced to migrate to an entirely new yet\nsimply pass-through API?\n\nOr another idea is to decouple these two discussions: go ahead and add\nthe single expert method to IW, but as a separate discussion/JIRA work\nout how we can simplify overall access/management of IR/IW instances?\n",
            "date": "2009-02-20T16:43:30.861+0000",
            "id": 19
        },
        {
            "author": "Jason Rutherglen",
            "body": "Ah yes, patch from the old directory that need deleting.  Here's the correct one.  Sorry about that.",
            "date": "2009-02-20T17:26:36.071+0000",
            "id": 20
        },
        {
            "author": "Grant Ingersoll",
            "body": "bq. Right, I'm just saying IndexAccessor will have many methods. And then\nyou're asking every app to make this switch, on upgrade. It's alot of\nAPI swapping/noise vs a single added expert method to IW.\n\nSure, but that is already the case w/ IW/IR anyway.\n\nI agree about the short term noise, but in the long run it seems cleaner.\n\nbq. But this will be an expert/advanced API, a single added method to IW.\nI wouldn't expect users to be confused: on upgrade I think most users\nwill not even notice its existence!\n\nHmm, I don't agree, but I guess it depends on the performance hit.  If given a choice between the semantics of a reader that sees changes as they are made versus having to do the whole reopen thing, I'm betting most users will say \"duh, I want to see my changes right away\" and choose the IR that is synced w/ the IW, b/c that is what people think is the logical thing to happen and it is how DBs work, which many devs. are used to.  As an app developer, if I don't have to think about IR lifecycle management, why would I want to as long as it performs?  What this patch is offering, AFAICT, is the removal of IR lifecycle managment from the user.\n\nIn other words, my guess is that over time, as the performance proves out, it will be the default choice, not \"expert\".  Now, if you're telling me this is going to be significantly slower even when updates are rare, then maybe I would stick to the current lifecycle, but if there isn't much difference, I'll take the one that pushes the lifecycle complexity down into Lucene instead of in my app.",
            "date": "2009-02-20T17:55:55.302+0000",
            "id": 21
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. a reader that sees changes as they are made versus having to do the whole reopen thing\n\nIt's hard keeping up with the current proposal in big issues/threads, but I don't think anyone is proposing a reader that automatically sees changes... i.e. the view of an IndexReader instance will still be fixed.",
            "date": "2009-02-20T20:29:55.610+0000",
            "id": 22
        },
        {
            "author": "Michael McCandless",
            "body": "bq. It's hard keeping up with the current proposal in big issues/threads, but I don't think anyone is proposing a reader that automatically sees changes... i.e. the view of an IndexReader instance will still be fixed.\n\nThat's right.  The current proposal is to add one method to IW:\n\n{code}\nIndexReader getReader()\n{code}\n\nthat returns a point-in-time view of the index plus all changes\nbuffered in IW up until that point.  Then you can reopen that reader\n(or call getReader() again, which does the same thing) to quickly get\na new point-in-time reader.\n\nI think the point-in-time semantics is important to keep.\n\nAlso, you can't easily emulate point-in-time if we implemented the\n\"live\" approach, but you can easily do vice/versa (assuming we can\nkeep reopen() time fast enough).\n\nEG the IndexAccessor convenience layer could do automatic reopening so\nthat when you ask it for the reader it always reopens it; this would\nemulate \"live updates\" and hide the lifecycle management.\n\n",
            "date": "2009-02-20T22:21:44.324+0000",
            "id": 23
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I guess it depends on the performance hit.\n\nIt's challenging to implement truly live updates w/ decent\nperformance: I think we'd need to build the reader impl that can\nsearch DocumentsWriter buffer.\n\nWhereas the approach (patch) here is actually quite simple (all the\nhard work was already done -- IndexReader.reopen,\ncollection/sorting/filtering by segment, etc.).\n\nbq. In other words, my guess is that over time, as the performance proves out, it will be the default choice, not \"expert\".\n\nI agree: realtime search will likely be a popular feature once we\nfinish it, release it, it proves stable, performant, etc.  Eventually\n(maybe soon) it should be made the default.\n\nI think IndexAccessor makes alot of sense, but it's a big change and\nI'd rather not couple it to this issue.  There are many questions to\nbe hashed out (under a new issue): is it a simple pass-through?  Or\ndoes it manage the lifecycle of the readers for you?  Does it warm new\nreaders?  Should it emulate \"live\" update semantics?  Should getReader\nget it from the writer if there is one (ie, making realtime search the\n\"default\")?  Etc.\n",
            "date": "2009-02-20T22:50:56.927+0000",
            "id": 24
        },
        {
            "author": "Grant Ingersoll",
            "body": "OK, I agree.  Let's just mark it as expert/subject to revision and then we're good.\n\nWe can revisit IndexAccessor separately.\n\n",
            "date": "2009-02-20T23:09:23.293+0000",
            "id": 25
        },
        {
            "author": "Jason Rutherglen",
            "body": "There's concurrency issues to work out.\n\n- IW.getReader returns a cloned read only reader \n- Removed IW.reopenReader \n- All test methods pass except testAddIndexesAndDoDeletesThreads. testAddIndexesAndDoDeletesThreads\ncurrently merges indexes concurrently (and fails). In the future the\nmethod will test merging, deleting, and searching concurrently. \n- Concurrent merges fail when \"ant test-core\" is run \n- DocumentsWriter.applyDeletes deletes again at the SegmentReader level\n\n",
            "date": "2009-02-21T01:36:11.733+0000",
            "id": 26
        },
        {
            "author": "Jason Rutherglen",
            "body": "- Added segmentReaders, segmentReaderClone to MergePolicy.OneMerge\n- mergeFinish decRefs the OneMerge.segmentReaders\n- _mergeInit clones segmentreaders\n- commitMergedDeletes uses OneMerge.segmentReaders instead of loading \nbitvectors from the directory\n- testAddIndexesAndDoDeletesThreads fails with 20 less documents (80 vs 100)\n(an index that was supposed to be added isn't showing up)\n- Getting exceptions from org.apache.lucene.TestSnapshotDeletionPolicy, such as:\n\n{code}\nCaused by: java.io.IOException: read past EOF\n    [junit] \tat org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:135)\n    [junit] \tat org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:92)\n    [junit] \tat org.apache.lucene.store.IndexOutput.copyBytes(IndexOutput.java:172)\n    [junit] \tat org.apache.lucene.index.TermVectorsWriter.addRawDocuments(TermVectorsWriter.java:185)\n    [junit] \tat org.apache.lucene.index.SegmentMerger.mergeVectors(SegmentMerger.java:447)\n    [junit] \tat org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:145)\n    [junit] \tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4823)\n    [junit] \tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:4408)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:218)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:274)\n{code}",
            "date": "2009-02-22T07:40:04.926+0000",
            "id": 27
        },
        {
            "author": "Michael McCandless",
            "body": "I think your latest test is missing the the new unit test source.",
            "date": "2009-02-22T22:10:02.578+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "Woops, sorry -- I downloaded the wrong patch.",
            "date": "2009-02-22T22:10:53.464+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "\nI think we can simplify the approach here:\n\n  * I don't think IW should track an \"internal reader\" at all.\n    Instead, it tracks a pool of held-open and re-used SegmentReader\n    instances.  Maybe make a new SegmentReaderPool private class.  It\n    exposes get() to load a SegmentReader, returning the pooled one if\n    present, else falling back to the normal SegmentReader.get.\n\n  * Then, the many places where we currently pull a segment reader (to\n    apply deletes, to do a merge, to carry over new deletes after\n    merge finishes) as well as the new getReader() method, should use\n    this pool to get the reader.\n\n  * We would never mergeSegmentInfos, never ask the \"internal reader\"\n    to commit, etc.\n\nOther things:\n\n  * Since IndexReader will always be readonly, you can simplify the\n    new DirectoryIndexReader.open method, eg we don't need to copy\n    over the writeLock nor the DeletionPolicy, and we don't need the\n    readOnly param, and closeDirectory is always false.  In fact we\n    could simply create a ReadOnly{Multi,}SegmentReader directly.\n\n  * I think IndexReader.clone() should not grab the writer's\n    segmentInfos?  Ie it should truly be a clone(), not a reopen.\n    Actually, when the reader is created we should make a private full\n    copy of the SegmentInfos.\n\n  * We lost some \"private\" modifiers, unecessarily; eg\n    DirectoryIndexReader.writeLock.\n\n  * I don't understand why we need merge.segmentReadersClone?  If we\n    only use to detect new deletions for carrying over deletes after\n    merge finishes, we should instead just grab the delCount when the\n    merge kicked off?\n\n  * I think the only reason for reader to hold a reference to writer\n    is so that on reopen, the reader realizes it was created from\n    IW.getReader and simply forwards the request to IW.  Otherwise\n    writer should not be used in reader.\n\n  * Once we no longer store/maintain/merge an \"internal\" reader,\n    IW.getReader simply becomes a call to IndexReader.open, with two\n    differences: 1) we pass in a SegmentInfos instance (instead of\n    looking for last segments_N in dir), and 2) we pass in our own\n    SegmentReaderPool that should be used to open the SR's.\n\n  * You need to add mainDir.close() in\n    testAddIndexesAndDoDeletesThreads.\n",
            "date": "2009-02-23T12:03:35.774+0000",
            "id": 30
        },
        {
            "author": "Jason Rutherglen",
            "body": "> make a new SegmentReaderPool private class\n\nI'd prefer the SegmentReaderPool model over the patch's existing one\nas it is simpler and closer to how the underlying system actually\nworks meaning it works directly with the segments in a systematized way.\n\n> We would never mergeSegmentInfos, never ask the \"internal reader\"\nto commit\n\nGood, merging the segmentInfos is confusing and tricky to debug\n\n> Since IndexReader will always be readonly, you can simplify the new\nDirectoryIndexReader.open method\n\n+1\n\n> why we need merge.segmentReadersClone?\n\nI was modeling after the segmentInfosClone. If it's not necessary\nI'll happily remove it.\n\n> I think the only reason for reader to hold a reference to writer is\nso that on reopen, the reader realizes it was created from\nIW.getReader and simply forwards the request to IW\n\n+1\n\n> Wouldn't delete-by-Query cover this? Ie one could always make a\nFilter implementing the \"look @ field cache, do some logic, provide\ndocIDs to delete\", wrap as Query, then delete-by-Query? (from a\nprevious coment) \n\nYes that will work. \n\nHow are these sample SegmentReaderPool method signatures? \n\n{code}\nprivate class SegmentReaderPool {\n  public SegmentReader get(SegmentInfo si) {}\n    \n  public void decRef(SegmentReader sr) {}\n}\n{code}\n",
            "date": "2009-02-23T17:09:27.222+0000",
            "id": 31
        },
        {
            "author": "Jason Rutherglen",
            "body": "In SegmentInfo it's hashCode only takes into\naccount the name and directory, why not the delete generation?",
            "date": "2009-02-23T17:12:44.426+0000",
            "id": 32
        },
        {
            "author": "Michael McCandless",
            "body": "bq. How are these sample SegmentReaderPool method signatures? \n\nLooks good for starters.  decRef would simply pass-through to the SR's\nnormal decRef, but would then also decRef its copy if the ref count has\ndropped to 1?\n\n{quote}\nIn SegmentInfo it's hashCode only takes into\naccount the name and directory, why not the delete generation?\n{quote}\n\nTwo SegmentInfo instances are considered equal if the dir is == and\nthe name is .equals, so hashCode must match that (be the same value\nwhen equals returns true).\n",
            "date": "2009-02-23T20:19:30.484+0000",
            "id": 33
        },
        {
            "author": "Jason Rutherglen",
            "body": "- Added SegmentReaderPool that maintains a map of SegmentInfoKey ->\nSegmentReader. SegmentInfoKey returns a hashcode of a SegmentInfo\nthat also includes the deletes and norms generations.\nSegmentReaderPool also maintains a map of SegmentReaders that\nrepresent \"working copies\" (cloned readers) for realtime deletes\noccurring. When IW.getReader is called the \"working copy\" readers\nreplace the existing readers keyed with the same segmentinfo in the\nSegmentReaderPool. This way IW.deleteDocument applies to the in\nmemory cloned \"working copies\" but does not alter SegmentReaders\nalready returned from SegmentReaderPool. \n- The test methods in TestIndexWriterReader pass \n- I'm hesitant to automatically flush on IW.getReader as IW.flush has\na number of parameters. However for updateDocument to work properly\nwe'll probably need to do this? \n- DocumentsWriter.applyDeletes should no longer need to apply deletes\nas they are already applied in realtime in IW.deleteDocument? It\nshould only need to call SR.commitChanges? \n- \"ant test-core\" fails in ConcurrentMergeScheduler with the same\nexception as above in TestSnapshotDeletionPolicy (and some others). I\nsuspect in the case of TestSnapshotDeletionPolicy the errors could be\ndue to the SegmentReader holding files open while they are being\ncopied. Perhaps this would explain the EOFExceptions? ",
            "date": "2009-02-24T22:53:02.924+0000",
            "id": 34
        },
        {
            "author": "Jason Rutherglen",
            "body": "- Fixed the basic concurrency merge issue (TestSnapshotDeletionPolicy passes)\n- Many unit tests fail due to other reasons",
            "date": "2009-02-25T01:13:32.862+0000",
            "id": 35
        },
        {
            "author": "Jason Rutherglen",
            "body": "- TestAddIndexesNoOptimize test passes\n- There's an incRef issue still",
            "date": "2009-02-25T02:07:22.269+0000",
            "id": 36
        },
        {
            "author": "Michael McCandless",
            "body": "Jason could you rebase the patch on current trunk?  (Or post the revision you're on?).",
            "date": "2009-02-25T14:00:15.598+0000",
            "id": 37
        },
        {
            "author": "Jason Rutherglen",
            "body": "- Revised to trunk\n- Still an incRef issue.\nTestIndexWriterReader.testDeleteFromIndexWriter fails.\nTestConcurrentMergeScheduler fails, as well as others\n- Removed the working copy stuff from SegmentReaderPool as the\ndeletes need to be applied to the internal readers, the external\nIW.getReader is a clone of the internal SRs.\n",
            "date": "2009-02-25T17:11:28.267+0000",
            "id": 38
        },
        {
            "author": "Jason Rutherglen",
            "body": "We'll need a method to check the SegmentReaderPool for readers that\nmay be removed. It seems it would check IW.segmentInfos, pending\nmerges, current merges for segmentinfo(s) that are in use, then all\nothers in the pool are expunged. I'm not sure if this method would\nneed to run in a background thread. ",
            "date": "2009-02-25T17:22:03.736+0000",
            "id": 39
        },
        {
            "author": "Michael McCandless",
            "body": "Looks good!:\n\n  * The SegmentReaderPool class should be package private.\n\n  * You're still passing readOnly & closeDirectory into the new\n    DirectoryIndexReader ctor; I think you can pass in only the\n    writer, and then it can get all it needs from that (pool, dir,\n    segmentInfos)?\n\n  * Hmm... I think we should change commitMergedDeletes to use the\n    already computed doc ID maps, instead of loading the old deletes.\n    This way you don't need the old SR.\n\n  * I think you don't need SegmentInfoKey?  Can't you key off of just\n    the SegmentInfo?  I think for each segment name we only ever need\n    one SR instance in the pool (dir is always the same)?  (The\n    readers we give out will make clones of these).\n\n  * Why do you applyDeletesImmediately?  I think we should\n    buffer/flush like we normally do?  That code shouldn't need to\n    change.\n\n  * I think getReader does need to flush, else added docs won't be\n    seen in the reopend reader.  You should call flush(true, true,\n    true) -- because you want to triggerMerges, flushDocStores,\n    and flushDeletes.\n\n  * Are segmentInfosList/printSegmentInfosList temporary debugging?\n    Can you stick \"nocommits\" on them?\n\nFor SegmentReaderPool:\n\n  * I think IW should always use the pool API internally to load &\n    decRef the SRs it uses.\n\n  * If getReader has never been called, then the pool should not hold\n    refs, ie IW should behave exactly as it does today (get SR, use\n    it, commit it, close it).  Ie, the pool should detect on decRef\n    that the RC is now 1, and release the SR.\n\n  * If getReader has been called, then the pool should hold onto an SR\n    as long as the writer's segmentInfos still references that\n    segment.  This means, after a merge commits we should prune the\n    pool.  Ie, we decRef and remove the SR from the pool, but we don't\n    close it since a reader may still be using it.  Maybe add a\n    prune() method?\n\n  * Somehow, we should not let the SR commit itself on close -- that's\n    up to IW to decide.  If getReader has not been called then IW must\n    commit deletes as soon as applies them (like today).  Else, it's\n    only on closing the pool that deletes are committed.  EG pending\n    deletes on SRs that have been merged can be freely discarded,\n    since those deletes \"made it\" into the newly merged segment.\n\n  * So I don't think we need a BG thread to do the culling.\n",
            "date": "2009-02-25T19:41:14.560+0000",
            "id": 40
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}\nI think we should change commitMergedDeletes to use the\nalready computed doc ID maps, instead of loading the old deletes.\n{quote}\n\nWhere do I get the already computed doc ID maps?  \n\n{quote}\nI think getReader does need to flush, else added docs won't be\nseen in the reopend reader. You should call flush(true, true,\ntrue) - because you want to triggerMerges, flushDocStores,\nand flushDeletes.\n{quote}\n\nI added a flushDeletesToDir flag that defaults to true except for IW.getReader.\n\n",
            "date": "2009-02-25T23:18:34.480+0000",
            "id": 41
        },
        {
            "author": "Jason Rutherglen",
            "body": "Found the doc ID map in SegmentMerger in IW.commitMerge",
            "date": "2009-02-25T23:27:44.023+0000",
            "id": 42
        },
        {
            "author": "Jason Rutherglen",
            "body": "-\tcommitMergedDeletes uses the docIdMap from commitMerge SegmentMerger to map new deletes to the newly merged segment\n\n-\tRemoved applyDeletesImmediately\n\n-\tHaven't solved the decRef issue which appears in TestIndexWriterReader.testDeleteFromIndexWriter.  It doesn't show up in the debugger.\n\n{code}\n[junit] java.lang.AssertionError\n    [junit] \tat org.apache.lucene.index.SegmentReader$Ref.incRef(SegmentReader.java:103)\n    [junit] \tat org.apache.lucene.index.SegmentReader.incRef(SegmentReader.java:341)\n    [junit] \tat org.apache.lucene.index.IndexWriter$SegmentReaderPool.get(IndexWriter.java:647)\n    [junit] \tat org.apache.lucene.index.IndexWriter$SegmentReaderPool.getReadOnlyClone(IndexWriter.java:615)\n    [junit] \tat org.apache.lucene.index.MultiSegmentReader.<init>(MultiSegmentReader.java:86)\n    [junit] \tat org.apache.lucene.index.ReadOnlyMultiSegmentReader.<init>(ReadOnlyMultiSegmentReader.java:35)\n    [junit] \tat org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:396)\n    [junit] \tat org.apache.lucene.index.TestIndexWriterReader.testDeleteFromIndexWriter(TestIndexWriterReader.java:159)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)\n\n{code}\n",
            "date": "2009-02-26T01:33:44.360+0000",
            "id": 43
        },
        {
            "author": "Jason Rutherglen",
            "body": "- The patch is not committable as the debugging and comments need cleaning\n- The clone issue was hacked around so more tests pass\n- TestAtomicUpdate fails",
            "date": "2009-02-27T01:20:38.352+0000",
            "id": 44
        },
        {
            "author": "Jeremy Volkman",
            "body": "I noticed the comments about IW.getReader() flushing the current state of the writer, launching merges, etc. Does this mean that the point of interaction between index writes and reads will still be the Directory? (i.e. the disk)",
            "date": "2009-02-27T20:52:33.807+0000",
            "id": 45
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Does this mean that the point of interaction between index writes and reads will still be the Directory? (i.e. the disk)\n\nFor added docs, yes.  But deletions will carry straight through in memory.",
            "date": "2009-02-27T21:12:58.240+0000",
            "id": 46
        },
        {
            "author": "Jason Rutherglen",
            "body": "- Most tests pass. Because SegmentReaders are held by the writer,\nfiles remain open whereas before this patch they did not. This causes\nsome tests to fail. \n- I'm working a way to remove unreferenced segments from the pool ",
            "date": "2009-03-02T23:50:28.490+0000",
            "id": 47
        },
        {
            "author": "Jason Rutherglen",
            "body": "* In IW.checkpoint, SegmentReaderPool.closeUnusedSegmentReaders is\ncalled which gathers a set of all currently active segment names and\ncloses the segment readers not in the set. When the IW is closed an\nassertion checks to insure the IW.segmentInfos names are the same as\nthose in the pool. \n\n* Bunches of private variables were made package protected and need to\nbe changed back\n",
            "date": "2009-03-03T00:33:26.556+0000",
            "id": 48
        },
        {
            "author": "Jason Rutherglen",
            "body": "* In TestConcurrentMergeScheduler.testNoWaitClose I'm seeing a couple\nof exceptions. I'm not quite sure what to make of them. The\ntestNoWaitClose method calls IW.close(false) a number of times on an\nIW that uses ConcurrentMergeScheduler. \n\n{code}\n[junit] Caused by: java.lang.AssertionError\n    [junit] \tat org.apache.lucene.index.IndexFileDeleter$RefCount.DecRef(IndexFileDeleter.java:553)\n    [junit] \tat org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:470)\n    [junit] \tat org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:481)\n    [junit] \tat org.apache.lucene.index.IndexWriter.decrefMergeSegments(IndexWriter.java:4485)\n    [junit] \tat org.apache.lucene.index.IndexWriter.commitMerge(IndexWriter.java:4473)\n    [junit] \tat org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4910)\n    [junit] \tat org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:4539)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:218)\n    [junit] \tat org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:274)\n{code}\n{code}\n[junit] java.lang.AssertionError\n    [junit] \tat org.apache.lucene.index.IndexWriter.finishMerges(IndexWriter.java:3350)\n    [junit] \tat org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:2090)\n    [junit] \tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:2045)\n    [junit] \tat org.apache.lucene.index.TestConcurrentMergeScheduler.testNoWaitClose(TestConcurrentMergeScheduler.java:211)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)\n{code}",
            "date": "2009-03-03T18:31:39.768+0000",
            "id": 49
        },
        {
            "author": "Jason Rutherglen",
            "body": "The IndexFileDeleter$RefCount.DecRef exception is related to an incorrect ref count of .del files when TestConcurrentMergeScheduler is run.",
            "date": "2009-03-04T00:34:44.885+0000",
            "id": 50
        },
        {
            "author": "Jason Rutherglen",
            "body": "Because the patch does not flush deletes to disk like the existing\ncode does, the SegmentInfo delGen etc isn't updated at the same\npoints as expected. In tracing the code the .del files that have 0\nrefs then are decRefed throwing the assertion error were never\nincRefed by IndexFileDeleter. This is because the deletes are carried\nover in the IW internal readers. Eventually the .del files are\nwritten but aren't increfed as they usually would be. Is there a good\nsolution to this? Do we need an interim SegmentInfo variable that\nindicates future flushed deletions? ",
            "date": "2009-03-04T00:51:27.934+0000",
            "id": 51
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. I added a flushDeletesToDir flag that defaults to true except for IW.getReader.\n\nI think that should not be necessary.  On releasing a reader, if it\nhas pending changes and we want to drop it from the pool, we should\nmove its deletes to the directory?\n\nSegmentMerger should clear the hasChanges on the reader after its done\nmerging, so we don't bother saving delete files for segments that were\nmerged away.\n\nbq. In TestConcurrentMergeScheduler.testNoWaitClose I'm seeing a couple of exceptions.\n\nI think the first is the root cause (and causes the 2nd one).\n\nWhich test case hits that?  The test cases that intentionally throw\nexceptions at interesting times are especially fun to debug ;)\n\n{quote}\nBecause the patch does not flush deletes to disk like the existing\ncode does, the SegmentInfo delGen etc isn't updated at the same\npoints as expected.\n{quote}\n\nIFD should be fine with this -- there must be something else at play,\ncausing us to over-decRef.\n\nOther notes:\n\n  * You still have SegmentInfoKey (see above)\n\n  * The new DirectoryIndexReader.open should just take IndexWriter\n    (see above)?\n\n  * Rather than the getSegmentsInUse \"polling\" approach, I think you\n    can incrementally add & remove readers from the pool?  EG after a\n    merge commits, drop the readers for the just-merged segments and\n    add a reader for the newly merged segment (and using it to hold\n    the deletes carried over in commitMergedDeletes).  I think for\n    just-flushed segments you can wait for a getReader() call to\n    happen again, to init their readers, unless something else does\n    first (flushing deletes, merging)?\n\n  * You commented out the last part of commitMergedDeletes, that\n    actually saves the deletes.  You need to instead get the reader\n    for the merged segment from the pool and hand it the new deletes.\n\n  * We lost the cutover to using the already computed docMap in\n    commitMergedDeletes?  We should put that back -- it saves having\n    to read in the prior deletions from disk.\n",
            "date": "2009-03-04T19:58:29.106+0000",
            "id": 52
        },
        {
            "author": "Jason Rutherglen",
            "body": "* TestConcurrentMergeScheduler is passing\n\n* A handful of other tests don't pass, looking into them next\n\n* DirectoryIndexReader.open with a writer isn't necessary so the\nrelated method is removed\n\n* I'm not sure we need to write out the deletes of unused segment\nreaders because they are no longer used and so should not be\nrequired. I realized the unused readers needs to also check with\nthe deletion policy?\n\n* The TestConcurrentMergeScheduler. testNoWaitClose exception is\ncaused by the way .del files are created when SR.commitChanges is\ncalled, altering the SR segmentinfo without IFD.incRefing the .del\nfile. The solution, manually incref the .del file after the pool\ncommits on the SRs. Before this process was handled transparently by\nIW.checkpoint. Also the other issue was the SegmentInfo files\nvariable was created before the delGen change, then the .del file was\nnot being returned by SI.files(). With the .del file being returned\nthe IFD related exception went away.\n\n* SegmentInfoKey is removed, SegmentInfo is used as the key for\nSegmentReaderPool.get\n\n> You commented out the last part of commitMergedDeletes, that\nactually saves the deletes. You need to instead get the reader for\nthe merged segment from the pool and hand it the new deletes.\n\nI wasn't sure what you meant by this, in the patch the deletes are\ncopied into the merged reader. Do you mean instead the merged reader\nshould not be opened and instead the deletes file needs to be written\nto?\n\n> We lost the cutover to using the already computed docMap in\ncommitMergedDeletes?\n\nSegmentMerger wasn't always returning the docMap so I stopped using\nit.  ",
            "date": "2009-03-05T00:42:51.570+0000",
            "id": 53
        },
        {
            "author": "Jason Rutherglen",
            "body": "A little over zealous in altering the flushDeletesToDisk flag in IW.flush",
            "date": "2009-03-05T01:11:10.655+0000",
            "id": 54
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Jason, good progress...\n\n{quote}\nI'm not sure we need to write out the deletes of unused segment\nreaders because they are no longer used and so should not be\nrequired.\n{quote}\n\nEG I think DocumentsWriter.applyDeletes should get the reader from\npool, do deletes, then call pool.release(reader), and that release\nwould write the changes to disk if we're not pooling readers.  Then we\ndon't need a new \"flushDeletesToDir\" propogated around.\n\nOn an explicit commit(), we should also sweep the pool and write\nchanges to disk for any SR that has pending changes.\n\n{quote}\n> You commented out the last part of commitMergedDeletes, that\nactually saves the deletes. You need to instead get the reader for\nthe merged segment from the pool and hand it the new deletes.\n\nI wasn't sure what you meant by this, in the patch the deletes are\ncopied into the merged reader. Do you mean instead the merged reader\nshould not be opened and instead the deletes file needs to be written\nto?\n{quote}\n\nWoops -- sorry: you are doing it correctly (applying to the\nmergedReader).  I missed that.\n\nbq. SegmentMerger wasn't always returning the docMap so I stopped using it.\n\nIt should always return it, unless there were no deletes on that\nsegment when the merge started, in which case the docMap is null.  But\nyou can handle that case (just check if there are now any deletes, and\ncarry them over).\n\n(Then we shouldn't need merge.segmentReadersClone, again).\n\n{quote}\nThe solution, manually incref the .del file after the pool\ncommits on the SRs.\n{quote}\n\nThat's spooky -- why exactly is it needed?  We should only do an extra\nincRef if we can explain exactly which decRef it will correspond to.\nShouldn't IW.checkpoint still work properly for the .del files?\n\nAlso, SegmentInfo.files should be cleared whenever delGen is advanced\n-- can you give more details on what path doesn't properly clear the\nfiles?\n\nThe fact that an SR can carry deletes in memory without writing a new\n.del file should not impact IFD.  Whenever we do advance delGen and\nwrite new .del files, then we must call checkpoint().\n\nSo I think we need to really explain the root cause here...\n\nOthers:\n\n  * You overrode decRef in DirectoryIndexReader, to not write changes\n    whenever a writer is present, but I think that a better approach\n    is to leave decRef as it is and then from writer, clear hasChanges\n    when you want to discard them (because they were merged away).\n\n  * You inserted applyDeletes at the top of commitMergedDeletes -- why\n    was that needed?\n\n  * There's alot of noise in the patch -- whitespace changes,\n    commented out debug code, etc.  Can you remove some of it?  It\n    makes it harder to separate signal from noise...\n",
            "date": "2009-03-05T10:10:33.733+0000",
            "id": 55
        },
        {
            "author": "Jason Rutherglen",
            "body": "Here's a summary of the which tests are failing grouped by the\nguessed cause:\n\n* TestDeletionPolicy, TestIndexFileDeleter, TestIndexReader fails (not\nall methods but they seem to be related, i.e. files are being left\nopen that should not be)\n\n* In TestIndexWriter most test methods pass, however a few such as\ntestCommitOnCloseDiskUsage fail.\nTestIndexWriterDelete.testUpdatesOnDiskFull fails for reasons\npresumably similar to TestIndexWriter.testCommitOnCloseDiskUsage\n\n* TestTransactions and TestStressIndexing2 fails. At first glance I'm\nnot sure why ",
            "date": "2009-03-05T21:11:03.917+0000",
            "id": 56
        },
        {
            "author": "Jason Rutherglen",
            "body": "On IW.close -> SegmentReaderPool.flushChanged ->\nsegmentreader.commitChanges was being called on SRs that had no\nchanges. TestDeletionPolicy and TestIndexFileDeleter passes.\n\n",
            "date": "2009-03-05T21:37:59.422+0000",
            "id": 57
        },
        {
            "author": "Jason Rutherglen",
            "body": "TestIndexReader.testDocsOutOfOrderJIRA140 fails because IW.close\nisn't called before dir.close. Is this a bug in the unit test?",
            "date": "2009-03-05T21:44:13.981+0000",
            "id": 58
        },
        {
            "author": "Jason Rutherglen",
            "body": "TestIndexWriterDelete.testOperationsOnDiskFull fails with\nMockRAMDirectory.close (still open files) because IW.close isn't\ncalled.\n\nTestIndexWriter.testImmediateDiskFullWithThreads fails because\nIW.close fails on the disk full exception. Should IW.closeInternal ->\nSegmentReaderPool.close be placed in the finally clause?\n\nTestTransactions fails because the purposely random failure occurs\nduring IW.close (SRP.close specifically) \n\nThe TestStressIndexing2 failure could be tricky",
            "date": "2009-03-05T22:46:53.111+0000",
            "id": 59
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nTestIndexReader.testDocsOutOfOrderJIRA140 fails because IW.close\nisn't called before dir.close. Is this a bug in the unit test?\n{quote}\nYes, this looks like a bug in the test.  This also means when we eventually commit this, we'll have to first fix that bug on the back-compat tests branch.\n\n{quote}\nTestIndexWriterDelete.testOperationsOnDiskFull fails with\nMockRAMDirectory.close (still open files) because IW.close isn't\ncalled.\n{quote}\nI don't understand: that test looks like it does call IW.close for all IW's opened?  (It's a little tricky, because modifier.close gets called the 2nd time the for(int x = 0...) loop runs).\n\n{quote}\nTestIndexWriter.testImmediateDiskFullWithThreads fails because\nIW.close fails on the disk full exception. Should IW.closeInternal ->\nSegmentReaderPool.close be placed in the finally clause?\n{quote}\nDoes the 2nd call to close (close(false)) also hit an exception?  Perhaps, modify the test so that if that 2nd close hits an exception, call abort?\n\nIt's good that you're down to mainly the exceptions-based test failures... though I think you should focus more on the bigger structural changes to the approach (eg switching back to docMap for merging deletes, adding SegmentReaderPool.release, which should write changes to the dir & calling it from all places that do a .get(), and the other comments above) before trying to get all tests to pass.",
            "date": "2009-03-06T12:25:30.671+0000",
            "id": 60
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote} On an explicit commit(), we should also sweep the pool and\nwrite changes to disk for any SR that has pending changes. {quote}\n\nI created SegmentReaderPool.commitAll which commits changes for all\nSRs in the pool in IW.startCommit.\n\nbq. applyDeletes at the top of commitMergedDeletes\n\nRemoved\n\nbq. switching back to docMap for merging deletes\n\nWhere should I get the numDeletedDocs from? (Used for docUpto +=\ndocCount - previousReader.numDeletedDocs()) Should the entire\ndocIdMap be scanned? Is there an expense in cloning the segment\nreaders besides the extra bitvectors?\n\n* SegmentReaderPool.release is implemented instead of using\nreader.decRef. I think you're saying put this patch's decRef logic in\nthe SRP.release method? \n\n* Added IW.close to TestIndexReader.testDocsOutOfOrderJIRA140\n\n* TestTransactions may sometimes be failing legitimately during the\nprepareCommit, the exception can't be reliably reproduced but perhaps\nanother test case can be written that does\n\n* TestIndexWriterDelete.testErrorAfterApplyDeletes fails due to\nIW.commit not throwing an expected exception ",
            "date": "2009-03-06T19:59:41.650+0000",
            "id": 61
        },
        {
            "author": "Michael McCandless",
            "body": "Jason can you resync to trunk?",
            "date": "2009-03-06T22:27:35.870+0000",
            "id": 62
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. Where should I get the numDeletedDocs from?\n\nSegmentMerger.getDelCounts()\n\nbq. Should the entire docIdMap be scanned?\n\nYes, when the before/after delCount is different.\n\nbq. Is there an expense in cloning the segment readers besides the extra bitvectors?\n\nCopy-on-write of the bitvectors (time and space) is the biggest cost I\nthink.  Since docMap already has everything we need, I think we should\nuse it... we could even separate out this change and do it first, if\nyou want.\n\n{quote}\nI think you're saying put this patch's decRef logic in\nthe SRP.release method?\n{quote}\n\nActually the release method should always commit changes, if there are\nany, when an SR is removed SR from the pool.\n\nThen, something higher up (merging, once successful) should clear\nchanges when it's safe, so that release doesn't save anything.\n\nThe \"decRef that never commits whenever writer is present\" in\nDirectoryIndexReader is too low level, I think.\n",
            "date": "2009-03-06T23:02:55.096+0000",
            "id": 63
        },
        {
            "author": "Jason Rutherglen",
            "body": "* The patch is updated to trunk, in most tests \"there are still open\nfiles\" failures occur now (maybe it's not related to the latest revision)\n\n* mergeMiddle isn't synchronized so if we use a pooled reader\n(instead of a frozen clone) couldn't deletes be applied to the SR as\nmerging is happening? ",
            "date": "2009-03-06T23:24:33.497+0000",
            "id": 64
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nmergeMiddle isn't synchronized so if we use a pooled reader\n(instead of a frozen clone) couldn't deletes be applied to the SR as\nmerging is happening?\n{quote}\nHmm -- good point.  You're right, we need to clone the reader up front, so deletions don't change during the merging process.\n\nIn which case let's just stick with the current approach in the patch?",
            "date": "2009-03-08T01:14:34.308+0000",
            "id": 65
        },
        {
            "author": "Michael McCandless",
            "body": "Jason can I have the conch here?  (Ie take your patch and make some changes / iterate).  Are all of your changes included in the last patch?",
            "date": "2009-03-17T15:09:12.375+0000",
            "id": 66
        },
        {
            "author": "Jason Rutherglen",
            "body": "Hi Mike, yes please!  I have not had time to work on it.  All my changes are in the last patch.  I can clean it up, but it would not be until later today.",
            "date": "2009-03-17T16:06:44.878+0000",
            "id": 67
        },
        {
            "author": "Michael McCandless",
            "body": "OK, don't worry about cleaning things up -- I'll start from what's here now.  Thanks!",
            "date": "2009-03-17T16:49:11.271+0000",
            "id": 68
        },
        {
            "author": "Michael McCandless",
            "body": "New patch attached.  All tests pass.  Back-compat tests pass with no\nchanges.\n\nI made a number of changes, added asserts, cleaned things up, added\njavadocs, CHANGES entry, etc.\n\nI try to consistently refer to this as \"near real-time search\".\nCalling it realtime is overstating it, especially when you compare it\nto what \"realtime operating systems\" do.\n\nI also added IndexWriter.setMergedSegmentWarmer(..) and abstract base\nclass so the caller can have a merged segment warmed before it's\ncommitted to the index.  This is important for reducing turnaround\ntime after merges are completed; eg it means you can go ahead adding\nnew docs/deletes, and opening new readers, while the merged segment is\nbeing warmed.\n\nNet/net I think the approach here is sound, and it's a good step\nforward.  I think we can get this in for 2.9.\n\nBut, I added a few nocommits that we still need to figure out.\n\nOtherwise this is close to committable, though I'd like to let it age\nfor a while so people can look/test.\n\nWe also still need to pursue in-memory transactional data structures\nfor representing deletes/norms (LUCENE-1526 is already open for\nthis, and it should not block this issue).\n\nNext up I plan to run some basic tests to see what the \"typical\" delay\nis for opening a near-realtime reader...\n",
            "date": "2009-03-27T12:45:11.775+0000",
            "id": 69
        },
        {
            "author": "Michael McCandless",
            "body": "OK I ran a basic initial test of the latency when opening a near\nreal-time reader.\n\nUsing contrib/benchmark, I index wikipedia docs like normal, but then\nI added a NearRealTimeReaderTask, which runs a BG thread that once\nevery N (I did 3) seconds it gets a new reader from the writer.\n\nThen it does a simple search for term \"1\" in the body, and sorts by\nthe docdate field.\n\nI measured milli-seconds to reopen and to run the search, and plot\nthose as a function of index size.\n\nI ran two tests.  The first (attached as ssd.png) stores the index on\nan Intel X25M solid-state disk; the second (attached as magnetic.png)\non a WD Velociraptor.\n\nNotes:\n\n  * The reopen time is ~700 msec in both cases, and doesn't change\n    much as index grows (which is nice).\n\n  * It is quite noisy, likely due to merges committing.\n\n  * I logged (but did not graph) the flush time vs actual reopen time,\n    and it's the flush time that dominates.  This is good because with\n    a slower indexing rate, this flush time would go way down.  My\n    guess is flushing is CPU bound not IO bound.\n\n  * The search time ramps up linearly (expected), and also shows\n    spikes due to merging.  There's one massive spike at the end of\n    the SSD one that's odd (did not correspond to reopening after a\n    merge, though perhaps during a merge).\n\n  * This is a somewhat overly stressful test because I'm indexing docs\n    at full speed.  Whereas I'd expect for the typical near realtime\n    search app, the docs would usually be trickling in more slowly and\n    a reopen would happen after just a few docs.\n\n  * SSD and magnetic look pretty darn similar, though magnetic shows\n    more noise and maybe is more affected by merges.\n\n  * I'm doing no deletions in this test, but the typical near\n    real-time app presumably would.\n",
            "date": "2009-03-27T19:43:20.950+0000",
            "id": 70
        },
        {
            "author": "Michael McCandless",
            "body": "Disregard the search time in the above results... we have a sneaky bug\n(LUCENE-1579) that is causing FieldCache to not be re-used for shared\nsegments in a reopened reader.  This makes the search time after\nreopen far worse than it should be.\n",
            "date": "2009-03-28T16:06:23.011+0000",
            "id": 71
        },
        {
            "author": "Michael McCandless",
            "body": "New patch:  Fixed a few small issues... and made some changes to\ncontrib/benchmark to help in running more realistic near real-time\ntests:\n\n  * Fixed LineDocMaker to properly set \"docid\" primary key field.\n\n  * Added UpdateDocTask that calls IndexWriter.updateDocument,\n    randomly picking a docid.\n\n  * Added NearRealTimeReader task, that creates BG thread that every N\n    seconds opens a reader, runs a static search and prints results.\n\nThis patch also contains patch from LUCENE-1579.\n\nSo, using this you can 1) create a large index, 2) create an alg that\ndoes doc updates at a fixed rate and then tests the near real-time\nreader performance.\n",
            "date": "2009-03-28T22:06:30.396+0000",
            "id": 72
        },
        {
            "author": "Michael McCandless",
            "body": "\nOK using the last patch, I ran another near real-time test, using this\nalg:\n\n{code}\n\nanalyzer=org.apache.lucene.analysis.standard.StandardAnalyzer\n\ndoc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n\nmerge.policy=org.apache.lucene.index.LogDocMergePolicy\n\ndocs.file=/Volumes/External/lucene/wiki.txt\ndoc.stored = false\ndoc.term.vector = false\ndoc.add.log.step=10\nmax.field.length=2147483647\n\ndirectory=FSDirectory\nautocommit=false\ncompound=false\nmerge.factor = 10\nram.flush.mb = 128\ndoc.maker.forever = false\ndoc.random.id.limit = 3204040\n\nwork.dir=/lucene/work\n\n{ \"BuildIndex\"\n  - OpenIndex\n  - NearRealtimeReader(1)\n   { \"UpdateDocs\" UpdateDoc > : 100000 : 50/sec\n  - CloseIndex\n}\n\nRepSumByPrefRound BuildIndex\n{code}\n\nIt opens a full (3.2M docs, previously built) wikipedia index, then\nrandomly selects a doc and updates it (deletes old, adds new) at the\nrate of 50 docs/sec.  Then, once per second I open a new reader, do\nthe same search (term \"1\", sorted by date).\n\nI attached another graph (ssd2.png) with the results, showing reopen &\nsearch time as a function of how many updates have been done; rough\ncomments:\n\n  * Search time is pretty constant ~35 msec, except occassional\n    glitches where it goes as high as ~340 msec.  Net/net very\n    reasonable I think.\n\n  * Search time is remarkably non-noisy, except for occasional\n    spikes.\n\n  * Reopen time is also fast (~ 40 msec) but is more noisy.\n\n  * It's not clear the merges are really impacting things that much.\n    It could simply be that I didn't run test for long enough for a\n    big merge to run.  Also, this index has no stored fields nor term\n    vectors, so if we added those, merges would get slower.\n\n  * This is a better test than last one, since it's doing some deletes\n\n  * Since I open writer with autoCommit false, and near-realtime\n    carries all pending deletes in RAM, no *.del file ever gets\n    written to the index\n",
            "date": "2009-03-29T13:55:58.146+0000",
            "id": 73
        },
        {
            "author": "Jason Rutherglen",
            "body": "Mike, nice work!  \n\nI will hopefully test this week.",
            "date": "2009-03-30T22:07:44.188+0000",
            "id": 74
        },
        {
            "author": "Michael McCandless",
            "body": "Nice work to you too -- I just picked things up where you left off!  I'm still working on the nocommits, but I think we can commit this soon.",
            "date": "2009-03-30T22:49:10.795+0000",
            "id": 75
        },
        {
            "author": "Michael McCandless",
            "body": "New patch: sync'd to trunk, cleaned up the nocommits (some fixing, some turned into TODOs).\n\nI'd like to add some more tests, but otherwise I think this is ready to commit.",
            "date": "2009-03-31T16:07:46.742+0000",
            "id": 76
        },
        {
            "author": "Michael McCandless",
            "body": "Attached patch: added more test cases, and fixed the near real-time reader to NOT try to open segments in a foreign Directory (which happens when addIndexes* is running).",
            "date": "2009-03-31T19:22:44.891+0000",
            "id": 77
        },
        {
            "author": "Michael McCandless",
            "body": "Added another test case to TestIndexWriterReader, stress testing adding/deleting docs while constant opening near real-time reader.",
            "date": "2009-04-02T09:00:53.970+0000",
            "id": 78
        },
        {
            "author": "Jason Rutherglen",
            "body": "In ReaderPool.get(SegmentInfo info, boolean doOpenStores, int readBufferSize) the readBufferSize needs to be passed into SegmentReader.get",
            "date": "2009-04-03T00:25:49.463+0000",
            "id": 79
        },
        {
            "author": "Michael McCandless",
            "body": "Good catch!  I'll fix.",
            "date": "2009-04-03T09:19:07.415+0000",
            "id": 80
        },
        {
            "author": "Michael McCandless",
            "body": "I think NRT search is finally ready to go in... I'll wait another day or two.",
            "date": "2009-04-07T09:08:00.787+0000",
            "id": 81
        },
        {
            "author": "Michael McCandless",
            "body": "I just committed this.  Thanks Jason!",
            "date": "2009-04-09T17:18:13.223+0000",
            "id": 82
        },
        {
            "author": "Jason Rutherglen",
            "body": "I noticed NearRealtimeReaderTask is in trunk, can you include the .alg?  How does one create a line based wiki file?",
            "date": "2009-04-17T22:19:31.608+0000",
            "id": 83
        },
        {
            "author": "Shai Erera",
            "body": "look at extractWikipedia.alg, which uses the EnwikiDocMaker, the enwiki.xml file and the WriteLineDoc task. BTW, with LUCENE-1591 you can run this alg with bzip.compression=true and feed it with the .bz2 file rather than the xml.",
            "date": "2009-04-18T02:32:00.604+0000",
            "id": 84
        },
        {
            "author": "Michael McCandless",
            "body": "This is the alg I use:\n\n{code}\n{ \"BuildIndex\"\n  - OpenIndex\n  - NearRealtimeReader(1)\n   { \"UpdateDocs\" UpdateDoc > : 100000 : 50/sec\n  - CloseIndex\n}\n{code}\n\nIe, using a BG thread, the NRT reader reopens once per second, runs a fixed search, and prints timing info.\n\nMeanwhile, a single thread replaces the docs at the rate of 50/sec up to 100K docs.",
            "date": "2009-04-18T11:08:08.311+0000",
            "id": 85
        },
        {
            "author": "Jason Rutherglen",
            "body": "We need an IndexWriter.getMergedSegmentWarmer method?",
            "date": "2009-04-20T19:19:54.420+0000",
            "id": 86
        },
        {
            "author": "Michael McCandless",
            "body": "bq. We need an IndexWriter.getMergedSegmentWarmer method?\n\nYes, I just committed; thanks!",
            "date": "2009-04-24T17:32:51.418+0000",
            "id": 87
        },
        {
            "author": "Jason Rutherglen",
            "body": "In LogMergePolicy.findMergesToExpungeDeletes I think we need to\nchange how deletes are checked for. I added this change into\nLUCENE-1313, however as we're about to release 2.9, we probably\nneed to implement it. \n\nCurrently we check the info for deletes, however with this\npatch, I think we need to check the segmentReader which could\nhave deletes that don't show up in the info.",
            "date": "2009-06-17T00:05:00.332+0000",
            "id": 88
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nCurrently we check the info for deletes, however with this\npatch, I think we need to check the segmentReader which could\nhave deletes that don't show up in the info.\n{quote}\n\nGood catch!  Can you open a new issue & attach patch?  Though: how would you do this?  Right now MergePolicy never receives a SegmentReader, and makes all its decisions based on the SegmentInfo.  Each SegmentReader tracks its own pendingDelCount... maybe we add a private pendingDelCount to SegmentInfo, and change SegmentReader to use that instead?  That'd be a single source, and then the merge policy could retrieve it...",
            "date": "2009-06-17T08:56:25.000+0000",
            "id": 89
        },
        {
            "author": "Jason Rutherglen",
            "body": "I think we will want to do something like what field cache does\nwith CreationPlaceholder for IndexWriter.readerPool. Otherwise\nwe have the (I think somewhat problematic) issue of all other\nreaderPool.get* methods waiting for an SR to warm. \n\n{code}\n  public synchronized SegmentReader get(SegmentInfo info, boolean doOpenStores, int readBufferSize) throws IOException {\n      if (poolReaders) {\n        readBufferSize = BufferedIndexInput.BUFFER_SIZE;\n      }\n\n      SegmentReader sr = (SegmentReader) readerMap.get(info);\n      if (sr == null) {\n        // TODO: we may want to avoid doing this while\n        // synchronized\n        // Returns a ref, which we xfer to readerMap:\n        sr = SegmentReader.get(info, readBufferSize, doOpenStores);\n        readerMap.put(info, sr);\n      } else if (doOpenStores) {\n        sr.openDocStores();\n      }\n\n      // Return a ref to our caller\n      sr.incRef();\n      return sr;\n    }\n{code}",
            "date": "2009-06-30T20:10:30.640+0000",
            "id": 90
        }
    ],
    "component": "",
    "description": "The current problem is an IndexReader and IndexWriter cannot be open\nat the same time and perform updates as they both require a write\nlock to the index. While methods such as IW.deleteDocuments enables\ndeleting from IW, methods such as IR.deleteDocument(int doc) and\nnorms updating are not available from IW. This limits the\ncapabilities of performing updates to the index dynamically or in\nrealtime without closing the IW and opening an IR, deleting or\nupdating norms, flushing, then opening the IW again, a process which\ncan be detrimental to realtime updates. \n\nThis patch will expose an IndexWriter.getReader method that returns\nthe currently flushed state of the index as a class that implements\nIndexReader. The new IR implementation will differ from existing IR\nimplementations such as MultiSegmentReader in that flushing will\nsynchronize updates with IW in part by sharing the write lock. All\nmethods of IR will be usable including reopen and clone. \n",
    "hasPatch": true,
    "hasScreenshot": true,
    "id": "LUCENE-1516",
    "issuetypeClassified": "REFACTORING",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Integrate IndexReader with IndexWriter ",
    "systemSpecification": true,
    "version": "2.4"
}