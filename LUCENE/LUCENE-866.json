{
    "comments": [
        {
            "author": "Michael Busch",
            "body": "Attaching the first version of this patch.",
            "date": "2007-04-21T01:43:10.929+0000",
            "id": 0
        },
        {
            "author": "Michael Busch",
            "body": "I ran a few performance tests with this patch. I built a rather big \nindex (about 1.2GB) using documents from Wikipedia and optimized the\nindex to get big posting lists.\n\nI compare query evaluation time with one-level skipping (old) and\nmulti-level skipping (new) and measure the amount of I/O by counting \nthe number of VInts read from disk. Each query runs several thousand\ntimes.\n\nThe following queries contain very frequent and very unique terms. \nFor these queries the speedup with multi-level skipping is \nsignificant:\n\n   Query: +lucene +search +engine +library\n   5 total matching documents\n   VInt reads: old: 143268000, new: 3933000, -97.25479520897898%\n   Time: old: 7234ms, new: 1157ms, -84.00608238871993%\n\n   Query: +apache +http +server\n   181 total matching documents\n   VInt reads: old: 155892000, new: 27849000, -82.13570933723346%\n   Time: old: 10656ms, new: 5703ms, -46.48085585585586%\n\n\nEven though I/O is reduced for the next query, it runs a bit slower.\nI believe the reason is that the same query runs several thousand\ntimes, so the posting lists will be loaded into the file system\ncache and the effect of less I/O is reduced, while the skipping\nalgorithm itself is a bit more complex:\n\n   Query: +multi +level +skipping\n   13 total matching documents\n   VInt reads: old: 42894000, new: 39096000, -8.854385228703315%\n   Time: old: 3875ms, new: 3922ms, 1.2129032258064516%\n\n\nFor the next query there is slightly more I/O necessary in the \nmulti-skipping case. This is because not many big skips can be made,\nbut more skipping data has to be read. The top 2 skip levels are \nbuffered in this first version of the patch and if no big skips\ncan be made than this buffering is overhead compared to the \nsingle-level case:\n\n   Query: +beatles +yellow +submarine\n   78 total matching documents\n   VInt reads: old: 38460000, new: 38685000, 0.5850234009360374%\n   Time: old: 3172ms, new: 3265ms, 2.9319041614123584%\n\nHowever, if I change the query a little bit, then the speed-up\nis significant (due to the very frequent stop word \"the\"):\n\n   Query: +\"the beatles\" +yellow +submarine\n   77 total matching documents\n   VInt reads: old: 382307000, new: 262331000, -31.38210914265237%\n   Time: old: 26703ms, new: 22828ms, -14.51147811107366%\n   \n   \nIt would be interesting to run more sophisticated benchmarks.\nTo run the same query several times is not very realistic \nbecause it reduces the effects of the I/O savings due to caching.\n\nI'm not that familiar with the new benchmark stuff that has\nbeen added recently, but I'll try to dig into that next week.",
            "date": "2007-04-21T02:21:22.368+0000",
            "id": 1
        },
        {
            "author": "Yonik Seeley",
            "body": "Looks interesting!\n\nHave you done any performance testing?  I guess best case for this patch would be \"skip to a random doc\", and worst case would be skipTo(currDoc+2) in a loop  (or a conjunction across terms that appear in almost all docs).\n\nI haven't dug into the code, but do you avoid storing extra levels when the docfreq is small?\nWhat is the size cost for terms with docfreq=1?\n\n",
            "date": "2007-04-21T02:43:13.310+0000",
            "id": 2
        },
        {
            "author": "Michael Busch",
            "body": "> and worst case would be skipTo(currDoc+2) in a loop \n> (or a conjunction across terms that appear in almost all docs).\n\nTrue that should be the worst case. A query with 3 stop words\ntakes about 2% longer:\n\n   Query: +the +and +a\n   682737 total matching documents\n   VInt reads: old: 481897700, new: 481416800, -0.09979296435737295%\n   Time: old: 27812ms, new: 28406ms, 2.1357687329210413%\n\nMaybe a possible optimization could be to avoid using the higher levels\nafter a certain amount of small skips have been performed. I will try out if \nthis will improve things.\n\n> do you avoid storing extra levels when the docfreq is small?\n> What is the size cost for terms with docfreq=1? \n\nI only store another level if it contains at least one SkipDatum.\nSo there is no overhead for terms with df=1.\n\nThe .frq file in my index grew by 1.3% in the multi-level case\nfor an index with about 170MB.\n\n",
            "date": "2007-04-21T03:09:54.729+0000",
            "id": 3
        },
        {
            "author": "Michael Busch",
            "body": "LUCENE-888 will introduce the new method BufferedIndexInput.setBufferSize(). Since the length of the different skip levels is known I can set the buffer length before I clone the skip stream in this patch.",
            "date": "2007-05-25T00:29:45.069+0000",
            "id": 4
        },
        {
            "author": "Michael Busch",
            "body": "I ran some more performance experiments to test the average speedup.\nI used the same index (about 1.2GB, optimized, docs from wikipedia)\nand created 50,000 random queries. Each query has 3 AND terms and \neach term has a df > 100. \n\nHere are the results:\n\nold (one level skipping):\nTime: 62141 ms.\nVInt reads: 752430441\n\nnew (multi level):\nTime: 51969 ms.\nVInt reads: 435504734\n\nThis is a speedup of about 16% and i/o savings of 42%.\n\nThen I changed the algorithm a bit to not always start on the \nhighest skip level to find the next skip but to start at level 0\nand walk up until the next skip point is greater than the target.\nThis speeds up things further so that the overall gain is about\n20%:\n\nTime: 49500 ms.\nBytes read: 435504734\n\nThis 20% speedup is for average AND queries. Certain queries\nbenefit even more from multi-level skipping as the results\nshow that I attached earlier.\n\n\nI will attach a new patch as soon as LUCENE-888 is committed.",
            "date": "2007-05-25T21:43:17.357+0000",
            "id": 5
        },
        {
            "author": "Michael Busch",
            "body": "New version of the patch with the following changes:\n\n- Applies cleanly on current trunk\n\n- Renamed AbstractSkipListReader and AbstractSkipListWriter\n  to MultiLevelSkipListReader and MultiLevelSkipListWriter\n  \n- The skipTo() algorithm now starts with the lowest level\n  and walks up until it finds the highest level that has a\n  skip for the target. \n  \n- Uses the new BufferedIndexInput.setBufferSize() method to\n  reduce memory consumption in case skip levels occupy less\n  space than BufferedIndexInput.BUFFER_SIZE (1024 bytes)\n  \n- Only preload the top level into memory. That saves memory\n  and almost doesn't affect performance.\n  \nAll unit tests pass.\n\nI think this patch is ready to commit now.",
            "date": "2007-05-29T18:12:47.149+0000",
            "id": 6
        },
        {
            "author": "Paul Elschot",
            "body": "Patch applies cleanly here.\nAll tests pass, except for org.apache.lucene.store.TestBufferedIndexInput:\n\n    [junit] Testsuite: org.apache.lucene.store.TestBufferedIndexInput\n    [junit] Tests run: 4, Failures: 0, Errors: 1, Time elapsed: 2.536 sec\n    [junit]\n    [junit] Testcase: testSetBufferSize(org.apache.lucene.store.TestBufferedIndexInput):        Caused an ERROR\n    [junit] null\n    [junit] java.lang.NullPointerException\n    [junit]     at org.apache.lucene.store.BufferedIndexInput.setBufferSize(BufferedIndexInput.java:52)\n    [junit]     at org.apache.lucene.store.TestBufferedIndexInput$MockFSDirectory.tweakBufferSizes(TestBufferedIndexInput.java:210)\n    [junit]     at org.apache.lucene.store.TestBufferedIndexInput.testSetBufferSize(TestBufferedIndexInput.java:164)\n    [junit]\n    [junit]\n    [junit] Test org.apache.lucene.store.TestBufferedIndexInput FAILED\n\nIt's possible that this is due to another diff in my working copy, but a quick look did\nnot reveal anything suspicious. In case I'm the only one with this test failing, I'll dig deeper.\n\n",
            "date": "2007-05-29T21:19:12.714+0000",
            "id": 7
        },
        {
            "author": "Doron Cohen",
            "body": "On a clean checkout it applies cleanly and this test (and all others) pass.",
            "date": "2007-05-29T21:36:52.512+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "Hmm -- that NullPointerException I think is from the assert I added for LUCENE-888.  I will fix.",
            "date": "2007-05-29T21:49:42.798+0000",
            "id": 9
        },
        {
            "author": "Paul Elschot",
            "body": "The last change to BufferedIndexInput indeed makes all core tests pass again here with this patch applied.",
            "date": "2007-05-30T18:15:17.460+0000",
            "id": 10
        },
        {
            "author": "Michael Busch",
            "body": "Thanks for reviewing Paul and Doron, and thanks for fixing the NPE, Mike!!\n\nI'm planning to commit this in a day or so.",
            "date": "2007-05-30T18:23:22.863+0000",
            "id": 11
        },
        {
            "author": "Yonik Seeley",
            "body": "Nice job!  \nStarting at the lowest level will probably improve the worst-case scenario for this patch (which wasn't nearly as bad as I expected in the first place).\n\n+1 to commit",
            "date": "2007-05-30T18:25:00.623+0000",
            "id": 12
        },
        {
            "author": "Michael Busch",
            "body": "This patch updates the fileformats document. It also adds a comment \nsaying that SkipDelta is only written to the tis file when DocFreq is not \nsmaller than SkipInterval (as recently mentioned by Jeff in LUCENE-349).",
            "date": "2007-05-31T06:06:07.304+0000",
            "id": 13
        },
        {
            "author": "Michael Busch",
            "body": "Committed.",
            "date": "2007-05-31T07:49:22.532+0000",
            "id": 14
        }
    ],
    "component": "core/index",
    "description": "To accelerate posting list skips (TermDocs.skipTo(int)) Lucene uses skip lists. \nThe default skip interval is set to 16. If we want to skip e. g. 100 documents, \nthen it is not necessary to read 100 entries from the posting list, but only \n100/16 = 6 skip list entries plus 100%16 = 4 entries from the posting list. This \nspeeds up conjunction (AND) and phrase queries significantly.\n\nHowever, the skip interval is always a compromise. If you have a very big index \nwith huge posting lists and you want to skip over lets say 100k documents, then \nit is still necessary to read 100k/16 = 6250 entries from the skip list. For big \nindexes the skip interval could be set to a higher value, but then after a big \nskip a long scan to the target doc might be necessary.\n\nA solution for this compromise is to have multi-level skip lists that guarantee a \nlogarithmic amount of skips to any target in the posting list. This patch \nimplements such an approach in the following way:\n\n  Example for skipInterval = 3:\n                                                      c            (skip level 2)\n                  c                 c                 c            (skip level 1) \n      x     x     x     x     x     x     x     x     x     x      (skip level 0)\n  d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d  (posting list)\n      3     6     9     12    15    18    21    24    27    30     (df)\n \n  d - document\n  x - skip data\n  c - skip data with child pointer\n \nSkip level i contains every skipInterval-th entry from skip level i-1. Therefore the \nnumber of entries on level i is: floor(df / ((skipInterval ^ (i + 1))).\n \nEach skip entry on a level i>0 contains a pointer to the corresponding skip entry in \nlist i-1. This guarantees a logarithmic amount of skips to find the target document.\n\n\nImplementations details:\n\n   * I factored the skipping code out of SegmentMerger and SegmentTermDocs to \n     simplify those classes. The two new classes AbstractSkipListReader and \n\t AbstractSkipListWriter implement the skipping functionality.\n   * While AbstractSkipListReader and Writer take care of writing and reading the \n     multiple skip levels, they do not implement an actual skip data format. The two \n\t new subclasses DefaultSkipListReader and Writer implement the skip data format \n\t that is currently used in Lucene (with two file pointers for the freq and prox \n\t file and with payload length information). I added this extra layer to be \n\t prepared for flexible indexing and different posting list formats. \n      \n   \nFile format changes: \n\n   * I added the new parameter 'maxSkipLevels' to the term dictionary and increased the\n     version of this file. If maxSkipLevels is set to one, then the format of the freq \n\t file does not change at all, because we only have one skip level as before. For \n\t backwards compatibility maxSkipLevels is set to one automatically if an index \n\t without the new parameter is read. \n   * In case maxSkipLevels > 1, then the frq file changes as follows:\n     FreqFile (.frq) --> <TermFreqs, SkipData>^TermCount\n\t SkipData        --> <<SkipLevelLength, SkipLevel>^(Min(maxSkipLevels, \n\t                       floor(log(DocFreq/log(skipInterval))) - 1)>, SkipLevel>\n\t SkipLevel       --> <SkipDatum>^DocFreq/(SkipInterval^(Level + 1))\n\n\t Remark: The length of the SkipLevel is not stored for level 0, because 1) it is not \n\t needed, and 2) the format of this file does not change for maxSkipLevels=1 then.\n\t \n\t \nAll unit tests pass with this patch.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-866",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Multi-level skipping on posting lists",
    "systemSpecification": true,
    "version": ""
}