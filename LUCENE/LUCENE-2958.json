{
    "comments": [
        {
            "author": "Doron Cohen",
            "body": "Attached patch modifies LineDocSource and WriteLineDocTask to allow the described extensions. \n\nBy default there are no modifications and behavior is as before.",
            "date": "2011-03-10T10:47:55.602+0000",
            "id": 0
        },
        {
            "author": "Doron Cohen",
            "body": "Updated patch (for 3x):\n- from 3x root (previous patch was from benchmark by mistake)\n- fixed typos in javadoc\n- simplified loop over the fields in WriteLineDocTask\n- removed *volatile* but added *final* for *fields/ToWrite*. \n\nWithout volatile one test was failing: TestPerfTasksLogic.testParallelDocMaker() but then I was unable to fail it again even after removing volatile. \n\nOnce marking these fields *final* definitely volatile is not required.\n\nBut I don't understand why was it needed in the first place - ParallelTask in TaskSequence clones the tasks, and since WriteLineDocTask does not implement clone() all (parallel) tasks will have a reference to same array... which in fact can be copied into a local copy by the JVM for efficiency.. but since the clone must take place only after the constructor is done, the array is initialized already... If I could fail this again I would investigate it but now it always passes even without final/volatile. \n\nSo keeping the final, as this is safe, but I don't like the voodooism of it and if anyone has a better explanation it would be appreciated.",
            "date": "2011-03-10T13:31:08.696+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "It's great to make line file docs more extensible!  Maybe, we should\nput the field names as a header line?  Then the source can get the\nfield names from there?\n\nfeilds is mis-spelled\n\nThe cutover to splitting by regex (vs prior split-by-char) makes me\nnervous.  Have you tested perf hit?  Or, can we go back to just using\na char sep?  We also make a String[] when we didn't before.  If we\nreally want to stick w/ regex then we should at least pre-compile to\na Pattern then use the split method on that?\n",
            "date": "2011-03-10T17:13:27.222+0000",
            "id": 2
        },
        {
            "author": "Doron Cohen",
            "body": "Thanks for reviewing Mike!\n\nNo I didn't perf the perf task change.. :-) I will look at this again. \n\nNice catch about the feild name.\n\nAs for putting the field names in the file - let's see how this would work\n\n- We need a matching pair: LineDocWriter and LineDocSource, both should expect the same fields in same order/\n- In addition, DocData has some fixed field names: ID, Body, Name, Date, Title, and a flexible Properties which can include anything else. \n- If we move to put the field names in the header line as you suggest, that would make a single line-doc-source for any lineDocWriter which is nice.\n- It would require to modify DocData so that all fields would be maintained in props )except for ID which would be still mandatory). \n\nI like it, except perhaps for performance... \nWhat do you think? Is this what you have in mind?",
            "date": "2011-03-10T17:33:47.930+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "That's exactly what I had in mind!\n\nI think for LineDocWriter we'd have to tell it up front what fields (in what order) it should write.\n\nBut then LineDocSource should be generic... though we'd still have to set up the Fields properly for indexing (ie some are stored, some are not tokenized, etc.).",
            "date": "2011-03-10T18:14:33.597+0000",
            "id": 4
        },
        {
            "author": "Shai Erera",
            "body": "bq. though we'd still have to set up the Fields properly for indexing (ie some are stored, some are not tokenized, etc.)\n\nI don't think that matters? I.e., LineDocSource returns DocData, it's the DocMaker which creates the actual Lucene Field and Document instances. So all LDS needs to know is the name of the field.\n\nbq. If we really want to stick w/ regex then we should at least pre-compile to a Pattern then use the split method on that?\n\nI agree. String.split compiles a Pattern inside, so we'd better pre-compile that pattern once, and then you pattern.split().\n\nbq. Or, can we go back to just using a char sep? We also make a String[] when we didn't before.\n\nThat's a good point Mike. But the alternative (splitting on char 'the old way') is not any better either, because you don't know in advance how many fields you expect, so you'd have to create a List or something to store them.\n\nI guess what we should be considering is super-duper optimized code vs. a nice readable one. String.split[] is easily understood, keeps the code compact and clear. Searching for SEP (the old code) is more complicated, especially when you want to handle a general case. We'll be searching for SEP both ways, so the only difference is whether an array is allocated or not.\n\nMaybe instead of doing the split ourselves, we can have a getDocData(String line), which will be implemented by default to search for TITLE, DATE and BODY, using the optimized code, and can be overridden by others to parse line differently? That way we don't impose any specific splitting behavior on everyone, but we lose the potential generality of LineDocSource.\n\nIs that array alloc() really critical?\n\nAbout writing the field names in the file -- that's a nice idea, but complicates DocData. We'd need to change it to store a Map<String,String> (or Properties) of name-value pairs. That will affect the performance of creating DocData, as well as constructing a Document out of it.\n\nIf we're willing to sacrifice some optimization here, we can do nice things. But if we want to insist on having the most optimized code, I don't think we can do much ... probably the best option is to have WLDT and LDS optimized for what they are today, and let users extend to write/read more fields. We can pass them the 'line' and let them split it however they want.",
            "date": "2011-03-10T19:11:07.183+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I don't think that matters? I.e., LineDocSource returns DocData, it's the DocMaker which creates the actual Lucene Field and Document instances. So all LDS needs to know is the name of the field.\n\nOK, that's nice.  So a simple string<SEP>string<SEP>... header could define the field names.\n\nbq. Is that array alloc() really critical?\n\nProbably fairly minor, but, this is a death-by-a-thousand-cuts situation?  Ie, these changes only make our index throughput tests slower; hopefully by a tiny amount, but it'll add up over time.\n\nbq. Maybe instead of doing the split ourselves, we can have a getDocData(String line), which will be implemented by default to search for TITLE, DATE and BODY, using the optimized code, and can be overridden by others to parse line differently?\n\nI think that's good?\n\nOr, if we do the header idea... then a given usage need not override getDocData?  Like it's generic at that point?",
            "date": "2011-03-10T19:47:51.739+0000",
            "id": 6
        },
        {
            "author": "Shai Erera",
            "body": "If we do the header idea, then we'll need to move to a more generic DocData. So instead of doing docData.title = title, you'll need to do docData.set(\"title\", title), which under the hood will store that pair in a Map or Properties. Similarly for 'getter'. That also has some implications on perf.\n\nWhat is better - generality or optimized code for the common Lucene tasks and let users extend for their own purposes?\n\nIf we want to have the most optimized code, then let's pass the line entirely to an overridable method. Lucene will offer an optimized way of tokenizing the current fields, while the user will have to either provide his own optimized way (for his fields), or decide that he can risk some cycles in favor of simpler code (e.g., calling line.split()).",
            "date": "2011-03-11T05:44:12.022+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "bq. If we do the header idea, then we'll need to move to a more generic DocData. So instead of doing docData.title = title, you'll need to do docData.set(\"title\", title), which under the hood will store that pair in a Map or Properties. Similarly for 'getter'. That also has some implications on perf.\n\nHmm, true.\n\nReally, it would be better if LineDocSource could directly set Field values.  Then, up front on parsing the header it could make a Field[], and then when parsing the line it'd just set these Field values.  \n\nBut that's a much larger change... so I think until then we should just pass the full String line to eg a processLine method?  And the default optimized one breaks it into the fixed name/date/body fields.",
            "date": "2011-03-11T11:37:14.438+0000",
            "id": 8
        },
        {
            "author": "Shai Erera",
            "body": "bq. Really, it would be better if LineDocSource could directly set Field values.\n\nThat will break the separation we have today -- ContentSource returns DocData which is not a Lucene Document, and DocMaker creates a Document out of it. Remember that we were in this design before -- DocMaker was responsible for both parsing the content and creating a Document out of it. The current design is much more flexible.\n\nbq. until then we should just pass the full String line to eg a processLine method\n\nI agree. Either processLine or getDocData or whatever, but something which receives a line and returns DocData.",
            "date": "2011-03-11T12:03:58.181+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "So the separation we have today of DocData from DocMaker allows what flexibility?  Is it just so that we can pull multiple docs from a single DocData?  EG the line file could have massive docs, but we want to index tiny docs, so DocMaker can split them up?\n\nI agree that's useful... but it does result in somewhat synthetic docs.  EG 20 docs in a row will have the same title and date (and any other properties).  If you are eval'ing a standard corpus, presumably you don't do this doc splitting, right?\n\nThe flexibility can only cost us performance (though maybe it's not so much of a hit).",
            "date": "2011-03-11T14:17:10.287+0000",
            "id": 10
        },
        {
            "author": "Shai Erera",
            "body": "No, the flexibility is in the ability to have a TrecContentSource emitting the TREC documents, and multiple DocMakers that consume them and build Lucene documents out of them.\n\nFor example, one DocMaker can decide to split each doc into N tiny docs. Another can choose to add facets to it. Yet another can do complex analysis on it and produce richer documents.\n\nBefore that, you'd have to write a DocMaker for every such combination. E.g., if you wanted to add facets, you'd need to write a DocMaker per source of data with the same impl.\n\nDocData as an intermediary object is not expensive, considering it's only bin over some already allocated Strings. And we reuse it always, so you don't even allocate it more than once ...\n\nI would hate to lose that flexibility.",
            "date": "2011-03-11T14:29:14.220+0000",
            "id": 11
        },
        {
            "author": "Doron Cohen",
            "body": "Hi, thanks Mike and Shai for the review and great comments.\n\nAttaching an updated patch.\n\nNow WriteLineDocTask writes the fields as a header line to the result file. \n\nIt always does this - perhaps a property to disable the header will be useful for allowing previous behavior (no header).\n\nThere are quite a few involved changes to LineDocSource:\n\n- replaced line.split(SEP) by original recurring search for SEP.\n- Method fillDocData(doc,fields[]) was changed to take a line String instead of the array of fields.\n- That method was wrapped in a new interface: DocDataFiller for which there are now two implementations: \n-- SimpleDocDataFiller is used when there is no header line in the input file. It is implementing the original logic before this change. This allows to continue using existing line-doc-files which have no header line.\n-- HeaderDocDataFiller is used when there exists a header line in the input file. Its implementation populates both fixed fields and flexible properties of DocData:\n--- At construction of the filler a mapping is created from the field position in the header line to a setter method of docData. That mapping is not by reflection, nor by a HashMap - simply an int[] posToM where if posToM[3] = 1, later, when handling the field no. 3 in the line, the method fillDate3() will be called, and it will, in turn, call docData.setDate(), through a switch statement. If there's no mapping to a DocData setter, its properties object will be populated. So, this is quite general, with some performance overhead, though less than reflection I think (I did not measure this).\n- An extension point for overriding the filler creation is through two new methods:\n-- createDocDataFiller() for the case of no header line\n-- createDocDataFiller(String[] header) when a header line is found in the input\n- Note that filler creation is done once, when reading the first line of the input file. \n\nSome tests were fixed to account for the existence (or absence) of a header line.\n\nI think more tests are required, but you can get the idea how this code will work.\n\nBottom line, LineDocSource is more general now, but the code became more complex.\n\nI have mixed feelings about this - preferring simple code, but the added functionality is appealing.",
            "date": "2011-03-12T00:01:58.366+0000",
            "id": 12
        },
        {
            "author": "Shai Erera",
            "body": "I haven't reviewed the patch yet, but I must say that from your description it sounds like LineDocSource has become very complicated. I'd prefer to keep things simple. Before this issue, LDS read a line and split it into 3 fields. Now we think it should be extend-able, such that users can read lines and tokenize them differently (for e.g. supporting more fields). I think that for that, a getDocData/processLine extension point is enough.\n\nAfter all, users can write their own WLDT and LDS, they don't have to use ours. The purpose here is to keep the common logic in those two classes (writing/reading lines to multiple in/output formats), only allow these classes to be somewhat more flexible.\n\nTherefore I think that the header line may not be that useful eventually. It seems to only complicate matters. Most people (judging by the fact that it hasn't come up as an issue yet) are either happy w/ the current capabilities, or wrote their own matching pair to support more fields. So let's keep the current impl as optimized as it was before, but allow for a simple extension point?",
            "date": "2011-03-13T06:33:08.644+0000",
            "id": 13
        },
        {
            "author": "Doron Cohen",
            "body": "bq. So let's keep the current impl as optimized as it was before, but allow for a simple extension point?\n\nI believe the original case in LineDocSource is as optimized as it was before.\n\nIf you take a look at the inner class SimpleDocDataFiller - it has exactly the same logic as before.\n\nThe more general logic - the one in HeaderDocDataFiller which processes any header line for you - is more complex, and perhaps somewhat less efficient - but only slightly I believe, as the additional cost is a switch statement per field.\n\nBut please do not review this code just yet - I'm in a middle of improving it:\n* By default LineDocSource should use the SimpleDocDataFiller not only when there's no header line in the file (this part is covered already), but also when the header line is the same as the default one (the default coming from WriteLineDocTask).\n* selecting the DocDataFiller to use should be possible through a property - as is the spirit of this package.\n* DocDataFiller should better be named DocDataLineReader.\n* DocDataLineReader inner methods like fillDate2() should be inlined (i.e. removed)\n* HeaderDocDataLineReader should switch on an enum rather than on ints.\n\nThese changes would make LineDocSource more efficient and more readable.\nI feel that the added functionality is worth the additional complexity in the code,\nAnd, for those wishing to save the extra cycles of the general HeaderDocDataLineReader, it is possible to implement a custom one and pass its name as the (new) property docdata.line.reader.\n\nI am working on an updated patch...",
            "date": "2011-03-13T15:40:43.862+0000",
            "id": 14
        },
        {
            "author": "Shai Erera",
            "body": "A thought -- how about we do the following:\n# LineDocSource remains basically as it is today, with a getDocData(String line) extendable method for whoever wants\n# Instead of introducing those Fillers, you create a HeaderLineDocSource which assumes the first line read is the header line, and parses the following ones as appropriate. It will create LDS extending getDocData.\n\nThis will not introduce a Filler in LDS, and those who don't care about it don't need to know about it at all. Also, it will showcase the 'extendability' of LDS.\n\nWill that be simpler?",
            "date": "2011-03-13T19:01:04.750+0000",
            "id": 15
        },
        {
            "author": "Doron Cohen",
            "body": "bq. Will that be simpler?\nIt will be simpler, I admit, but it will harder to manage:\n* when re-reading the input file (with repeat=true) special treatment of the header line is needed. And cannot assume that the header line exists, because there are 1-line files out there without this line, which, is possible, I would not like to force recreating, and it is possible.\n* the simple LDS as today handles no header line. As such, if there is one, it will wrongly treat it as a regular line. But I would like it to be able to handle both old files (with no header) and new files, with the header. Mmmm,,, e could for that write the header only if it differs from the default header. Perhaps this will work.\n\nI'll take a look at that again, meanwhile attaching updated patch with the two inner DocDataLineReader's.",
            "date": "2011-03-13T19:57:04.682+0000",
            "id": 16
        },
        {
            "author": "Doron Cohen",
            "body": "Rethinking this suggestion, I am afraid it will easily lead users to errors/mistakes - users would have to be aware: \n\n\"did I create that file with a header? Mmm... so I must use the source which handles the header, and that file is with the default settings, so it needs the simple reader, but man, did I set it to create the header anyhow... I don't remember, I'll recreate the file...\" \n\nMaybe some users will remember such things, but I know that I will not remember, and a line-reader that handles correctly all inputs out-of-the-box is much more convenient... which is what I liked in the header suggestion.",
            "date": "2011-03-13T20:17:14.806+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "bq. For example, one DocMaker can decide to split each doc into N tiny docs. Another can choose to add facets to it. Yet another can do complex analysis on it and produce richer documents.\n\nI like the flexibility to \"enrich\" the docs produced by the source\n(set up facets, semantic extraction, etc.) but the ability to split up\ndocs is... dangerous, I think.\n\nIe, it feels to me like DocData should in fact just be a Document. The\ntwo-step process we have now (fill fields in a DocData, then,\nseparately ask this DocData to make one or more Docments) feels\nwrong.\n\nSplitting a big doc into N smaller docs can't be done well.  It's\nsynthetic data (eg 20 docs in a row will have same title/data) and so\nyou'll draw synthetic conclusions.\n\nThe enrichment can \"simply\" be a Document processing pipeline that\nruns after the source document was produced from the line file.  EG,\nUIMA.\n\nWhen we run perf tests w/ luceneutil, we do in fact do this split, but\nthen we shuffle the resulting line file so that you don't see 20 docs\nw/ same title in a row which skews eg compression results since a\ngiven term foo in its title will have 20 adjacent docIDs assigned and\nthus be unnaturally easy to compress.  Likewise for the date field,\nwhich makes the NRQ performance unnaturally good.\n\nIf you want to chop docs up really you do it as a pre-processing step\nin building your line file...\n\nbq. Before that, you'd have to write a DocMaker for every such combination. E.g., if you wanted to add facets, you'd need to write a DocMaker per source of data with the same impl.\n\nBut, if LineDocSource returned a Document, can't you take that\nDocument and run with it?  We'd still have a single class that pulls\nDocument from a line file, just different \"Document processors\" that\nrun after it.\n\nI'm still not really seeing why DocData is needed, except for the\nsomewhat dangerous split-up-docs case.\n\nBut we don't need to change/fix this, today...\n",
            "date": "2011-03-14T10:37:55.438+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks great!  Some small things:\n\n  * I think we should throw an exc if any of the field names contain\n    the SEP char?\n\n  * Can we name it \"parseLine\" instead of \"readLine\"?  Ie, the line\n    has already been read (from the file); what remains is to parse it\n    (and, as a side effect, change DocData to reflect that parsing).\n\n  * Typo: sedDocData -> setDocData (in HeaderDocDataLineReader).\n\nI do think we should move to all line files having the field header\nline (w/ back compat handled for existing line files out there).  The\napproach in the patch looks great -- the [common] fixed case of just\ntitle/date/body that we have today is specialized and should still be\nfast (SimpleDocDataLineReader).\n",
            "date": "2011-03-14T10:39:38.131+0000",
            "id": 19
        },
        {
            "author": "Doron Cohen",
            "body": "Thanks for reviewing Mike!\n\nbq. I think we should throw an exc if any of the field names contain the SEP char?\nRight, good catch!\n\nbq. Can we name it \"parseLine\"\nYes, I like it better, readLine() felt wrong, parseLine it will be.\nAlso the inner class should better be called LineParser (rather than DocDataLineReader).\n\nI'll patch these - and fix the typo - totogether with more tests...",
            "date": "2011-03-14T10:55:35.553+0000",
            "id": 20
        },
        {
            "author": "Doron Cohen",
            "body": "Updated patch, tests added for better coverage, and added a Changes entry.",
            "date": "2011-03-20T22:54:55.687+0000",
            "id": 21
        },
        {
            "author": "Doron Cohen",
            "body": "Hmmm, while reviewing again before committing I noticed that the HeaderLineParser constructor never assigns FieldName.PROP in posToF. I intended to do this but forgot. Indeed, emma shows that Properties handling in LineDocSource is not tested. So I enhanced LineDocSourceTest to also test for nonstandard fields and for properties. The test failes as expected, and the fix was trivial.\n\nAttaching updated patch, planning to commit this shortly.",
            "date": "2011-03-21T13:30:52.141+0000",
            "id": 22
        },
        {
            "author": "Doron Cohen",
            "body": "Committed:\n- r1083789 - 3x\n- r1083812 - 3x undo added unused imports\n- r1083816 - trunk\n\nThanks Shai and Mike for reviewing and suggestions!",
            "date": "2011-03-21T15:34:46.072+0000",
            "id": 23
        },
        {
            "author": "Robert Muir",
            "body": "Bulk closing for 3.2",
            "date": "2011-06-03T16:37:15.241+0000",
            "id": 24
        }
    ],
    "component": "modules/benchmark",
    "description": "Make WriteLineDocTask and LineDocSource more flexible/extendable:\n* allow to emit lines also for empty docs (keep current behavior as default)\n* allow more/less/other fields",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2958",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "WriteLineDocTask improvements",
    "systemSpecification": true,
    "version": ""
}