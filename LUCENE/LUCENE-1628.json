{
    "comments": [
        {
            "author": "Robert Muir",
            "body": "patch file",
            "date": "2009-05-03T17:04:16.597+0000",
            "id": 0
        },
        {
            "author": "Robert Muir",
            "body": "farsi stopwords file moved to resources folder and test to ensure it loads.\n",
            "date": "2009-05-19T05:57:21.199+0000",
            "id": 1
        },
        {
            "author": "Mark Miller",
            "body": "Thanks Robert, looks cool.\n\nAnyone know what the policy on the stop word list being BSD license is? I assume its compatible with Apache? Whats our BSD license policy? I don't see anything definitive on a quick mailing list search.\n\n- Mark",
            "date": "2009-06-11T02:09:40.428+0000",
            "id": 2
        },
        {
            "author": "Mark Miller",
            "body": "Okay, I see that the stopword list for Arabic was committed by Grant with the BSD license. I'll take that as an \"its okay\" unless anyone speaks up.\n\nThanks for all these great Analyzers Robert.",
            "date": "2009-06-11T02:11:54.090+0000",
            "id": 3
        },
        {
            "author": "Robert Muir",
            "body": "mark, on the same topic: if possible, at some time it would be great to know which licenses are OK, and which ones are not.\n",
            "date": "2009-06-11T04:03:55.302+0000",
            "id": 4
        },
        {
            "author": "Mark Miller",
            "body": "bq. mark, on the same topic: if possible, at some time it would be great to know which licenses are OK, and which ones are not. \n\nFound it.\n\nNo Problem:\n    * Apache License 2.0\n    * ASL 1.1\n    * BSD\n    * MIT/X11\n    * NCSA\n    * W3C Software license\n    * X.Net\n    * zlib/libpng\n\nwith some hassle:\n    * CDDL 1.0\n    * CPL 1.0\n    * EPL 1.0\n    * IPL 1.0\n    * MPL 1.0 and MPL 1.1\n    * SPL 1.0\n\nhttp://www.apache.org/legal/3party.html",
            "date": "2009-06-18T03:51:02.485+0000",
            "id": 5
        },
        {
            "author": "Mark Miller",
            "body": "Looks pretty good. Not sure if we should update to the new token api here or just commit and hit it with the other issue. I guess we might as well get it here first.\n\nIs it better to put the raw text in there like that (in the tests) or do you think it would be better to use utf8 codes with maybe the raw text in a comment? I'm just remembering running into issues with such things in a past life as I moved around source code.",
            "date": "2009-06-18T04:06:21.063+0000",
            "id": 6
        },
        {
            "author": "Robert Muir",
            "body": "mark: thanks for the followup on the licenses!\n\nwrt non-english text, I will say that if you set encoding to UTF-8 (such as in eclipse under project>properties>text encoding) then things are fine.\nthe ant build also does the right thing, and there are definitely other analyzers that behave like this too, and will break if things aren't set right.\n\nalso, if you do not set encoding to UTF-8, most editors (such as eclipse) will not be able to save the file, and will error out with encoding issues... even if the text is inside a comment!\n\nnot really (ok a little) trying to talk you out of this, but I'm just not sure it would really help anything...\n\nthat being said... (my) eclipse still jacks up if you team->apply patch from file. if you open the patch in notepad, ctrl-a,ctrl-c, and then team->apply patch from clipboard, it works fine... very annoying!\n",
            "date": "2009-06-18T04:43:05.788+0000",
            "id": 7
        },
        {
            "author": "Mark Miller",
            "body": "Okay, fair enough. I figured you'd know better than me, just wanted to check. Certainly if we have other code that way, no reason to change it here. And of course it makes sense that you would still run into issues with the comments - garbalage at best.\n\nI only ever use apply to/from clipboard so I have luckily never seen that issue :)\n\nWe should be good to put this in then - I'll wait till we get squared away with the new token api patch then commit.",
            "date": "2009-06-18T04:50:46.941+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "I think we should go ahead and commit this and cutover to the new API as a separate step?",
            "date": "2009-07-14T18:24:42.383+0000",
            "id": 9
        },
        {
            "author": "Mark Miller",
            "body": "Should we add a coulple tests Robert?\n\n+public class TestPersianAnalyzer extends TestCase {\n\n+  \n\n+  /** This test fails with NPE when the \n\n+   * stopwords file is missing in classpath */\n\n+  public void testResourcesAvailable() {\n\n+    new PersianAnalyzer();\n\n+  }\n\n+  \n\n+  /* TODO: more tests */\n\n+\n\n+}",
            "date": "2009-07-14T21:41:41.186+0000",
            "id": 10
        },
        {
            "author": "Robert Muir",
            "body": "Mark, no problem. \n\nI will upload a new patch showing some behavior of the analyzer as a whole...",
            "date": "2009-07-14T21:46:06.916+0000",
            "id": 11
        },
        {
            "author": "Robert Muir",
            "body": "add additional tests, showing behavior of this analyzer as a whole.",
            "date": "2009-07-14T22:53:15.057+0000",
            "id": 12
        },
        {
            "author": "Mark Miller",
            "body": "Thanks a lot Robert, looks great!\n\nHere is a quick tiny update thats been formatted with the Lucene/Solr eclipse formatter file and with a few unused imports removed.\n\nI'd rather just wait for some finalization with the new token api, but I'll defer to Mike on whether we wait to commit or not.",
            "date": "2009-07-14T23:05:11.011+0000",
            "id": 13
        },
        {
            "author": "Robert Muir",
            "body": "mark, i'm sorry you had to reformat it.\n\nI am using the lucene formatter file :) apparently something slipped thru, along with the unused imports... ugh.\n\n\n",
            "date": "2009-07-14T23:13:48.728+0000",
            "id": 14
        },
        {
            "author": "Mark Miller",
            "body": "bq. mark, i'm sorry you had to reformat it. \n\nNo worries - I certainly didn't have to. I just ran it because I recently re-added it to eclipse today. Certainly wasn't necessary, and perhaps there are more than one of these files floating around out there with a slight difference?\n\nNo big deal at all, just wanted to mention the change - I wouldn't have even made the patch other than to remove the imports and they are not a big deal either. There are a bunch in Lucene right now. And there is some crazy, whacky formatting as well. Its easy to be anal about the small stuff when someone else has done all the work on the big stuff ;)",
            "date": "2009-07-14T23:21:12.071+0000",
            "id": 15
        },
        {
            "author": "Robert Muir",
            "body": "mark, I will be more careful in the future!\n\nproblem is some habit of mine (ctrl-I versus a real format)... also its a tad difficult to spot real warnings (not a real excuse) because of deprecated token api... hopefully we can fix that soon!\n\nthanks for cleaning it up.",
            "date": "2009-07-14T23:26:39.909+0000",
            "id": 16
        },
        {
            "author": "Robert Muir",
            "body": "Mark, I think we should figure out a plan with the order of this, LUCENE-1460, and LUCENE-1728...\nI'm not really sure what the best order would be, I think just as long as we coordinate it won't be difficult.\n\nIf we move things around in 1728 it might make life difficult, trying to avoid that.\n",
            "date": "2009-07-22T01:58:38.468+0000",
            "id": 17
        },
        {
            "author": "Robert Muir",
            "body": "analyzers/fa -> analyzers/common/fa\nmake PersianNormalizationFilter final\nswitch to new API.\n",
            "date": "2009-07-24T13:37:39.720+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "Robert/Mark is this one ready to be committed?",
            "date": "2009-07-28T17:25:07.923+0000",
            "id": 19
        },
        {
            "author": "Robert Muir",
            "body": "add lowercasefilter, consistent with the arabic analyzer, its userfriendly for the common case where there is also some english text.\n",
            "date": "2009-07-30T05:14:27.419+0000",
            "id": 20
        },
        {
            "author": "Robert Muir",
            "body": "I have been looking this over, I think this one is ready. any comments/concerns? \n",
            "date": "2009-08-07T09:43:29.853+0000",
            "id": 21
        },
        {
            "author": "Robert Muir",
            "body": "implement reusableTokenStream here too.",
            "date": "2009-08-10T12:41:01.803+0000",
            "id": 22
        },
        {
            "author": "Robert Muir",
            "body": "Committed revision 802955.",
            "date": "2009-08-10T23:37:25.904+0000",
            "id": 23
        }
    ],
    "component": "modules/analysis",
    "description": "A simple persian analyzer.\n\ni measured trec scores with the benchmark package below against http://ece.ut.ac.ir/DBRG/Hamshahri/ :\n\nSimpleAnalyzer:\nSUMMARY\n  Search Seconds:         0.012\n  DocName Seconds:        0.020\n  Num Points:           981.015\n  Num Good Points:       33.738\n  Max Good Points:       36.185\n  Average Precision:      0.374\n  MRR:                    0.667\n  Recall:                 0.905\n  Precision At 1:         0.585\n  Precision At 2:         0.531\n  Precision At 3:         0.513\n  Precision At 4:         0.496\n  Precision At 5:         0.486\n  Precision At 6:         0.487\n  Precision At 7:         0.479\n  Precision At 8:         0.465\n  Precision At 9:         0.458\n  Precision At 10:        0.460\n  Precision At 11:        0.453\n  Precision At 12:        0.453\n  Precision At 13:        0.445\n  Precision At 14:        0.438\n  Precision At 15:        0.438\n  Precision At 16:        0.438\n  Precision At 17:        0.429\n  Precision At 18:        0.429\n  Precision At 19:        0.419\n  Precision At 20:        0.415\n\nPersianAnalyzer:\nSUMMARY\n  Search Seconds:         0.004\n  DocName Seconds:        0.011\n  Num Points:           987.692\n  Num Good Points:       36.123\n  Max Good Points:       36.185\n  Average Precision:      0.481\n  MRR:                    0.833\n  Recall:                 0.998\n  Precision At 1:         0.754\n  Precision At 2:         0.715\n  Precision At 3:         0.646\n  Precision At 4:         0.646\n  Precision At 5:         0.631\n  Precision At 6:         0.621\n  Precision At 7:         0.593\n  Precision At 8:         0.577\n  Precision At 9:         0.573\n  Precision At 10:        0.566\n  Precision At 11:        0.572\n  Precision At 12:        0.562\n  Precision At 13:        0.554\n  Precision At 14:        0.549\n  Precision At 15:        0.542\n  Precision At 16:        0.538\n  Precision At 17:        0.533\n  Precision At 18:        0.527\n  Precision At 19:        0.525\n  Precision At 20:        0.518\n\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1628",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Persian Analyzer",
    "systemSpecification": true,
    "version": ""
}