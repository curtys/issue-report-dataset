{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "Attached my current patch.  It compiles, but, quite a few tests fail!",
            "date": "2007-12-04T13:07:41.639+0000",
            "id": 0
        },
        {
            "author": "Shai Erera",
            "body": "So Mike - just to clarify. If I have 3 segments: A (0-52), B (53-124) and C (125-145), and you decide to merge A and C, what will be the new doc IDs of all segments? will they start from 53? or will you shift all the documents so that the segments will be B (0-71) and A+C (72-145)?",
            "date": "2009-07-21T18:21:59.243+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "Well... one option might be \"the newly merged segment always replaces the leftmost segment\".  Another option could be to leave it undefined, ie IW makes no commitment as to where it will place the newly merged segment so you should not rely on it.  Presumably apps that rely on Lucene's internal doc ID to \"mean something\" would not use a merge policy that selects non-contiguous segments.\n\nUnfortunately, with the current index format, there's a big cost to allowing non-contiguous segments to be merged: it means the doc stores will always be merged.  Whereas, today, if you build up a large new index, no merging is done for the doc stores.\n\nIf we someday allowed a single segment to reference multiple original doc stores (logically concatenating [possibly many] slices out of them), which would presumably be a perf hit when retrieving the stored doc or term vectors, then this cost would go away.",
            "date": "2009-07-21T18:30:03.457+0000",
            "id": 2
        },
        {
            "author": "Shai Erera",
            "body": "Well ... what I was thinking of is that even if the app does not care about internal doc IDs, the Lucene code may very well care. If we don't shift doc IDs back, it means maxDoc will continue to grow, and at some point (extreme case though), maxDoc will equal 1M, while there will be just 50K docs in the index.\n\nAFAIU, maxDoc is used today to determine array length in FieldCache, I've seen it used in IndexSearcher to sort the sub readers (at least in the past) etc. So perhaps alongside maxDoc we'll need to keep a curNumDocs member to track the actual number of documents?\n\nBut I have a feeling this will also get complicated.",
            "date": "2009-07-21T18:46:28.646+0000",
            "id": 3
        },
        {
            "author": "Michael McCandless",
            "body": "maxDoc() is computed by simply summing docCount of all segments in the index; it shouldn't ever grow.",
            "date": "2009-07-21T18:51:13.611+0000",
            "id": 4
        },
        {
            "author": "Shai Erera",
            "body": "But how is a new doc ID allocated? Not by calling maxDoc()? So if maxDoc()  = 50K, but there is a document w/ ID 1M (and possibly another one w/ 50K), won't that be a problem?",
            "date": "2009-07-21T18:55:12.547+0000",
            "id": 5
        },
        {
            "author": "Shai Erera",
            "body": "Besides Mike, there's something I don't understand from a previous comment you've made: You commented that today if I build a large index, the doc stores are not merged, while if we'll move to merging non contiguous segments, they will. I'm afraid I'm not familiar with this area of Lucene well -- if I merge two consecutive segments, how come I don't merge their doc stores?",
            "date": "2009-07-21T18:58:23.951+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "bq. But how is a new doc ID allocated?\n\nDoc IDs are logically assigned by summing docCount of all segments before me, as my base, and then adding to the \"index\" of the doc within my segment.  Ie, the base of a given segment is not stored anywhere, so we are always free to shuffle up the order of segments and nothing in Lucene should care (but, the app might).",
            "date": "2009-07-21T20:47:29.642+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "bq.  if I merge two consecutive segments, how come I don't merge their doc stores\n\nMultiple segments are able to share a single set of doc-store (=\nstored fields & term vectors) files, today.  This only happens when\nmultiple segments are written in a single IndexWriter session with\nautoCommit=false.\n\nEG if I open a writer, index all of wikipedia w/ autoCommit false, and\nclose it, you'll see a single large set of doc store files (eg _0.fdt,\n_0.fdx, _0.tvf, _0.tvd, _0.tvx).\n\nWhenever RAM is full (with postings & norms data), a new segment is\nflushed, but the doc store files are kept open & shared with further\nflushed segments.\n\nA single segment then refers to the shared doc stores, but records its\n\"offset\" within them.\n\nSo, when we merge contiguous segments, because the resulting docs are\nalso contiguous in the doc stores, we are able to store a single doc\nstore offset in the merged segment, referencing the orignial doc\nstore, and it works fine.\n\nBut if we merge non-contiguous segments, we must then pull out & merge\nthe \"slices\" from the doc stores into a new [private to the new\nsegment] set of doc store files.\n\nFor apps that store term vectors w/ positions & offsets, and have many\nstored fields, and have heterogenous field name -> number assignments\n(see LUCENE-1737 to fix that), the merging of doc stores can easily\ndominate the merge cost.\n",
            "date": "2009-07-21T20:57:13.513+0000",
            "id": 8
        },
        {
            "author": "Tim Smith",
            "body": "i suppose you could do a preliminary round of merging that would merge together segments that share doc store/termvector data\n\nonce this preliminary round of merging is done, you would then no longer have the need to slice the doc stores up, just merge them together (contiguous or non-contiguous wouldn't matter anymore, however if a \"segmented session\" still exists higher up, this would prevent you from selecting these segments, or newer segments)\n\nit might even be desirable to have a \"commit()\" optionally perform this merging prior to the commit finishing as this will result in each commit producing one segment, regardless of the number of flushes that were done under the hood",
            "date": "2009-07-21T21:16:42.214+0000",
            "id": 9
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}if I merge two consecutive segments, how come I don't\nmerge their doc stores?{quote}\n\nYou may want to take a look at\nSegmentInfo.docStoreOffset/docStoreSegment which is the pointer\nto the docstore file data for that SI. ",
            "date": "2009-07-21T21:51:36.698+0000",
            "id": 10
        },
        {
            "author": "Shai Erera",
            "body": "Thanks for the education everyone.\n\nMike - it feels to me, even though I can't pin point it at the moment (FieldCache maybe?), that if maxDoc won't reflect the number of documents in the index we'll run into troubles. Therefore I suggest you consider introducing another numDocs() method which returns the actual number of documents there are in the index.",
            "date": "2009-07-22T15:08:19.368+0000",
            "id": 11
        },
        {
            "author": "Michael McCandless",
            "body": "maxDoc() does reflect the number of docs in the index.  It's simply the sum of docCount for all segments.  Shuffling the order of the segments, or allowing non-contiguous segments to be merged, won't change how maxDoc() is computed.\n\nNew docIDs are allocating by incrementing an integer (starting with 0) for the buffered docs.  When a segment gets flushed, we reset that to 0.  Ie, docIDs are stored within one segment; they have no \"context\" from prior segments.",
            "date": "2009-07-22T15:55:34.656+0000",
            "id": 12
        },
        {
            "author": "Shai Erera",
            "body": "Oh. Thanks for correcting me. In that case, I take what I said back.\n\nI think this together w/ LUCENE-1750 can really help speed up segment merges in certain scenarios. Will wait for you to come back to it :)",
            "date": "2009-07-22T16:01:18.970+0000",
            "id": 13
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Will wait for you to come back to it\n\nFeel free to take it, too :)\n\nI think LUCENE-1737 is also very important for speeding up merging, especially because it's so \"unexpected\" that just by adding different fields to your docs, or the same fields in different orders, can so severely impact merge performance.",
            "date": "2009-07-22T17:01:23.103+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "Unassigning myself.",
            "date": "2009-07-22T17:02:12.536+0000",
            "id": 15
        },
        {
            "author": "Shai Erera",
            "body": "bq. Feel free to take it, too\n\nI don't mind to take a stab at it. But this doesn't mean you can unassign yourself. I'll need someone to commit it :).",
            "date": "2009-07-22T17:06:54.581+0000",
            "id": 16
        },
        {
            "author": "Shai Erera",
            "body": "Can someone please help me understand what's going on here? After I applied the patch to trunk, TestIndexWriter.testOptimizeMaxNumSegments2() fails. The failure happens only if CMS is used, and doesn't when SMS is used. I dug deeper into the test and what happens is that the test asks to optimize(maxNumSegments) and expects that either: (1) if the number of segments was < maxNumSegments than the resulting number of segments is exactly as it was before and (2) otherwise it should be exactly maxNumSegments.\n\nFirst, the javadocs of optimize(maxNumSegments) say that it will result in <= maxNumSegments, but I understand the LogMergePolicy ensures that if you ask for maxNumSegments, that's the number of segments you'll get.\n\nWhile trying to debug what's wrong w/ the change so far, I managed to reduce the test to this code:\n\n{code}\npublic void test1() throws Exception {\n    MockRAMDirectory dir = new MockRAMDirectory();\n\n    final Document doc = new Document();\n    doc.add(new Field(\"content\", \"aaa\", Field.Store.YES, Field.Index.ANALYZED));\n\n    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);\n//    writer.setMergeScheduler(new SerialMergeScheduler());\n    LogDocMergePolicy ldmp = new LogDocMergePolicy();\n    ldmp.setMinMergeDocs(1);\n    writer.setMergePolicy(ldmp);\n    writer.setMergeFactor(3);\n    writer.setMaxBufferedDocs(2);\n\n    MergeScheduler ms = writer.getMergeScheduler();\n//  writer.setInfoStream(System.out);\n    \n    // Add enough documents to create several segments (uncomitted) and kick off\n    // some threads.\n    for (int i = 0; i < 20; i++) {\n      writer.addDocument(doc);\n    }\n    writer.commit();\n    \n    if (ms instanceof ConcurrentMergeScheduler) {\n      // Wait for all merges to complete\n      ((ConcurrentMergeScheduler) writer.getMergeScheduler()).sync();\n    }\n    \n    SegmentInfos sis = new SegmentInfos();\n    sis.read(dir);\n    \n    System.out.println(\"numSegments after add + commit ==> \" + sis.size());\n    \n    final int segCount = sis.size();\n    \n    int maxNumSegments = 3;\n    writer.optimize(maxNumSegments);\n    writer.commit();\n    \n    if (ms instanceof ConcurrentMergeScheduler) {\n      // Wait for all merges to complete\n      ((ConcurrentMergeScheduler) writer.getMergeScheduler()).sync();\n    }\n    \n    sis = new SegmentInfos();\n    sis.read(dir);\n    final int optSegCount = sis.size();\n    \n    System.out.println(\"numSegments after optimize (\" + maxNumSegments + \") + commit ==> \" + sis.size());\n    \n    if (segCount < maxNumSegments)\n      Assert.assertEquals(segCount, optSegCount);\n    else\n      Assert.assertEquals(maxNumSegments, optSegCount);\n}\n{code}\n\nThis fails almost every time that I run it, so if you try it - make sure to run it a couple of times. I then switched to trunk, but it fails almost consistently on trunk also !?!?\n\nCan someone please have a look and tell me what's wrong (is it the test, or did I hit a true bug in the code?)?",
            "date": "2009-07-28T03:46:18.434+0000",
            "id": 17
        },
        {
            "author": "Shai Erera",
            "body": "Hmm ... I think I found the problem. I added commit() after cms.sync() and it never failed again. So I checked the output of infoStream in two cases (failure and success) and found this: in the success case, the pending merges occurred before the last addDocument calls happened (actually the last 2), therefore commit() committed those pending merges output, and sync() afterwards did nothing.\n\nIn the failure case, the last pending merge happened *after* commit() was called, either as (or not) part of the sync() call, but it was never committed.\n\nSo it looks to me that I should add this test case as testOptimizeMaxNumSegments3() (even though it has nothing to do w/ optimize()), just to cover this aspect and also document CMS.sync() to mention that a commit() after it is required if the outcome of the merges should be reflected in the index (i.e., committed).\n\nDid I get it right?",
            "date": "2009-07-28T07:40:38.571+0000",
            "id": 18
        },
        {
            "author": "Shai Erera",
            "body": "BTW, the second sync() call comes after optimize(), which is redundant as far as I understand, since optimize() or optimize(int) will wait for all merges to complete, which CMS merges.\n\nI wonder then if it won't be useful to have a commit(doWait=true), which won't require calling sync() or waitForMerges()?",
            "date": "2009-07-28T07:45:59.229+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I added commit() after cms.sync() and it never failed again. \n\nI think you are right!  Since we try to read the dir (sis.read), if we don't commit then the changes won't be present since IndexWriter is opened w/ autoCommit false.  Another simple check would be to call getReader().getSequentialSubReaders() and check how many segments there are, instead of having to go through the Directory to check it.\n\nbq. BTW, the second sync() call comes after optimize(), which is redundant as far as I understand\n\nI agree.\n\nbq. I wonder then if it won't be useful to have a commit(doWait=true), which won't require calling sync() or waitForMerges()?\n\nI think we can leave this separate (ie you should call waitForMerges() if you need to), because commit normally has nothing to do w/ merging, since merging doesn't change any docs in the index.  Commit only ensure that changes to the index are pushed to stable storage.  Whereas eg optimize is all about doing merges so it makes sense for it to have a doWait?",
            "date": "2009-07-28T17:01:44.645+0000",
            "id": 20
        },
        {
            "author": "Shai Erera",
            "body": "Ok I agree that commit should not wait for merges. It does seem not related to segment merging.",
            "date": "2009-07-28T18:32:49.556+0000",
            "id": 21
        },
        {
            "author": "Michael McCandless",
            "body": "I'll take a crack at this.  It's compelling, now that we always bulk merge doc stores...",
            "date": "2011-01-24T18:55:56.497+0000",
            "id": 22
        },
        {
            "author": "Michael McCandless",
            "body": "Patch.  I think it's ready to commit...\n\nTo stress test ooo merging, I made a fun new MockRandomMergePolicy\n(swapped in half the time by LTC.newIndexWriterConfig) which randomly\ndecides when to do a merge, then randomly picks how many segments to\nmerge, and then randomly picks which ones.  Also, I modified\nLogMergePolicy to add a boolean get/setRequireContiguousMerge,\ndefaulting to false.\n\nMany tests rely on in-order docIDs during merging, so I had to wire\nthem to use in-order LogMP.\n\nI also reworked how buffered deletes are managed, so that each\n\"packet\" of buffered deletes, as well as each flushed segment, is now\nassigned an incrementing gen.  This way, when it's time to apply\ndeletes, the algorithm is easy: only delete packets with gen >= this\nsegment should coalesce and apply.\n\nSeparately, eventually, I'd like to switch to a better default MP,\nsomething like BSMP where immense merges are done w/ small mergeFactor\n(eg, 2), and tiny merges are done w/ large mergeFactor.\n",
            "date": "2011-01-28T23:26:56.823+0000",
            "id": 23
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}I also reworked how buffered deletes are managed, so that each\n\"packet\" of buffered deletes, as well as each flushed segment, is now\nassigned an incrementing gen.  This way, when it's time to apply\ndeletes, the algorithm is easy: only delete packets with gen >= this\nsegment should coalesce and apply.{quote}\n\nI saw this and thought it was interesting.  Why is the gen needed?",
            "date": "2011-01-29T01:45:40.605+0000",
            "id": 24
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I saw this and thought it was interesting. Why is the gen needed?\n\nSo, at first I added it because the pushing of merged delete packets\ngot too hairy, eg when merges interleave you'd have to handle deletes\nbeing pushed onto each other's internal merged segments.\n\nAlso, we really needed a transactional data structure here, because\nbefore DW could push more deletes into an existing packet (ie the\npacket was not write once), which made tracking problematic if the\nmerge wanted to record that the first batch of deletes had been\napplied but not any subsequent pushes.\n\nBut, after making the change, I realized that today (trunk, 3.1) we\nare badly inefficient!  We apply deletes to segments being merged, but\nthen we place the merged segment back in the same position.  This is\ninefficient because later when this segment gets merged, we wastefully\nre-apply the same deletes (plus, new ones, which do need to be\napplied).  This is a total waste.\n\nSo, by decoupling tracking of where you are in the deletes packet\nstream, from the physical location of your segment in the index, we\nfix this waste.  Also, it's quite a bit simpler now -- we no longer\nhave to merge deletes on completing a merge.\n",
            "date": "2011-01-29T13:36:06.784+0000",
            "id": 25
        },
        {
            "author": "Michael McCandless",
            "body": "Another benefit of the transaction log for deletes is, because they are write-once (ie, after a set of buffered deletes is pushed, they are never changed), we can switch to a more efficient data structure than TreeMap on push.\n\nEg, we can pull the del Terms (sorted) and store them in an array.",
            "date": "2011-01-29T15:18:56.191+0000",
            "id": 26
        },
        {
            "author": "Michael McCandless",
            "body": "Committed to trunk; I'll let this age for a while before back porting.\n\nOn the backport, I'll leave the default contiguous merges.",
            "date": "2011-01-29T19:49:49.210+0000",
            "id": 27
        },
        {
            "author": "Michael McCandless",
            "body": "I think we can do this for 3.1, but I'll leave the default as always doing contiguous merges.",
            "date": "2011-02-03T15:25:27.052+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "I changed my mind! Pushing this to 3.2 now.",
            "date": "2011-02-04T12:06:07.700+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "Phew, this patch almost fell below the event horizon of my TODO list...\n\nI'm attaching new modernized one.  I also mod'd the policy to not select two max-sized merges at once.  I think it's ready to commit...",
            "date": "2011-04-01T16:41:02.481+0000",
            "id": 30
        },
        {
            "author": "Shai Erera",
            "body": "Patch against 3x. This is not ready to commit yet, as many tests fail on exceptions like this:\n\n{noformat}\n    [junit] java.lang.IndexOutOfBoundsException\n    [junit]     at java.util.AbstractList.subList(AbstractList.java:763)\n    [junit]     at java.util.Vector.subList(Vector.java:975)\n    [junit]     at org.apache.lucene.index.IndexWriter.commitMerge(IndexWriter.java:3550)\n    [junit]     at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:4057)\n    [junit]     at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3631)\n{noformat}\n\nMike says there was  an earlier commit (handled how deletes are flushed) that is a dependency of that, and that I can continue only he back-ports that.\n\nIn the meantime, I've fixed tests that assumed LogMP (for setting compound and mergeFactor) by adding LTC.setUseCompoundFile and LTC.setMergeFactor as utility methods.\n\nWill continue after Mike back-ports the dependencies.",
            "date": "2011-05-03T12:09:00.129+0000",
            "id": 31
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Shai!",
            "date": "2011-05-05T23:23:50.120+0000",
            "id": 32
        },
        {
            "author": "Michael McCandless",
            "body": "I think we should change 3.2's default MP to TieredMP?\n\nThis means docIDs may be reordered, since Tiered MP can merge out-of-order segments.",
            "date": "2011-05-10T18:34:18.452+0000",
            "id": 33
        },
        {
            "author": "Robert Muir",
            "body": "+1 Mike",
            "date": "2011-05-11T22:26:52.376+0000",
            "id": 34
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. This means docIDs may be reordered, since Tiered MP can merge out-of-order segments.\nI think this is a very hard break and it should depend on the Version you pass to IWC. Stuff like that is really a good usecase for Version. I had customers in the past that heavily depend on the lucene doc ID while it is not recommended but with this change their app will suddenly not work anymore. so we should make sure that they can upgrade seamlessly!",
            "date": "2011-05-12T06:41:43.255+0000",
            "id": 35
        },
        {
            "author": "Uwe Schindler",
            "body": "{quote}\n\nbq. This means docIDs may be reordered, since Tiered MP can merge out-of-order segments.\nI think this is a very hard break and it should depend on the Version you pass to IWC. Stuff like that is really a good usecase for Version. I had customers in the past that heavily depend on the lucene doc ID while it is not recommended but with this change their app will suddenly not work anymore. so we should make sure that they can upgrade seamlessly!\n{quote}\n\nI think we should also warn people that have this problem to use IndexUpgrader (LUCENE-3082), because it has the same problem. Segments are reordered (segments that were upgraded before a call to MP's optimize come first, then the upgraded ones). Maybe we should add this to JavaDocs in 3.x.\n\nI'll reopen LUCENE-3082.",
            "date": "2011-05-12T07:09:39.437+0000",
            "id": 36
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I think this is a very hard break and it should depend on the Version you pass to IWC.\n\n+1\n\nI'll make it TieredMP if version >= 3.2, else LogByteSizeMP.",
            "date": "2011-05-12T10:32:38.834+0000",
            "id": 37
        },
        {
            "author": "Michael McCandless",
            "body": "Patch against 3.x, changing default if matchVersion is >= LUCENE_32 passed to IWC.",
            "date": "2011-05-12T16:37:44.783+0000",
            "id": 38
        },
        {
            "author": "Robert Muir",
            "body": "Bulk closing for 3.2",
            "date": "2011-06-03T16:37:19.432+0000",
            "id": 39
        }
    ],
    "component": "core/index",
    "description": "I started work on this but with LUCENE-1044 I won't make much progress\non it for a while, so I want to checkpoint my current state/patch.\n\nFor backwards compatibility we must leave the default MergePolicy as\nselecting contiguous merges.  This is necessary because some\napplications rely on \"temporal monotonicity\" of doc IDs, which means\neven though merges can re-number documents, the renumbering will\nalways reflect the order in which the documents were added to the\nindex.\n\nStill, for those apps that do not rely on this, we should offer a\nMergePolicy that is free to select the best merges regardless of\nwhether they are continuguous.  This requires fixing IndexWriter to\naccept such a merge, and, fixing LogMergePolicy to optionally allow\nit the freedom to do so.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1076",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Allow MergePolicy to select non-contiguous merges",
    "systemSpecification": true,
    "version": "2.3"
}