{
    "comments": [
        {
            "author": "Steven Tamm",
            "body": "Here's the patch\n\nSorry about my lack of proofreading, I saved right as I was leaving work.  \n\nThe main point is that the look ahead caching done by TermScorer is unnecessary.  It is only of benefit if you are scoring in a given locality (i.e. query doc 0, then 30, then 10, then 3, etc).  Nearly all use cases are sequential: the use of seek vs. next() is fine because the underlying BufferedIndexInput has an efficient seek function for sequential access.  \n\nHere's an HPROF run from a set of sequential wildcard searches (with many terms per search).  Since this never performs sequential access on documents, the \"cache\" is completely unnecessary. \n\n          percent          live          alloc'ed  stack class\n rank   self  accum     bytes objs     bytes  objs trace name\n   29  0.79% 58.64%   1029312 7148   1801296 12509 387945 float[]\n   30  0.79% 59.43%   1029312 7148   1801296 12509 387944 int[]\n   31  0.79% 60.23%   1029312 7148   1801296 12509 387943 int[]\n\nTRACE 387943:\n\torg.apache.lucene.search.TermScorer.<init>(TermScorer.java:30)\n\torg.apache.lucene.search.TermQuery$TermWeight.scorer(TermQuery.java:64)\n\torg.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:165)\n\torg.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:158)\nTRACE 387944:\n\torg.apache.lucene.search.TermScorer.<init>(TermScorer.java:31)\n\torg.apache.lucene.search.TermQuery$TermWeight.scorer(TermQuery.java:64)\n\torg.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:165)\n\torg.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:158)\nTRACE 387945:\n\torg.apache.lucene.search.TermScorer.<init>(TermScorer.java:36)\n\torg.apache.lucene.search.TermQuery$TermWeight.scorer(TermQuery.java:64)\n\torg.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:165)\n\torg.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:158)\n\t",
            "date": "2006-03-01T14:33:33.000+0000",
            "id": 0
        },
        {
            "author": "Steven Tamm",
            "body": "If you're using a WildcardTermEnum, this optimization saves a ton.  We usually do wildcard searches which retrieve 50-5000 terms.  Since each one of these corresponds to a new TermScorer, removing the caching saves a lot.  For a query that has 1800 terms, it saves 800K/query, plus it's also quicker by about 15%.\n\nDon't double buffer.",
            "date": "2006-03-04T02:59:48.000+0000",
            "id": 1
        },
        {
            "author": "Doug Cutting",
            "body": "It is not clear to me that your uses are typical uses.  These optimizations were added because they made big improvements.  They were not premature.  In some cases JVM's may have evolved so that some of them are no longer required.  But some of them may still make significant improvements for lots of users.  We really need a benchmark suite to better understand the effects of things like this...\n",
            "date": "2006-03-04T03:11:40.000+0000",
            "id": 2
        },
        {
            "author": "Steven Tamm",
            "body": "The main point is this:  When you are using TermScorer to score one document, it is doing a lot of extra work.  It's reading 31 extra documents from the disk and calculating the weight factors for 31 documents.   The question is how does the caching help when you have multiple documents.  My analysis is that (with a modern VM) it helps you only if the docFreq of a term is 16-31 and you are using a ConjunctiveScorer (i.e. not Wildcard searches).  I would imagine this is a use case that is not uncommon.  Anyone using Wildcard searches will have *immediate* benefit from installing this patch.\n\nSo I'm going to analyze this from the \"amount of work to do\" perspective.\nTermScorer.next():  If you are calling TermScorer.next() there is no real difference.  SegmentTermDocs.read(int[], float[]) is no different from calling SegmentTermDocs.next() 32 times.  The change in the patch switches TermScorer.next() to always calling next on the underlying SegmentTermDocs.  The only cost I'm removing is the caching and I'm not adding any new ones.  Therefore there's no change, with the exception of adding the cache for use in skipTo().\n\nTermScorer.skipTo():  The only case where my patch is worse is if the frequency of the term is greater than the skip interval (i.e >= 16 documents per term).  In this case, if you are retrieving more than 16 documents (but less than 32), you can avoid accessing the skipStream entirely.  If you are retrieving more than 32 documents, then you need to access the skipStream anyway, and since both of the underlying IndexInput's are cached, repositioning the freqStream will be only pointer manipulation.\n\nTermScorer.score():\n\"In some cases JVM's may have evolved so that some of them are no longer required.\"  I can imagine that the scoreCache made a lot of sense in JDK 1.1 when the cost of Math.sqrt would be high.  However, if the TermScorer is only going to be used for a single document, this is obviously wrong.   Like I said before, caching DefaultSimilarity.tf(int) inside DefaultSimilarity would end up inlined by the HotSpot compiler, but Math.sqrt is inlined into a processor trap, so it's not a big deal.\n\nI want other people to test this and tell me any problems with it.  Whether or not you accept the patches into are less important to me than providing them to other people that have similar performance problems.  Perhaps I should have created a parallel structure to TermScorer that you can use when you have a low hit/term ratio. ",
            "date": "2006-03-04T03:57:47.000+0000",
            "id": 3
        },
        {
            "author": "Doug Cutting",
            "body": "> The question is how does the caching help when you have multiple documents.  My analysis is that (with a modern VM) it helps you only if the docFreq of a term is 16-31 and you are using a ConjunctiveScorer (i.e. not Wildcard searches).\n\nThe conjunctive scorer does not call score(HitCollector,int).  This is only called in a few cases anymore.  It can help a lot with a single-term query for a very common term, or for disjunctive queries involving very common terms, although BooleanScorer2 no longer uses it in this case.  That's too bad.  If all clauses to a query are optional, then the old BooleanScorer was faster.  But it didn't always return documents in order...  So it indeed may be time to retire this method.\n\n>SegmentTermDocs.read(int[], int[]) is no different from calling SegmentTermDocs.next() 32 times.\n\nIf that were the case, then then termDocs(int[], int[]) method would never have been added!  Benchmarking showed this to be much faster.   There's also optimized C++ code that implements this method in src/gcj.  In C++, with a memory-mapped index, the i/o completely inlines.  When I last benchmarked this in GCJ, it was twice as fast as anything HotSpot could do.\n\nBut without score(HitCollector,int), TermDocs.read(int[], int[]) will never be called.  Sigh.\n\nAs for the scoreCache, this is certainly useful for terms that occur in thousands of documents, and useless for terms that occur only once.  Perhaps we should have two TermScorer implementations, one for common terms and one for rare terms, and have TermWeight select which to use.",
            "date": "2006-03-04T04:52:24.000+0000",
            "id": 4
        },
        {
            "author": "Steven Tamm",
            "body": "> The conjunctive scorer does not call score(HitCollector,int).  This is only called in a few cases anymore. \nHowever, in your comments to LUCENE-505 you said this: \"For example, in TermScorer.score(HitCollector, int), Lucene's innermost loop, you change two array accesses into a call to an interface. That could make a substantial difference.\"  Which is true?   Or, as it seems likely, TermScorer was optimized for a case that is no longer valid (i.e. ConjunctiveScorer).\n\n> If that were the case, then then termDocs(int[], int[]) method would never have been added!\nThis hasn't been true for at least 3 years.  Inlining by hand is not necessary anymore with hotspot (I don't know about gcj).  Run a benchmark on JDK 1.5 to prove this to yourself.\n\nIn short, we should have two TermScorer implementations.  One for low documents/term, and one for high documents/term.\n",
            "date": "2006-03-04T05:10:21.000+0000",
            "id": 5
        },
        {
            "author": "Paul Elschot",
            "body": ">> The question is how does the caching help when you have multiple documents. My analysis is that (with a modern VM) it helps you only if the docFreq of a term is 16-31 and you are using a ConjunctiveScorer (i.e. not Wildcard searches). \n \n> The conjunctive scorer does not call score(HitCollector,int). This is only called in a few cases anymore. It can help a lot with a single-term query for a very common term, or for disjunctive queries involving very common terms, although BooleanScorer2 no longer uses it in this case. That's too bad. If all clauses to a query are optional, then the old BooleanScorer was faster. But it didn't always return documents in order... So it indeed may be time to retire this method. \n\nWith BooleanScorer2 It is quite possible to use different versions of DisjunctionScorer:\none for query top level that does not need skipTo(), and one for lower level that allows\nskipTo(). The top level one can be implemented just like the \"old\" BooleanScorer.\n\nIirc the method to implement such different behaviour are already in place (for scoring a range of documents),\nit only needs to be implemented for DisjunctionScorer, and the top level BooleanScorer2 should then\nuse it when appropriate.\n\nRegards,\nPaul Elschot\n",
            "date": "2006-03-04T05:54:26.000+0000",
            "id": 6
        },
        {
            "author": "Doug Cutting",
            "body": ">  Which is true? Or, as it seems likely, TermScorer was optimized for a case that is no longer valid (i.e. ConjunctiveScorer). \n\nNo, it was optimized for BooleanScorer's *disjunctive* scoring algorithm, which is no longer used by default, but is faster than BooleanScorer2's disjunctive scoring algorithm.  This applies to a very common type of query: classic vector-space searches.  So this optimization may not be leveraged much in the current codebase, but that does not mean that it is no longer valid.  But it may slow other sorts of searches, like your wildcards.  The challenge is not just how to figure out how to make your application as fast as possible, but how to do this without making other's and future applications slower.\n\n> In short, we should have two TermScorer implementations. One for low documents/term, and one for high documents/term.\n\nYes, I think that would be useful.  Classically, total query processing time is dominated by common terms, so that's an important case to optimize.  But It seems that with wildcard queries over smaller collections that these optimizations become costly.  So two implementations seems like it would make everyone happy.",
            "date": "2006-03-04T06:27:32.000+0000",
            "id": 7
        },
        {
            "author": "Mark Miller",
            "body": "Are we interested in this optimization?\n\nHere is an attempted patch. \n\nTwo issues:\n\n1. Seems it might be better to try and use IDF to determine which scorer to use (TermScorer or LowFreqTermScorer) rather than doc freq so that doc freq doesn't need to be accessed twice.\n\n2. I don't know at what 'level' the LowFreqTermScorer should be cut out for the TermScorer. Some benching may help.",
            "date": "2008-11-13T03:43:42.158+0000",
            "id": 8
        },
        {
            "author": "Robert Muir",
            "body": "Can we close this one?\n\nIt seems it was geared towards multitermqueries, but these are using different codepaths these days (e.g. Filter rewrite).\nI think as this issue stands though, I would be against changing the defaults...\n\n(separately: in LUCENE-2392, you can control this in your Similarity yourself)\n",
            "date": "2011-01-26T12:29:23.503+0000",
            "id": 9
        },
        {
            "author": "Robert Muir",
            "body": "In trunk, there is no longer a score cache in TermScorer because this is just an optimization for the TF/IDF scoring formula.\n\nInstead this optimization is in TFIDFSimilarity, if you want or don't want similar pre-computations in your scoring you can adjust this by plugging in your own Similarity.\n",
            "date": "2011-07-15T03:00:22.262+0000",
            "id": 10
        }
    ],
    "component": "core/search",
    "description": "TermScorer aggressively caches the doc and freq of 32 documents at a time for each term scored.  When querying for a lot of terms, this causes a lot of garbage to be created that's unnecessary.  The SegmentTermDocs from which it retrieves its information doesn't have any optimizations for bulk loading, and it's unnecessary.\n\nIn addition, it has a SCORE_CACHE, that's of limited benefit.  It's caching the result of a sqrt that should be placed in DefaultSimilarity, and if you're only scoring a few documents that contain those terms, there's no need to precalculate the SQRT, especially on modern VMs.\n\nEnclosed is a patch that replaces TermScorer with a version that does not cache the docs or feqs.  In the case of a lot of queries, that saves 196 bytes/term, the unnecessary disk IO, and extra SQRTs which adds up.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-502",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "TermScorer caches values unnecessarily",
    "systemSpecification": false,
    "version": "1.9"
}