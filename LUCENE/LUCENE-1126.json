{
    "comments": [
        {
            "author": "Hoss Man",
            "body": "bq. Switching to using JFlex's [:letter:] and [:digit:] predefined character classes ties (most of) these decisions to the user's choice of JVM version, and this seems much more reasonable to me than the current status quo.\n\nClarification: I don't think this would actually tie anything to the \"user's\" choice of JVM, wouldn't it make the behavior of the resulting code depend on the JVM choice of whoever generated StandardTokenizerImpl.java from StandardTokenizerImpl.jflex (not to mention possible discrepancies based on the version of JFLex in use at the time) ?\n\nI'm not positive, but couldn't this result in situations where a committer using a 1.5 JVM could generate and commit a StandardTokenizerImpl.java that had would have a different behavior then if he was using 1.4 -- all of which would be completely independent of whether or not the release engineer of the next release compiled the resulting grammer using 1.4?",
            "date": "2008-01-11T00:29:48.130+0000",
            "id": 0
        },
        {
            "author": "Steve Rowe",
            "body": "bq. I'm not positive, but couldn't this result in situations where a committer using a 1.5 JVM could generate and commit a StandardTokenizerImpl.java that had would have a different behavior then if he was using 1.4 - all of which would be completely independent of whether or not the release engineer of the next release compiled the resulting grammer using 1.4?\n\nYes, you're right - I was conceiving of the resulting lexer building the tables at runtime, but looking at the code in StandardTokenizerImpl.java, that's patently false - this, as you point out, is a compile-time operation.\n\nAnd when I compile with 1.4.2, the resulting StandardTokenizerImpl.java file is significantly different from the result with 1.5.0.\n\nI'm quite sure, despite this, that switching to the (compile-time) JVM's Unicode version's definitions of letters and digits is a better policy than depending on a non-existent update mechanism for the grammar.",
            "date": "2008-01-11T01:01:47.061+0000",
            "id": 1
        },
        {
            "author": "Steve Rowe",
            "body": "Compiled using JFlex 1.4.1, JDK 1.4.2",
            "date": "2008-01-11T01:07:13.135+0000",
            "id": 2
        },
        {
            "author": "Hoss Man",
            "body": "bq. this, as you point out, is a compile-time operation.\n\nagain, minor clarification ... if it were \"compile-time\" it would be *.java -> *.class conversion ... what we're talking about is ... i dunno ... \"jflex-time\" or \"source-generation-time\" ... where the JVM in use during the  *.flex -> *.java conversion affects the behavior of the final code.\n\n(i'm not saying it's a bad thing ... i just want to be clear about what happens where)",
            "date": "2008-01-11T07:22:02.656+0000",
            "id": 3
        },
        {
            "author": "Steve Rowe",
            "body": "In part my imprecise characterization of the process comes from what is likely a misunderstanding of the Lucene-Java release process - when you said:\n\nbq. I'm not positive, but couldn't this result in situations where a committer using a 1.5 JVM could generate and commit a StandardTokenizerImpl.java that had would have a different behavior then if he was using 1.4 - all of which would be completely independent of whether or not the release engineer of the next release compiled the resulting grammer using 1.4?\n\nI assumed you meant that during the release process, the lexical scanner source (.java file) would be regenerated from the grammar (.jflex file).  And in this scenario, I meant to refer to \"compile-time\" as the entire build process - raw source to jar assembly, *including* lexical scanner generation - undertaken when producing a binary release.\n\nBut of course you're right :) .  The JVM version being used during source-generation-time (occurring prior to, and potentially not contiguously with, bytecode-generation-time) determines the version of Unicode used to define the meaning of \"letter\" and \"digit\".\n",
            "date": "2008-01-11T13:47:42.323+0000",
            "id": 4
        },
        {
            "author": "Grant Ingersoll",
            "body": "{quote}\n'm not positive, but couldn't this result in situations where a committer using a 1.5 JVM could generate and commit a StandardTokenizerImpl.java that had would have a different behavior then if he was using 1.4 - all of which would be completely independent of whether or not the release engineer of the next release compiled the resulting grammer using 1.4?\n{quote}\n\nTrue, but no committer should ever be doing this, right?  :-)  At least not until 3.0.  The ANT settings specify 1.4 right?  If it does happen, it is a bug, no?",
            "date": "2008-01-11T14:33:10.289+0000",
            "id": 5
        },
        {
            "author": "Steve Rowe",
            "body": "bq. The ANT settings specify 1.4 right? If it does happen, it is a bug, no?\n\nThe ANT settings specify that the bytecode compiler should produce 1.4-JVM-compatible byte code (javac.target property), and that the Java source code should be interpreted as complying with a particular version of the Java language specification (javac.source property).\n\nBut the JVM is a different tool from the bytecode compiler, and as Hoss pointed out, it is the JVM under which JFlex runs that makes the difference.\n\nAs I noted above:\n\nbq. And when I compile with 1.4.2, the resulting StandardTokenizerImpl.java file is significantly different from the result with 1.5.0.\n\n({{s/compile/generate the scanner source/g}} :) )",
            "date": "2008-01-11T17:14:30.834+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "This patch looks reasonable?  We are replacing our own hardwired regexp for DIGIT with JFlex's :digit: which then falls back to the Character.isDigit() [equivalent] on the JVM that ran jflex.",
            "date": "2008-08-27T10:19:19.756+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "I will commit this in a day or two.",
            "date": "2008-08-28T11:51:53.361+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "Hmm -- I'm now seeing an failure with this patch, in TestThaiAnalyzer (in contrib/analyzers):\n\n{code}\n    [junit] Testcase: testAnalyzer(org.apache.lucene.analysis.th.TestThaiAnalyzer):\tFAILED\n    [junit] expected:<?[]> but was:<?[??]>\n    [junit] junit.framework.ComparisonFailure: expected:<?[]> but was:<?[??]>\n    [junit] \tat org.apache.lucene.analysis.th.TestThaiAnalyzer.assertAnalyzesTo(TestThaiAnalyzer.java:43)\n    [junit] \tat org.apache.lucene.analysis.th.TestThaiAnalyzer.testAnalyzer(TestThaiAnalyzer.java:54)\n    [junit] \n{code}\n\nDoes anyone else see this?",
            "date": "2008-09-03T09:27:03.808+0000",
            "id": 9
        },
        {
            "author": "Steve Rowe",
            "body": "Yeah, I see this too.\n\nThe issue is that the entire Thai range {{\\u0e00-\\u0e5b}} is included in the unpatched grammar's {LETTER} definition, which contains the huge range {{\\u0100-\\u1fff}}, much of which is not actually letters.  The patched grammar instead substitutes the Unicode 3.0 {{Letter}} general category (via JFlex's [:letter:]), which excludes some characters in the Thai range: non-spacing marks, a currency symbol, numerals, etc.\n\nThaiAnalyzer uses ThaiWordFilter, which uses Java's BreakIterator to tokenize the contiguous text (i.e. without whitespace) provided by StandardTokenizer.\n\nThe failing test expects to see {{\"\\u0e17\\u0e35\\u0e48\"}}, but instead gets {{\"\\u0e17\"}}, because {{\\u0e35}} is a non-spacing mark, which the patched StandardTokenizer doesn't pass to ThaiWordFilter.\n\nBecause of this problem, I guess I'm -1 on applying the patch I provided.\n\nOne solution would be to switch from using the {{Letter}} general category to the derived property {{Alphabetic}}, which includes both general categories {{Letter}} and {{Mark}}. (see Annex C of [the Unicode Regular Expressions Technical Standard|http://www.unicode.org/unicode/reports/tr18/#Compatibility_Properties] under \"alpha\" for discussion of this).  The current version of JFlex does not support Unicode property references in its syntax, though, so simplifying -- and correcting -- the grammar may have to wait for the next version of JFlex, which will support syntax like {{\\p{Alphabetic}}}.\n",
            "date": "2008-09-03T19:33:11.056+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nOne solution would be to switch from using the Letter general category to the derived property Alphabetic, which includes both general categories Letter and Mark. (see Annex C of the Unicode Regular Expressions Technical Standard under \"alpha\" for discussion of this). The current version of JFlex does not support Unicode property references in its syntax, though, so simplifying - and correcting - the grammar may have to wait for the next version of JFlex, which will support syntax like \\p{Alphabetic}.\n{quote}\nCould we, alternatively, modify the patch to explicitly add back in the full Thai range into ALPHANUM, and then upgrade to \\p{Alphabetic} once the next version of JFlex is released?\n\nOr are there other languages, besides Thai, that we might break with this patch?",
            "date": "2008-09-04T11:12:26.924+0000",
            "id": 11
        },
        {
            "author": "Steve Rowe",
            "body": "{quote}\nCould we, alternatively, modify the patch to explicitly add back in the full Thai range into ALPHANUM, and then upgrade to \\p{Alphabetic} once the next version of JFlex is released?\n\nOr are there other languages, besides Thai, that we might break with this patch?\n{quote}\n\nI noticed in looking at the Unicode database that Lao, which is contiguous with Thai and contained in the unpatched {{{LETTER}}} range, has exactly the same issue as Thai.  However, the Lucene code base doesn't contain a Lao Analyzer.  And I think ThaiAnalyzer is depending on faulty behavior from StandardTokenizer, so \"fixing\" the issue for other languages would be to make the assumption that they too would depend on bad behavior.\n\nI'll shortly provide a redone patch that allows the ThaiAnalyzer to work again, but unless we have evidence of actual use by other language analyzers, I don't think we should be addressing them right now.",
            "date": "2008-09-04T16:12:58.014+0000",
            "id": 12
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I'll shortly provide a redone patch that allows the ThaiAnalyzer to work again, but unless we have evidence of actual use by other language analyzers, I don't think we should be addressing them right now.\n\nOK this sounds like the right approach.",
            "date": "2008-09-04T17:06:29.225+0000",
            "id": 13
        },
        {
            "author": "Steve Rowe",
            "body": "Refreshed original patch to include the Thai range {{[\\u0e00-\\u0e59]}} in the {{{ALPHANUM}}} definition.\n\nAll tests, including TestThaiAnalyzer, now pass.",
            "date": "2008-09-04T17:31:57.599+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "Steven, it looks like you ran JFlex with a 1.5 or 1.6 JRE?  Should we stick with that, or, go with a 1.4 JRE (which is indeed significantly different)?",
            "date": "2008-09-04T17:54:01.397+0000",
            "id": 15
        },
        {
            "author": "Steve Rowe",
            "body": "bq. Steven, it looks like you ran JFlex with a 1.5 or 1.6 JRE? Should we stick with that, or, go with a 1.4 JRE (which is indeed significantly different)?\n\nYou're right, Mike - I was inadvertently using a 1.5 JRE.  I'll put up another patch produced with 1.4 JRE shortly - we should definitely not go with 1.5..",
            "date": "2008-09-04T18:31:59.284+0000",
            "id": 16
        },
        {
            "author": "Michael McCandless",
            "body": "Steven, it's OK I can regen; I just wanted to confirm which JRE (1.4) we should use.\n\nI'm also going to add a comment at the top of StandardTokenizerImpl.jflex stating this.",
            "date": "2008-09-04T18:41:09.979+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "Committed revision 692211.\n\nThanks Steven!",
            "date": "2008-09-04T19:49:56.890+0000",
            "id": 18
        }
    ],
    "component": "modules/analysis",
    "description": "Summary of thread entitled \"Fullwidth alphanumeric characters, plus a question on Korean ranges\" begun by Daniel Noll on java-user, and carried over to java-dev:\n\nOn 01/07/2008 at 5:06 PM, Daniel Noll wrote:\n> I wish the tokeniser could just use Character.isLetter and\n> Character.isDigit instead of having to know all the ranges itself, since\n> the JRE already has all this information.  Character.isLetter does\n> return true for CJK characters though, so the ranges would still come in\n> handy for determining what kind of letter they are.  I don't support\n> JFlex has a way to do this...\n\nThe DIGIT macro could be replaced by JFlex's predefined character class [:digit:], which has the same semantics as java.lang.Character.isDigit().\n\nAlthough JFlex's predefined character class [:letter:] (same semantics as java.lang.Character.isLetter()) includes CJK characters, there is a way to handle this using JFlex's regex negation syntax {{!}}.  From [the JFlex documentation|http://jflex.de/manual.html]:\n\nbq. [T]he expression that matches everything of {{a}} not matched by {{b}} is !(!{{a}}|{{b}}) \n\nSo to exclude CJ characters from the LETTER macro:\n\n{code}\n    LETTER = ! ( ! [:letter:] | {CJ} )\n{code}\n \nSince [:letter:] includes all of the Korean ranges, there's no reason (AFAICT) to treat them separately; unlike Chinese and Japanese characters, which are individually tokenized, the Korean characters should participate in the same token boundary rules as all of the other letters.\n\nI looked at some of the differences between Unicode 3.0.0, which Java 1.4.2 supports, and Unicode 5.0, the latest version, and there are lots of new and modified letter and digit ranges.  This stuff gets tweaked all the time, and I don't think Lucene should be in the business of trying to track it, or take a position on which Unicode version users' data should conform to.  \n\nSwitching to using JFlex's [:letter:] and [:digit:] predefined character classes ties (most of) these decisions to the user's choice of JVM version, and this seems much more reasonable to me than the current status quo.\n\nI will attach a patch shortly.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1126",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Simplify StandardTokenizer JFlex grammar",
    "systemSpecification": true,
    "version": "2.2"
}