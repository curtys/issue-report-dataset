{
    "comments": [
        {
            "author": "Uwe Schindler",
            "body": "First version of a patch for contrib-queries. It includes my classes for package o.a.l.search.trie. The TrieRangeFilter was refactored to be a separate class (in contrast to my original implementation). The class is Java 1.4 compatible (like contrib-queries). JavaDocs were updated and a general information page for the package was given.\n\nA first test case to test the trie-encoded values was added. The filter and query tests must be written.",
            "date": "2008-11-26T14:12:06.438+0000",
            "id": 0
        },
        {
            "author": "Uwe Schindler",
            "body": "See also the discussion in LUCENE-1461, as it inspired me to give my code to contrib.",
            "date": "2008-11-26T14:16:16.785+0000",
            "id": 1
        },
        {
            "author": "Uwe Schindler",
            "body": "fixed patch",
            "date": "2008-11-26T14:23:59.974+0000",
            "id": 2
        },
        {
            "author": "Earwin Burrfoot",
            "body": "Lucene (at least 2.3.2 version) cannot index Strings containing 0xFFFF characters, as it is used internally as EOS marker.\nI've implemented binary encoding similar to yours, but after hitting this issue with some converted Date, switched to base-2 ^15^ encoding, leaving high bit always zero.\n\n{code}\n  public String toLucene(Long n) {\n    if (n == null) {\n      return null;\n    }\n\n    long u = n + Long.MAX_VALUE + 1;\n\n    return new String(new char[]{\n        (char) ((u & 0xFFFE000000000000L) >> 49),\n        (char) ((u & 0x0001FFFC00000000L) >> 34),\n        (char) ((u & 0x00000003FFF80000L) >> 19),\n        (char) ((u & 0x000000000007FFF0L) >> 4),\n        (char) ((u & 0x000000000000000FL) << 11),\n    });\n  }\n\n  public Long fromLucene(String string) {\n    if (string == null) {\n      return null;\n    }\n\n    return (((long) string.charAt(0)) << 49 |\n            ((long) string.charAt(1)) << 34 |\n            ((long) string.charAt(2)) << 19 |\n            ((long) string.charAt(3)) << 4 |\n            ((long) string.charAt(4)) >> 11\n    ) - Long.MAX_VALUE - 1;\n  }\n{code}",
            "date": "2008-11-26T14:34:50.660+0000",
            "id": 3
        },
        {
            "author": "Earwin Burrfoot",
            "body": "Read your code closer. Silly me, I assumed you went with base-2 ^16^.",
            "date": "2008-11-26T15:02:33.373+0000",
            "id": 4
        },
        {
            "author": "Uwe Schindler",
            "body": "No problem, I have choosen the two chars / byte approach for two reasons:\n\na) it is simplier to generate lower precision terms (for 1-8 bytes precision) you just have to remove 2 chars per byte. in base 2^15, you only have 4 precisions and some more bits. The number of terms in my special TrieRangeFilter per precision would not be 256 values, it would be 32768, making it slower. Another posibility would be to cast each byte to one char (+offset), but see the next point:\nb) Java sometimes has strange string comparisons, and I did not want to walk into incompatiblities with String.compareTo()\n\nBut one other point you noted in the other JIRA issue is hurting me: I did not try to sort the results to my combined, prefixed field since long time, and you are right, it is not possible, if all different precisions are in the same field. A earlier version of TrieRangeQuery used a suffix after the field-name, automatically added by the TrieUtils.addXXXXTrieDocumentField. I reverted removed this about two years ago, but never tried to sort a numerical field since then. I think, I have to write a bug report for my own project :) The question is, if this is included into contrib and I put a dependency in my project to this contrib pacakge, the index format of panFMP may change, which is not good.\n\nJust one question to other developers: Why is it not possible, to sort by a field with more than one term/doc, if you would restrict this to only use the *first* added term to the document as sort key in FieldCache?",
            "date": "2008-11-26T15:28:35.921+0000",
            "id": 5
        },
        {
            "author": "Earwin Burrfoot",
            "body": "bq. in base 2^15, you only have 4 precisions and some more bits\nWe have slightly different approaches. Yours is universal, and mine requires hand-tuning. I have an abstract FastRangeFilter, which I extend, specifying functions that lower precision before encoding, thus I have any required amount of type/field-dependent precision levels. For further ease of use that one is extended by FastDateRangeFilter, which accepts an array of date parts, like {HOUR_OF_DAY, DAY_OF_MONTH}.\nThat allows me to exploit known statistical properties of my requests/data, for example most date ranges are rolling day/week/month/3 months windows, or salaries which tend to be attracted to certain values.\n\nbq. Java sometimes has strange string comparisons, and I did not want to walk into incompatiblities with String.compareTo()\nFact that java strings are UCS-16 (UTF-16 minus special non-16bit characters) is written into java language specification, so you can trust String.compareTo() - anything that blows up there, is not Java(tm). Problems usually come from within libraries.\n\nbq. I did not try to sort the results to my combined, prefixed field since long time, and you are right, it is not possible, if all different precisions are in the same field.\nActually, right now I'm -stealing- borrowing your idea of storing various precisions in the same field. I have my own custom field cache, so broken sort can be fixed easily. Having everything in the same field allows you to do exactly one pass through TermEnum/TermDocs, and that is what I'm going to exploit :)",
            "date": "2008-11-26T16:48:29.963+0000",
            "id": 6
        },
        {
            "author": "Uwe Schindler",
            "body": "You are right, my intentation was to make it universal by casting everything to unsigned long. For the sorting problem:\nWith standard lucene, you can only sort when you have one term/field/doc. I am currently testing my code after modifying it to use a more compact version of the encoding:\n\na) I do not encode half bytes, I encode full bytes per char. I still have 8 precisions after it, but code gets simplier and you have terms with length=8 chrs after encoding. After reading docs of String.compareTo() again, I understood the \"Java-Way\" of binary sort order and verified it with my test cases.\n\nb) the full precision is stored in the user-given document field name, all 7 lower precisions are prefixed (as before) and put into a \"helper\" field with suffix \"#trie\" after the name. This field is only indexed, not stored or anything else. This field is only used in TrieRangeFilter for the trie algorithm.\n\nFor my project *panFMP* it is not such a big problem, if you inform the \"few\" users using it (I know all of them) and ask them to do a index rebuild (for that a tool is available). But the encoding format of this contrib package should be fixed and discussed before it is released!",
            "date": "2008-11-26T17:05:16.306+0000",
            "id": 7
        },
        {
            "author": "Earwin Burrfoot",
            "body": "bq. But the encoding format of this contrib package should be fixed and discussed before it is released!\nMake it pluggable? I have my encoding separate from range filters and precision loss, you can specify fields to use, etc-etc. Anyway, people can always copy your code and tweak it to their specific needs. For contribs, imho, the idea is much more valuable than actual implementation details.",
            "date": "2008-11-26T17:26:37.245+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\n> Why is it not possible, to sort by a field with more than one\n> term/doc, if you would restrict this to only use the first added term\n> to the document as sort key in FieldCache?\n{quote}\n\nThis was discussed, but not resolved, in LUCENE-1372.\n\nIf we wanted to allow \"use the first term\" as a policy, we'd have to\nfix how FieldCacheImpl computes the StringIndex: currently it gets a\nTermEnum, walks through all terms, for each term walks through all\ndocs, and records docID->termNumber as well as termNumber->termText\narrays, un-inverting the field.  I guess we could get a TermPositions,\ninstead, and only pay attention to position=0 terms, and then only\nincrement t if there were any docs with position=0 occurrences of the\nterm.\n",
            "date": "2008-11-26T18:36:05.618+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "\nIt seems like there are two things you might want to make \"pluggable\":\n\n  - Which hierarchical ranges to create & logic that creates them for\n    a given value.  For certain types (dates) I think there's one\n    obvious set of ranges (year, quarter, month, day, etc.).  For\n    something like \"price\" it's probably more app dependent, eg if you\n    present certain filters/facets to the user.\n\n  - What label is attached to each range, and the correspond decoder\n    to map that label back into its upper/lower values.\n\nSo we could provide good defaults for eg Date.  The labels would\nideally be things like 2007, 2008 and \"Jan 2008\", \"Feb 2008\", etc.  I\nthink?  Hmm -- except for the need to sort them \"properly\".\n\nWe really should change Lucene's terms so that they are opaque objects\n(as KS/Lucy is doing).  This way you could store a Term like \"Feb\n2008\" in the index, along with details like \"this is a generated\nranged datetime term\" and the numeric date details ie 02/2008, and\nyou'd just have to export a compareTo to Lucene.\n",
            "date": "2008-11-26T19:05:05.479+0000",
            "id": 10
        },
        {
            "author": "Uwe Schindler",
            "body": "An updated version of the patch. The format of encoded terms is incompatible to the previous patch.\n\nChanged:\n- Each byte of the 8 byte longs is encoded as 1 char (see coments before) -> more compact\n- each byte is shifted by 0x30 when converted to char (e.g. value 0x05 is saved as char 0x35)\n- the lower precision terms are stored in a separate field (using the original fieldname+\"#trie\"). With that it is possible to sort the original field. The mentioned helper field is only index. The prefix for the lower precisions starts at char 0x20. By separating prefix and data ranges, later merging the separate and the original field is easily possible. Because of this lower precisions would be listed before higher precisions or the full precision in the ordered term list.\n- fix a small issue with checking a term two times (between two ranges)\n- added test for TrieRangeQuery with a full-bounded and half-open range.\n\nI will say something about Mike's suggestions tomorrow, now it is to late!",
            "date": "2008-11-26T22:19:04.974+0000",
            "id": 11
        },
        {
            "author": "Uwe Schindler",
            "body": "I wanted to go to bed, but it is the same every time, cannot sleep :-)\n\nThe discussions about TrieRangeQuery are the same like two years ago when I prepared the original paper cited in the documentation with my mentor/colleague Michael Diepenbroek (the second author on the paper) -- here my comments about the suggestions to Mike McCandless and Earwin, after looking into my notices, I can explain:\n\nWhen looking for the first time on the algorithm to split the range, the ideas of Mike and Earwin are very clear: You are both interested in Dates and both of you have the same idea, how to divide them into parts with different \"precisions\". For dates it is rather simple: If you have a range between March 1999 and August 2006, the two precisions \"full years\" and \"months\" are very easy to understand when creating the ranges. You would have three range queries: Mar'1999 ... Dec'1999, 2000 ... 2005, Jan'2006 ... Aug'2006 very easy. You have to visit 10+6+8=24 terms. Simple. The maximum number of terms would be 12+12+numberOfTotalPossibleYears. This is easy to understand and very effective.\n\nMy approach in principle does the same, but the values are simply devided into bytes. There is no problem with it to store the number of milliseconds since 1970-01-01 in different precisions. The total range of this long would be very very large (I do not want to calculate now how many years BC the value -2^-31ms would be). If you have a range like the one above you have e.g. longs like 0x0000323113422134 TO 0x0000325113422174. You see the frront is identical, the center of the range maybe only goes to the precision of 12 digits. I said the long ist divided into 8 bytes, so you would use only the end of the range in highest precsision, the center in lower. As the lowest precsison does not change (it would only change if you are thousands of years from start to end), my algorithm would not use it, it would only go downto the precision that exists. The if-clause \"(length==1 || minShort.compareTo(maxShort)>=0)\" is responsible for that. If the lower and upper end of the next lower precision range is zero or inverted, it is not further calculated.\n\nBecause of this shortcut it does not make a difference if the term values are distributed in the index very far from each other or are all on one place (the algorithm works as good, if your index contains prices between 1$ and 100$ or if you have double that give ages of millions of years and you want to do a range select on them. Independent on the selected range the number of terms to be visited is limited by my algorithm to the number of 3825 = (a maximum of 255 terms in the lowermost range)+(a maximum of 255 terms in the uppermost range)+(255)+(255)+......+(255 in center of range). This is a hard limit, and this limit can only be reached if you would have an index with 2^64-2 documents, that have 2^64-2 different long values and you have a range that selects all of them (only in this case you have to visit 3825 terms!).\n\nYou can test a little bit with the new test case in the latest patch (change some numbers) and uncomment the System.out.println() for the number of terms seen. You will see, even with large indexes, the number of terms in mostly about 400. Only very very seldom more terms were visited (I have seen in our 500,000 docs index only one case with 750 terms, which can happen if you have a non-homogenous dispersion of values that has a local maximum near one of the range bounds). Important: The TrieRangeQuery only makes sense if your index as a minimum of about 500 docs, if there are less, you mostly have to visit all terms. Just play with the test case and do some statistics!\n\nBecause of all this, I see no real need for giving the user a customized way to convert the values into Terms and calculate lower precisions of it. The above code works performant (you have tested it yesterday with our index) even for doubles, that are very seldom homogenous distributed through the whole LONG range if stored as IEEE-754 bitmap. So TrieRangeQuery in the current version is effective for longs, doubles and Dates (as they are longs). The only problem would be fixed comma numbers with many digits (e.g. BigDecimals), they cannot simply casted to longs. For these type of numbers, TrieUtils could be extended to generate longer strings.\n\nTo space usage: You are right, if you only want to have ranges on year/month, a long is too much. But keep in mind, the length of terms is not so important for index size, important is the number of different terms in the TermEnum. So I see no problem with storing integers and doubles and dates as a 64bit value in index - its only little bit extra term length with \"unused bits\".",
            "date": "2008-11-26T23:56:58.957+0000",
            "id": 12
        },
        {
            "author": "Paul Elschot",
            "body": "{quote}Independent on the selected range the number of terms to be visited is limited by my algorithm to the number of 3825 = (a maximum of 255 terms in the lowermost range)(a maximum of 255 terms in the uppermost range)(255)(255)......+(255 in center of range).{quote}\n\nThe code uses a trie factor of 256, or 8 bits in a long of 64 bits.\nWould it be possible to use lower values for this trie factor, like 16 (4 bits) or even 4 (2 bits)?\nIn such cases the (rough) maximum number of terms for a single ended range becomes smaller:\n(256-1) * (64/8) = 255 * 8 = 2040\n(16-1) * (64/4) = 15 * 16 = 240\n(4-1) * (64/2) = 3 * 32 = 96\nThis reduction comes at the cost doubling or quadrupling the number of the indexed terms in the lower precision field.\n\nThe number of characters in the lower precision terms is not really relevant in the term index, because terms are indexed with common prefixes. Therefore in these cases one could just use a character to encode the 4 bits or 2 bits.\n\nSo the question is would it be possible to specify the trie factor when building and using the index?\n",
            "date": "2008-11-27T08:20:29.616+0000",
            "id": 13
        },
        {
            "author": "Uwe Schindler",
            "body": "I think, this would easyly be possible. Refactoring some code parts, special implementations of the encoder and incrementTrieCoded/decrementTrieCoded would be possible.\nA possibility to make this configuraeable would be to use an instance of TrieUtils, that takes the trie factor as constructor argument. The user would then encode/decode all his values using the instance. TrieRangeFilter would also get an instance of the encoder and use it to calculate the prefix terms and so on.\n\nbq. The number of characters in the lower precision terms is not really relevant in the term index, because terms are indexed with common prefixes. Therefore in these cases one could just use a character to encode the 4 bits or 2 bits.\nbq. So the question is would it be possible to specify the trie factor when building and using the index?\n\nYes thats good for jumping to the correct term to start in the range query. The problem with shorter trie factors would be, that for each precision (e.g. 4 bits, 2 bits) you need one full char in the encoded variant. As length is not a problem for terms, I think the common prefixed cannot be used so effective (a lot of terms with two low-cardinality chars at the beginning).",
            "date": "2008-11-27T08:39:39.775+0000",
            "id": 14
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. In such cases the (rough) maximum number of terms for a single ended range becomes smaller:\n\nAre you sure? My algorithm is more effective on single ended ranges (the number of terms is half the size of the terms of a two-ended range). When the lower bound is open, it is enough to use the lowest precision in the lower half of the range beginning (this special case is done in splitRange()). Higher precision terms only occur at bthe higher bound. This is why I use 255 and not 256 for my formula of the maximum number of terms (a range of all 256 terms in higher precision could easily reduced to the lower precision).",
            "date": "2008-11-27T08:45:50.941+0000",
            "id": 15
        },
        {
            "author": "Paul Elschot",
            "body": "I simply used the maximum number of terms at each level times the number of levels.\nFor a trie factor of 2**b (using b bits), and a long of 64 bits, that is:\n(2**b - 1) * (64/b)\nThe actual values for b = 8, b = 4 and b = 2 are shown above.",
            "date": "2008-11-27T08:59:06.831+0000",
            "id": 16
        },
        {
            "author": "Uwe Schindler",
            "body": "You are right, thats correct! I misunderstood your answer. The number of terms for single ended ranges is about half of the terms for a two-ended range.",
            "date": "2008-11-27T09:03:41.021+0000",
            "id": 17
        },
        {
            "author": "Paul Elschot",
            "body": "So going from 8 bits to 4 bits will reduce the number of filter terms by a factor 8.5, for a cost of doubling the number of indexed terms. I'd like to see actual performance figures, but on the face of it I'd rather use 4 bits.",
            "date": "2008-11-27T09:15:29.373+0000",
            "id": 18
        },
        {
            "author": "Uwe Schindler",
            "body": "Just a update of the patch, before I want to implement Paul's suggestions on trie factor.\n\nThe current patch has some performance improvements by avoid seeking back and forth in IndexReader's TermEnum. IndexReader's TermEnum may also better use caching. The parts of the range, that use Terms, that come earlier in the TermEnum are done first. The order is now:\n- Highest precision (because the field name of highest precision is not suffixed, and so the terms come earlier, \"fieldname\"<\"fieldname#trie\")\n- Lower precision starting with the lowest one. The prefix of the lowest precision is the smallest (0x20) and goes up to 0x27\n\nWhen looking into SegmentTermEnum's code, I realized that IndexReader.terms(Term t) is faster than only getting the complete TermEnum and then seekTo(Term). Why this difference? I first wanted to change my code to use only one instance (like with the TermDocs) of the TermEnum through the wohle range split prcess and seek the enum for each range, but I dropped that change.\n\nFurther improvements of that patch are more comments and a cleaner (and more elegant) code in TrieUtils (without using a StringBuffer).",
            "date": "2008-11-27T10:19:54.218+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "Sheesh, doesn't anyone sleep anymore?\n\nbq. When looking into SegmentTermEnum's code, I realized that IndexReader.terms(Term t) is faster than only getting the complete TermEnum and then seekTo(Term).\n\nDid you mean TermEnum.skipTo?  See LUCENE-1437.",
            "date": "2008-11-27T10:33:59.719+0000",
            "id": 20
        },
        {
            "author": "Michael McCandless",
            "body": "bq. My approach in principle does the same, but the values are simply devided into bytes.\n\nI do very much like that your approach requires no \"application or type-specific knowledge\" on what might make good ranges.  Ie, it's generic, and so any type that can be cast into long and back w/o losing any information, can make use of this.",
            "date": "2008-11-27T10:39:29.220+0000",
            "id": 21
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Did you mean TermEnum.skipTo? See LUCENE-1437.\n\nYes exactly that, I wrote seekTop(), but meant skipTo(). The patch in this issue is only a faster scanning approach, but the best would be to make skipTo() use the same logic like SegmentInfoReader.terms that uses seeking on disk and so on, but this comment would be  better for the above issue.\n\nI am fine with calling IndexReader.terms(Term) to use the cache and faster seeking. The cost of creating new instances of TermEnums is less than doing disk reads.",
            "date": "2008-11-27T10:44:38.183+0000",
            "id": 22
        },
        {
            "author": "Paul Elschot",
            "body": "When the trie factor can be chosen, it might make sense to encode it in the trie field name, something like:\n{code}fieldName#trieNN{code}\nwith NN the trie factor or its number of bits.",
            "date": "2008-11-27T12:21:18.391+0000",
            "id": 23
        },
        {
            "author": "Uwe Schindler",
            "body": "Here is a new version of the patch, supporting 8bit, 4bit and 2bit trie variant. The API is similar, but instead of using TrieUtils as static class, you can choose 3 singleton converters: e.g. TrieUtils.VARIANT_8BIT.convertSomething(). TrieRangeQuery and TrieRangeFilter both accept a parameter for choosing the variant. A default can be set with a static TrieRangeFilter.setDefaultTrieVariant(...) and the corresponding getter.\n\nTo Paul's suggestion: You could put the trie variant into the filed name, but the main fieldname, where the full precision value is indexed/stored/whatever, has no suffix. This would make it inconsistent.\n\nIn my opinion, one should choose the same trie variant when indexing values and doing range queries. It is the same like choosing another analyzer for indexing and searching. Maybe setting the default trie variant with this static method maybe better hosted in TrieUtils, but this is room for discussion.\n\nNow it is time to do some performance tests with really big indexes  comparing the trie variants :-) Does anybody has a synthetic benchmarker that can be easily used for this? The test case is not so interesting (uses RAMDirectory). I could reindex our PANGAEA index for performance testing in real world (doubles, dates).\n\nThe testcase (if you uncomment the printout in TrieRangeFilter of number of terms) clearly shows the lower number of terms visited for 4bit or 2bit. Formulas are in the package description.\n\nIn my opinion 4bit is a good alternative (about 3 times space requirement for about 8.5x less terms), but the impact of 2bit is to low for the about 6 times larger space.",
            "date": "2008-11-27T15:08:11.183+0000",
            "id": 24
        },
        {
            "author": "Mark Harwood",
            "body": "A note of caution - I noticed when moving from Lucene 2.3 to 2.4 that my similar scheme for encoding information meant that I couldn't encode information using byte arrays using bytes with values > 216.\nThe changes (I think in Lucene-510) introduced some code that modified the way the bytes were written/read and corrupted my encoding.\n\nNot sure if your proposed approach is prone to this or if anyone can cast further light on these encoding issues.\n\nGood to see this making its way into Lucene, Uwe.",
            "date": "2008-11-27T21:19:06.381+0000",
            "id": 25
        },
        {
            "author": "Uwe Schindler",
            "body": "I do not know what you had done in your code. Did you directly converted the byte[] arrays to a string, e.g. by using new String(byte[],charset) or without charset (in this case Java would use whats actual, but maybe its UTF-8)? Then, the character 216 (0xd8) would be interpreted by new String() as an UTF-8/16 sequence or whatever and map to some unknown char. UnicodeUtils, on the other hand, encodes the java chars to UTF-8 when storing in index, but does not like chars >0xd800 and replaces them by the replacement char for unknown chars. And this is a modification, as the char >0xd800 is not valid (see source of UnicodeUtils). and 0xd800 looks like 0xd8 (==216).\n\nMy code does not transform a byte[] directly to a string, it creates a 16 bit standard java char of each byte with some offset. As the trie code only produces bytes between 0-255 and it adds a offset of 0x30 to it, the range of chars is 0x30..0x12f. This range is unicode safe and can be easily encoded to UTF-8. But I will make a explicit testcase for that tomorrow!\n\nMaybe your problem was the mentioned mis-use of directly generating strings from byte arrays using unknown/incorrect charset. Keep me informed!\n\nBy the way Earwin Burrfoot: are you sure, your 15bit encoding works with the latest Lucene version, maybe this affects you, too?",
            "date": "2008-11-27T22:29:57.978+0000",
            "id": 26
        },
        {
            "author": "Mark Harwood",
            "body": "Yep, I was simply using new String(byte[]) \n",
            "date": "2008-11-27T22:44:51.946+0000",
            "id": 27
        },
        {
            "author": "Nadav Har'El",
            "body": "Hi, I just wanted to comment that a similar method was proposed in 2005 by several researchers in IBM (all of them have moved to Yahoo by now, actually). See \"Inverted Index Support for Numeric Search\", by M Fontoura, R Lempel, R Qi, J Zien\nhttp://webcourse.cs.technion.ac.il/236620/Winter2006-2007/hw/WCFiles/paramsearch.pdf\nThe paper contains some discussion on the tradeoffs between the number of layers, number of terms, index size, and so on, so it might contain some valuable ideas even if you don't use their scheme exactly.",
            "date": "2008-12-01T13:52:09.930+0000",
            "id": 28
        },
        {
            "author": "Uwe Schindler",
            "body": "Thank you, I did not know this paper. It is often rather complicated to find all papers about such subjects, very nice work!",
            "date": "2008-12-01T22:07:03.539+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "Uwe, is this ready to be committed?  It looks good, and the tests pass.  Thanks!",
            "date": "2008-12-02T11:55:41.823+0000",
            "id": 30
        },
        {
            "author": "Uwe Schindler",
            "body": "It is almost complete. In my opinion the only change would be the setting of defaults. I wanted to move this into TrieUtils directly. Let me iterate one more patch and then we could commit. As I currently have no contributor status, it is simplier to first iterate the patch enough before committing. I was waiting for any additional comments before releasing a new patch version.\n\nI did not had time to do some benchmarks, so testing the three trie variants for speed/disk io/indexing speed is not yet done. My current benchmarks affect only the performance of my \"old\" trie code, not the new one using the more binary encoding. I asked for a good \"benchmarking framework\", is contrib/benchmark useful for that? For benchmarking you need to create rather big indexes, maybe containing random numeric values. So the benchmark may also need much disk space (1 Gig?), ok you can leave out stored fields and additional full text fields, but the benchamrk should at last have also a normal tokenized field for performance with combining trie / ordinal term queries (like in the paper given by Nadav).\n\nDo you think, it is good to directly include it into contrib-search? In my opinion, it is, but maybe others think different.",
            "date": "2008-12-02T12:14:59.888+0000",
            "id": 31
        },
        {
            "author": "Uwe Schindler",
            "body": "New version of the patch:\n- moved default trie variant handling to TrieUtils\n- method in TrieRangeFilter that returns the number of visited terms after execution of getDocIdSet()\n- tests print out the number of terms, other improvements to tests\n\nJust one question: is it ok to have a static initializer in the test to prevent the test from generating the rather large index for each test?",
            "date": "2008-12-02T15:30:39.613+0000",
            "id": 32
        },
        {
            "author": "Uwe Schindler",
            "body": "From my point of view: the patch is now ready to commit, all tests pass.\n\nThe last patch also improved javadocs. Maybe some native speaker may please look into my javadocs, it is \"Denglish\" (as it is called in Germany)? :-)",
            "date": "2008-12-02T16:03:17.390+0000",
            "id": 33
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I asked for a good \"benchmarking framework\", is contrib/benchmark useful for that?\n\nI think it is good.  You should probably make your own DocMaker to create random number fields, or, make a big line file and use LineDocMaker.\n\nAnd then also create your own QueryMaker to create TrieRangeQuery's.\n\nOne nice test to add, to assert correctness, would be to iterate N times, randomly picking lower/upper bounds, and compare the builtin RangeFilter vs the TrieRangeFilter, asserting that they pull the same docs?\n\nbq. is it ok to have a static initializer in the test to prevent the test from generating the rather large index for each test?\n\nI think that's fine.",
            "date": "2008-12-02T23:29:33.102+0000",
            "id": 34
        },
        {
            "author": "Uwe Schindler",
            "body": "I have some first \"benchmarks\" on the three trie implementations. They were done using three indexes with same content, but encoded in 8bit, 4bit and 2bit, containing real-world data in 575,000 documents of the PANGAEA repository (see www.pangaea.de as mentioned before). The same range queries were executed after some warmup and time was measured until TopDocs returned the first 10 documents.\n\nThe indexes each contain 13 numeric, tree encoded fields (doubles and Dates). Index size (including the \"normal\" fields) was:\n- 8bit: 4.8 GiB\n- 4bit: 5.1 GiB\n- 2bit: 5.7 GiB\n\nSo the addition 13*8*575,000 extra trie terms (with longer) size took 300 MB going from 8 to 4bit. Going from 4 to 2bit, the additional 13*16*575,000 extra terms took 600 MB (additional to the step from 8 to 4 bit, but its linear to the number of additional terms).\n\nThe performance impact was neglectible (or better the standard deviation was bigger than the performance plus), so I think still, 8bit is a good idea and index size is the smallest.\n\nMy idea, why this is so: Using fewer bits increases number of terms in index (I do not know how this decreases performance), needs more TermEnum seeks (for each trie precision, 2 seeking operations are needed). These term enum seeks are faster for 8bit trie varaint, because the 2 chars prefix can be used extensively (first prefix=precision, second char = first byte of numeric value). With 4bit and 2bit you get only 4bit or 2bits in addition to precision marker. So seeks in term enum are slower.\n\nIn comparison a ConstantScoreRangeQuery on the full precision trie-coded values took about 10-20 times longer. Here the numbers:\nFor PANGAEA, all queries are half open (we have the problem, that data sets have ittself bounding boxes - minlatitude, maxlatitude, minlongitude, maxlongitude) and the user searches for bounding boxes, hits are datasets that *intersect*  the search bounding box. So for a complete lat/lon constraint you have 4 half open ranges (with the current Trie implementation, this is not a problem, because two ANDed filters just withit half of the number of terms needed for a full range. The only performance impact is ANDing the two OpenBitSets). So 4 such half-open ranges ANDed together take the following times on the given index after some warmup, inkl fetching the first 10 documents:\n\nConstantScoreRange: 1500-4000 ms\nTrieRangeQuery 8bit: 40ms-100ms\nTrieRangeQuery 4bit: 30ms-80ms\nTrieRangeQuery 2bit: 20ms-80ms\n\nThe old RangeQuery was not tested, but that hits the MaxClauseCount constraint, and if this is raised to Integer.MAX_VALUE, it tooks approx 1 minute to finish). The numbers are pure rnage queries, if you additionally add some search terms, time goes up a little bit. Used was only TrieRangeQuery (so rewrite to constant score), no docs were filtered in the classical way: termqueries + filtering of results.\n\nMore benchmark results follow, when I finish the benchmarker. But these are real world examples. The search engine machine was a Sun X4600 machine with 16 opteron cores, 64bit Java 1.5, Sun Java System Web Server, -Xmx 8192M (our current configuration). On lower cost machnies like desktop pcs, the ConstantScoreRangeQuery takes much more time, but TrieRangeQuery is approx the same time, as disk io seeks is more the limiting factor. Processor speed and number of processors is not the limiting factor (if only one query is run in parallel).",
            "date": "2008-12-03T13:27:35.001+0000",
            "id": 35
        },
        {
            "author": "Uwe Schindler",
            "body": "Just one note: The queries in the comment before were taken on the same index with typical \"user queries\", the hit count and ranges were rather large (e.g. queries with about 120,000 total hits, so the typical case water around Africa, like Mike McCandless did in his first tests). They were not synthetic, done on the real running productive system (I only have two X4600 sun machines..., so tests are most time done on the life system, e.g. like a open-heart surgery).",
            "date": "2008-12-03T14:06:18.727+0000",
            "id": 36
        },
        {
            "author": "Paul Elschot",
            "body": "Uwe, thanks very much for the size and performance info. The space/time tradeoff between 8 and 4 bits is clear, and it's great to have that fexibility.",
            "date": "2008-12-03T17:26:04.571+0000",
            "id": 37
        },
        {
            "author": "Uwe Schindler",
            "body": "New Patch, I think this is really ready to commit. Includes Mike's suggestion to compare the result count of RangeQuery with TrieRangeQuery with random values. Also contains a further test on result count of ranges with an index containing values with distance=1. This is to detect errors, when the code generating the splitted range may fail to correctly attach the range parts to each other.\n\nDuring changing my own project panFMP to use the new contrib package, I needed a function to read stored, trie-encoded values for re-indexing with another trie variant. The new static methods in TrieUtils choose the variant for decoding using the encoded string length. These static methods can be used for easy decoding of stored fields that use the trie encoding without knowing the encoding. For range queries, the encoding cannot be autodetected (which is clear).\n\nFurther work maybe a optimized sort algorith for the trie encoded fields. Current FieldCache cannot handle them as longs, only as Strings which is memory intensive. Having a FieldCache implementation for longs that support this encoding would be good. I do not want to do this with the current unflexible FieldCache implementation, so I wait for LUCENE-831. Has anybody an idea, how to plugin the correct FieldCache impl for this encoding in current Lucene, respecting the TrieUtils variant?",
            "date": "2008-12-03T18:50:06.873+0000",
            "id": 38
        },
        {
            "author": "Michael McCandless",
            "body": "Those results are nice, thanks Uwe!  The open-heart surgery part was spooky though ;)\n\nbq. Index size (including the \"normal\" fields) was:\n\nAs a baseline, how large is an index with just the normal fields?\n\nI'll commit shortly.  This is a great addition!",
            "date": "2008-12-03T18:54:42.266+0000",
            "id": 39
        },
        {
            "author": "Uwe Schindler",
            "body": "I am sorry, one JavaDoc bug in the new static method, has a @link too much after @throws",
            "date": "2008-12-03T19:08:09.640+0000",
            "id": 40
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. As a baseline, how large is an index with just the normal fields?\n\nThis is not so simply to check out, I do not have such an index available and it takes long time to reindex :) The 3 variants were generated last night on the X4600 monster machine, which took 2 hours there (in parallel).\n\nBut as the step from 8 to 4 bits was about 300 MBytes space more and the step doubled the number of terms, the index without the numeric fields (if its still linear) should be about (!) 4.5 GiB.\n\nSometimes, when changing program code of \"live\" systems where users are working on (sometimes you cannot avoid it), I feel really like doing a open-heart surgery :-)",
            "date": "2008-12-03T19:20:26.693+0000",
            "id": 41
        },
        {
            "author": "Michael McCandless",
            "body": "bq. But as the step from 8 to 4 bits was about 300 MBytes space more and the step doubled the number of terms, the index without the numeric fields (if its still linear) should be about  4.5 GiB\n\nOK thanks!",
            "date": "2008-12-03T19:30:29.009+0000",
            "id": 42
        },
        {
            "author": "Michael McCandless",
            "body": "Committed revision 723031.\n\nThanks Uwe!",
            "date": "2008-12-03T19:39:26.843+0000",
            "id": 43
        },
        {
            "author": "Michael McCandless",
            "body": "Uwe, can't you use ExtendedFieldCache.getLongs, passing in your own LongParser?",
            "date": "2008-12-03T23:11:27.328+0000",
            "id": 44
        },
        {
            "author": "Uwe Schindler",
            "body": "The question is, how to plugin the whole FieldCache code from contrib? I see there is only one default ExtendedFieldCache named ExtendedFieldCacheImpl, but how do I implement a custom field cache and tell the searchers to use it whenever they see a Trie encoded field (e.g. when Sort.AUTO is used).? This is one thing, that I do not understand how to do.\n\nA very simple LongParser may be:\n{code}\n  new LongParser() {\n      public long parseLong(String value) {\n        return TrieUtils.VARIANT_8BIT.trieCodedToLong(value);\n      }\n  };\n{code}\n\nBut what to do with it for sorting search results...",
            "date": "2008-12-03T23:57:55.260+0000",
            "id": 45
        },
        {
            "author": "Michael McCandless",
            "body": "I think if you call ExtendedFieldCache.EXT_DEFAULT.getLongs, first, with your parser, and then sort by that field as a long, it will just use what's already populated into the cache?  (It is sort of messy though; ideally there would be some way to record the LongParser you'd like used for a given field).",
            "date": "2008-12-04T00:14:30.209+0000",
            "id": 46
        },
        {
            "author": "Uwe Schindler",
            "body": "This is messy and not easy to understand for a beginner. I was thinking of a possibility to supply a class in the contrib package o.a.l.search.trie that extends SortField or Sort or whatever else to mark a field explicitely as \"trie-encoded\" (which could also use the automatic trie variant detection in TrieUtils.trieCodedToLongAuto()), that can be used for calling Searcher.search(). And if TrieUtils would not be in Contrib, the AUTO parser could be extended to also try TrieUtils for deconding the field value (as it also throws NumberFormatException, it would fit in wonderful).\n\nAnother idea was to overwrite ExtendedFieldCacheImpl and set it in the (non-final) static member ExtendedFieldCache.EXT_DEFAULT (which would be possible), but ExtendedFieldCacheImpl is package-protected. The whole sort thing is a bit messy, this is why I was hoping that LUCENE-831 may fix this problem in future, maybe we move this discussion to this issue.\n\nFor the beginning, I could simply supply a static field parser instance in TrieUtils, e.g. LongParser named TrieUtils.TRIE_FIELD_CACHE_PARSER, that uses autodetection or the concrete trie variant.",
            "date": "2008-12-04T00:44:08.554+0000",
            "id": 47
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. I think if you call ExtendedFieldCache.EXT_DEFAULT.getLongs, first, with your parser, and then sort by that field as a long, it will just use what's already populated into the cache?\n\nI think, this cannot work. The Cache is keyed by FieldCacheImpl.Entry containing the parser to use. If search code would later call getLongs() with the default parser, it would get a new array of longs, not the one previously parsed with the specific parser, as there is no cache hit (because Entry is not equal). Do I miss something here?",
            "date": "2008-12-04T01:08:36.594+0000",
            "id": 48
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Mike,\nthe last Hudson build failed. It seems it did not like that I used \"LuceneTestCase\" instead of standard JUnit TestCase in one of the tests. I copied the source of this test from some other class. Maybe I cannot use LuceneTestCase in Contrib. But this is not a problem, it works identical with the JUNIT default class.",
            "date": "2008-12-04T07:45:17.625+0000",
            "id": 49
        },
        {
            "author": "Uwe Schindler",
            "body": "Sorry for again a new patch: When again looking into the test, I missed a test for the automatic encoding detection by string length (TrieUtils.trieCodedToXxxAuto()).\nThe appended patch fixes the hudson build and adds this test.",
            "date": "2008-12-04T08:30:28.509+0000",
            "id": 50
        },
        {
            "author": "Michael McCandless",
            "body": "Hmm -- I would prefer that contrib tests subclass LiaTestCase.  We must be missing a dependency in the ant build files.\n\nOK this seems to fix it:\n\n{code}\nIndex: contrib/contrib-build.xml\n===================================================================\n--- contrib/contrib-build.xml\t(revision 723145)\n+++ contrib/contrib-build.xml\t(working copy)\n@@ -61,7 +61,7 @@\n   </target>\n \n   \n-  <target name=\"init\" depends=\"common.init,build-lucene\"/>\n+  <target name=\"init\" depends=\"common.init,build-lucene,build-lucene-tests\"/>\n   <target name=\"compile-test\" depends=\"init\" if=\"contrib.has.tests\">\n     <antcall target=\"common.compile-test\" inheritRefs=\"true\" />\n   </target>\n{code}\n\nI'll commit that, and the fix to the test case.  Thanks Uwe!",
            "date": "2008-12-04T11:02:15.871+0000",
            "id": 51
        },
        {
            "author": "Michael McCandless",
            "body": "bq.  Hmm - I would prefer that contrib tests subclass LiaTestCase\n\nWoops, I meant LuceneTestCase ;)  Time sharing not working very well in my brain this morning...",
            "date": "2008-12-04T11:04:02.822+0000",
            "id": 52
        },
        {
            "author": "Michael McCandless",
            "body": "Committed revision 723287.",
            "date": "2008-12-04T11:07:43.952+0000",
            "id": 53
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I think, this cannot work. The Cache is keyed by FieldCacheImpl.Entry containing the parser to use.\n\nSigh, you are correct.  How would you fix FieldCache?\n\nI guess the workaround is to also index the original value (unencoded by TrieUtils) as an additional field, for sorting.",
            "date": "2008-12-04T11:55:03.861+0000",
            "id": 54
        },
        {
            "author": "Uwe Schindler",
            "body": "Thanks, then I would also change TestTrieRangeQuery to also use LuceneTestCase, just for completeness.\n\nbq. Sigh, you are correct. How would you fix FieldCache?\n\nI would fix FieldCache by giving in SortField the possibility to supply a parser instance. So you create a SortField using a new constructor SortField(String field, int type, Object parser, boolean reverse). The parser is \"object\" bcause all parsers have no super-interface. The ideal solution would be to have:\n\nSortField(String field, int type, FieldCache.Parser parser, boolean reverse)\n\nand FieldCache.Parser is a super-interface (just empty, more like a marker-interface) of all other parsers (like LongParser...)\n\nbq. I guess the workaround is to also index the original value (unencoded by TrieUtils) as an additional field, for sorting.\n\nThe problem with the extra field would be, that it works good for longs or doubles (with some extra work), but Dates still keep as String, or you use Date.getTime() as long. But this is not very elegant and needs more fields and terms. I prefer a clean solution.",
            "date": "2008-12-04T12:02:20.788+0000",
            "id": 55
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Thanks, then I would also change TestTrieRangeQuery to also use LuceneTestCase, just for completeness. \n\nOK done.\n\nbq. would fix FieldCache by giving in SortField the possibility to supply a parser instance. So you create a SortField using a new constructor SortField(String field, int type, Object parser, boolean reverse). The parser is \"object\" bcause all parsers have no super-interface.\n\nThis seems OK for now?  Can you open an issue?  Retro-fitting a super-interface would break back-compat for (admittedly very advanced) existing Parser instances external to Lucene, right?\n\nbq. but Dates still keep as String, or you use Date.getTime() as long\n\nYeah.  But if we open the new issue (to allow external FieldCache parsers to be used when sorting) then one could parse to long directly from a TrieUtil encoded Date field, right?",
            "date": "2008-12-04T13:23:21.720+0000",
            "id": 56
        },
        {
            "author": "Uwe Schindler",
            "body": "Yes, I will open an issue! Maybe I maybe create a first patch after looking into the problem.\n\nbq. This seems OK for now? Can you open an issue? Retro-fitting a super-interface would break back-compat for (admittedly very advanced) existing Parser instances external to Lucene, right?\n\nI am not sure, but I think its better to leave it as now. On the other hand, if we just have a \"marker\" super-interface, it should be backwards compatible, because the new super-interface is new and existing code would only use the existing interfaces. New methods are not added by the super interface, so code would be source and binary compatible (as it only references the existing interfaces). I think we had this discussion some time in the past in another issue (Fieldable???), but this was another problem.\n\nbq. Yeah. But if we open the new issue (to allow external FieldCache parsers to be used when sorting) then one could parse to long directly from a TrieUtil encoded Date field, right?\n\nCorrect. As soon as this works, I would simply add as \"extra bonus\" o.a.l.search.trie.TrieSortField, that automatically supplys a correct parser for easy usage. Date, Double and Long trie fields can always be sorted as longs without knowing the correct meaning (because the trie format was designed like so).\n\nCurrently my code would just sort the trie encoded fields using SortField.STRING, but this resource expensive (butI have no example currently running, as it was not needed for panFMP/PANGAEA and other projects).",
            "date": "2008-12-04T13:48:39.649+0000",
            "id": 57
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Mike,\n\nI opened issue LUCENE-1478 and attached a first patch.\n\nAbout the current issue: I have seen that TrieRangeQuery is missing in /lucene/java/trunk/contrib/queries/README.txt. Can you add it there or should I write a small patch? I think it should at least be mentioned there for what it is for, but the JavaDocs are much more informative and the corresponding paper / code credits are cited there.\n\nThank you very much for helping to get this into Lucene!",
            "date": "2008-12-04T23:19:25.138+0000",
            "id": 58
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Thank you very much for helping to get this into Lucene!\n\nYou're welcome!  But, that was the easy part ;)  Thank you for creating it & getting it into Lucene!\n\nbq. About the current issue: I have seen that TrieRangeQuery is missing in /lucene/java/trunk/contrib/queries/README.txt.\n\nI agree -- can you create a patch?  Thanks.",
            "date": "2008-12-05T00:50:09.626+0000",
            "id": 59
        },
        {
            "author": "Uwe Schindler",
            "body": "Here the readme changes.",
            "date": "2008-12-05T09:48:07.424+0000",
            "id": 60
        },
        {
            "author": "Michael McCandless",
            "body": "Committed revision 723701.\n\nThanks Uwe!",
            "date": "2008-12-05T10:17:18.903+0000",
            "id": 61
        },
        {
            "author": "Yonik Seeley",
            "body": "Attaching completely untested prototype TrueUtils.java\n\nsome discussion:\nhttp://www.lucidimagination.com/search/document/d62c0fd21d88f880\n\nFeatures:\n  - same encode/decode code works for any variant... no 2,4,8 bit specific instances\n  - decouples \"slicing\" of the value into different precisions and encoding of the slice to a String, allowing for the most efficient String encoding to be used for every prevision variant.\n  - 7 bit char encoding to optimize for UTF8 index storage\n  - right justified to allow lucene to prefix compress efficiently\n  - separates creation of sortableBits from trie encoding of those bits to avoid so many methods\n  - allows indexing into multiple fields, or all in the same field\n  - much smaller code should be much easier to understand\n  - left out \"Date\" support - the average Java developer understands how to go from a Date to a long (unlike double, etc).\n  - relatively trivial to add 32 bit (int/float) support and reuse code like addIndexedFields (which is just an agnostic helper method).\n",
            "date": "2009-02-07T17:25:50.861+0000",
            "id": 62
        },
        {
            "author": "Uwe Schindler",
            "body": "Thanks for the code fragments, I can take this as basis and will implement the TrieRangeFilter counterpart. I have some more ideas (e.g. for the beginners API there is missing the possibility to *store* the full-precision value). And norms should be disabled per default (in beginners API). 32bit support is also simple, as you noted.\n\nThe problem is now, how to make TrieRangeFilter so generic, to support all posible encodings and field names possible (with your code, it would also be possible to put each precision in a separate field). I work on that, I want to have clean API on the TrieRangeFilter!\n\nI would name both addIndexedFields() and addField with the same name (just overload), but different options. I prepare that!\n\nI agree, date support is not needed. And if I would again add Date support, Calendars should also be possible etc. No need for that - Date should be deprecated by Sun!\n\nOne question: The encoding is now different from NumberUtils again - NumberUtils tries to get the most out of each char vs. this tries to not affect UTF-8 encoding and use ASCII only? Would Solr use this encoding in future (7bit chars) for numeric values or should be both separate? These TrieUtils now also make it possible to not use trie coding, but encode doubles/longs for other use (and use the currently missing LongParser/SortField generators).",
            "date": "2009-02-07T18:19:48.149+0000",
            "id": 63
        },
        {
            "author": "Uwe Schindler",
            "body": "Adding that as comment:\n\n> On Sat, Feb 7, 2009 at 12:29 PM, Uwe Schindler <uwe@thetaphi.de> wrote:\n> > This is only a minimal optimization, suitable for very large indexes.\n> The\n> > problem is: if you have many terms in highest precission (a lot of\n> different\n> > double values), seeking is more costly if you jump from higher to lower\n> > precisions.\n> \n> That's my point... in very large indexes this should not result in any\n> difference at all on average because the terms would be no where near\n> each other.\n\nOK.\n\nI prepare a new TrieRangeFilter implementation, just taking the String[] fieldnames and the sortableLong and the precisionStep.\n\nAnd I think, you are right. We could completely remove the \"storing\" API. If one wants to add stored fields, he could use NumberUtils and do this separately (add stored, not indexed fields). For TrieRangeFilter it is not neded.\n\n> As an example: in a very big index, one wants to independently collect\n> all documents that match \"apple\" and all documents that match \"zebra\",\n> which term you seek to first should not matter.\n\nOK, I agree :)\n",
            "date": "2009-02-07T18:29:09.179+0000",
            "id": 64
        },
        {
            "author": "Uwe Schindler",
            "body": "Reopening this issue. I will later prepare a patch (have no time now, its now Saturday evening here in Germany), completely changing the current API and encoding, thanks Yonik! I think, the TrieRangeFilter will get smaller, too (ok, the c'tors may stay for beginners). The Javadocs need to be updated too.\n\nI will also add missing methods for LongParser and SortField, as the encoded fields can be stored in ExtendedFieldCache (but there as real Longs, not sortableLongs) - as before.\n\nI will also add 32 bit API.\n\nThanks, Yonik!",
            "date": "2009-02-07T18:35:02.715+0000",
            "id": 65
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. for the beginners API there is missing the possibility to store the full-precision value\n\nThey could simply store it in a different field, in whatever format they desire, right?  It seems like TrieRange should be about range matching, not the format of stored fields.\n\nbq. NumberUtils tries to get the most out of each char vs. this tries to not affect UTF-8 encoding and use ASCII only?\n\nNumberUtils in Solr was developed a *long* time ago, before Parser support in the FieldCache, etc (Lucene 1.4).  I chose 14 bit numbers to minimize size in FieldCache using a StringIndex, and because I didn't understand Lucene prefix compression at the time :-)\n\nIf there are to be many in-memory representations, then using 14 bit chars might be better.  Otherwise it seems like 7 bit might be preferable (better prefix compression, more predictable branches in the UTF8 encoder/decoder).  Of course it's a trivial switch, so perhaps we should just try and benchmark it when everything else is done.\n\nAs for TrieRangeFilter, I guess the most generic constructor would look like:\n{code}\nTrieRangeFilter(int precisionStep, String[] fields, long lowerSortableBits, long upperSortableBits, boolean includeLower, boolean includeUpper)\n{code}\n\n\n",
            "date": "2009-02-07T18:44:23.360+0000",
            "id": 66
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. NumberUtils in Solr was developed a long time ago, before Parser support in the FieldCache, etc (Lucene 1.4). I chose 14 bit numbers to minimize size in FieldCache using a StringIndex, and because I didn't understand Lucene prefix compression at the time \n\nThe same here :). By the way, the new SortField constructors taking a LongParser (LUCENE-1478) as parameter make this very simple. You could so sort by the trie encoded field (and do not need to index them separately). Just use the LongParser supplied by TrieUtils to sort. No String sorting needed. I only wanted to supply static parsers based on TrieUtils for that.",
            "date": "2009-02-07T18:52:08.619+0000",
            "id": 67
        },
        {
            "author": "Uwe Schindler",
            "body": "For TrieRangeFilter I now also need something similar to the current increment/decrementTrieCoded, that adds 1 to a prefix coded value (in principle, just add 1<<shift to the long an use it, something like that) This is needed for the matching of the parts of the range. More later.",
            "date": "2009-02-07T18:57:07.603+0000",
            "id": 68
        },
        {
            "author": "Uwe Schindler",
            "body": "I modified the proposal TrieUtils a little bit. Maybe this class could get the new NumberUtils with the extra option to trie encode the values.\n\n - Added 32bit support\n - Merged the unsigned int/long handling into prefixCode methods. For TrieRangeFilter the raw bits will not be needed (I need compareable signed ints/longs).\n - The conversion from doubles and floats was renamed and returns the standard signed long/int: doubleToSortableLong and floatToSortableInt, Date is removed (as just Date.getTime() can be used, as everybody knows).\n - Still missing are Long/IntParser for FieldCache and a SortField factory.\n\nIt's still untested!\n\nI will implement tomorrow (now its time to go to bed) the TrieRangeFilter in two variants (one for 32 bit ints another for 64 bit longs). The min/max values are the ints/longs. The trie coding is also done using the shift value and bit magic. The results of the range split are then encoded using TrieUtils.xxxToPrefixCode().",
            "date": "2009-02-07T23:32:18.528+0000",
            "id": 69
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. Merged the unsigned int/long handling into prefixCode methods. For TrieRangeFilter the raw bits will not be needed (I need compareable signed ints/longs).\n\nNice Uwe... we're really on the same wavelength now -  I came to the exact same conclusion and made the exact same changes!  It's much nicer having ints and longs as the exposed \"interface\"  so if a<b in java then a will come before b in the term index.",
            "date": "2009-02-08T14:30:08.084+0000",
            "id": 70
        },
        {
            "author": "Uwe Schindler",
            "body": "A fixed TrieUtils (a small bug in the encoding routine caused endless loop), the increment in array was missing :-)",
            "date": "2009-02-08T14:35:59.750+0000",
            "id": 71
        },
        {
            "author": "Uwe Schindler",
            "body": "A first test version of TrieRangeFilter. Much of the functionality iss missing (no hashCode, some dead parts,....). But: it works. I modified the tests and get it running. Both variants (old and new) visit the same number of terms and get exact results.\n\nTo do further testing it is now important, to generate Ranges with negative long. It seems to work, but I need more tests. It was a hell to get it running, I hate Sun for not having unsigned ints/longs in Java :-(\n\nTrieRangeQuery is obsolete now. You can wrap the Filter using a ConstantScore query or user TrieRangeFilter.asQuery() [which will return a nicer toString() output).\n\nI will now clean up everything and create a patch!",
            "date": "2009-02-08T14:39:56.505+0000",
            "id": 72
        },
        {
            "author": "Uwe Schindler",
            "body": "I forget to mention:\nI modified the TrieUtils to have another SHIFT_START for ints and longs. By this you can earlier throw a NumberFormatException of you try to decode long using the int method or vice versa. I added these checks, to fail early, when old indexes using the different encoding are used or sorting may use the wrong encoding. TrieUtils still needs the FieldCache parser, but this is a trivial addition.",
            "date": "2009-02-08T14:48:15.994+0000",
            "id": 73
        },
        {
            "author": "Uwe Schindler",
            "body": "Yonik:\nHow would you hande a TrieRangeFilter for int's? I could create a separate class that looks identical (but uses the other TrieUtils methods and int instead of longs), or combine both in one class (which is hard). The problem is, you cannot have the same codebase, because masks, shifts, 31 vs. 63 is everywhere. And the other datatype.\nIf I do it, I will have two classes: TrieRangeFilter32, TrieRangeFilter64, alternatively TrieRangeFilterInt, TrieRangeFilterLong. Whats your opinion?",
            "date": "2009-02-08T14:52:31.392+0000",
            "id": 74
        },
        {
            "author": "Uwe Schindler",
            "body": "Now the next step: A corrected TrieUtils again.\n\n - contains the FieldCache parsers for easy usage and sort field factories.\n - changes the SHIFT for ints (0x80 was too much, UTF-8 optimization needs chars <0x80)\n\nSorting tests now also work.",
            "date": "2009-02-08T15:15:30.075+0000",
            "id": 75
        },
        {
            "author": "Yonik Seeley",
            "body": "Uploading a new version of TrieUtils.\n\nI've implemented my own range splitting logic (it will be interesting to see how it compares to yours, and if mine actually works!  so many edge cases...)\n\nIt separates the range splitting logic from the execution of anything, so we can use the same logic to generate queries that match the same range (no building an OpenBitSet first), or other things like printable queries for debugging.\n\nAfter reflection, the RangeBuilder interface could be simplified to just have a single addRange() method... with the implicit assumption that all ranges are ORd together.",
            "date": "2009-02-08T15:37:24.976+0000",
            "id": 76
        },
        {
            "author": "Yonik Seeley",
            "body": "Since filters are a symbolic representation (like Query), it makes sense to have separate TrieRangeFilter32 and TrieRangeFilter64 classes.\nHopefully most of the logic that is in common can be shared somehow.",
            "date": "2009-02-08T15:41:55.466+0000",
            "id": 77
        },
        {
            "author": "Yonik Seeley",
            "body": "I *think* the range splitting code I wrote is generic enough to also automatically handle 32 bits too.\nI think it's just a matter of starting with a different shift?\n\nSo for longs we have:\n{code}\n  public static Object buildRange(RangeBuilder builder, long ll, long uu, int precisionStep) {\n    // figure out where to start shift at\n    int shift = ((64-1)/precisionStep) * precisionStep;\n    return buildRange(builder, ll, uu, shift, precisionStep);\n  }\n{code}\n\nAnd for ints, it would simply (hopefully) be:\n{code}\n  public static Object buildRange(RangeBuilder builder, int ll, int uu, int precisionStep) {\n    // figure out where to start shift at\n    int shift = ((32-1)/precisionStep) * precisionStep;\n    return buildRange(builder, (long)ll, (long)uu, shift, precisionStep);\n  }\n{code}\n\nDoes that sound right?\n",
            "date": "2009-02-08T15:46:06.049+0000",
            "id": 78
        },
        {
            "author": "Uwe Schindler",
            "body": "I am still \"reading\" your code, but the main parts look identical (also the break condition, if no inner range available -> this condition is very important and must be min>=max). There are only differences between both are the generation of the inner range bounds. I just rewrote my old code, did you do it completely from scratch?\n\nI would change RangeBuilder to be only a interface/abstract class with no Object return code that has a addRange method. The range will always be or'ed (anding makes no sense). The same idea came me, when I tried to unify the 32bit and 64 bit variant in my TrieRangeFilter code.\n\nFor performance reasons, it is better to use the same TermEnum and TermDocs and only one OpenBitSet when executing the range. But this can be easily handled using the interface (RangeBuilder initializes the Openbitset, TermDocs and TermEnums, like in my code. When the range was built, the OpenBitset is retrieved).",
            "date": "2009-02-08T16:04:33.864+0000",
            "id": 79
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. did you do it completely from scratch? \n\nYep, took me a while...  An interesting exercise and good news if it's so similar to yours.  Hard code like this is actually fun too :-)\n\nbq. I would change RangeBuilder to be only a interface/abstract class with no Object return code that has a addRange method.\n\nSounds fine - that means that RangeBuilder instances will always need to be stateful, but that seems flexible enough and simpler to understand than passing around Objects and casting.\n\n",
            "date": "2009-02-08T16:17:31.389+0000",
            "id": 80
        },
        {
            "author": "Uwe Schindler",
            "body": "I like your idea, I will also use a generic Range splitter (as described before), but use my algorithm for it. But I am really interested how both compare. You can test this, if you use mine and enable the System.out in setBits (which is identical to your addRange()), that returns hexadecimals for each range.\n\nFor nicer code, I would supply two interface to build ranges (int, long), because the TrieUtils always to correctly use int/long and the user may be confused if he gets longs (even if the api is very special for experts). But it can e.g. be used to create a BooleanQuery using classic RangeQueries or'ed.",
            "date": "2009-02-08T16:19:55.249+0000",
            "id": 81
        },
        {
            "author": "Uwe Schindler",
            "body": "There may be one issue with signedness in your's and mine code for the special case of a precisionStep of 1. In this case the outermost/innermost calculation of the range bounds can fail, but I am not sure. I wanted to write a test for it.\n\nI am currently testing only 8, 4, 2 - as the results for that *must* be identical to my old code (I also printed a lot of System.outs in my old code to generate a exact match of everything). There are some special cases in range splitting that are very sensitive. It took me also very long time to reimplement it. But the usage of longs/ints is really nicer and cleaner than incrementing/decrementing string values, as I did before.\n\nIt is now also possible to use a completely other encoding, without changing the range splitting (e.g. to compare 7bit chars to 13/14 bit chars from NumberUtils).",
            "date": "2009-02-08T16:27:17.146+0000",
            "id": 82
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Yonik,\n\nI implemented the generic interface. I changed it a little bit (it throws now Exception, because without that the building of a range then needs wrapping the IOExceptions of IndexReader with a RuntimeException, not so nice). There are now 2 times the same implementation for 32 and 64 bit and two interfaces (long, int).\n\nThe TrieRangeFilter is now a implementation using the LongRangeBuilder. The test, I used, is also there (just replace the files in contrib). The TrieUtilsTest is currently not working, TrieRangeQuery is obsolete.\n\nI think I will create a helper, that does the TermEnum/TermDocs iteration and reuse it in both LongTrieRangeFilter and IntTrieRangeFilter (maybe they get both the same superclass containing important methods useable for both).",
            "date": "2009-02-08T17:19:36.274+0000",
            "id": 83
        },
        {
            "author": "Yonik Seeley",
            "body": "Oh I see.... one difference between our code is that you start at the lowest shift and I start at the highest.  Counting up has a nice effect of getting rid of the calculation of what shift to start at... I just had a harder time thinking about the recursion in that direction.\n\nAnyway, it's all looking great!\n\nDo we have test code that tests that the most efficient precision is used (as opposed to just the right bits matching?  i.e. for a precisionStep of 4\n0x300-0x4ff could be matched with 3-4 with a shift of 8, or 30-4f with a shift of 4, or 300-4ff with a shift of 0.",
            "date": "2009-02-09T15:09:26.916+0000",
            "id": 84
        },
        {
            "author": "Uwe Schindler",
            "body": "I was preparing the whole day the final version, including all javadoc, but then i overwrote the wron file and the whole work of today (according trie) is away. Give me one more day, and I will redo everything. I changed since yesterday a lot in the trie code. Your code was a little bit better when the range bound of one precision was exact on the range's start or end (in this case the precision could be left out, in your code the boolean needUpper and needLower). I implemented this similar.\n\nI also extended the interface a little bit, but this is work I have to redo. So it takes now longer. Most work is writing documentation and javadocs. If everything had worked ok (and I did not overwrite/update svn in the wrong way, I would be finished now :(\n\nbq. Do we have test code that tests that the most efficient precision is used (as opposed to just the right bits matching? i.e. for a precisionStep of 4\n0x300-0x4ff could be matched with 3-4 with a shift of 8, or 30-4f with a shift of 4, or 300-4ff with a shift of 0.\n\nThe most efficent precision is sometimes hard, but the optimization above with needUpper/needLower is really good sometimes (depends on the range). I think about it.\n\nUwe",
            "date": "2009-02-09T16:14:40.297+0000",
            "id": 85
        },
        {
            "author": "Uwe Schindler",
            "body": "{quote}\nDo we have test code that tests that the most efficient precision is used (as opposed to just the right bits matching? i.e. for a precisionStep of 4\n0x300-0x4ff could be matched with 3-4 with a shift of 8, or 30-4f with a shift of 4, or 300-4ff with a shift of 0.\n{quote}\n\nI misunderstood your note. My last optimizations (now they are again restored in my svn) exactly handle this. There are two cases:\n - if the left range of a precision has ((min & mask) == 0L) [starts exactly at the beginning of the next more unprecise range], I leave out the left range for the actual precision and directly use the next lower prec\n - if the right range of a precision has ((max & mask) == mask) [ends exactly at the end of the next more unprecise range], I do the same for the right one.\n\nMy new code is now cleaner and easier to understand (there were some other unneeded extra shifts/ands in it). I also merged the 64 bit and 32 bit range splitting and wrap the RangeBuilder classes accordingly (now abstract with two different range collecting possibilities).\n\nThe old code was not aware of this, leading to sometimes left/right precisions that are not needed. I will add test code in TestTrieUtils, that tests the range split (without an index). The problem here is, how to test this effectively. I could generate some examples and test for the resulting range bounds using a custom XxxRangeBuilder in the test, that collects the ranges into a List and compares this list). Do you have another prossibility to test this without a lot of manually checks example ranges?\n\nI have now restored all my changes (and did an extra backup). I will now write again the new Javadocs of TrieUtils and package.html. After that I will post a patch with the final API. The extra and new tests will be added later. First I want to fixate and document  the API.",
            "date": "2009-02-09T20:05:50.873+0000",
            "id": 86
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. I could generate some examples and test for the resulting range bounds using a custom XxxRangeBuilder in the test, that collects the ranges into a List and compares this list).\n\n+1\n\nI think that, in conjunction with some random testing should be sufficient.\n",
            "date": "2009-02-09T21:28:23.017+0000",
            "id": 87
        },
        {
            "author": "Uwe Schindler",
            "body": "One of the trie filter test also checks, that the range is completely matched (when using a index with incrementing values). An additional test in TestTrieUtils can test for not having overlapping ranges. This can easily be tested using a OpenBitSet in which a RangeBuilder sets the bits. If it hits a bit two times, fails the test. I wrote this down here, to not forget it later, when writing this test.",
            "date": "2009-02-09T21:47:26.305+0000",
            "id": 88
        },
        {
            "author": "Uwe Schindler",
            "body": "This is the first patch for the revamp of TrieRange. It contains the complete API, a complete API documentation and some tests.\n\nStill missing is:\n - TestIntTrieRangeFilter\n - TestTrieUtils (the old one is still there but commented out)\n - Tests as described in previous comments (in TestTrieUtils): No range overlap, most optimized rangeSplit\n\nYonik: Can you look over it and say, if this is, what you would like to have for full flexibility and Solr (which needs this full flexibility)?\n\nAll others: Is something missing in the API (like shortcuts), any comments?\n\nIf everything is OK, I will commit the patch after adding the important tests.",
            "date": "2009-02-09T23:49:50.475+0000",
            "id": 89
        },
        {
            "author": "Uwe Schindler",
            "body": "Typos and wrong english in the docs can be fixed after the commit, merging of additional patches is simplier without all those small changes. The important things are API and functionality.\n\nUwe",
            "date": "2009-02-09T23:54:00.355+0000",
            "id": 90
        },
        {
            "author": "Uwe Schindler",
            "body": "This is (hopefully) my last patch. It contains all tests and the final API (with some modifications).\n\nThe split range test is a bit ugly, but it just test, if the algorithm works exactly like it should (but currently accepts no other order of addRange() calls when splitting the range) - and it tests only one \"reference\" range.\n\nPlease give some comments, Yonik!",
            "date": "2009-02-11T00:05:43.862+0000",
            "id": 91
        },
        {
            "author": "Yonik Seeley",
            "body": "Looking forward to the latest incarnation Uwe,  but I'm traveling through the rest of the week... I'll definitely check it out at some point, but I liked the previous ones so you should go ahead and commit if you feel it's ready.",
            "date": "2009-02-11T13:14:43.121+0000",
            "id": 92
        },
        {
            "author": "Uwe Schindler",
            "body": "Thanks for the answer,\n\nMy time is limited at the moment, too. I will commit Friday or the weekend!\n\nUwe",
            "date": "2009-02-11T13:34:34.280+0000",
            "id": 93
        },
        {
            "author": "Uwe Schindler",
            "body": "New patch with:\n - some optimizations in the splitRange (now it works even better) and more understandable again\n - split range now also works correct for precisionStep=1\n - new tests for splitRange (special cases are also tested, e.g. Long.MIN_VALUE..Long.MAX_VALUE can easily be done with only one range on the lowest precision)\n\nI will commit soon.",
            "date": "2009-02-13T17:15:40.629+0000",
            "id": 94
        },
        {
            "author": "Uwe Schindler",
            "body": "Committed revision #744207",
            "date": "2009-02-13T18:29:15.292+0000",
            "id": 95
        },
        {
            "author": "Uwe Schindler",
            "body": "I forgot: Thanks Yonik for the good ideas and discussions about API and help with coding this new trie implementation!",
            "date": "2009-02-13T22:40:58.875+0000",
            "id": 96
        },
        {
            "author": "Yonik Seeley",
            "body": "OK, I got a chance to further check things out and do some manual testing to ensure that the most efficient forms are always used.  Everything looks good!",
            "date": "2009-02-15T19:17:09.480+0000",
            "id": 97
        },
        {
            "author": "Uwe Schindler",
            "body": "Cool. So you like the new code? Are you happy with the API and how it works, is it good to use in Solr? If yes, it would be great and I am happy, that it is of use :-)\n\nThere seem to be no problems, I converted my own code to use the new TrieRange API and I like it. No problems. 8bit precisionStep works good for me, my index with 13 numeric trie fields and 600,000 docs works fine, no performance differences, queries run amazingly fast. Index size is almost identical.\n\nI hope I will get some synthetic performance the next days, do you have some code for the performance contrib to check performance (I am not so familar with the performance code, I will check it out).\n\nUwe",
            "date": "2009-02-15T21:42:46.293+0000",
            "id": 98
        },
        {
            "author": "Ning Li",
            "body": "Good stuff!\n\nIs it worth to also have an option to specify the number of precisions to index a value?\n\nWith a large precision step (say 8), a value is indexed in fewer terms (8) but the number of terms for a range can be large. With a small precision step (say 2), the number of terms for a range is smaller but a value is indexed in more terms (32). With precision step 2 and number of precisions set to 24, the number of terms for a range is still quite small but a value is indexed in 24 terms instead of 32. For applications usually query small ranges, the number of precisions can be further reduced.\n\nWe can provide more options to make things more flexible. But we probably want a balance of flexibility vs. the complexity of user options. Does this number of precisions look like a good one?",
            "date": "2009-02-16T16:06:04.748+0000",
            "id": 99
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Ning,\nthanks for suggesting. I was thinking abou that, too. In general an idea, would be to use 32 bit integers or floats, if you do not need  that much accuracy. In this case, the number of terms is reduced, too.\nBut it may be a good option, to specify a option, that values are indexed with the most possible precision and additionally indexed with lower precision values, too. But The precision step may be dynamic, like:\na) precision step gets bigger for lower precisions\nb) after a precision of XXbits no mor lower precisions are generated and queried. This may be possible to implement by e.g. an array of precision step values that give the splitting of the whole long/int into different precisions (like 2-2-2-2-8-8-8-8-8-16, so precisie values use 2 bit precision step, e.g. from shift 0 to 2, but from shift 48 to 64 a step value of 16 is used).\n\nUwe",
            "date": "2009-02-16T16:44:21.491+0000",
            "id": 100
        },
        {
            "author": "Ning Li",
            "body": "Hi Uwe,\n\nI had something similar in mind when I said we can \"make things more flexible\". Do you think it'll be too complex for users to specify? On the other hand, this is for experts so let experts have all the flexibility. :) We can open a different JIRA issue if we decide to go for it.",
            "date": "2009-02-16T20:12:48.760+0000",
            "id": 101
        },
        {
            "author": "Uwe Schindler",
            "body": "In principle it could be in the same way like the field names in the API: An array of precision step values as parameter where now only a single precisionStep is used. A shortcut would be the current API, that internally passes a one-item-array with the only precisionStep (if the array is shorter, the same logic with Math.min like on the field array). The simple-user API has only (like the current), one fieldname and one precision step, the full feaured api has an array of field names for each step and an array of precision step values.\nBut the problem with all this is, that the api gets complexer and complexer, so the simple shortcuts should also be provided and should be recommended.",
            "date": "2009-02-16T21:15:39.501+0000",
            "id": 102
        },
        {
            "author": "Ning Li",
            "body": "Agree. Do you want to open a new issue? If you want, I can take a crack at it, but probably sometime next week.",
            "date": "2009-02-17T15:55:46.895+0000",
            "id": 103
        },
        {
            "author": "Uwe Schindler",
            "body": "A new issue would be good, can you open one? The idea for the patch is almost finished, I can attach a patch shortly. There are some minor things to solve and think about, but its not a big thing.",
            "date": "2009-02-17T18:10:18.417+0000",
            "id": 104
        },
        {
            "author": "Uwe Schindler",
            "body": "Removed the splitRange recursion and replaced by a simple loop. Committed rev #745533",
            "date": "2009-02-18T15:15:04.279+0000",
            "id": 105
        },
        {
            "author": "Uwe Schindler",
            "body": "A change for a more universal RangeBuilder API (as preparation for LUCENE-1541). This patch also included the last commit that removes the recursion (for completeness).",
            "date": "2009-02-22T18:15:49.744+0000",
            "id": 106
        },
        {
            "author": "Uwe Schindler",
            "body": "Committed revision 746790.",
            "date": "2009-02-22T18:18:42.816+0000",
            "id": 107
        }
    ],
    "component": "modules/other",
    "description": "According to the thread in java-dev (http://www.gossamer-threads.com/lists/lucene/java-dev/67807 and http://www.gossamer-threads.com/lists/lucene/java-dev/67839), I want to include my fast numerical range query implementation into lucene contrib-queries.\n\nI implemented (based on RangeFilter) another approach for faster\nRangeQueries, based on longs stored in index in a special format.\n\nThe idea behind this is to store the longs in different precision in index\nand partition the query range in such a way, that the outer boundaries are\nsearch using terms from the highest precision, but the center of the search\nRange with lower precision. The implementation stores the longs in 8\ndifferent precisions (using a class called TrieUtils). It also has support\nfor Doubles, using the IEEE 754 floating-point \"double format\" bit layout\nwith some bit mappings to make them binary sortable. The approach is used in\nrather big indexes, query times are even on low performance desktop\ncomputers <<100 ms (!) for very big ranges on indexes with 500000 docs.\n\nI called this RangeQuery variant and format \"TrieRangeRange\" query because\nthe idea looks like the well-known Trie structures (but it is not identical\nto real tries, but algorithms are related to it).\n\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1470",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Add TrieRangeFilter to contrib",
    "systemSpecification": true,
    "version": "2.4"
}