{
    "comments": [
        {
            "author": "Grant Ingersoll",
            "body": "First draft at a patch, provides two different approaches:\n\n1.  CachedAnalyzer and CachedTokenizer take in a list of Tokens and output them as appropriate.  Similar to CachingTokenFilter, but assumes you already have the Tokens\n\n2. In contrib/analyzers/buffered, add CollaboratingAnalyzer and related classes for creating a Analyzer, etc. that work in the stream.\n\nStill not sure if and how this plays with the Token reuse (I think it doesn't)",
            "date": "2007-11-19T13:52:55.215+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "I think the discussion in LUCENE-1063 is relevant to this issue: if you store (& re-use) Tokens you may need to return a copy of the Token from the next() method to ensure that nay filters that alter the Token don't mess up your private copy.",
            "date": "2007-11-20T22:10:19.039+0000",
            "id": 1
        },
        {
            "author": "Grant Ingersoll",
            "body": "Some javadoc comments for the modifyToken method in BufferingTokenFilter should be sufficient, right?  Something to the effect that if this TokenFilter is not the last in the chain that it should make a full copy.  \n\nAs for the CachedTokenizer and CachedAnalyzer, those should be implied, since the user is passing them in to begin with.\n\nThe other thing of interest, is that calling Analyzer.tokenStream(String, Reader) is not needed.  In fact, this somewhat suggests having a new Fieldable property akin to tokenStreamValue(), etc. that says don't even ask the Fieldable for a value.  \n\nLet me take a crack at what that means and post a patch.  It will mean some changes to invertField() in DocumentsWriter and possibly changing it to not require that one of tokenStreamValue, readerValue() or stringValue() be defined.  Not sure if that is a good idea or not.  \n\n",
            "date": "2007-11-21T02:21:15.123+0000",
            "id": 2
        },
        {
            "author": "Grant Ingersoll",
            "body": "Here's a patch that modifies the DocumentsWriter to not throw an IllegalArgumentException if no Reader is specified.  Thus, an Analyzer needs to be able to handle a null Reader (this still needs to be documented).  Basically, the semantics of it are that the Analyzer is producing Tokens from some other means.  I probably should spell this out in a new Field constructor as well, but this should suffice for now, and I will revisit it after the break.\n\n I also added in a TestCollaboratingAnalyzer.  All tests pass.",
            "date": "2007-11-21T02:51:51.059+0000",
            "id": 3
        },
        {
            "author": "Grant Ingersoll",
            "body": "A new version of this with the following changes/additions:\n\nDocumentsWriter no longer requires that a Field have a value (i.e. stringValue, etc.)  Added a new Field constructor that allows for the construction of a Field without a value.  This would allow for Analyzer implementations that produce their own tokens (whatever that means)\n\nMoved CollaboratingAnalyzer, et. al to the core under analysis.buffered as I thought these items should be in core given the changes to Field and DocsWriter.\n\nNote, I think this is a subtle, but important change in DocumentsWriter/Field behavior.",
            "date": "2007-11-27T01:57:26.300+0000",
            "id": 4
        },
        {
            "author": "Grant Ingersoll",
            "body": "Added some more documentation, plus a test showing it is bad to use the no value Field constructor w/o support from the Analyzer to produce tokens.\n\nIf no objections, I will commit on Thursday or Friday of this week.",
            "date": "2007-11-27T14:10:51.906+0000",
            "id": 5
        },
        {
            "author": "Grant Ingersoll",
            "body": "fixed a failing test",
            "date": "2007-11-27T17:30:21.466+0000",
            "id": 6
        },
        {
            "author": "Michael Busch",
            "body": "Grant,\n\nI'm not sure why we need this patch.\n\nFor the testcase that you're describing:\n{quote}\nFor example, if you want to have two fields, one lowercased and one not, but all the other analysis is the same, then you could save off the tokens to be output for a different field.\n{quote}\n\ncan't you simply do something like this:\n{code:java}\nDocument d = new Document();\nTokenStream t1 = new CachingTokenFilter(new WhitespaceTokenizer(reader));\nTokenStream t2 = new LowerCaseFilter(t1);\nd.add(new Field(\"f1\", t1));\nd.add(new Field(\"f2\", t2));\n{code}\n\nMaybe I'm missing something?\n",
            "date": "2007-11-27T18:48:56.933+0000",
            "id": 7
        },
        {
            "author": "Yonik Seeley",
            "body": "Maybe I'm not looking at it the right way yet, but I'm not sure this feels \"right\"...\nSince Field has a tokenStreamValue(), wouldn't it be easiest to just use that?\nIf the tokens of two fields are related, one could just pre-analyze those fields and set the token streams appropriately.  Seems more flexible and keeps any convoluted cross-field logic in the application domain.",
            "date": "2007-11-27T18:58:23.346+0000",
            "id": 8
        },
        {
            "author": "Grant Ingersoll",
            "body": "{quote}\nMaybe I'm missing something?\n{quote}\n\nNo, I don't think you are missing anything in that use case, it's just an example of its use.  And I am not totally sold on this approach, but mostly am :-) \n\nI had originally considered your option, but didn't feel it was satisfactory for the case where you are extracting things like proper nouns or maybe it is generating a category value.  The more general case is where not all the tokens are needed (in fact, very few are).  In those cases, you have to go back through the whole list of cached tokens in order to extract the ones you want.  In fact, thinking some more of on it, I am not sure my patch goes far enough in the sense that what if you want it to buffer in mid stream.  \n\nFor example, if you had:\nStandardTokenizer\nProper Noun TF\nLowerCaseTF\nStopTF\n\nand Proper Noun TF is solely responsible for setting aside proper nouns as it comes across them in the stream.\n\nAs for the convoluted cross-field logic, I don't think it is all that convoluted.  There are only two fields and the implementing Analyzer takes care of all of it.  Only real requirement the application has is that the fields be ordered correctly.  \n\nI do agree somewhat about the pre-analysis approach, except for the case where there may be a large number of tokens in the source field, in which case, you are holding them around in memory (maxFieldLength mitigates to some extent.)  Also, it puts the onus on the app. writer to do it, when it could be pretty straight forward for Lucene to do it w/o it's usual analysis pipeline.\n\nAt any rate, separate of the CollaboratingAnalyzer, I do think the CachedTokenFilter is useful, especially in supporting the pre-analysis approach.\n\n",
            "date": "2007-11-27T20:31:07.523+0000",
            "id": 9
        },
        {
            "author": "Yonik Seeley",
            "body": "{quote}As for the convoluted cross-field logic, I don't think it is all that convoluted.{quote}\n\nBut it's baked into CollaboratingAnalyzer... it seems like this is better left to the user.  What if they wanted 3 fields instead of two?\n\n{quote}\nI do agree somewhat about the pre-analysis approach, except for the case where there may be a large number of tokens in the source field, in which case, you are holding them around in memory\n{quote}\nIsn't this what your current code does?\n",
            "date": "2007-11-27T20:43:05.719+0000",
            "id": 10
        },
        {
            "author": "Grant Ingersoll",
            "body": "{quote}\nWhat if they wanted 3 fields instead of two?\n{quote}\nTrue.  I'll have to think about a more generic approach.  In some sense, I think 2 is often sufficient, but you are right it isn't totally generic in the spirit of Lucene.  \n\nTo some extent, I was thinking that this could help optimize Solr's copyField mechanism.  In Solr's case, I think you often have copy fields that have marginal differences in the filters that are applied.  It would be useful for Solr to be able to optimize these so that it doesn't have to go through the whole analysis chain again.\n\n{quote}\nIsn't this what your current code does?\n{quote}\nNo, in my main use case (# of buffered tokens is << # of source tokens) the only tokens kept around is the (much) smaller subset of buffered tokens.  In the pre-analysis approach you have to keep the source field tokens and the buffered tokens.  Not to mention that you are increasing the work by having to iterate over the cached tokens in the list in Lucene.  Thus, you have the cost of the analysis in your application plus the storage of both token lists (one large, one small, likely) then in Lucene you have the cost of iterating over two lists.  In my approach, I think, you have the cost of analysis plus the cost of storage of one list of tokens (small) and the cost of iterating that list.",
            "date": "2007-11-27T21:03:45.637+0000",
            "id": 11
        },
        {
            "author": "Yonik Seeley",
            "body": "{quote}To some extent, I was thinking that this could help optimize Solr's copyField mechanism.{quote}\nMaybe... it would take quite a bit of work to automate it though I think.\n\nAs far as pre-analysis costs. iteration is pretty much free in comparison to everything else.  Memory is the big factor.\n\nThings like entity extraction are normally not done by lucene analyzers AFAIK... but if one wanted a framework to do that, the problem is more generic.  Your really want to be able to add to multiple fields from multiple other fields.\n",
            "date": "2007-11-27T21:13:06.966+0000",
            "id": 12
        },
        {
            "author": "Grant Ingersoll",
            "body": "Any objection to me committing the CachedAnalyzer and CachedTokenizer pieces of this patch, as I don't think they are effected by the other parts of this and they solve the pre-analysis portion of this discussion.  In the meantime, I will think some more about the generic field case, as I do think it is useful.  I am also trying out some basic benchmarking on this.\n\n{quote}\nThings like entity extraction are normally not done by lucene analyzers AFAIK\n{quote}\n\nConsider yourself in the \"know\" now, as I have done this on a few occasions, but, yes, I do agree a one to many approach is probably better if it can be done in a generic way.",
            "date": "2007-11-27T22:11:53.519+0000",
            "id": 13
        },
        {
            "author": "Yonik Seeley",
            "body": "I dunno... it feels like we should have the right generic solution (many-to-many) before committing anything in this case, simply because this is all user-level code (the absence of this patch doesn't prohibit the user from doing anything... no package protected access rights are needed, etc).\n",
            "date": "2007-11-27T22:27:13.937+0000",
            "id": 14
        },
        {
            "author": "Michael Busch",
            "body": "I think the ideas here make sense, e. g. to have a buffering\nTokenFilter that doesn't buffer all tokens but enables the \nuser to control which tokens to buffer. \n\nWhat is still not clear to me is why we have to introduce a\nnew API for this and a new kind of analyzer? To allow creating\nan no-value field seems strange. Can't we achieve all this\nby using the Field(String, TokenStream) API without the\nanalyzer indirection?\n\nThe javadocs should make clear that the IndexWriter processes\nfields in the same order the user added them. So if a user \nadds TokenStream ts1 and thereafter ts2, they can be sure \nthat ts1 is processed first. With that knowledge ts1 can\nbuffer certain tokens that ts2 uses then. Adding even more\nfields that use the same tokens is straightforward.",
            "date": "2007-11-27T22:31:00.908+0000",
            "id": 15
        },
        {
            "author": "Grant Ingersoll",
            "body": "OK, I am trying not be fixated on the Analyzer.   I guess I haven't fully synthesized the new TokenStream use in DocsWriter\n\nI agree, I don't like the no-value Field, and am open to suggestions.\n\nSo, I guess I am going to push back and ask, how would you solve the case of where you have two fields and the Analysis given by:\nsource field:\nStandardTokenizer\nProper Noun TF\nLowerCaseTF\nStopTF\n\nbuffered1 Field:\nProper Noun Cache TF  (cache of all terms found to be proper nouns by the Proper Noun TF)\n\nbuffered2 Field:\nAll terms lower cased\n\nAnd the requirement is that you only do the Analysis phase once (i.e. for the source field) and the other two fields are from memory.\n\nI am just not seeing it yet, so I appreciate the explanation as it will better cement my understanding of the new Token Stream stuff and DocsWriter\n\n",
            "date": "2007-11-27T22:56:37.404+0000",
            "id": 16
        },
        {
            "author": "Michael Busch",
            "body": "We need to change the CachingTokenFilter a bit (untested code):\n\n{code:java}\npublic class CachingTokenFilter extends TokenFilter {\n  private List cache;\n  private Iterator iterator;\n  \n  public CachingTokenFilter(TokenStream input) {\n    super(input);\n    this.cache = new LinkedList();\n  }\n  \n  public Token next() throws IOException {\n    if (iterator != null) {\n      if (!iterator.hasNext()) {\n        // the cache is exhausted, return null\n        return null;\n      }   \n      return (Token) iterator.next();\n    } else {\n      Token token = input.next();\n      addTokenToCache(token);\n      return token;\n    }\n  }\n  \n  public void reset() throws IOException {\n    if(cache != null) {\n    \titerator = cache.iterator();\n    }\n  }\n  \n  protected void addTokenToCache(Token token) {\n    if (token != null) {\n      cache.add(token);\n    }\n  }\n}\n{code}\n\nThen you can implement the ProperNounTF:\n\n{code:java}\nclass ProperNounTF extends CachingTokenFilter {\n  protected void addTokenToCache(Token token) {\n    if (token != null && isProperNoun(token)) {\n      cache.add(token);\n    }\n  }\n  \n  private boolean isProperNoun() {...}\n}  \n{code}\n\nAnd then you add everything to Document:\n\n{code:java}\nDocument d = new Document();\nTokenStream properNounTf = new ProperNounTF(new StandardTokenizer(reader));\nTokenStream stdTf = new CachingTokenFilter(new StopTokenFilter(properNounTf));\nTokenStrean lowerCaseTf = new LowerCaseTokenFilter(stdTf);\n\n\nd.add(new Field(\"std\", stdTf));\nd.add(new Field(\"nouns\", properNounTf));\nd.add(new Field(\"lowerCase\", lowerCaseTf));\n{code}\n\nAgain, this is untested, but I believe should work? ",
            "date": "2007-11-27T23:50:55.934+0000",
            "id": 17
        },
        {
            "author": "Yonik Seeley",
            "body": "Very similar to what I came up with I think... (all untested, etc)\n\n{code}\nclass ListTokenizer extends Tokenizer {\n  protected List<Token> lst = new ArrayList<Token>();\n  protected Iterator<Token> iter;\n\n  public ListTokenizer(List<Token> input) {\n    this.lst = input;\n    if (this.lst==null) this.lst = new ArrayList<Token>();\n  }\n\n  /** only valid if tokens have not been consumed,\n   * i.e. if this tokenizer is not part of another tokenstream\n   */\n  public List<Token> getTokens() {\n    return lst;\n  }\n\n  public Token next(Token result) throws IOException {\n    if (iter==null) iter = lst.iterator();\n    return iter.next();\n  }\n\n  /** Override this method to cache only certain tokens, or new tokens based\n   * on the old tokens.\n   */\n  public void add(Token t) {\n    if (t==null) return;\n    lst.add((Token)t.clone());\n  }\n\n  public void reset() throws IOException {\n    iter = lst.iterator();\n  }\n}\n\nclass TeeTokenFilter extends TokenFilter {\n  ListTokenizer sink;\n\n  protected TeeTokenFilter(TokenStream input, ListTokenizer sink) {\n    super(input);\n    this.sink = sink;\n  }\n\n  public Token next(Token result) throws IOException {\n    Token t = input.next(result);\n    sink.add(t);\n    return t;\n  }\n}\n{code}",
            "date": "2007-11-28T00:07:51.879+0000",
            "id": 18
        },
        {
            "author": "Yonik Seeley",
            "body": "I think having the \"tee\" solves the many-to-many case... you can have many fields contribute tokens to a new field.\n\n{code}\nListTokenizer sink1 = new ListTokenizer(null);\nListTokenizer sink2 = new ListTokenizer(null);\n\nTokenStream source1 = new TeeTokenFilter(new TeeTokenFilter(new WhitespaceTokenizer(reader1), sink1), sink2);\nTokenStream source2 = new TeeTokenFilter(new TeeTokenFilter(new WhitespaceTokenizer(reader2), sink1), sink2);    \n\n// now sink1 and sink2 will both get tokens from both reader1 and reader2 after whitespace tokenizer\n// now we can further wrap any of these in extra analysis, and more \"tees\" can be inserted if desired.\n\nTokenStream final1 = new LowerCaseFilter(source1);\nTokenStream final2 = source2;\nTokenStream final3 = new EntityDetect(sink1);\nTokenStream final4 = new URLDetect(sink2);\n    \nd.add(new Field(\"f1\", final1));\nd.add(new Field(\"f2\", final2));\nd.add(new Field(\"f3\", final3));\nd.add(new Field(\"f4\", final4));\n{code}",
            "date": "2007-11-28T00:31:14.524+0000",
            "id": 19
        },
        {
            "author": "Michael Busch",
            "body": "I like the TeeTokenFilter! +1",
            "date": "2007-11-28T00:44:28.487+0000",
            "id": 20
        },
        {
            "author": "Grant Ingersoll",
            "body": "OK, looks good to me and is much simpler.  Only thing that gets complicated is the constructors, but that should be manageable.  Thanks for bearing w/ me :-)\n\nOne of you want to whip up a patch w/ tests or do you want me to do it?",
            "date": "2007-11-28T01:06:26.055+0000",
            "id": 21
        },
        {
            "author": "Michael Busch",
            "body": "I'm quite busy currently with other stuff. Feel free to go ahead ;)",
            "date": "2007-11-28T01:10:14.410+0000",
            "id": 22
        },
        {
            "author": "Grant Ingersoll",
            "body": "Will do.  Patch to follow shortly",
            "date": "2007-11-28T14:32:03.665+0000",
            "id": 23
        },
        {
            "author": "Grant Ingersoll",
            "body": "Whew.  I think we are there and I like it!  \n\nI renamed Yonik's suggestions to be SinkTokenizer and SourceTokenFilter to model the whole source/sink notion.  Hopefully people won't think the SourceTokenFilter is for processing code.  :-)\n\nI will commit tomorrow if there are no objections.",
            "date": "2007-11-28T15:16:29.407+0000",
            "id": 24
        },
        {
            "author": "Yonik Seeley",
            "body": "The SinkTokenizer name could make sense, but I think TeeTokenFilter makes more sense than SourceTokenFilter (it is a tee, it splits a single token stream into two, just like the UNIX tee command).\n",
            "date": "2007-11-28T15:27:43.063+0000",
            "id": 25
        },
        {
            "author": "Grant Ingersoll",
            "body": "Tee it is.  And here I just thought you liked golf!  I guess I have never used the tee command in UNIX.",
            "date": "2007-11-28T15:57:21.159+0000",
            "id": 26
        },
        {
            "author": "Grant Ingersoll",
            "body": "Committed revision 599478.",
            "date": "2007-11-29T15:18:39.619+0000",
            "id": 27
        },
        {
            "author": "Michael Busch",
            "body": "Sorry, this review is a bit late. Only a simple remark:\n\nIn SinkTokenizer you could initalize the iterator in the constructor,\nthen you can avoid the check if (iter==null) in next()?\n\n",
            "date": "2007-11-29T20:02:12.497+0000",
            "id": 28
        },
        {
            "author": "Yonik Seeley",
            "body": "> In SinkTokenizer you could initalize the iterator in the constructor\n\nIterators are generally fail-fast, hence it would throw an exception when you tried to use it later after adding some elements.",
            "date": "2007-11-29T20:15:04.199+0000",
            "id": 29
        },
        {
            "author": "Michael Busch",
            "body": "I see. Then we should set iter=null in add() in case after reset() more tokens\nare added to the list, right?",
            "date": "2007-11-29T20:27:08.974+0000",
            "id": 30
        },
        {
            "author": "Grant Ingersoll",
            "body": "In looking again, I also wonder whether the getTokens() shouldn't return an immutable list?  Or should we allow adding tokens outside of the Tee process?",
            "date": "2007-11-29T20:31:18.800+0000",
            "id": 31
        },
        {
            "author": "Yonik Seeley",
            "body": "> I see. Then we should set iter=null in add() in case after reset() more tokens are added to the list, right?\nDepends on what we consider valid usecases... reset could also just null out iter.\n\n> Or should we allow adding tokens outside of the Tee process?\nWhy not?  Seems more flexible, and this is an expert level API.\n\n",
            "date": "2007-11-29T20:37:46.651+0000",
            "id": 32
        },
        {
            "author": "Grant Ingersoll",
            "body": "And, of course, if we are calling add() outside of the Tee process, we probably don't need to clone the token, either.",
            "date": "2007-11-29T20:39:05.303+0000",
            "id": 33
        },
        {
            "author": "Grant Ingersoll",
            "body": "{quote}\nWhy not? Seems more flexible, and this is an expert level API.\n{quote}\nThen we should document that they must call reset before calling next(), right?  Same could go for the add() method.",
            "date": "2007-11-29T20:58:47.426+0000",
            "id": 34
        },
        {
            "author": "Grant Ingersoll",
            "body": "I also added, in SinkTokenizer, to override the close() method on Tokenizer so that it doesn't try to close the Reader, which throws an NullPointerEx.",
            "date": "2007-12-06T14:42:11.700+0000",
            "id": 35
        },
        {
            "author": "Grant Ingersoll",
            "body": "http://www.gossamer-threads.com/lists/lucene/java-dev/56255\n\nChanged to override next() instead of next(Token)",
            "date": "2007-12-28T14:17:55.264+0000",
            "id": 36
        }
    ],
    "component": "modules/analysis",
    "description": "In some cases, it would be handy to have Analyzer/Tokenizer/TokenFilters that could siphon off certain tokens and store them in a buffer to be used later in the processing pipeline.\n\nFor example, if you want to have two fields, one lowercased and one not, but all the other analysis is the same, then you could save off the tokens to be output for a different field.\n\nPatch to follow, but I am still not sure about a couple of things, mostly how it plays with the new reuse API.\n\nSee http://www.gossamer-threads.com/lists/lucene/java-dev/54397?search_string=BufferingAnalyzer;#54397",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1058",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "New Analyzer for buffering tokens",
    "systemSpecification": true,
    "version": ""
}