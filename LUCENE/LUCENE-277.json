{
    "comments": [
        {
            "author": "Daniel Naber",
            "body": "I'm not familiar with the sorting implementation, but maybe this information \nhelps debugging the problem: \n-it does not happen anymore if you change the \"reverse\" paramater \n-it does not happen anymore if you remove the \"if (i % 20 == 0)\" \n-it does not happen anymore if in search.Hits you change getMoreDocs(50) to \ngetMoreDocs(100) ",
            "date": "2004-09-16T04:53:31.000+0000",
            "id": 0
        },
        {
            "author": "Daniel Naber",
            "body": "Thanks for your test case. The bug is fixed now, I think. You can see the \npatch here, please test it: \nhttp://cvs.apache.org/viewcvs.cgi/jakarta-lucene/src/java/org/apache/lucene/search/FieldSortedHitQueue.java?r1=1.11&r2=1.12&diff_format=h \n ",
            "date": "2004-09-19T01:25:17.000+0000",
            "id": 1
        },
        {
            "author": "Jiri Kuhn",
            "body": "I have been testing the patch in our application in real life by common users \nand no duplicity have occured. Many thanks for all who helped. I'm closing this \nbug as solved. Thanks.",
            "date": "2004-09-22T14:02:50.000+0000",
            "id": 2
        }
    ],
    "component": "core/search",
    "description": "If you run the code below the exception will be thrown. I believe that it isn't \ncorrect behaviour (the duplicities, of course), index id of hits should be \nunique as it is without sort.\n\nLucene versions:\n1.4-final\n1.4.1\nCVS 1.5-rc1-dev\n\n\nimport org.apache.lucene.analysis.standard.StandardAnalyzer;\nimport org.apache.lucene.document.Document;\nimport org.apache.lucene.document.Field;\nimport org.apache.lucene.index.IndexReader;\nimport org.apache.lucene.index.IndexWriter;\nimport org.apache.lucene.queryParser.ParseException;\nimport org.apache.lucene.queryParser.QueryParser;\nimport org.apache.lucene.search.Hits;\nimport org.apache.lucene.search.IndexSearcher;\nimport org.apache.lucene.search.Query;\nimport org.apache.lucene.search.Searcher;\nimport org.apache.lucene.search.Sort;\nimport org.apache.lucene.search.SortField;\nimport org.apache.lucene.store.Directory;\nimport org.apache.lucene.store.RAMDirectory;\n\nimport java.io.IOException;\nimport java.util.HashSet;\nimport java.util.LinkedList;\nimport java.util.ListIterator;\nimport java.util.Set;\n\n/**\n * Run this test with Lucene 1.4 final or 1.4.1\n */\npublic class DuplicityTest\n{\n    public static void main(String[] args) throws IOException, ParseException\n    {\n        Directory directory = create_index();\n\n        search_index(directory);\n    }\n\n    private static void search_index(Directory directory) throws IOException, \nParseException\n    {\n        IndexReader reader = IndexReader.open(directory);\n        Searcher searcher = new IndexSearcher(reader);\n\n        Sort sort = new Sort(new SortField(\"co\", SortField.INT, false));\n\n        Query q = QueryParser.parse(\"sword\", \"text\", new StandardAnalyzer());\n\n        find_duplicity(searcher.search(q), \"no sort\");\n\n        find_duplicity(searcher.search(q, sort), \"using sort\");\n\n        searcher.close();\n        reader.close();\n    }\n\n    private static void find_duplicity(Hits hits, String message) throws \nIOException\n    {\n        System.out.println(message + \" hits size: \" + hits.length());\n\n        Set set = new HashSet();\n        for (int i = 0; i < hits.length(); i++) {\n//            System.out.println(hits.id(i) + \": \" + hits.doc(i).toString());\n            Integer id = new Integer(hits.id(i));\n            if (!set.contains(id))\n                set.add(id);\n            else\n                throw new RuntimeException(\"duplicity found, index id: \" + id);\n        }\n        System.out.println(\"no duplicity found\");\n    }\n\n    private static LinkedList words;\n\n    static {\n        words = new LinkedList();\n\n        words.add(\"word\");\n        words.add(\"sword\");\n        words.add(\"dwarf\");\n        words.add(\"whale\");\n        words.add(\"male\");\n    }\n\n    private static Directory create_index() throws IOException\n    {\n        Directory directory = new RAMDirectory();\n\n        ListIterator e_words1 = words.listIterator();\n        ListIterator e_words2 = words.listIterator(words.size());\n\n        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), \ntrue);\n\n        int co = 1;\n\n        for (int i = 0; i < 300; i++) {\n\n            if (!e_words1.hasNext()) {\n                e_words1 = words.listIterator();\n                e_words1.hasNext();\n            }\n            String word1 = (String)e_words1.next();\n            if (!e_words2.hasPrevious()) {\n                e_words2 = words.listIterator(words.size());\n                e_words2.hasPrevious();\n            }\n            String word2 = (String)e_words2.previous();\n\n            Document doc = new Document();\n\n            doc.add(Field.Keyword(\"co\", String.valueOf(co)));\n            doc.add(Field.Text(\"text\", word1 + \" \" + word2));\n            writer.addDocument(doc);\n\n            if (i % 20 == 0)\n                co++;\n        }\n        writer.optimize();\n        System.err.println(\"index size: \" + writer.docCount());\n        writer.close();\n\n        return directory;\n    }\n}",
    "hasPatch": false,
    "hasScreenshot": false,
    "id": "LUCENE-277",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Sorting produces duplicates",
    "systemSpecification": false,
    "version": "1.4"
}