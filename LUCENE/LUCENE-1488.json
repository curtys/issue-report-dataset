{
    "comments": [
        {
            "author": "Grant Ingersoll",
            "body": "Very interesting, Robert.  I'd like to see your patch.  I don't think we need to think of it as a Std. Analyzer replacement, but I could totally see offering it as the the ICUAnalyzer or some other better name.  In other words, I'd approach this as another Analyzer in the arsenal of Analyzers, otherwise, we'll have to deal with back-compatibility issues, etc.",
            "date": "2008-12-11T23:26:28.428+0000",
            "id": 0
        },
        {
            "author": "Robert Muir",
            "body": "thats a good idea. you know, currently trying to get it to pass all the standard analyzer unit tests causes some problems since lucene has some rather obscure definitions of 'number' (i think ip addresses, etc are included) which differ dramatically from the basic unicode definition.\n\nOther things of note:\n\ninstantiating the analyzer takes a long time (couple seconds) because ICU must \"compile\" the rules. I'm not sure of the specifics but by compile I think that means building massive FSM or similar based on all the unicode data. Its possible to precompile the rules into binary format but I think this is not currently exposed in ICU.\n\nthe lucene tokenization pipeline makes the implementation a little hairy. I hack around it by tokenizing on whitespace first, then acting as a token filter (just like the Thai analyzer does, which also uses RBBI). I don't think this really is that bad from a linguistic standpoint because the rare cases where 'token' can have whitespace inside of it (persian, etc) need serious muscle somewhere else and should be handled by a language analyzer.\n\ni'll try to get this thing in reasonable shape at least to document the approach.\n",
            "date": "2008-12-12T01:04:12.657+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "i've attached a patch for 'ICUAnalyzer'. I see that some things involving Token have changed but I created it before that point.\n\nI stole the unit tests from standard analyzer and put comments as to why certain ones arent appropriate and disabled those.\n\ni added some unit tests that demonstrate some of the value, correct analysis for arabic numerals, hindi text, decomposed latin diacritics, hebrew punctuation, cantonese and linear-b text outside of the BMP, etc.\n\none issue is that setMaxTokenLength() doesnt work correctly for values > 255 because CharTokenizer has a hardcoded private limit of 255 that i can't override. This is a problem since i use WhitespaceTokenizer first and then break down those tokens with the RBBI.\n",
            "date": "2008-12-12T05:58:16.576+0000",
            "id": 2
        },
        {
            "author": "Robert Muir",
            "body": "as soon as I figure out how to invoke the ICU RBBI compiler i'll see if i can update the patch with compiled rules so instantiation of this thing is cheap...",
            "date": "2008-12-12T15:38:32.033+0000",
            "id": 3
        },
        {
            "author": "uday kumar maddigatla",
            "body": "hi,\n\ni too just facing the same problem. my documet contains english as well as danish elements.\n\nI tried to use this analyzer. when i try to use this i got this error .\n\nException in thread \"main\" java.lang.ExceptionInInitializerError\n\tat org.apache.lucene.analysis.icu.ICUAnalyzer.tokenStream(ICUAnalyzer.java:74)\n\tat org.apache.lucene.analysis.Analyzer.reusableTokenStream(Analyzer.java:48)\n\tat org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:117)\n\tat org.apache.lucene.index.DocFieldConsumersPerField.processFields(DocFieldConsumersPerField.java:36)\n\tat org.apache.lucene.index.DocFieldProcessorPerThread.processDocument(DocFieldProcessorPerThread.java:234)\n\tat org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:765)\n\tat org.apache.lucene.index.DocumentsWriter.addDocument(DocumentsWriter.java:743)\n\tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1918)\n\tat org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1895)\n\tat com.IndexFiles.indexDocs(IndexFiles.java:87)\n\tat com.IndexFiles.indexDocs(IndexFiles.java:80)\n\tat com.IndexFiles.main(IndexFiles.java:57)\nCaused by: java.lang.IllegalArgumentException: Error 66063 at line 2 column 17\n\tat com.ibm.icu.text.RBBIRuleScanner.error(RBBIRuleScanner.java:505)\n\tat com.ibm.icu.text.RBBIRuleScanner.scanSet(RBBIRuleScanner.java:1047)\n\tat com.ibm.icu.text.RBBIRuleScanner.doParseActions(RBBIRuleScanner.java:484)\n\tat com.ibm.icu.text.RBBIRuleScanner.parse(RBBIRuleScanner.java:912)\n\tat com.ibm.icu.text.RBBIRuleBuilder.compileRules(RBBIRuleBuilder.java:298)\n\tat com.ibm.icu.text.RuleBasedBreakIterator.compileRules(RuleBasedBreakIterator.java:316)\n\tat com.ibm.icu.text.RuleBasedBreakIterator.<init>(RuleBasedBreakIterator.java:71)\n\tat org.apache.lucene.analysis.icu.ICUBreakIterator.<init>(ICUBreakIterator.java:53)\n\tat org.apache.lucene.analysis.icu.ICUBreakIterator.<init>(ICUBreakIterator.java:45)\n\tat org.apache.lucene.analysis.icu.ICUTokenizer.<clinit>(ICUTokenizer.java:58)\n\t... 12 more\n\nplease help me in this.",
            "date": "2009-04-28T11:31:12.149+0000",
            "id": 4
        },
        {
            "author": "Robert Muir",
            "body": "what version of icu4j are you using? needs to be >= 4.0",
            "date": "2009-04-28T13:58:14.178+0000",
            "id": 5
        },
        {
            "author": "Robert Muir",
            "body": "updated patch, not ready yet but you can see where i am going.\n\nICUTokenizer: Breaks text into words according to UAX #29: Unicode Text Segmentation. Text is divided across script boundaries so that this segmentation can be tailored for different writing systems; for example Thai text is segmented with a different method. The default and script-specific rules can be tailored. In the resources folder i have some examples for Southeast Asian scripts, etc.  Since i need script boundaries for tailoring, i stuff the ISO 15924 script code constant in the flags; this could be useful for downstream consumers.\n\nICUCaseFoldingFilter: Fold case according to Unicode Default Caseless Matching; Full case folding. This may change the length of the token, for example german sharp s is folded to 'ss'. This filter interacts with the downstream normalization filter in a special way, so you can provide a hint as to what the desired normalization form will be. In the NFKC or NFKD case it will apply the NFKC_Closure set so you do not have to Normalize(Fold(Normalize(Fold(x))))\n\nICUDigitFoldingFilter: Standardize digits from different scripts to the latin values, 0-9.\n\nICUFormatFilter: Remove identifier-ignorable codepoints, specifically those from the Format category. \n\nICUNormalizationFilter: Apply unicode normalization to text. This is accelerated with a quick-check.\n\nICUAnalyzer ties all this together. All of these components should also work correctly with surrogate-pair data. \n\nNeeds more doc and tests. any comments appreciated.\n",
            "date": "2009-06-04T14:20:05.339+0000",
            "id": 6
        },
        {
            "author": "Robert Muir",
            "body": "here's a simple description of what the current functionality buys you, its this:\n\nall indic languages (Hindi, Bengali, Tamil, ...), middle eastern languages (Arabic, Hebrew, etc) will work pretty well here (by that I mean tokenized, normalized, etc). Most of these lucene cannot parse correctly with any of the built-in analyzers.\n\nobviously european languages lucene handles quite well already, but unicode still has some improvements here, i.e. better case-folding.\n\nAnd finally, of course, the situation where you have data in a bunch of these different languages!\n\nin general, the unicode defaults work quite well for almost all languages, with the exception of CJK and southeast-asian languages. \nits not my intent to really solve those harder cases, only to provide a mechanism for someone else to deal with it if they don't like the defaults.\n\na great example is the arabic tokenizer, it should not exist. unicode defaults work great for that language. and it would be silly to think about HindiTokenizer, BengaliTokenizer, etc etc when unicode defaults will tokenize those correctly as well. \n\nthere's still some annoying complexity here, and any comments are appreciated. Especially tricky is the complexity-performance-maintenance balance, i.e. the case-folding filter could be a lot faster, but then it would have to be updated when a new unicode version is released... Another thing is i didn't optimize the BMP case anywhere [i.e. working at 32-bit codepoint to ensure surrogate data works], and I think thats worth considering... like 99.9% of data is in the BMP :)\n\nThanks,\nRobert",
            "date": "2009-06-05T17:42:21.126+0000",
            "id": 7
        },
        {
            "author": "Robert Muir",
            "body": "just an update, still more work to be done.\n\nsome of the components are javadoc'ed and have pretty good tests (case folding and normalization). These might be useful to someone in the meantime.\n\nalso added some tests to TestICUAnalyzer for various jira issues (LUCENE-1032, LUCENE-1215, LUCENE-1343, LUCENE-1545, etc) that are solved here. \n",
            "date": "2009-06-13T03:59:48.059+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "ICUAnalyzer looks very useful!  Good work Robert.  (And, thanks!).\n\nDo you think this'll be ready to go in time for 2.9 (which we are\ntrying to wrap up soonish)?\n\nIt seems like this absorbs the functionality of many of Lucene's\ncurrent analyzers.  EG you mentioned ArabicAnalyzer already.  What\nother analyzers (eg in contrib/analyzers/*) would you say are\nlogically subsumed by this?\n\nAlso, this seems quite different from StandardAnalyzer, in that it\nfocuses entirely on doing \"good\" tokenization, by relying on the\nUnicode standard (defaults) instead of fixed char ranges in\nStandardAnalyzer.  So it fixes many bugs in how StandardAnalyzer\ntokenizes, especially on non-European languages.\n\nAlso, StandardAnalyzer goes beyond making the initial tokens: it also\ntries to label things as acronym, host name, number, etc.; tries to\nfilter out stop words.\n\nI assume ICUCaseFoldingFilter logically subsumes LowercaseFilter?\n\nbq. Especially tricky is the complexity-performance-maintenance balance, i.e. the case-folding filter could be a lot faster, but then it would have to be updated when a new unicode version is released.\n\nI think it's fine to worry about this later.  Correctness is more\nimportant than performance at this point.\n",
            "date": "2009-06-13T10:21:22.010+0000",
            "id": 9
        },
        {
            "author": "Robert Muir",
            "body": "Michael, I don't think it will be ready for 2.9, here is some answers to your questions.\n\ngoing with your arabic example:\nThe only thing this absorbs is language-specific tokenization (like ArabicLetterTokenizer), because as mentioned I think thats generally the wrong approach.\nBut this can't replace ArabicAnalyzer completely, because ArabicAnalyzer stems arabic text in a language-specific way, which has a huge effect on retrieval quality for Arabic language text.\n\nSome of what it does the language-specific analyzers don't do though.\n\nIn this specific example, it would be nice if ArabicAnalyzer really used the functionality here, then did its Arabic-specific stuff!\nBecause this functionality will do things like normalize 'Arabic Presentation Forms' and deal with Arabic digits and things that aren't in the ArabicAnalyzer. It also will treat any non-Arabic text in your corpus very nicely!\n\nYes, you are correct about the difference from StandardAnalyzer and I would argue there are tokenization bugs in how StandardAnalyzer works with European languages too, just see LUCENE-1545!\n\nI know StandardAnalyzer does these things. This tokenizer has some built-in types already, such as number. If you want to add more types, its easy. Just make a .txt file with your grammar, create a RuleBasedBreakIterator with it, and pass it along to the tokenizer constructor. you will have to subclass the tokenizer's getType() for any new types though, because RBBI 'types' are really just integer codes in the rule file, and you have to map them to some text such as \"WORD\".\n\nYes, case-folding will work better than lowercase for a few european languages.\n",
            "date": "2009-06-13T14:47:52.119+0000",
            "id": 10
        },
        {
            "author": "Earwin Burrfoot",
            "body": "bq. But this can't replace ArabicAnalyzer completely, because ArabicAnalyzer stems arabic text in a language-specific way, which has a huge effect on retrieval quality for Arabic language text.\nWhat about separating word-tokenizing from morphological processing?",
            "date": "2009-06-14T19:25:22.510+0000",
            "id": 11
        },
        {
            "author": "Robert Muir",
            "body": "Earwin, I don't understand your question... \nThere is no morphological processing or any other language-specific functionality in this patch...\n",
            "date": "2009-06-14T22:08:17.002+0000",
            "id": 12
        },
        {
            "author": "Robert Muir",
            "body": "add analysis tests for a few languages to demonstrate what this does.\n",
            "date": "2009-06-16T21:40:14.521+0000",
            "id": 13
        },
        {
            "author": "Earwin Burrfoot",
            "body": "bq. There is no morphological processing or any other language-specific functionality in this patch... \nI'm speaking of stemming in ArabicAnalyzer. Why can't you use its stemming tokenfilter over all ICU goodness from this patch? Everything else ArabicAnalyzer consists of might as well be deleted right after.",
            "date": "2009-07-02T17:34:49.718+0000",
            "id": 14
        },
        {
            "author": "Robert Muir",
            "body": "Earwin, you are absolutely correct.\n\nthough i would also want to keep the ArabicNormalizationFilter as it does \"non-standard\" normalization that is usually helpful for arabic text.",
            "date": "2009-07-02T20:42:00.063+0000",
            "id": 15
        },
        {
            "author": "Robert Muir",
            "body": "this is latest copy of my code (in response to java-user discussion).\n\nnot many changes except tokenstream changes and work for writing systems with no word separation: lao, myanmar, cjk, etc.\nfor these, the tokenizer does not break text into words, but subwords (syllables), and unigrams&bigrams of these are indexed.\n",
            "date": "2009-09-08T04:57:39.799+0000",
            "id": 16
        },
        {
            "author": "Robert Muir",
            "body": "here I complete Lao support (fully implementing http://www.panl10n.net/english/final%20reports/pdf%20files/Laos/LAO06.pdf)\n\nAlso fix a tokenstream bug (not back-compat issue!) in the bigramfilter.\n\nI think all language/unicode features are done, basically we can get better language support in the future from ICU automatically, but I think all languages are handled in a reasonable way for now. \n\nimho all that is left is:\n* fix docs, improve tests, java api, rbbi grammars, any bugs, TODOs\n* decide if we want to merge this with the collation contrib (I think it might be a good idea)\n* test various versions of ICU to know which ones it works with\n\nit works and the tests pass, but some tests are slow (10+ seconds, though I made them faster).\nThe problem is these slow tests have found bugs and will help test version compatibility, so I like them.\n",
            "date": "2009-09-25T19:03:56.360+0000",
            "id": 17
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Robert: If you do a restoreState() no clearAttributes() is needed before, as the restoreState overwrites all attributes. Everything else looks good.",
            "date": "2009-09-25T20:30:53.198+0000",
            "id": 18
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, thanks for taking a look! I'll fix this.\n",
            "date": "2009-09-25T20:40:40.070+0000",
            "id": 19
        },
        {
            "author": "Robert Muir",
            "body": "setting a fix version, setting a correct description of the issue",
            "date": "2009-11-13T23:44:33.211+0000",
            "id": 20
        },
        {
            "author": "DM Smith",
            "body": "Robert, just finished reviewing the code. Looks great! Doesn't look like there's too much left. All I see is a bit of JavaDoc and an extraneous unused variable (ICUTokenizer: private PositionIncrementAttribute posIncAtt;)\n\nThe documentation in ICUNormalizationFilter is very instructive. Kudos. The only part that's hard to for me to understand is the filter order dependency, but then again that's a hard topic in the first place.\n\nI'm wondering whether it would make sense to have multiple representations of a token with the same position in the index. Specifically, transliterations and case-folding. That is, the one is a \"synonym\" for the other. Is that possible and does it make sense? I'm imagining a use case where a end user enters for a search request a Latin script transliteration of Greek \"uios\" but might also enter \"\u03c5\u03b9\u03bf\u03c2\".\n\nThe other question on my mind is that given a text of German, Greek and Hebrew (three distinct scripts) does it make sense to apply stop words to them based on script? And should stop words be normalized on load with the ICUNormalizationFilter? Or is it a given that they work as is?\n\nCan/How does all this integrate with stemmers?\n\nAgain, many thanks! (Btw, special thanks for this working with 2.9 and Java 1.4!)\n\n",
            "date": "2009-12-02T22:45:47.002+0000",
            "id": 21
        },
        {
            "author": "Robert Muir",
            "body": "DM, I really appreciate your review. You have brought up some good ideas that I haven't yet thought about.\n\nbq. All I see is a bit of JavaDoc and an extraneous unused variable (ICUTokenizer: private PositionIncrementAttribute posIncAtt\n\nYeah there are some TODOs, and cleanup on the tokenstreams, and the API in general. its not easy to customize the way its supposed to be: where you as a user can actually supply BreakIterator impls to the tokenizer and say \"use these rules/dictionary/whatever for tokenizing XYZ script only\".\n\nbq. I'm wondering whether it would make sense to have multiple representations of a token with the same position in the index. Specifically, transliterations and case-folding. That is, the one is a \"synonym\" for the other. Is that possible and does it make sense? I'm imagining a use case where a end user enters for a search request a Latin script transliteration of Greek \"uios\" but might also enter \"\u03c5\u03b9\u03bf\u03c2\".\n\nYeah this is something to consider. I don't think it makes sense for the case folding filter, but maybe for the transform filter? will have to think about it.\nThere's use cases here like what you mentioned, also real-world ones like invoking Serbian-Latin or something, where you want users to search in either writing system and there actually is a clearly defined transformation.\n\nI guess on the other hand, you could always use a separate field (with different analysis/transforms) for each and search both.\n\nbq. The other question on my mind is that given a text of German, Greek and Hebrew (three distinct scripts) does it make sense to apply stop words to them based on script? And should stop words be normalized on load with the ICUNormalizationFilter? Or is it a given that they work as is?\n\nYou could put them all in one list with regular stopfilter now. They won't clash since they are different unicode Strings. Obviously I would normalize this list with the same stuff (normalization form/case folding/whatever) that your analyzer users.\n\nI don't put any stopwords in this, because thats language dependent, trying to stick with language-independent (either stuff that applies to unicode as a whole, or specific writing systems, which can be accurately detected).\n\nbq. Can/How does all this integrate with stemmers?\n\nRight this is just supposed to be what \"StandardTokenizer\"-type stuff does, and you would add stemming on top of it. The idea is you would use this even if you think you only have english text, maybe then applying your porter english stemmer. But if it happens to stumble upon some CJK or Thai or something along the way, everything will be ok.\n\nIn all honesty, I probably put 90% of the work into the Khmer, Myanmar, Lao, etc cases. Having good tokenization I think makes a usable search engine, for a lot of languages stemming is only a bonus.\n\nHowever, one thing it also does is put the script value in the flags for each token. This can work pretty well: if its Greek script, its probably Greek language, but if its Hebrew script, well it could be Yiddish too.  If its Latin script, could be english, german, etc. Its ended only to make life easier since the information is already available... but I don't know yet how to make use of it in a nice way.\n\nbq. Again, many thanks! (Btw, special thanks for this working with 2.9 and Java 1.4!)\n\nYeah i haven't updated it to java 5/Lucene 3.x yet, started working it, but kinda forgot about that so far. I guess this is a good thing, so you can play with it if you want.\n\n",
            "date": "2009-12-02T23:19:23.965+0000",
            "id": 22
        },
        {
            "author": "Robert Muir",
            "body": "Linking this issue to LUCENE-2124. Once contrib/collation has been renamed contrib/icu, I want to split out two of the tokenfilters (case folding and normalization) out as a separate smaller issue to start. \n\nThese are useful on their own, yet inseparable because of the special K mappings: you must tell the case folding filter what your targeted normalization form will be.",
            "date": "2009-12-07T16:29:42.330+0000",
            "id": 23
        },
        {
            "author": "Vilaythong Southavilay",
            "body": "I am developing an IR system for Lao. I've been searching for this kind of analyzers to be used in my development to index documents containing languages like Lao, French and English in one single passage.\n\nI tested it for Lao language for Lucene 2.9 and 3.0 using my short passage. It worked correctly for both versions as I expected, especially for segmenting Lao single syllables. I also tried it with the bi-gram filter option for two syllables, which worked fine for simple words. The result contained some two-syllable words which do not make sense in Lao language. I guess this not a big issue. As Robert pointed out (in an email to me), we still need dictionary-based word segmentation for Lao, which can be integrated in ICU and used by this analyzer.\n\nAny way, thanks for your assistance. This work will be helpful not only for Lao, but others as well because it's good to have a common analyzer for unicode characters.\n\nI'll continue testing it and report any problems if I find one. ",
            "date": "2010-01-20T00:35:59.145+0000",
            "id": 24
        },
        {
            "author": "Robert Muir",
            "body": "Thanks for sharing those results! Yes the bigram behavior (right now enabled for Han, Lao, Khmer, and Myanmar) is an attempt to boost relevance in a consistent way since we do not have dictionary-based word segmentation for those writing systems, only the ability to segment into syllables.\n\nIn the next patch I'll make it easier to configure this behavior, and turn it off when you want, without writing your own analyzer.\n\nI am glad to hear the syllable segmentation algorithm is working well! \nThe credit really belongs to the Pan Localization Project, I simply implemented the algorithm described here: http://www.panl10n.net/english/final%20reports/pdf%20files/Laos/LAO06.pdf\nYou can see the code in Lao.rbbi in the patch, warning, as it mentions, I am pretty sure Lao numeric digits are not yet working correctly, but hopefully I will fix those too in the next version.\n",
            "date": "2010-01-20T01:25:05.183+0000",
            "id": 25
        },
        {
            "author": "Vilaythong Southavilay",
            "body": "I tested Lao numbers. It only worked for 2 digit numbers (because of two syllable segmentation),  but the result tokens were converted to Arabic numbers (instead of Lao). This is not too bad for analyzing heading numbers and ordered lists with less than 100 items (the meaning and order are preserved).\n\nIn the documents i encountered most of scientific numerals or financial figures (complex numeric digits) were written using Arabic numbers.\n\nNevertheless, recognizing long Lao numeric digits is a \"must-to-have\" to complete this set for Laos.\n\n",
            "date": "2010-01-20T23:37:08.231+0000",
            "id": 26
        },
        {
            "author": "Robert Muir",
            "body": "Hi,this is not intentional to split them into 2 digits, it is really only because of rbbi rule-chaining turned off.\nSo now \u0ed0\u0ed1\u0ed2\u0ed3 stays as a single token, and later becomes 0123.\n\nI've written tests for, and fixed numerics in my local copy for lao, myanmar, and khmer. I will post an updated patch hopefully soon with all the improvements.\n\nbq. but the result tokens were converted to Arabic numbers (instead of Lao).\n\nYes this is intentional, later there is a filter that converts all numeric digits to Arabic so the search will match either.",
            "date": "2010-01-20T23:42:57.162+0000",
            "id": 27
        },
        {
            "author": "Robert Muir",
            "body": "uploading a dump of my workspace, so Uwe can review the new attribute.",
            "date": "2010-03-16T14:22:05.799+0000",
            "id": 28
        },
        {
            "author": "Uwe Schindler",
            "body": "Attribute looks good! I would only fix toString() to match the defaulkt impl by using syntax variableName + \"=\" + value, here  \"code=\"+getName(code). This makes AttrubuteSource.toString() look nice.",
            "date": "2010-03-16T14:36:11.659+0000",
            "id": 29
        },
        {
            "author": "Robert Muir",
            "body": "Thanks for the review Uwe! moving forwards...",
            "date": "2010-03-16T15:55:43.754+0000",
            "id": 30
        },
        {
            "author": "David Bowen",
            "body": "I have a possibly naive question on the bigram filter: why would you want to index the individual one-character-tokens, as well as the bigrams?   The CJK Tokenizer just emits the bigrams.  Wouldn't indexing and searching on the unigrams as well as the bigrams just slow down search?\n\n",
            "date": "2010-03-31T03:44:37.392+0000",
            "id": 31
        },
        {
            "author": "Robert Muir",
            "body": "bq. I have a possibly naive question on the bigram filter\n\nIts not naive at all really! I think we to do exactly what you suggest.\n\nchange it slightly to behave just like CJKTokenizer (except of course, working with mixed language text, and supporting UCS-4)\n",
            "date": "2010-03-31T03:55:36.200+0000",
            "id": 32
        },
        {
            "author": "Robert Muir",
            "body": "Marking this fixed, as all the icu functionality has been broken into smaller issues and now resolved (and simpler due to ICU 4.4 changes)",
            "date": "2010-05-03T13:22:21.504+0000",
            "id": 33
        },
        {
            "author": "Grant Ingersoll",
            "body": "Bulk close for 3.1",
            "date": "2011-03-30T15:50:04.781+0000",
            "id": 34
        }
    ],
    "component": "modules/analysis",
    "description": "The standard analyzer in lucene is not exactly unicode-friendly with regards to breaking text into words, especially with respect to non-alphabetic scripts.  This is because it is unaware of unicode bounds properties.\n\nI actually couldn't figure out how the Thai analyzer could possibly be working until i looked at the jflex rules and saw that codepoint range for most of the Thai block was added to the alphanum specification. defining the exact codepoint ranges like this for every language could help with the problem but you'd basically be reimplementing the bounds properties already stated in the unicode standard. \n\nin general it looks like this kind of behavior is bad in lucene for even latin, for instance, the analyzer will break words around accent marks in decomposed form. While most latin letter + accent combinations have composed forms in unicode, some do not. (this is also an issue for asciifoldingfilter i suppose). \n\nI've got a partially tested standardanalyzer that uses icu Rule-based BreakIterator instead of jflex. Using this method you can define word boundaries according to the unicode bounds properties. After getting it into some good shape i'd be happy to contribute it for contrib but I wonder if theres a better solution so that out of box lucene will be more friendly to non-ASCII text. Unfortunately it seems jflex does not support use of these properties such as [\\p{Word_Break = Extend}] so this is probably the major barrier.\n\nThanks,\nRobert\n\n\n\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1488",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "multilingual analyzer based on icu",
    "systemSpecification": true,
    "version": ""
}