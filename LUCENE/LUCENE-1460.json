{
    "comments": [
        {
            "author": "Robert Muir",
            "body": "is anyone working on this? I have some functionality that needs some of these to be new-api so i have at least half of them done.\n",
            "date": "2009-05-22T22:28:19.377+0000",
            "id": 0
        },
        {
            "author": "Mark Miller",
            "body": "wanna post what you have Robert? perhaps then someone will finish off the other half.",
            "date": "2009-06-11T02:23:25.361+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "only partial solution...\nsome of the analyzers don't have any tests, so I think thats a bit more work!\nthe AsciiFoldingFilter fix is in here too, i know its not in contrib but it doesnt support new API either.\n\n",
            "date": "2009-06-11T04:12:44.917+0000",
            "id": 2
        },
        {
            "author": "Simon Willnauer",
            "body": "Robert, would you divide your patch into contrib/core. That would make it easier to help out and eventually commit it.\nThanks",
            "date": "2009-06-11T08:01:45.742+0000",
            "id": 3
        },
        {
            "author": "Robert Muir",
            "body": "split patch: core/contrib",
            "date": "2009-06-11T08:28:37.405+0000",
            "id": 4
        },
        {
            "author": "Uwe Schindler",
            "body": "I added the ASCIIFoldingFilter patch into LUCENE-1693 (patch comes shortly). So this part is finished.",
            "date": "2009-07-14T14:20:04.561+0000",
            "id": 5
        },
        {
            "author": "Michael Busch",
            "body": "Now that LUCENE-1693 is almost committed, we can tackle this one.\n\nWe need to keep these points in mind that Uwe posted on 1693:\n\n    *  rewrite and replace next(Token)/next() implementations by new API\n    * if the class is final, no next(Token)/next() methods needed (must be removed!!!)\n    * if the class is non-final add the following methods to the class:\n\n{code:java}\n      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should\n       * not be overridden. Delegates to the backwards compatibility layer. */\n      public final Token next(final Token reusableToken) throws java.io.IOException {\n        return super.next(reusableToken);\n      }\n\n      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should\n       * not be overridden. Delegates to the backwards compatibility layer. */\n      public final Token next() throws java.io.IOException {\n        return super.next();\n      }\n{code}\n\n      Also the incrementToken() method must be final in this case.\n",
            "date": "2009-07-22T01:21:23.194+0000",
            "id": 6
        },
        {
            "author": "Robert Muir",
            "body": "Michael, I think we should try to coordinate timing on this with LUCENE-1728, where we are trying to reorganize contrib/analyzers...\n",
            "date": "2009-07-22T01:49:19.793+0000",
            "id": 7
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nMichael, I think we should try to coordinate timing on this with LUCENE-1728, where we are trying to reorganize contrib/analyzers... \n{quote}\n\nIt seems like 1728 is ready to commit? Simon said on java-dev he will try to finish it by end of this week?\n\nIn that case we should commit 1728 first. But we can finish the patch here I think... after 1728 is committed we just do a find&replace on the patch and replace \"contrib/analyzers/\" with \"contrib/analyzers/common/\" ?",
            "date": "2009-07-22T05:02:20.463+0000",
            "id": 8
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. It seems like 1728 is ready to commit? Simon said on java-dev he will try to finish it by end of this week?\n\nThat is correct.  I can commit it today I think. Will make this issue dependent on 1728 and finish it by the end of today.\n\nsimon\n",
            "date": "2009-07-22T06:46:59.225+0000",
            "id": 9
        },
        {
            "author": "Michael Busch",
            "body": "Cool! Thanks, Simon.",
            "date": "2009-07-22T07:22:43.885+0000",
            "id": 10
        },
        {
            "author": "Robert Muir",
            "body": "Michael, after 1728 I can take another look at this. the reason is, that I added some tests to these analyzers and found a bug in the Thai offsets.\n\nWhen i submitted this, i only duplicated the existing behavior, but I don't want to reintroduce the bug into incrementToken()\n",
            "date": "2009-07-22T12:44:08.751+0000",
            "id": 11
        },
        {
            "author": "Simon Willnauer",
            "body": "LUCENE-1728 is committed. You can go ahead on this.\n\nThanks",
            "date": "2009-07-23T18:46:15.915+0000",
            "id": 12
        },
        {
            "author": "Robert Muir",
            "body": "I'll work on updating the patch.\n\nQuestion: instead of converting the tests to new api, would it be more beneficial to test both? testNewAPI() / testOldAPI() \n",
            "date": "2009-07-23T19:19:58.492+0000",
            "id": 13
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Question: instead of converting the tests to new api, would it be more beneficial to test both? testNewAPI() / testOldAPI() \n\nNo need to do this. The correct backwards-compatibility of the new API is checked separately by the extra test in LUCENE-1693.\n\nIt is enough if you remove all next() methods, add incrementToken() and make the above mentioned changes to make the token streams final where possible (see the comment of Michael Busch above -- which is the \"howto\" for the conversion).\n\nThe testy should only test the new API, but until they are also converted, the old tests should also still work (because of the backwards-compatibility layer). Be sure to apply LUCENE-1693 which does not change anything in contrib before starting to create a patch to have the newest API. Hopefully Michael has committed 1693 tomorrow.\n\nLUCENE-1460_core.txt is already in LUCENE-1693 as it is part of core token streams.",
            "date": "2009-07-23T20:44:34.256+0000",
            "id": 14
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, thanks!\n\nYes, I will follow your guidelines that Michael re-posted here.\n\nadditionally, some tokenstreams are not final in contrib, but have not been put into a release (example: arabic, my fault)\nAny objection to making these final, so I can just remove the old api instead of declaring final, deprecated next() methods?\n",
            "date": "2009-07-23T20:57:48.445+0000",
            "id": 15
        },
        {
            "author": "Uwe Schindler",
            "body": "no, make them final. the same happened with asciifoldingfilter, was never released, so can be changed.\n\nIn general, all tokenstreams should be final or have final impls (see LUCENE-1753 for explanation). The only real example of a non-final one is CharTokenizer, which is abstract but the tokenization impls are final :)",
            "date": "2009-07-23T21:09:41.614+0000",
            "id": 16
        },
        {
            "author": "Uwe Schindler",
            "body": "Make next() final is only the last chance to prevent users from overriding never-called next-methods, if the class was not final before. This makes the backwards-break as small as possible (by only making these methods final). See the discussion in 1693.",
            "date": "2009-07-23T21:16:04.797+0000",
            "id": 17
        },
        {
            "author": "Robert Muir",
            "body": "i updated the previous patch to the current reality, still incomplete.\n\n",
            "date": "2009-07-24T00:58:31.791+0000",
            "id": 18
        },
        {
            "author": "Robert Muir",
            "body": "some more done (the rest of the various language analyzers at least).\n",
            "date": "2009-07-24T06:34:01.171+0000",
            "id": 19
        },
        {
            "author": "Robert Muir",
            "body": "i have visitors coming in town, so if you feel like punishing yourself, just let me know so we don't duplicate efforts. \nI promise I left all the fun ones still to be converted.\n",
            "date": "2009-07-24T06:42:10.441+0000",
            "id": 20
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\ni have visitors coming in town, so if you feel like punishing yourself, just let me know so we don't duplicate efforts.\nI promise I left all the fun ones still to be converted. \n{quote}\n\nYay I'm excited!\n\nI think when these patches are committed I don't want to hear the word \"TokenStream\" for a while... :)\n\nI can work on this Friday and during the weekend. Let me know which ones you're gonna work on and which ones I should take.\n\nBtw: Thanks for all your work here, Robert!!",
            "date": "2009-07-24T09:03:01.002+0000",
            "id": 21
        },
        {
            "author": "Robert Muir",
            "body": "Michael, I'm not gonna be able to work on this at all until monday at least, that's what I am saying.\nSo I will check with you Monday and see how things are.",
            "date": "2009-07-24T12:16:02.134+0000",
            "id": 22
        },
        {
            "author": "Robert Muir",
            "body": "Michael, oh also, as Uwe mentioned, I applied LUCENE-1693 before creating this patch.\n\nIf you do this all the tests will pass, otherwise I am not so sure!\n",
            "date": "2009-07-24T12:46:25.642+0000",
            "id": 23
        },
        {
            "author": "Robert Muir",
            "body": "Michael/anyone else, if you apply the patch (it should go cleanly now that 1693 is committed), then the easiest way to see what remains to be done is to declare TokenStream.incrementToken abstract. \n\nThis still causes my eclipse to light up like a christmas tree so there is a lot to do :)\n",
            "date": "2009-07-24T23:26:29.706+0000",
            "id": 24
        },
        {
            "author": "Michael Busch",
            "body": "Thanks, Robert!\n\nI'm currently working on LUCENE-1448, will finish that tonight.\nTomorrow and/or Sunday I'll work on this one.",
            "date": "2009-07-24T23:44:13.325+0000",
            "id": 25
        },
        {
            "author": "Michael Busch",
            "body": "I converted some more contribs...",
            "date": "2009-07-25T07:34:32.903+0000",
            "id": 26
        },
        {
            "author": "Michael Busch",
            "body": "More progress... \nngram was a bit tricky. But I think it is much more efficiently implemented now. It used to clone every token it returns. Now it only clones the term that it receives from the input stream.\n\nWould be good if someone could take a look at the ngram changes... well, the testcases pass.",
            "date": "2009-07-25T10:07:21.154+0000",
            "id": 27
        },
        {
            "author": "Michael Busch",
            "body": "Btw: I'm taking a tokenstream break today... so if anyone feels the sudden urge to convert some of the remaining streams: don't hesitate - it won't conflict with my work, the patch I posted late last night is still current.\n\nI'll try to continue tomorrow.",
            "date": "2009-07-25T19:16:01.514+0000",
            "id": 28
        },
        {
            "author": "Robert Muir",
            "body": "Michael, looks like you got a ton done. I'll take a look and see late sunday/monday at what you did with ngram for curiousity at least.\n\nif you get a moment maybe someone could review what I did with Thai, I didn't look to hard to see if save/restore state was worse than the previous cloning... \n\nthanks for tackling the tougher ones here :)",
            "date": "2009-07-25T22:45:53.340+0000",
            "id": 29
        },
        {
            "author": "Michael Busch",
            "body": "Some more progress - mostly in contrib/memory.",
            "date": "2009-07-26T08:42:10.738+0000",
            "id": 30
        },
        {
            "author": "Robert Muir",
            "body": "Michael, I looked at your patch. \n\nWhat do you think about the remaining ones? should they be left as is for now?\nor do you think some of these should still expose Token (i.e. in their public/protected methods) but just as back compat/convenience and work w/ the new api behind the scenes?\n",
            "date": "2009-07-27T01:25:40.045+0000",
            "id": 31
        },
        {
            "author": "Robert Muir",
            "body": "with analyzers/compound",
            "date": "2009-07-27T02:55:23.238+0000",
            "id": 32
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nwith analyzers/compound \n{quote}\n\nCool. Thanks for all your work here! \n\n{quote}\nif you get a moment maybe someone could review what I did with Thai\n{quote}\n\nI'll review it before we commit this.",
            "date": "2009-07-29T04:58:37.407+0000",
            "id": 33
        },
        {
            "author": "Robert Muir",
            "body": "Michael, sorry to leave it incomplete, I think I am not the best for the remaining ones.\n\nFor example I am a little intimidated by things such as this note in ShingleMatrix: \n{code}\n  * This method exists in order to avoid reursive calls to the method\n  * as the complexity of a fairlt small matrix then easily would require\n  * a gigabyte sized stack per thread.\n{code}\n",
            "date": "2009-07-29T13:53:17.087+0000",
            "id": 34
        },
        {
            "author": "Michael Busch",
            "body": "I wonder if we should just deprecate PrefixAwareTokenFilter and PrefixAndSuffixAwareTokenFilter. I don't really understand what they're supposed to be used for, and I find the implementation a bit strange.\n\nI don't think it's possible to convert them in a backwards-compatible way anyways, and writing a replacement seems not really worthwhile - or does someone use these?",
            "date": "2009-07-31T03:57:58.016+0000",
            "id": 35
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nFor example I am a little intimidated by things such as this note in ShingleMatrix:\n{quote}\n\nSame here. The shingle ones are the only remaining ones... I'll try, but I don't know the code at all. They do a lot of caching and stuff - so probably the hardest to convert.\n\n",
            "date": "2009-07-31T22:47:01.130+0000",
            "id": 36
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nso probably the hardest to convert.\n{quote}\nI was thinking, there are varying degrees of conversion. \nI think w/ the new api you could reduce cloning and really optimize these, or the other extreme you are almost merely pushing the back compat logic up to that level... kinda like my compound hack.\n(i tried the latter and was unsuccessful even doing that for the shingles).\n",
            "date": "2009-08-01T02:21:20.519+0000",
            "id": 37
        },
        {
            "author": "Michael Busch",
            "body": "I'll commit this current patch soon and then make a separate patch for the shingle filters.",
            "date": "2009-08-01T20:44:19.972+0000",
            "id": 38
        },
        {
            "author": "Michael Busch",
            "body": "Latest patch that converts all streams, except the single ones.\n\nI'll commit this soon.",
            "date": "2009-08-01T22:37:02.747+0000",
            "id": 39
        },
        {
            "author": "Michael Busch",
            "body": "Committed.",
            "date": "2009-08-01T22:58:07.667+0000",
            "id": 40
        },
        {
            "author": "Michael Busch",
            "body": "Robert, thank you for all your help here!!!",
            "date": "2009-08-01T23:00:01.721+0000",
            "id": 41
        },
        {
            "author": "Uwe Schindler",
            "body": "You two guys are the best, great work! I had no time to completely review it, but I am happy that the new TokenStream API seems so successful :-)",
            "date": "2009-08-02T08:37:47.563+0000",
            "id": 42
        }
    ],
    "component": "",
    "description": "Now that we have the new TokenStream API (LUCENE-1422) we should change all contrib modules to use it.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1460",
    "issuetypeClassified": "REFACTORING",
    "issuetypeTracker": "OTHER",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Change all contrib TokenStreams/Filters to use the new TokenStream API",
    "systemSpecification": true,
    "version": ""
}