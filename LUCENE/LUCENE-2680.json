{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "Hmm... I think there's another silliness going on inside IW: when applying deletes, we one-by-one open the SR, apply deletes, close it.\n\nBut then immediately thereafter we open the N segments to be merged.\n\nWe should somehow not do this double open, eg, use the pool temporarily, so that the reader is opened to apply deletes, and then kept open in order to do the merging.  Using the pool should be fine because the merge forcefully evicts the sub readers from the pool after completion.",
            "date": "2010-10-02T09:47:21.925+0000",
            "id": 0
        },
        {
            "author": "Jason Rutherglen",
            "body": "Maybe we should implement this as pending deletes per segment rather than using a generational system because with LUCENE-2655, we may need to maintain the per query/term docidupto per segment.  The downside is the extraneous memory consumed by the hash map, however, if we use BytesRefHash this'll be reduced, or would it?  Because we'd be writing the term bytes to a unique byte pool per segment?  Hmm... Maybe there's a more efficient way.",
            "date": "2010-10-10T00:57:48.402+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "Tracking per-segment would be easier but I worry about indices that have large numbers of segments... eg w/ a large mergeFactor and frequent flushing you can get very many segments.\n\nSo if we track per-segment, suddenly the RAM required (as well as CPU cost of copying these deletions to the N segments) is multiplied by the number of segments.",
            "date": "2010-10-11T10:05:15.923+0000",
            "id": 2
        },
        {
            "author": "Jason Rutherglen",
            "body": "The general approach is to reuse BufferedDeletes though place them into a segment info keyed map for those segments generated post lastSegmentIndex as per what has been discussed here https://issues.apache.org/jira/browse/LUCENE-2655?focusedCommentId=12922894&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12922894 and below.\n\n* lastSegmentIndex is added to IW\n\n* DW segmentDeletes is a map of segment info -> buffered deletes.  In the apply deletes method buffered deletes are pulled for a given segment info if they exist, otherwise they're taken from deletesFlushedLastSeg.  \n\n* I'm not entirely sure what pushDeletes should do now, probably the same thing as currently, only the name should change slightly in that it's pushing deletes only for the RAM buffer docs.\n\n* There needs to be tests to ensure the docid-upto logic is working correctly\n\n* I'm not sure what to do with DW hasDeletes (it's usage is commented out)\n\n* Does there need to be separate deletes for the ram buffer vis-\u00e0-vis the (0 - lastSegmentIndex) deletes?\n\n* The memory accounting'll now get interesting as we'll need to track the RAM usage of terms/queries across multiple maps.  \n\n* In commitMerge, DW verifySegmentDeletes removes the unused info -> deletes\n\n* testDeletes deletes a doc in segment 1, then merges segments 1 and 2.  We then test to insure the deletes were in fact applied only to segment 1 and 2.  \n\n* testInitLastSegmentIndex insures that on IW init, the lastSegmentIndex is in fact set\n",
            "date": "2010-11-01T17:58:06.593+0000",
            "id": 3
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. The most recent one \"wins\", and we should do only one delete (per segment) for that term.\n\nHow should we define this recency and why does it matter?  Should it be per term/query or for the entire BD?\n\nI think there's an issue with keeping lastSegmentIndex in DW, while it's easy to maintain, Mike had mentioned keeping the lastSegmentIndex per BufferedDeletes object.  Coalescing the BDs should be easier to maintain after successful merge than maintaining a separate BD for them.  We'll see.\n\nI'll put together another patch with these changes.",
            "date": "2010-11-02T16:41:13.087+0000",
            "id": 4
        },
        {
            "author": "Jason Rutherglen",
            "body": "Here's a new patch with properly working last segment index.  \n\nThe trunk version of apply deletes has become applyDeletesAll and is functionally unchanged.\n\nThere's a new method, DW applyDeletesToSegments called by _mergeInit for segments that are about to be merged.  The deleted terms and queries for these segments are kept in hash sets because docid-uptos are not needed.  \n\nLike the last patch DW maintains the last segment index.  There's no need to maintain the last-segindex per BD, instead I think it's only useful per DW, and for trunk we only have one DW being used at a time.  \n\nOn successful merge, the last segment index is set to the segment index previous to the start segment of the merge.  The merged segments deletes are coalesced into the startIndex-1's segment deletes.\n",
            "date": "2010-11-03T01:22:17.720+0000",
            "id": 5
        },
        {
            "author": "Jason Rutherglen",
            "body": "I'm redoing things a bit to take into account the concurrency of merges.  For example, if a merge fails, we need to not have removed those segments' deletes to be applied.  Also probably the most tricky part is that lastSegmentIndex could have changed since a merge started, which means we need to be careful about how and which deletes we coalesce.",
            "date": "2010-11-03T18:38:58.502+0000",
            "id": 6
        },
        {
            "author": "Jason Rutherglen",
            "body": "Another use case that can be wacky is if commit is called and a merge is finishing before or after, in that case all (point-in-time) deletes will have been applied by commit, however do we want to clear all per-segment deletes at the end of commit?  This would blank out deletes being applied by the merge, most of which should be cleared out, however if new deletes arrived during the commit (is this possible?), then we want these to be attached to segments and not lost.  I guess we want to DW sync'd clear out deletes in the applyDeletesAll method.  ADA will apply those deletes, any incoming will queue up and be shuffled around.",
            "date": "2010-11-03T20:07:31.022+0000",
            "id": 7
        },
        {
            "author": "Jason Rutherglen",
            "body": "In my head at least I think the concurrency issues are worked\nout in this patch. We're not taking into account recency of\ndeletes as I'm not sure it matters. DW applyDeletesToSegments\ntakes care of the coalescing of segment deletes as this is a\nsynced DW method called by a synced IW method, meaning\nnothing'll be changing anywhere, so we're good with the possible\nconcurrency issues. I'm still a little worried about concurrent\nincoming deleted terms/queries, however those can't be added\nuntil after a successful ADTS call due to the DW sync. \n\nGuess it's time for the complete tests to run.",
            "date": "2010-11-03T22:48:53.516+0000",
            "id": 8
        },
        {
            "author": "Jason Rutherglen",
            "body": "There's an issue in that we're redundantly applying deletes in the applyDeletesAll case because the deletes may have already been applied to a segment when a merge happened, ie, by applyDeletesToSegments.  In the ADA case we need to use applyDeletesToSegments up to the segment point when the buffered deletes can be used.  ",
            "date": "2010-11-04T01:00:29.699+0000",
            "id": 9
        },
        {
            "author": "Jason Rutherglen",
            "body": "This brings up another issue which is we're blindly iterating over docs in a segment reader to delete, even if we can know ahead of time that the reader's docs are going to exceed the term/query's docid-upto (from the max doc of the reader).  In applyDeletes we're opening a term docs iterator, though I think we're breaking at the first doc and moving on if the docid-upto is exceeded.  This term docs iterator opening can be skipped.",
            "date": "2010-11-04T01:09:26.498+0000",
            "id": 10
        },
        {
            "author": "Jason Rutherglen",
            "body": "Here's a nice little checkpoint with more tests passing.  \n\n* A last known segment is recorded, which is the last segment seen when\nadding a delete term/query per segment. This is for a applyDeletesAll\ncheck to ensure a given query/term has not already been applied to a\nsegment. If a term/query exists in the per-segment deletes and is in\ndeletesFlushed, we delete, unless we're beyond the last known segment, at\nwhich point we simply delete (adhering of course to the docid-upto).\n\n* In the interest of accuracy I nixed lastSegmentIndex in favor of\nlastSegmentInfo which is easier for debugging and implementation when\nsegments are shuffled around and/or removed/added. There's not too much of\na penalty in terms of performance. \n\n* org.apache.lucene.index tests pass\n\n* I need to address the applying deletes only on readers within the\ndocid-upto per term/query, perhaps that's best left to a different Jira\nissue.\n\n* Still not committable as it needs cleaning up, complete unit tests, who\nknows what else.",
            "date": "2010-11-04T22:52:42.553+0000",
            "id": 11
        },
        {
            "author": "Jason Rutherglen",
            "body": "All tests pass except org.apache.lucene.index.TestIndexWriterMergePolicy testMaxBufferedDocsChange.  Odd.  I'm looking into this.\n\n{code}\n[junit] junit.framework.AssertionFailedError: maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10; \nsegs=_65:c5950 _5t:c10->_32 _5u:c10->_32 _5v:c10->_32 _5w:c10->_32 _5x:c10->_32 _5y:c10->_32 _5z:c10->_32 _60:c10->_32 _61:c10->_32 _62:c3->_32 _64:c7->_62\n{code}\n\nAlso, in IW deleteDocument(*) we're calling a new method, getSegmentInfos which is sync'ed on IW.  Maybe we should use an atomic reference to a read only segment infos instead?",
            "date": "2010-11-06T03:21:00.419+0000",
            "id": 12
        },
        {
            "author": "Jason Rutherglen",
            "body": "Sorry, spoke too soon, I made a small change to not redundantly delete, in apply deletes all and TestStressIndexing2 is breaking.  I think we need to \"push\" segment infos changes to DW as they happen.  I'm guessing that segment infos are being shuffled around and so the infos passed into DW in IW deleteDoc methods may be out of date by the time deletes are attached to segments.  Hopefully there aren't any lurking deadlock issues with this.",
            "date": "2010-11-06T05:05:31.063+0000",
            "id": 13
        },
        {
            "author": "Jason Rutherglen",
            "body": "Pushing the segment infos seems to have cleared up some of the tests failing, however intermittently (1/4 of the time) there's the one below.\n\nI'm going to re-add lastSegmentInfo/Index, and assert that if we're not using it, that the deletes obtained from the segmentinfo -> deletes map is the same.  \n\n{code}\n[junit] Testsuite: org.apache.lucene.index.TestStressIndexing2\n    [junit] Testcase: testRandom(org.apache.lucene.index.TestStressIndexing2):\tFAILED\n    [junit] expected:<12> but was:<11>\n    [junit] junit.framework.AssertionFailedError: expected:<12> but was:<11>\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:878)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:844)\n    [junit] \tat org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:278)\n    [junit] \tat org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:271)\n    [junit] \tat org.apache.lucene.index.TestStressIndexing2.testRandom(TestStressIndexing2.java:89)\n{code}",
            "date": "2010-11-06T21:59:20.512+0000",
            "id": 14
        },
        {
            "author": "Jason Rutherglen",
            "body": "I wasn't coalescing the merged segment's deletes, with that implemented, TestStressIndexing2 ran successfully 49 of 50 times.  Below is the error:\n\n{code}\n[junit] Testsuite: org.apache.lucene.index.TestStressIndexing2\n    [junit] Testcase: testMultiConfig(org.apache.lucene.index.TestStressIndexing2):\tFAILED\n    [junit] expected:<5> but was:<4>\n    [junit] junit.framework.AssertionFailedError: expected:<5> but was:<4>\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:878)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:844)\n    [junit] \tat org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:278)\n    [junit] \tat org.apache.lucene.index.TestStressIndexing2.verifyEquals(TestStressIndexing2.java:271)\n    [junit] \tat org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:115)\n{code}",
            "date": "2010-11-06T22:25:51.322+0000",
            "id": 15
        },
        {
            "author": "Jason Rutherglen",
            "body": "Putting a sync on DW block around the bulk of the segment alterations in IW commitMerge seems to have quelled the TestStressIndexing2 test failures.  Nice.",
            "date": "2010-11-06T22:37:26.283+0000",
            "id": 16
        },
        {
            "author": "Jason Rutherglen",
            "body": "Here's a check point patch before I re-add lastSegmentInfo/Index.  All tests pass except for what's below.  I'm guessing segments with all docs deleted, are deleted before the test expects.\n\n{code}\n[junit] Testcase: testCommitThreadSafety(org.apache.lucene.index.TestIndexWriter):\tFAILED\n    [junit] \n    [junit] junit.framework.AssertionFailedError: \n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:878)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:844)\n    [junit] \tat org.apache.lucene.index.TestIndexWriter.testCommitThreadSafety(TestIndexWriter.java:4699)\n    [junit] \n    [junit] \n    [junit] Testcase: testCommitThreadSafety(org.apache.lucene.index.TestIndexWriter):\tFAILED\n    [junit] Some threads threw uncaught exceptions!\n    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:878)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:844)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:437)\n    [junit] \n    [junit] \n    [junit] Tests run: 116, Failures: 2, Errors: 0, Time elapsed: 159.577 sec\n    [junit] \n    [junit] ------------- Standard Output ---------------\n    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=1826133140332330367:8102643307925777745\n    [junit] NOTE: test params are: codec=MockFixedIntBlock(blockSize=564), locale=es_CR, timezone=Asia/Urumqi\n    [junit] ------------- ---------------- ---------------\n    [junit] ------------- Standard Error -----------------\n    [junit] The following exceptions were thrown by threads:\n    [junit] *** Thread: Thread-1106 ***\n    [junit] java.lang.RuntimeException: java.lang.AssertionError: term=f:0_8; r=DirectoryReader(_0:c1  _1:c1  _2:c1  _3:c1  _4:c1  _5:c1  _6:c1  _7:c2  _8:c4 ) expected:<1> but was:<0>\n    [junit] \tat org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4690)\n    [junit] Caused by: java.lang.AssertionError: term=f:0_8; r=DirectoryReader(_0:c1  _1:c1  _2:c1  _3:c1  _4:c1  _5:c1  _6:c1  _7:c2  _8:c4 ) expected:<1> but was:<0>\n    [junit] \tat org.junit.Assert.fail(Assert.java:91)\n    [junit] \tat org.junit.Assert.failNotEquals(Assert.java:645)\n    [junit] \tat org.junit.Assert.assertEquals(Assert.java:126)\n    [junit] \tat org.junit.Assert.assertEquals(Assert.java:470)\n    [junit] \tat org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4684)\n    [junit] NOTE: all tests run in this JVM:\n    [junit] [TestMockAnalyzer, TestByteSlices, TestFilterIndexReader, TestIndexFileDeleter, TestIndexReaderClone, TestIndexReaderReopen, TestIndexWriter]\n{code}",
            "date": "2010-11-06T23:10:45.787+0000",
            "id": 17
        },
        {
            "author": "Jason Rutherglen",
            "body": "I placed (for now) the segment deletes directly into the segment info object.  There's applied term/queries sets which are checked against when apply deletes all is called.  All tests pass except for TestTransactions and TestPersistentSnapshotDeletionPolicy only because of an assertion check I added, that the last segment info is in fact in the newly pushed segment infos.  I think in both cases segment infos is being altered in IW in a place where the segment infos isn't being pushed, yet.  I wanted to checkpoint this though as it's a fairly well working at this point, including the last segment info/index, which is can be turned on or off via a static variable.  ",
            "date": "2010-11-07T22:34:52.183+0000",
            "id": 18
        },
        {
            "author": "Jason Rutherglen",
            "body": "Everything passes, except for tests that involve IW rollback.  We need to be able to rollback the last segment info/index in DW, however I'm not sure how we want to do that quite yet.",
            "date": "2010-11-08T01:02:56.986+0000",
            "id": 19
        },
        {
            "author": "Jason Rutherglen",
            "body": "In DW abort (called by IW rollbackInternal) we should be able to simply clear all per segment pending deletes, however, I'm not sure we can do that, in fact, if we have applied deletes for a merge, then we rollback, we can't undo those deletes thereby breaking our current rollback model?",
            "date": "2010-11-08T01:31:08.577+0000",
            "id": 20
        },
        {
            "author": "Jason Rutherglen",
            "body": "Here's an uncleaned up cut with all tests passing. I nulled out\nthe lastSegmentInfo on abort which fixes the my own assertion\nthat was causing the rollback tests to not pass. I don't know if\nthis is cheating or not yet just to get the tests to pass.",
            "date": "2010-11-08T01:55:25.349+0000",
            "id": 21
        },
        {
            "author": "Jason Rutherglen",
            "body": "I'm running test-core multiple times and am seeing some lurking test\nfailures (thanks to the randomized tests that have been recently added).\nI'm guessing they're related to the syncs on IW and DW not being in \"sync\"\nsome of the time. \n\nI will clean up the patch so that others may properly review it and\nhopefully we can figure out what's going on. ",
            "date": "2010-11-08T16:46:17.065+0000",
            "id": 22
        },
        {
            "author": "Jason Rutherglen",
            "body": "Here's a cleaned up patch, please take a look.  I ran 'ant test-core' 5 times with no failures, however running the below several times does eventually produce a failure.\n\nant test-core -Dtestcase=TestThreadedOptimize -Dtestmethod=testThreadedOptimize -Dtests.seed=1547315783637080859:5267275843141383546\n\nant test-core -Dtestcase=TestIndexWriterMergePolicy -Dtestmethod=testMaxBufferedDocsChange -Dtests.seed=7382971652679988823:-6672235304390823521",
            "date": "2010-11-08T21:29:50.986+0000",
            "id": 23
        },
        {
            "author": "Jason Rutherglen",
            "body": "The problem could be that IW deleteDocument is not synced on IW,\nwhen I tried adding the sync, there was deadlock perhaps from DW\nwaitReady. We could be adding pending deletes to segments that\nare not quite current because we're not adding them in an IW\nsync block.",
            "date": "2010-11-08T23:25:22.479+0000",
            "id": 24
        },
        {
            "author": "Jason Rutherglen",
            "body": "Ok, TestThreadedOptimize works when the DW sync'ed pushSegmentInfos method\nisn't called anymore (no extra per-segment deleting is going on), and stops\nworking when pushSegmentInfos is turned back on. Something about the sync\non DW is causing a problem.  Hmm... We need another way to pass segment\ninfos around consistently. ",
            "date": "2010-11-09T04:02:05.102+0000",
            "id": 25
        },
        {
            "author": "Jason Rutherglen",
            "body": "I think I've taken LUCENE-2680 as far as I can, though I'll\nprobably add some more assertions in there for good measure,\nsuch as whether or not a delete has in fact been applied etc. It\nseems to be working though again I should add more assertions to\nthat effect. I think there's a niggling sync issue in there as\nTestThreadedOptimize only fails when I try to run it 100s of\ntimes. I think the sync on DW is causing a wait notify to be\nmissed or skipped or something like that, as occasionally the\nisOptimized call fails as well. This is likely related to the\nappearance of deletes not being applied to segment(s) as\nevidenced by the difference in the actual doc count and the\nexpected doc count.\n\nBelow is the most common assertion failure. Maybe I should\nupload my patch that includes a method that iterates 200 times\non testThreadedOptimize?\n\n{code}\n[junit] ------------- ---------------- ---------------\n    [junit] Testsuite: org.apache.lucene.index.TestThreadedOptimize\n    [junit] Testcase: testThreadedOptimize(org.apache.lucene.index.TestThreadedOptimize):\tFAILED\n    [junit] expected:<248> but was:<266>\n    [junit] junit.framework.AssertionFailedError: expected:<248> but was:<266>\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:878)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:844)\n    [junit] \tat org.apache.lucene.index.TestThreadedOptimize.runTest(TestThreadedOptimize.java:119)\n    [junit] \tat org.apache.lucene.index.TestThreadedOptimize.testThreadedOptimize(TestThreadedOptimize.java:141)\n    [junit] \n    [junit] \n    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 1.748 sec\n{code}",
            "date": "2010-11-10T17:23:23.669+0000",
            "id": 26
        },
        {
            "author": "Jason Rutherglen",
            "body": "I think I've isolated this test failure to recording the applied deletes.\nBecause we're using last segment index/info, I was adding deletes that may\nor may not have been applied to a particular segment to the last segment\ninfo. I'm not sure what to do in this case as if we record the applied\nterms per segment, but keep the pending terms in last segment info, we're\neffectively not gaining anything from using last segment info because\nwe're then recording all of the terms per-segment anyways. In fact, this\nis how I've isolated that this is the issue, I simply removed the usage of\nlast segment info, and instead went to maintaining pending deletes\nper-segment. I'll give it some thought.\n\nIn conclusion, when deletes are recorded per-segment with no last segment\ninfo, the test passes after 200 times. ",
            "date": "2010-11-10T23:53:05.048+0000",
            "id": 27
        },
        {
            "author": "Jason Rutherglen",
            "body": "Alright, we needed to clone the per-segment pending deletes in the\n_mergeInit prior to the merge, like cloning the SRs. There were other\nterms arriving after they were applied to a merge, then the coalescing of\napplied deletes was incorrect. I believe that this was the remaining\nlingering issue. The previous failures seem to have gone away, I ran the\ntest 400 times. I'll upload a new patch shortly.",
            "date": "2010-11-11T07:42:32.884+0000",
            "id": 28
        },
        {
            "author": "Jason Rutherglen",
            "body": "I'm still seeing the error no matter what I do. Sometimes the index is not\noptimized, and sometimes there are too many docs. It requires thousands of\niterations to provoke either test error. Perhaps it's simply related to\nmerges that are scheduled but IW close isn't waiting on properly.",
            "date": "2010-11-11T18:54:00.932+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "TestThreadedOptimize is a known intermittent failure -- I'm trying to track it down!!  (LUCENE-2618)",
            "date": "2010-11-11T18:58:38.707+0000",
            "id": 30
        },
        {
            "author": "Jason Rutherglen",
            "body": "Ah, nice, I should have looked for previous intermittent failures via Jira.  ",
            "date": "2010-11-11T19:07:59.237+0000",
            "id": 31
        },
        {
            "author": "Jason Rutherglen",
            "body": "Now that the intermittent failures have been successfully dealt with, ie,\nLUCENE-2618, LUCENE-2576, and LUCENE-2118, I'll merge this patch to trunk,\nthen it's probably time for benchmarking. That'll probably include\nsomething like indexing, then updating many documents and comparing the\nindex time vs. trunk? ",
            "date": "2010-11-15T22:09:05.222+0000",
            "id": 32
        },
        {
            "author": "Jason Rutherglen",
            "body": "Straight indexing and deleting will probably not show much of an\nimprovement from this patch. In trunk, apply deletes (all) is called on\nall segments prior to a merge, so we need a synthetic way to measure the\nimprovement. One way is to monitor the merge time of small segments (of an\nindex with many deletes, and many existing large segments) with this patch\nvs. trunk. This'll show that this patch in that case is faster (because\nwe're only applying deletes to the smaller segments). \n\nI think I'll add a merge start time variable to OneMerge that'll be set in\nmergeinit. The var could also be useful for the info stream debug log. The\nbenchmark will simply print out the merge times (which'll be manufactured\nsynthetically). ",
            "date": "2010-11-16T15:51:27.476+0000",
            "id": 33
        },
        {
            "author": "Michael McCandless",
            "body": "Why do we still have deletesFlushed?  And why do we still need to\nremap docIDs on merge?  I thought with this new approach the docIDUpto\nfor each buffered delete Term/Query would be a local docID to that\nsegment?\n\nOn flush the deletesInRAM should be carried directly over to the\nsegmentDeletes, and there shouldn't be a deletesFlushed?\n\nA few other small things:\n\n  * You can use SegmentInfos.clone to copy the segment infos? (it\n    makes a deep copy)\n\n  * SegmentDeletes.clearAll() need not iterate through the\n    terms/queries to subtract the RAM used?  Ie just multiply by\n    .size() instead and make one call to deduct RAM used?\n\n  * The SegmentDeletes use less than BYTES_PER_DEL_TERM because it's a\n    simple HashSet not a HashMap?  Ie we are over-counting RAM used\n    now?  (Same for by query)\n\n  * Can we store segment's deletes elsewhere?  The SegmentInfo should\n    be a lightweight class... eg it's used by DirectoryReader to read\n    the index, and if it's read only DirectoryReader there's no need\n    for it to allocate the SegmentDeletes?  These data structures\n    should only be held by IndexWriter/DocumentsWriter.\n\n  * Do we really need to track appliedTerms/appliedQueries?  Ie is\n    this just an optimization so that if the caller deletes by the\n    Term/Query again we know to skip it?  Seems unnecessary if that's\n    all...\n",
            "date": "2010-11-17T11:47:15.438+0000",
            "id": 34
        },
        {
            "author": "Michael McCandless",
            "body": "Also: why are we tracking the last segment info/index?  Ie, this should only be necessary on cutover to DWPT right?  Because effectively today we have only a single \"DWPT\"?",
            "date": "2010-11-17T13:32:55.463+0000",
            "id": 35
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}Why do we still have deletesFlushed? And why do we still need to\nremap docIDs on merge? I thought with this new approach the docIDUpto for\neach buffered delete Term/Query would be a local docID to that\nsegment?{quote}\n\nDeletes flushed can be removed if we store the docid-upto per segment.\nThen we'll go back to having a hash map of deletes. \n\n{quote}The SegmentDeletes use less than BYTES_PER_DEL_TERM because it's a\nsimple HashSet not a HashMap? Ie we are over-counting RAM used now? (Same\nfor by query){quote}\n\nIntuitively, yes, however here's the constructor of hash set:\n\n{code} public HashSet() { map = new HashMap<E,Object>(); } {code}\n\nbq. why are we tracking the last segment info/index?\n\nI thought last segment was supposed to be used to mark the last segment of\na commit/flush. This way we save on the hash(set,map) space on the\nsegments upto the last segment when the commit occurred.\n\n{quote}Can we store segment's deletes elsewhere?{quote}\n\nWe can, however I had to minimize places in the code that were potentially\ncausing errors (trying to reduce the problem set, which helped locate the\nintermittent exceptions), syncing segment infos with the per-segment\ndeletes was one was one of those places. That and I thought it'd be worth\na try simplify (at the expense of breaking the unstated intention of the\nSI class).\n\n{quote}Do we really need to track appliedTerms/appliedQueries? Ie is this\njust an optimization so that if the caller deletes by the Term/Query again\nwe know to skip it? {quote}\n\nYes to the 2nd question. Why would we want to try deleting multiple times?\nThe cost is the terms dictionary lookup which you're saying is in the\nnoise? I think potentially cracking open a query again could be costly in\ncases where the query is indeed expensive.\n\n{quote}not iterate through the terms/queries to subtract the RAM\nused?{quote}\n\nWell, the RAM usage tracking can't be completely defined until we finish\nhow we're storing the terms/queries. ",
            "date": "2010-11-17T15:39:53.545+0000",
            "id": 36
        },
        {
            "author": "Michael McCandless",
            "body": "\n{quote}\nDeletes flushed can be removed if we store the docid-upto per segment.\nThen we'll go back to having a hash map of deletes.\n{quote}\n\nI think we should do this?\n\nIe, each flushed segment stores the map of del Term/Query to\ndocid-upto, where that docid-upto is private to the segment (no\nremapping on merges needed).\n\nWhen it's time to apply deletes to about-to-be-merged segments, we\nmust apply all \"future\" segments deletions unconditionally to each\nsegment, and then conditionally (respecting the local docid-upto)\napply that segment's deletions.\n\n{quote}\nIntuitively, yes, however here's the constructor of hash set:\n\n{noformat}\npublic HashSet() { map = new HashMap<E,Object>(); }\n{noformat}\n{quote}\n\nUgh I forgot about that.  Is that still true?  That's awful.\n\n{quote}\nbq. why are we tracking the last segment info/index?\n\nI thought last segment was supposed to be used to mark the last segment of\na commit/flush. This way we save on the hash(set,map) space on the\nsegments upto the last segment when the commit occurred.\n{quote}\n\nHmm... I think lastSegment was needed only for the multiple DWPT\ncase, to record the last segment already flushed in the index as of\nwhen that DWPT was created.  This is so we know \"going back\" when we\ncan start unconditionally apply the buffered delete term.\n\nWith the single DWPT we effectively have today isn't last segment\nalways going to be what we just flushed?  (Or null if we haven't yet\ndone a flush in the current session).\n\n{quote}\nbq. Do we really need to track appliedTerms/appliedQueries? Ie is this just an optimization so that if the caller deletes by the Term/Query again we know to skip it?\n\nYes to the 2nd question. Why would we want to try deleting multiple times?\nThe cost is the terms dictionary lookup which you're saying is in the\nnoise? I think potentially cracking open a query again could be costly in\ncases where the query is indeed expensive.\n{quote}\n\nI'm saying this is unlikely to be worthwhile way to spend RAM.\n\nEG most apps wouldn't delete by same term again, like they'd\n\"typically\" go and process a big batch of docs, deleting by an id\nfield and adding the new version of the doc, where a given id is seen\nonly once in this session, and then IW is committed/closed?\n",
            "date": "2010-11-17T18:25:38.363+0000",
            "id": 37
        },
        {
            "author": "Jason Rutherglen",
            "body": "DWPT deletes has perhaps confused this issue a little bit. \n\n{quote}Tracking per-segment would be easier but I worry about indices that\nhave large numbers of segments... eg w/ a large mergeFactor and frequent\nflushing you can get very many segments.{quote}\n\nI think we may be back tracking here as I had earlier proposed we simply\nstore each term/query in a map per segment, however I think that was nixed\nin favor of last segment + deletes per segment afterwards. We're not\nworried about the cost of storing pending deletes in a map per segment\nanymore?\n\n{quote}With the single DWPT we effectively have today isn't last segment\nalways going to be what we just flushed? (Or null if we haven't yet done a\nflush in the current session).{quote}\n\nPretty much. \n\n{quote}EG most apps wouldn't delete by same term again, like they'd\n\"typically\" go and process a big batch of docs, deleting by an id field\nand adding the new version of the doc, where a given id is seen only once\nin this session, and then IW is committed/closed?{quote}\n\nIn an extreme RT app that uses Lucene like a database, it could in fact\nupdate a doc many times, then we'd start accumulating and deleting the\nsame ID over and over again. However in the straight batch indexing model\noutlined, that is unlikely to happen. \n\n{quote}When it's time to apply deletes to about-to-be-merged segments, we\nmust apply all \"future\" segments deletions unconditionally to each\nsegment, and then conditionally (respecting the local docid-upto) apply\nthat segment's deletions.{quote}\n\nI'll use this as the go-ahead design then.\n\nbq. Is that still true?\n\nThat's from Java 1.6.",
            "date": "2010-11-17T19:01:34.933+0000",
            "id": 38
        },
        {
            "author": "Jason Rutherglen",
            "body": "Additionally we need to decide how accounting'll work for\nmaxBufferedDeleteTerms. We won't have a centralized place to keep track of\nthe number of terms, and the unique term count in aggregate over many\nsegments could be a little too time consuming calculate in a method like\ndoApplyDeletes. An alternative is to maintain a global unique term count,\nsuch that when a term is added, every other per-segment deletes is checked\nfor that term, and if it's not already been tallied, we increment the number\nof buffered terms.",
            "date": "2010-11-17T21:54:30.458+0000",
            "id": 39
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nI think we may be back tracking here as I had earlier proposed we simply\nstore each term/query in a map per segment, however I think that was nixed\nin favor of last segment + deletes per segment afterwards. We're not\nworried about the cost of storing pending deletes in a map per segment\nanymore?\n{quote}\n\nOK sorry now I remember.\n\nHmm but, my objection then was to carrying all deletes backward to all\nsegments?\n\nWhereas now I think what we can do is only record the deletions that\nwere added when that segment was a RAM buffer, in its pending deletes\nmap?  This should be fine, since we aren't storing a single deletion\nin multiple places (well, until DWPTs anyway).  It's just that on\napplying deletes to a segment because it's about to be merged we have\nto do a merge sort of the buffered deletes all \"future\" segments.\n\nBTW it could also be possible to not necessarily apply deletes when a\nsegment is merged; eg if there are few enough deletes it may not be\nworthwhile.  But we can leave that to another issue.\n\n{quote}\nAdditionally we need to decide how accounting'll work for\nmaxBufferedDeleteTerms. We won't have a centralized place to keep track of\nthe number of terms, and the unique term count in aggregate over many\nsegments could be a little too time consuming calculate in a method like\ndoApplyDeletes. An alternative is to maintain a global unique term count,\nsuch that when a term is added, every other per-segment deletes is checked\nfor that term, and if it's not already been tallied, we increment the number\nof buffered terms.\n{quote}\n\nMaybe we should change the definition to be total number of pending\ndelete term/queries?  (Ie, not dedup'd across segments).  This seems\nreasonable since w/ this new approach the RAM consumed is in\nproportion to that total number and not to dedup'd count?\n",
            "date": "2010-11-17T23:12:04.220+0000",
            "id": 40
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}Maybe we should change the definition to be total number of pending\ndelete term/queries? {quote}\n\nLets go with this, as even though we could record the total unique term\ncount, the approach outlined is more conservative.\n\n{quote}I think what we can do is only record the deletions that were added\nwhen that segment was a RAM buffer, in its pending deletes map{quote}\n\nOk, sounds like a design that'll work well.",
            "date": "2010-11-18T02:45:23.321+0000",
            "id": 41
        },
        {
            "author": "Jason Rutherglen",
            "body": "Flush deletes equals true means that all deletes are applied, however when it's false, that means we're moving the pending deletes into the newly flushed segment, as is, with no docId-upto remapping.  ",
            "date": "2010-11-18T03:29:04.392+0000",
            "id": 42
        },
        {
            "author": "Jason Rutherglen",
            "body": "We can \"upgrade\" to an int[] from an ArrayList<Integer> for the aborted docs.",
            "date": "2010-11-18T03:42:21.824+0000",
            "id": 43
        },
        {
            "author": "Jason Rutherglen",
            "body": "I'm seeing the following error which is probably triggered by the new per-segment deletes code, however also could be related to the recent CFS format changes?\n\n{code}\nMockDirectoryWrapper: cannot close: there are still open files: {_0.cfs=1, _1.cfs=1}\n    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_0.cfs=1, _1.cfs=1}\n    [junit] \tat org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:395)\n    [junit] \tat org.apache.lucene.index.TestIndexReader.testReopenChangeReadonly(TestIndexReader.java:1717)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:921)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:859)\n    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput\n    [junit] \tat org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:350)\n    [junit] \tat org.apache.lucene.store.Directory.openInput(Directory.java:138)\n    [junit] \tat org.apache.lucene.index.CompoundFileReader.<init>(CompoundFileReader.java:67)\n    [junit] \tat org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:121)\n    [junit] \tat org.apache.lucene.index.SegmentReader.get(SegmentReader.java:527)\n    [junit] \tat org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:628)\n    [junit] \tat org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:603)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:1081)\n    [junit] \tat org.apache.lucene.index.IndexWriter.applyDeletesAll(IndexWriter.java:4300)\n    [junit] \tat org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3440)\n    [junit] \tat org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3276)\n    [junit] \tat org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3266)\n    [junit] \tat org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:3131)\n    [junit] \tat org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:3206)\n{code}",
            "date": "2010-11-19T23:57:34.596+0000",
            "id": 44
        },
        {
            "author": "Jason Rutherglen",
            "body": "In trying to implement the per-segment deletes I encountered this error\nfrom TestStressIndexing2. So I started over with a new checkout of trunk,\nand started slowly adding in the per-segment code, running\nTestStressIndexing2 as each part was added. The attached patch breaks,\nthough the deletes are still using the old code. There's clearly some kind\nof synchronization issue, though nothing esoteric has been added, yikes.\n\n{code}\n[junit] Testcase: testMultiConfigMany(org.apache.lucene.index.TestStressIndexing2):\tFAILED\n    [junit] expected:<20> but was:<19>\n    [junit] junit.framework.AssertionFailedError: expected:<20> but was:<19>\n{code}",
            "date": "2010-11-22T00:19:53.837+0000",
            "id": 45
        },
        {
            "author": "Jason Rutherglen",
            "body": "Running TestStressIndexing2 500 times on trunk causes this error which is probably intermittent:\n\n{code}\n[junit] Testsuite: org.apache.lucene.index.TestStressIndexing2\n    [junit] Testcase: testMultiConfigMany(org.apache.lucene.index.TestStressIndexing2):\tCaused an ERROR\n    [junit] Array index out of range: 0\n    [junit] java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 0\n    [junit] \tat java.util.Vector.get(Vector.java:721)\n    [junit] \tat org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:1049)\n    [junit] \tat org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:4291)\n    [junit] \tat org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3444)\n    [junit] \tat org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3279)\n    [junit] \tat org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3269)\n    [junit] \tat org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1760)\n    [junit] \tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1723)\n    [junit] \tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1687)\n    [junit] \tat org.apache.lucene.index.TestStressIndexing2.indexRandom(TestStressIndexing2.java:233)\n    [junit] \tat org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:123)\n    [junit] \tat org.apache.lucene.index.TestStressIndexing2.testMultiConfigMany(TestStressIndexing2.java:97)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:950)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:888)\n{code}",
            "date": "2010-11-22T01:58:28.148+0000",
            "id": 46
        },
        {
            "author": "Jason Rutherglen",
            "body": "The above isn't on trunk, I misread the screen.",
            "date": "2010-11-22T02:10:33.442+0000",
            "id": 47
        },
        {
            "author": "Jason Rutherglen",
            "body": "I've isolated the mismatch in num docs between the CMS vs. SMS generated\nindexes to applying the deletes to the merging segments (whereas currently\nwe were/are not applying deletes to merging segments and\nTestStressIndexing2 passes). Assuming the deletes are being applied\ncorrectly to the merging segments, perhaps the logic of gathering up\nforward segment deletes is incorrect somehow in the concurrent merge case.\nWhen deletes were held in a map per segment, this test was passing. ",
            "date": "2010-11-22T02:50:55.591+0000",
            "id": 48
        },
        {
            "author": "Jason Rutherglen",
            "body": "A test to see if the problem is the deletes per-segment go forward logic is to iterate over the deletes flushed map using the docid-upto to stay within the boundaries of the segment(s) being merged.",
            "date": "2010-11-22T03:25:55.652+0000",
            "id": 49
        },
        {
            "author": "Michael McCandless",
            "body": "What a nice small patch :)\n\nI think the getDeletesSegmentsForward shouldn't be folding in the\ndeletesInRAM?  Ie, that newly flushed info will have carried over the\nprevious deletes in RAM?\n\nI think pushDeletes/pushSegmentDeletes should be merged, and we should\nnuke DocumentsWriter.deletesFlushed?  Ie, we should push directly from\ndeletesInRAM to the new SegmentInfo?  EG you are now pushing all\ndeletesFlushed into the new SegmentInfo when actually you should only\npush the deletes for that one segment.\n\nWe shouldn't do the remap deletes anymore.  We can remove\nDocumentsWriter.get/set/updateFlushedDocCount too.\n\nHmm... so what are we supposed to do if someone opens IW, does a bunch\nof deletes, then commits?  Ie flushDocs is false, so there's no new\nSegmentInfo.  I think in this case we can stick the deletes against\nthe last segment in the index, with the docidUpto set to the maxDoc()\nof that segment?\n",
            "date": "2010-11-24T18:53:20.899+0000",
            "id": 50
        },
        {
            "author": "Jason Rutherglen",
            "body": "* I added pushDeletesLastSegment to doc writer\n\n* Deletes flushed is gone, only deletesInRAM exists\n\n* In the apply merge deletes case, won't we want to add deletesInRAM in\nthe getForwardDeletes method?\n\n* The TestStressIndexing2 test still fails so there is still something\nincorrect.\n\n* Though for the failing unit test it does not matter, we need to figure\nout a solution for the pending doc ids deletions, eg, they can't simply\ntransferred around, they probably need to be applied as soon as possible.\nOtherwise they require remapping. ",
            "date": "2010-11-27T19:44:21.298+0000",
            "id": 51
        },
        {
            "author": "Michael McCandless",
            "body": "bq. In the apply merge deletes case, won't we want to add deletesInRAM in the getForwardDeletes method?\n\nNo, we can't add those deletes until the current buffered segment is successfully flushed.\n\nEg, say the segment hits a disk full on flush, and DocsWriter aborts (discards all buffered docs/deletions from that segment).  If we included these deletesInRAM when applying deletes then suddenly the app will see that some deletes were applied yet the added documents were not.  So on disk full during flush, calls to .updateDocument may wind up deleting the old doc but not adding the new one.\n\nSo we need to keep them segregated for proper error case semantics.\n\n{quote}\nThough for the failing unit test it does not matter, we need to figure\nout a solution for the pending doc ids deletions, eg, they can't simply\ntransferred around, they probably need to be applied as soon as possible.\nOtherwise they require remapping.\n{quote}\n\nHmm why must we remap?  Can't we carry these buffered deleteByDocIDs along with the segment?  The docIDs would be the segment's docIDs (ie no base added) so no shifting is needed?\n\nThese deleted docIDs would only apply to the current segment, ie would not be included in getForwardDeletes?",
            "date": "2010-11-27T19:54:25.831+0000",
            "id": 52
        },
        {
            "author": "Jason Rutherglen",
            "body": "I made the changes listed above, ie, docids and deletesInRAM aren't included in getForwardDeletes.  However TestStressIndexing2 still fails, and the numbers are still off significantly which probably indicates it's not a synchronization issue.",
            "date": "2010-11-27T20:08:01.152+0000",
            "id": 53
        },
        {
            "author": "Michael McCandless",
            "body": "So nice to see remapDeletes deleted!\n\n  * Don't forget to remove DocumentsWriter.get/set/updateFlushedDocCount too.\n\n  * Can you move the deletes out of SegmentInfo?  We can just use a\n    Map<SegmentInfo,BufferedDeletes>?  But remember to delete segments\n    from the map once we commit the merge...\n\n  * I think DocsWriter shouldn't hold onto the SegmentInfos; we should\n    pass it in to only those methods that need it.  That SegmentInfos\n    is protected under IW's monitor so it makes me nervous if it's\n    also a member on DW.\n\n  * Hmm we're no longer accounting for RAM usage of per-segment\n    deletes?  I think we need an AtomicInt, which we incr w/ RAM used\n    on pushing deletes into a segment, and decr on clearing?\n\n  * The change to the message(...) in DW.applyDeletes is wrong (ie\n    switching to deletesInRAM); I think we should just remove the\n    details, ie so it says \"applying deletes on N segments\"?  But then\n    add a more detailed message per-segment with the aggregated\n    (forward) deletes details?\n\n  * I think we should move this delete handling out of DW as much as\n    possible... that's really IW's role (DW is \"about\" flushing the\n    next segment, not tracking details associated with all other\n    segments in the index)\n\n  * Instead of adding pushDeletesLastSegment, can we just have IW call\n    pushDeletes(lastSegmentInfo)?\n\n  * Calling .getForwardDeletes inside the for loop iterating over the\n    infos is actually O(N^2) cost, and it could matter for\n    delete-intensive many-segment indices.  Can you change this,\n    instead, to walk the infos backwards, incrementally building up\n    the forward deletes to apply to each segment by adding in that\n    infos deletions?\n",
            "date": "2010-11-27T20:33:55.880+0000",
            "id": 54
        },
        {
            "author": "Jason Rutherglen",
            "body": "I guess you think the sync on doc writer is the cause of the\nTestStressIndexing2 unit test failure?\n\nbq. I think we should move this delete handling out of DW\n\nI agree, I originally took this approach however unit tests were failing\nwhen segment infos was passed directly into the apply deletes method(s).\nThis'll be the 2nd time however apparently the 3rd time's the charm.\n\nI'll make the changes and cross my fingers.",
            "date": "2010-11-28T22:49:22.161+0000",
            "id": 55
        },
        {
            "author": "Jason Rutherglen",
            "body": "I started on taking the approach of moving deletes to a SegmentDeletes class\nthat's a member of IW. Removing DW's addDeleteTerm is/was fairly trivial. \n\nIn moving deletes out of DW, how should we handle the bufferDeleteTerms sync on\nDW and the containing waitReady? The purpose of BDT is to check if RAM\nconsumption has reached it's peak, and if so, balance out the ram usage and/or\nflush pending deletes that are ram consuming. This is probably why deletes are\nintertwined with DW. We could change DW's BDT method though I'm loathe to\nchange the wait logic of DW for fear of causing a ripple effect of inexplicable\nunit test failures elsewhere.",
            "date": "2010-11-29T00:15:26.186+0000",
            "id": 56
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nI guess you think the sync on doc writer is the cause of the\nTestStressIndexing2 unit test failure?\n{quote}\n\nI'm not sure what's causing the failure, but, I think getting the net approach roughly right is the first goal, and then we see what's failing.\n\n{quote}\nbq. I think we should move this delete handling out of DW\n\nI agree, I originally took this approach however unit tests were failing\nwhen segment infos was passed directly into the apply deletes method(s).\nThis'll be the 2nd time however apparently the 3rd time's the charm.\n{quote}\n\nNot only moving the SegmentInfos out of DW as a member, but also move all the applyDeletes logic out.  Ie it should be IW that pulls readers from the pool, walks the merged del term/queries/per-seg docIDs and actually does the deletion.\n\nbq. In moving deletes out of DW, how should we handle the bufferDeleteTerms sync on DW and the containing waitReady?\n\nI think all the bufferDeleteX would move into IW, and timeToFlushDeletes. The RAM accounting can be done fully inside IW.\n\nThe waitReady(null) is there so that DW.pauseAllThreads also pauses any threads doing deletions.  But, in moving these methods to IW, we'd make them sync on IW (they are now sync'd on DW), which takes care of pausing these threads.",
            "date": "2010-11-29T10:30:28.350+0000",
            "id": 57
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. The waitReady(null) is there so that DW.pauseAllThreads also pauses any threads doing deletions\n\nwaitReady is used in getThreadState as well as bufferDeleteX, we may need to redundantly add it to SegmentDeletes?  Maybe not.  We'll be sync'ing on IW when adding deletions, that seems like it'll be OK.  \n\n{quote}in moving these methods to IW, we'd make them sync on IW (they are now sync'd on DW), which takes care of pausing these threads{quote}\n\nBecause we're sync'ing on IW we don't need to pause the indexing threads?  Ok this is because doFlushX is sync'd on IW.  \n\n{quote}The RAM accounting can be done fully inside IW.{quote}\n\nWell, inside of SegmentDeletes.",
            "date": "2010-11-29T20:58:35.213+0000",
            "id": 58
        },
        {
            "author": "Jason Rutherglen",
            "body": "This patch separates out most of the deletes storage and processing into a\nSegmentDeletes class. The segments are walked backwards to coalesce the pending\ndeletions. I think/hope this logic is correct. \n\nUpdate document is a bit tricky as we cannot sync on IW to insure correctness\nof del term addition, nor can we really sync inside of DW without probably\ncausing deadlock. When I simply delete the doc after adding it in IW,\nTestStressIndexing2 fails miserably with 0 num docs.",
            "date": "2010-11-30T19:12:33.082+0000",
            "id": 59
        },
        {
            "author": "Jason Rutherglen",
            "body": "Now we're returning the DocumentsWriterThreadState to IW to record the exact doc id as the del term limit.  TestStressIndexing2 fails but far less from the mark, eg, 1 when it does and actually passes some of the time.",
            "date": "2010-11-30T19:24:19.120+0000",
            "id": 60
        },
        {
            "author": "Jason Rutherglen",
            "body": "Here's a random guess, I think because with this patch we're applying deletes\nsometimes multiple times, whereas before we were applying all of them and\nclearing them out at once, there's a mismatch in terms of over/under-applying\ndeletes. Oddly when deletes are performed in _mergeInit on all segments vs.\nonly on the segments being merged, the former has a much higher success rate.\nThis is strange because all deletes will have been applied by the time\ncommit/getreader is called anyways. ",
            "date": "2010-11-30T22:48:19.132+0000",
            "id": 61
        },
        {
            "author": "Michael McCandless",
            "body": "\nOK I started from the last patch and iterated quite a bit.\n\nThe per-segment deletes are now working!  All tests pass.  It turned\nout to be a little more hairy than we thought because you also must\naccount for deletes that are pushed backwards onto a segment being\nmerged (eg due to a flush, or a merge just ahead) while that merge is\nstill running.\n\nI swapped the two classes -- SegmentDeletes tracks deletes for one\nsegment, while BufferedDeletes tracks deletes for all segments.\n\nI also wound up doing a fair amount of cleanup/refactoring on how\nDW/IW interact, I think a good step towards DWPT.  EG IW's flush is\nnow much smaller (could *almost* become unsync'd) since I moved\nflushing doc stores, building CFS, etc. down into DW.\n\nI added a new FlushControl class to manage (external to DW) triggering\nof flushing due to RAM, add doc count, buffered del count.  This way\nDWPT can share the single FlushControl instance in IW.\n",
            "date": "2010-12-08T19:44:32.349+0000",
            "id": 62
        },
        {
            "author": "Jason Rutherglen",
            "body": "When patching there are errors on IndexWriter.",
            "date": "2010-12-08T21:41:53.432+0000",
            "id": 63
        },
        {
            "author": "Michael McCandless",
            "body": "Woops, sorry about that Jason -- I wasn't fully updated.  Try this one?",
            "date": "2010-12-09T10:58:42.574+0000",
            "id": 64
        },
        {
            "author": "Jason Rutherglen",
            "body": "We're close, I think SegmentDeletes is missing?",
            "date": "2010-12-09T15:02:58.677+0000",
            "id": 65
        },
        {
            "author": "Michael McCandless",
            "body": "Ugh, OK new patch.  3rd time's a charm?",
            "date": "2010-12-09T15:14:40.215+0000",
            "id": 66
        },
        {
            "author": "Jason Rutherglen",
            "body": "The patch applied.\n\nOk, a likely cause of the TestStressIndexing2 failures was that when we're\nflushing deletes to the last segment (because a segment isn't being flushed),\nwe needed to move deletes also to the newly merged segment?\n\nIn the patch we've gone away from sync'ing on IW when deleting, which was a\nchallenge because we needed the sync on DW to properly wait on flushing threads\netc.",
            "date": "2010-12-09T16:38:25.189+0000",
            "id": 67
        },
        {
            "author": "Jason Rutherglen",
            "body": "All tests pass.",
            "date": "2010-12-09T17:20:41.531+0000",
            "id": 68
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nOk, a likely cause of the TestStressIndexing2 failures was that when we're\nflushing deletes to the last segment (because a segment isn't being flushed),\nwe needed to move deletes also to the newly merged segment?\n{quote}\n\nRight, and also the case where a merge just-ahead of you kicks off and dumps its merged deletes onto you.",
            "date": "2010-12-09T20:08:00.247+0000",
            "id": 69
        },
        {
            "author": "Michael McCandless",
            "body": "OK I committed this to trunk.\n\nSince it's a biggish change I'll hold off on back-porting to 3.x for now... let's let hudson chew on it some first.",
            "date": "2010-12-11T11:09:42.857+0000",
            "id": 70
        },
        {
            "author": "Grant Ingersoll",
            "body": "Bulk close for 3.1",
            "date": "2011-03-30T15:49:59.342+0000",
            "id": 71
        },
        {
            "author": "Roman Alekseenkov",
            "body": "Hey, is it something that was ported to 3.x, or not really?\n",
            "date": "2011-10-28T19:03:42.445+0000",
            "id": 72
        },
        {
            "author": "Robert Muir",
            "body": "Hi, this was backported since lucene 3.1",
            "date": "2011-10-28T19:14:51.320+0000",
            "id": 73
        },
        {
            "author": "Roman Alekseenkov",
            "body": "thank you, Robert\n\nI was asking because we are having issues with 3.4.0 where applyDeletes() takes an large amount of time on commit for 150GB index, and this is stopping all indexing threads. it looks like applyDeletes() is re-scanning an entire index, even though it's unnecessary as we are only adding documents to the index but not deleting them\n\nif this optimization was backported, then I will probably have to find a solution for my problem elsewhere...",
            "date": "2011-10-28T19:55:22.091+0000",
            "id": 74
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\neven though it's unnecessary as we are only adding documents to the index but not deleting them\n{quote}\n\nHi Roman, i saw your post.\n\nI think by default when you add a document with unique id X, Solr deletes-by-term of X.\n\nBut I'm pretty sure it has an option (sorry i dont know what it is), where you can tell it \nthat you are sure that the documents you are adding are new and it won't do this.\n",
            "date": "2011-10-28T19:59:29.772+0000",
            "id": 75
        }
    ],
    "component": "",
    "description": "IndexWriter buffers up all deletes (by Term and Query) and only\napplies them if 1) commit or NRT getReader() is called, or 2) a merge\nis about to kickoff.\n\nWe do this because, for a large index, it's very costly to open a\nSegmentReader for every segment in the index.  So we defer as long as\nwe can.  We do it just before merge so that the merge can eliminate\nthe deleted docs.\n\nBut, most merges are small, yet in a big index we apply deletes to all\nof the segments, which is really very wasteful.\n\nInstead, we should only apply the buffered deletes to the segments\nthat are about to be merged, and keep the buffer around for the\nremaining segments.\n\nI think it's not so hard to do; we'd have to have generations of\npending deletions, because the newly merged segment doesn't need the\nsame buffered deletions applied again.  So every time a merge kicks\noff, we pinch off the current set of buffered deletions, open a new\nset (the next generation), and record which segment was created as of\nwhich generation.\n\nThis should be a very sizable gain for large indices that mix\ndeletes, though, less so in flex since opening the terms index is much\nfaster.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2680",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Improve how IndexWriter flushes deletes against existing segments",
    "systemSpecification": true,
    "version": ""
}