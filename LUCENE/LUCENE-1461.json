{
    "comments": [
        {
            "author": "Tim Sturge",
            "body": "Base code which builds the integer array.",
            "date": "2008-11-19T01:17:30.079+0000",
            "id": 0
        },
        {
            "author": "Tim Sturge",
            "body": "Constructs a virtual RangeFilter on top of an already existing DisjointMultiFilter. Note that the RangeFilter costs almost nothing once the DisjointMultiFilter already exists. ",
            "date": "2008-11-19T01:19:14.921+0000",
            "id": 1
        },
        {
            "author": "Tim Sturge",
            "body": "Here's some benchmark data to demonstrate the utility. Results on a 45M document index:\n\nFirstly without an age constraint as a baseline:\n\nQuery \"+name:tim\" \nstartup: 0 \nHits: 15089\nfirst query: 1004\n100 queries: 132 (1.32 msec per query)\n\nNow with a cached filter. This is ideal from a speed standpoint but as with most range based queries there are too many possible start/end combinations to cache all the filters.\n\nQuery \"+name:tim age:[18 TO 35]\" (ConstantScoreQuery on cached RangeFilter)\nstartup: 3\nHits: 11156\nfirst query: 1830\n100 queries: 287 (2.87 msec per query)\n\nNow with an uncached filter. This is awful.\n\nQuery \"+name:tim age:[18 TO 35]\" (uncached ConstantScoreRangeQuery)\nstartup: 3\nHits: 11156\nfirst query: 1665\n100 queries: 51862 (yes, 518 msec per query, 200x slower)\n\nA RangeQuery is slightly better but still bad (and has a different result set)\n\nQuery \"+name:tim age:[18 TO 35]\" (uncached RangeQuery)\nstartup: 0\nHits: 10147\nfirst query: 1517\n100 queries: 27157 (271 msec is 100x slower than the filter)\n\nNow with the prebuilt column stride filter:\n\nQuery \"+name:tim age:[18 TO 35]\" (ConstantScoreQuery on prebuilt column stride filter)\nstartup: 2811\nHits: 11156\nfirst query: 1395\n100 queries: 441 (back down to 4.41msec per query)\n\nThis is less than 2x slower than the dedicated bitset and more than 50x faster than the range boolean query.\n\n",
            "date": "2008-11-19T01:26:36.460+0000",
            "id": 2
        },
        {
            "author": "Paul Elschot",
            "body": "This is a nice tradeoff: reduced space (the equivalent of 32 bit set range filters) for increased time (less than 2x slower) against any number of range filters on a single term field.",
            "date": "2008-11-19T19:31:53.854+0000",
            "id": 3
        },
        {
            "author": "Tim Sturge",
            "body": "Thanks Paul.\n\nThis solved a nasty performance itch with the system we are building at hi5. \n\nI'm looking into whether using a byte[] or short[] makes sense when there are less total terms (it will certainly save space, don't know about performance). \n\nThe other thing I wonder is whether you can use this for a set based query (for example a set of location grid-blocks). What I need there is a very fast java integer set (hopefully much faster than java.util.HashSet<Integer>)\n\n",
            "date": "2008-11-19T22:43:17.082+0000",
            "id": 4
        },
        {
            "author": "Paul Elschot",
            "body": "For fields that have no more distinct values than fit into a short (2^16 at best, 65536), using a short[] would make sense I think. As the number of distinct field values can simply be counted in this context, it would make sense to simply replace the int[] by a short[] in that case. But it would only help to reduce space, and only a factor two.\n\nFor a set based query, the problem boils down to doing integer set membership in the iterator. For small sets, binary search should be fine. For larger ones an OpenBitSet would be preferable, but in this context that would only be feasible when the number of different terms is a lot smaller than the number of documents in the index.\n\nFor location grid-blocks one needs to deal with more than one dimension. In such cases my first thought is to use indexed hierarchical prefixes in each dimension, because this allows skipTo() to be used on the documents for the intersection between the dimensions. (But there may be better ways, it's a long time ago that I had a look at the literature for this.)\nDo you need to index separate lower bounds and upper bounds on the data? That would complicate things.\nWithout indexed bounds (i.e. point data only) for each dimension it could make sense to use this multi range filter.\n\n",
            "date": "2008-11-20T08:05:55.699+0000",
            "id": 5
        },
        {
            "author": "Tim Sturge",
            "body": "For small subsets of a large set (in my case around 1000 out of 1million) I suspect a simple open hash may perform better than a binary search. \n\nFor location blocks (point data) my plan is just to number the grid with N^2 numbers and create a set based on a circle around the desired place. Ideally this solution doesn't degrade with circle size so it's not necessary to do hierarchical prefixes, but I don't have benchmarks to support or refute that assumption. Agreed bounded locations make this much trickier.\n\n\n",
            "date": "2008-11-20T21:47:49.280+0000",
            "id": 6
        },
        {
            "author": "Tim Sturge",
            "body": "I tried a short[] array and it is about 20% faster than the int[] array (I'm assuming this is a memory bandwidth issue.)\n\nI also tried replacing catching the ArrayIndexOutOfBoundsException with a check in the loop and discovered that the exception handling is about 3% faster.\n\nFinally, I implemented TermMultiFilter as well which has about the same performance characteristics.",
            "date": "2008-11-21T02:11:35.628+0000",
            "id": 7
        },
        {
            "author": "Tim Sturge",
            "body": "Added TermMultiFilter.java",
            "date": "2008-11-21T02:12:10.767+0000",
            "id": 8
        },
        {
            "author": "Paul Elschot",
            "body": "{quote}I tried a short[] array and it is about 20% faster than the int[] array (I'm assuming this is a memory bandwidth issue.) {quote}\n20% is more than I expected. Have a look at LUCENE-1410 for optimal bit packing in a frame of reference. There are also some performance numbers there for different numbers of frame bits. (A short[] is equivalent to 16 frame bits.)\nThis 20% means that it could well be wortwhile to always use such a frame for the docContents here.\n\nI would not expect that TermMultiFilter has an advantage over a TermFilter, since it does a linear search even for skipTo(). The only advantage it has it that it does the linear search from memory where TermFilter does its skipping using the skip info in the index.\n\nWould anyone else have an idea where this could be added, in core or contrib, and what (new) package name could be used?",
            "date": "2008-11-21T10:03:26.995+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "It seems like the core class here (DisjointMultiFilter) is doing the same thing as FieldCache's StringIndex?  Ie, it builds a data structure that maps String <-> ord and docID -> ord.  So maybe we can merge DisjointMultiFilter into the FieldCache API.\n\nAnd then RangeMultiFilter is a great addition for quickly \"spawning\" numerous new RangeFilters, having pulled & stored the StringIndex from the FieldCache?  So I think it should live in core org.apache.lucene.search.*?  I'd prefer a different name (RangeMultiFilter implies it can filter over multiple ranges) but can't think of one.  Or maybe we absorb it into RangeFilter, as a different \"rewrite\" method like \"useFieldCache=true|false\"?",
            "date": "2008-11-21T10:34:05.642+0000",
            "id": 10
        },
        {
            "author": "Tim Sturge",
            "body": "Paul,\n\nWow, I didn't realize people spent so much time on integer packing. I think there's lots of opportunities here, particularly if this ends up in the index (so the potential I/O cost becomes a factor as well as mem bandwidth). \n\nI agree that TermMultiFilter is not that useful; I mostly have it because I'm looking at TermsMultiFilter for location matching and wanted some benchmarks versus regular filters so I could compare set implementations.\n\nMike,\n\nI hadn't looked at fieldcache before, but StringIndex does seem to be the same thing as DisjointMultiFilter (modulo using a String[] instead of a TreeMap).  I'll port RangeMultiFilter to run on top of FieldCache and check it is identical and performs similarly (which seems like a fairly sure bet once I figure out the FieldCache API.) \n\nFieldCacheRangeFilter?  (yeah, I know :-) )\n\n\n\n\n\n",
            "date": "2008-11-21T19:55:12.455+0000",
            "id": 11
        },
        {
            "author": "Paul Elschot",
            "body": "{quote}I didn't realize people spent so much time on integer packing.{quote}\nWell, it appears that the memory-CPU bus really is getting to be a bottleneck, and you're not the first one to discover that, see the papers on which LUCENE-1410 is based.\nNevertheless I was surprised by a 20% performance increase when moving from int[] to short[].\n\n{quote}I'll port RangeMultiFilter to run on top of FieldCache.{quote}\nThat means that bit packing could be confined to the FieldCache lateron, which is good.\nAt the moment I'm factoring out the exceptions in the 1410 code. The FieldCache may need to wait for that because it will probably not be using exceptions.\nJust think of the extreme case of a field that has only two indexed values, it would be effectively cached as a bit set.",
            "date": "2008-11-21T21:06:21.174+0000",
            "id": 12
        },
        {
            "author": "Tim Sturge",
            "body": "This is a version of RangeMultiFilter built on top of FieldCache. This is much cleaner; it automatically handles changing the IndexReader and no longer requires the user to manually build a separate DisjointMultiFilter.\n\nPerformance is the same:\n\nCached Range Filter:\nstartup: 2\nHits: 167390\nfirst query: 2009\n100 queries: 4733\n\nRangeMultiFilter + FieldCache\nstartup: 0\nHits: 167390\nfirst query: 5405\n100 queries: 8091\n\nConstantScoreRangeQuery\nstartup: 3\nHits: 167390\nfirst query: 2012\n100 queries: 56620\n\nBoolean Query for Range\nstartup: 0\nHits: 121151\nfirst query: 3518\n100 queries: 118690\n\n\n\n",
            "date": "2008-11-24T19:38:38.770+0000",
            "id": 13
        },
        {
            "author": "Tim Sturge",
            "body": "Paul, Mike,\n\nFieldCache.StringIndex doesn't behave in the way I expect. In particular, the first element of the lookup[] array is null (which causes the binarySearch to NPE when you select a range wider than the one that actually exists.)\n\nIs this a bug in FieldCache? I would expect it to only contain terms actually in the index and I'm sincerely hoping that null is not a valid term.\n",
            "date": "2008-11-24T20:04:44.410+0000",
            "id": 14
        },
        {
            "author": "Tim Sturge",
            "body": "Looking at FieldCache and FieldDocSortedHitQueue I am very tempted to change the null sentinel (which already causes lots of grief in the comparator) to an empty string. \n\nI'm not sure that lucene is prepared to distinguish a field that is completely missing with an empty field (or field that is analyzed away), and I think the null exception handling is a significant pain.\n\n",
            "date": "2008-11-24T20:15:28.142+0000",
            "id": 15
        },
        {
            "author": "Paul Elschot",
            "body": "Here's a patch for the latest RangeMultiFilter. I've changed the package to o.a.l.search, changed the layout (where's the tool to automatically do that?), and I've added the Apache Licence, assuming that's ok from the earlier licence grant.\n",
            "date": "2008-11-25T08:52:26.014+0000",
            "id": 16
        },
        {
            "author": "Paul Elschot",
            "body": "Tim,\n\nIf there is code that depends on some particular null/empty string behaviour of FieldCache\nthere should be a test for that, so just try and patch FieldCache as you need it, and then see whether all tests still pass.\nThis way is a bit pushing, but it gets things going, and it's still no more than a patch.\n\nCould you add some test code for RangeMultiFilter?",
            "date": "2008-11-25T09:01:30.293+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "In fact, could you add a test that actually indexes an empty-string token (you'll have to make your own \"degenerate\" TokenStrem to do this I think), to ensure that switching to empty-string as sentinel doesn't break anything?",
            "date": "2008-11-25T11:04:38.454+0000",
            "id": 18
        },
        {
            "author": "Michael McCandless",
            "body": "Should we absorb RangeMultiFilter into RangeFilter, and add a \"setMethod\" to RangeFilter?\n\nSomeday, I think RangeFilter and RangeQuery should be implemented using hierarchical ranges (there was a reference to a page in the wiki, specifically about date range searching, recently), which would be another method.",
            "date": "2008-11-25T11:12:06.374+0000",
            "id": 19
        },
        {
            "author": "Paul Elschot",
            "body": "{quote}Someday, I think RangeFilter and RangeQuery should be implemented using hierarchical ranges (there was a reference to a page in the wiki, specifically about date range searching, recently), which would be another method.{quote}\n\nI think you're referring to the hierarchical prefixes here:\nhttp://wiki.apache.org/jakarta-lucene/DateRangeQueries\nThese hierarchical ranges require an analyzer to output all tokens \"to the root\" and a disjunction filter on the terms corresponding to the levels in the range. At each level a RangeMultiFilter could be used, but that would require a lot of memory. TermFilters would be better in that case I think.\n",
            "date": "2008-11-25T13:38:17.390+0000",
            "id": 20
        },
        {
            "author": "Uwe Schindler",
            "body": "I understood this in the same way. This is why I reported to the java-dev-Mailing list my developments going in this directions, perhaps for contrib: http://www.gossamer-threads.com/lists/lucene/java-dev/67807",
            "date": "2008-11-25T13:46:16.345+0000",
            "id": 21
        },
        {
            "author": "Paul Elschot",
            "body": "Uwe,\n\nAs it is already under APL 2.0, TrieRangeQuery and its utilities would make a nice addition to Lucene as a contrib package.",
            "date": "2008-11-25T14:32:15.119+0000",
            "id": 22
        },
        {
            "author": "Tim Sturge",
            "body": "Paul,\n\nThanks for the updates. I'll see about  toString() and hashCode() methods. Are we settled on RangeMultiFilter as the least confusing name?\n\nMike, Paul,\n\nI'll play with the lucene test infrastructure. Right now all the tests have been in my application but I can make a clean build to try them out.\n\nMike,\n\nI have a slight bias against adding RangeMultiFilter to RangeFilter due to the slight difference in semantics. RangeMultiFilter only works on single term fields (which should probably be mentioned in the java docs) whereas RangeFilter works on multiple term fields as well.\n\nWhile I expect more than 95% of the RangeFilter use cases are met by RangeMultiFilter (I suspect they are primarily dates, and otherwise prices and other numeric ranges.) I bet there are some people who really do a text range search between \"aardvark\" and \"antelope\". Those people will unexpectedly break if they set \"useFieldCache=true\" or setMethod(). I would rather we add a comment in the RangeFilter javadocs to the effect of:\n\n\"If you have a single term field (for example a date, or a price) that is repeatedly used in a RangeFilter with many different ranges, you should consider using RangeMultiFilter as a faster alternative to building a RangeFilter on this field for each query. You need to ensure that this field is untokenized, or that it always tokenizes to a single term.\"\n\n\n\n",
            "date": "2008-11-25T19:02:21.896+0000",
            "id": 23
        },
        {
            "author": "Michael McCandless",
            "body": "OK that makes sense -- let's leave it as a separate class, and can you add that difference to the javadocs?",
            "date": "2008-11-25T19:16:21.877+0000",
            "id": 24
        },
        {
            "author": "Paul Elschot",
            "body": "{quote}Are we settled on RangeMultiFilter as the least confusing name?{quote}\nNames are important, although not as important as javadocs.\nI'm happy to leave the choice of name to someone with an English/... mother tongue. ",
            "date": "2008-11-25T19:32:54.378+0000",
            "id": 25
        },
        {
            "author": "Tim Sturge",
            "body": "Progress report:\n\nHaving written some javadocs, I think FieldCacheRangeFilter is a better name; without DisjointMultiFilter the \"multi\" in RangeMultiFilter is confusing (after all, it indexes a field containing a *single* term). So at the risk of repainting the bikeshed I will go with FieldCacheRangeFilter.\n\nThe null versus \"\" distinction is completely confusing to me. I see this in ConstantScoreRangeQuery:\n\n    // Map to RangeFilter semantics which are slightly different...\n    RangeFilter rangeFilt = new RangeFilter\n        (fieldName, lowerVal != null?lowerVal:\"\", upperVal,\n         lowerVal==\"\"?false:includeLower, upperVal==null?false:includeUpper,\n         collator);\n\nwhich makes no sense to me at all.\n\nI'm also not sure it makes sense to allow the indexing of an empty field and distinguishing that case from there being nothing there. Please let me know if there is a usecase. The lowest impedance solution may be to write a version of binarySearch() that allows there to be a null in the first element and use that instead of Arrays.binarySearch().\n\n",
            "date": "2008-11-25T22:43:29.072+0000",
            "id": 26
        },
        {
            "author": "Tim Sturge",
            "body": "Here's the first cleanup\n\nChanges:\n\nRangeMultiFilter now FieldCacheRangeFilter\n\nFieldCache.StringIndex gains a binarySearchLookup() method that handles null\n\ntoString(), hashCode() and equals() methods.\n\nThis hasn't been tested very well; but I wanted to post something before I left for Thanksgiving.\n",
            "date": "2008-11-26T01:58:14.233+0000",
            "id": 27
        },
        {
            "author": "Earwin Burrfoot",
            "body": "Somewhat off topic, but nonetheless, my two techniques for superfast range queries/filters:\n1. cache [from, null]+[null, to] filters instead of [from, to] and intersect them\n-> can tremendously improve cache hits for certain setups\n\n2. when indexing a field that will be used for range filter, index lower-resolution versions of it additionally, than use a union of rangefilters over different resolution fields, ie:\na. we have severalM documents with a date field spanning few years with say minute precision (we'd like to sort on it afterward)\nb. we index additional fields with dates rounded down to something like years, months, days, hours (best combination depends on width of the queries you're most likely to perform, let's say it's day+hour for queries rarely spanning more than a month)\nc. we have a query like [2008-05-05 18:00 .. 2008-06-01 10:53], it is converted to -> hour:[05-05 18 .. 05-06 00) or day:[05-06 .. 06-01) or hour:[06-01 00 .. 06-01 10) or minute:[06-01 10:00 .. 06-01 10:53]\n-> massive win for ranges over fields having lots of high-selectivity terms, with timestamps being a good example, also salaries, coordinates, whatever",
            "date": "2008-11-26T07:26:59.487+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "bq. my two techniques for superfast range queries/filters\n\nI like those approaches -- I think 2 is similar to [above|#action_12650562] and similar to Uwe's approach (described on java-dev).  One nice property of these \"factor the range into a set of OR/AND clauses\" is RangeQuery no longer relies on the sort order of the terms, which means tricks like padding numeric terms are no longer needed, I think?\n\nThis sudden burst of innovation around RangeQuery is very exciting!",
            "date": "2008-11-26T10:16:20.148+0000",
            "id": 29
        },
        {
            "author": "Uwe Schindler",
            "body": "The RangeQuery still relies on the sort order of terms (this is how it works). For storing terms with lower precision you have two possiblities:\n\na) use another field name for each precision\nb) prefix the terms with a precision marker. The prefix is important for the sort order, so that all terms of one precision are in one \"bunch\" and not distributed between higher precsion terms.\n\nThe first version of my TrieRangeQuery was invented before the RangeFilter occurred first in Lucene. This version did exactly what was proposed here: combining more range queries with OR.\n\nFor my last implementation, based on filters I did not use a BooleanQuery with OR'ed ranges because of resource usage: Each RangeFilter needs an OpenBitSet instance, and all of them must be OR'ed during query execution. Using only one OpenBitSet for all range parts is more effective, I think. I am currently working on including my extension to the contrib-query package. I refactored the code a little bit, so the TrieRangeFilter is now separated from the query (and because of that could be used with e.g. filter caching). I think, I will start n issue this afternoon.",
            "date": "2008-11-26T10:37:31.183+0000",
            "id": 30
        },
        {
            "author": "Michael McCandless",
            "body": "This patch looks good!  I think it's ready to commit?  I plan to commit in a day or two.\n\nI made some small changes -- added CHANGES.txt entry, fixed whitespace, removed one unnecessary import.\n\nThanks Tim!",
            "date": "2008-11-26T10:39:39.706+0000",
            "id": 31
        },
        {
            "author": "Michael McCandless",
            "body": "bq. The RangeQuery still relies on the sort order of terms (this is how it works)\n\nAhh OK. Allowing each field to provide its own Comparator may still be helpful then (Lucene doesn't allow this today since fields are always sorted in java char order) to avoid padding and other binary conversion tricks.",
            "date": "2008-11-26T10:48:12.198+0000",
            "id": 32
        },
        {
            "author": "Earwin Burrfoot",
            "body": "bq. RangeQuery no longer relies on the sort order of the terms, which means tricks like padding numeric terms are no longer needed, I think?\nI do rely on sort order for speed and simplicity, though I never used padding for numeric/date terms :) All dates/numbers/somethingelsespecial are converted to strings using base-2 ^15^ (to keep high bit=0, as 0xFFFF is used somewhere within Lucene intestines as EOS marker, darn it!) encoding. Plus adjustment to preserve sort order for negative numbers in face of unsigned java char. This transformation is insanely fast, and produces well-compressed results (I have FAT read->mem/write->mem+disk indexes).\n\nbq. b) prefix the terms with a precision marker. The prefix is important for the sort order, so that all terms of one precision are in one \"bunch\" and not distributed between higher precsion terms.\nAnd you can no longer use this field for sorting, as it has more than one term for each document.\n\nbq. For my last implementation, based on filters I did not use a BooleanQuery with OR'ed ranges because of resource usage\nUsing filters here too\n\nbq. Allowing each field to provide its own Comparator may still be helpful then\nBut you still store strings in the index. So essentially you'll convert your value from T to String, store it, retrieve it, convert back to T in such a custom comparator, and finally compare. Why should I need that second conversion and custom comparators, if I can have order-preserving bijective T<->String relation?\n\n",
            "date": "2008-11-26T12:03:32.950+0000",
            "id": 33
        },
        {
            "author": "Michael McCandless",
            "body": "bq. But you still store strings in the index. So essentially you'll convert your value from T to String, store it, retrieve it, convert back to T in such a custom comparator, and finally compare. Why should I need that second conversion and custom comparators, if I can have order-preserving bijective T<->String relation?\n\nTrue, since you'll need to xform anyway for non-textual fields.  Or maybe eventually we can simply allow T to be the key in the terms dict (so long as T.compareTo(T) exists), which KS/Lucy apparently does.",
            "date": "2008-11-26T13:32:49.896+0000",
            "id": 34
        },
        {
            "author": "Michael McCandless",
            "body": "See also LUCENE-1470, which is another way to achieve faster RangeFilter/Query, by pre-aggregating ranges during indexing and then factoring queries at search time to use the aggregates when possible.",
            "date": "2008-11-26T18:42:52.949+0000",
            "id": 35
        },
        {
            "author": "Michael McCandless",
            "body": "Committed revision 721663.\n\nThanks Tim!",
            "date": "2008-11-29T11:51:01.060+0000",
            "id": 36
        },
        {
            "author": "Tim Sturge",
            "body": "Mike,\n\nThanks for committing. I have a slightly more tested version which uncovered a couple of bugs:\n\n- using null for the upper limit didn't work\n- bad combinations of ranges weren't rejected with IllegalArgumentException\n\nThis also includes the correct version of the test suite (I accidentally included the pre edit version before)\n\n",
            "date": "2008-12-01T19:07:03.907+0000",
            "id": 37
        },
        {
            "author": "Tim Sturge",
            "body": "New patch. Only changes are:\n\n- initialize() in FieldCacheRangeFilter\n- correct version of TestFieldCacheRangeFilter.",
            "date": "2008-12-01T19:08:00.058+0000",
            "id": 38
        },
        {
            "author": "Michael McCandless",
            "body": "No problem!  Could you redo the patch relative to what's now committed (on trunk)?  This way I can more easily see the new changes.\n\n\"svn diff\" (after checking out the trunk) is the simplest way to generate a patch.",
            "date": "2008-12-01T19:22:59.246+0000",
            "id": 39
        },
        {
            "author": "Tim Sturge",
            "body": "Patch from trunk fixing upper bound in FieldCacheRangeFilter",
            "date": "2008-12-01T19:36:57.997+0000",
            "id": 40
        },
        {
            "author": "Tim Sturge",
            "body": "Patch from incorrect to correct test suite",
            "date": "2008-12-01T19:37:26.610+0000",
            "id": 41
        },
        {
            "author": "Michael McCandless",
            "body": "Committed revision 722203.\n\nThanks Tim!",
            "date": "2008-12-01T19:53:15.239+0000",
            "id": 42
        },
        {
            "author": "Otis Gospodnetic",
            "body": "Is this related to LUCENE-855?  The same?  Aha, I see Paul asked the reverse question in LUCENE-855 already... Tim?\n",
            "date": "2008-12-04T17:03:31.208+0000",
            "id": 43
        },
        {
            "author": "Tim Sturge",
            "body": "That's amazing. LUCENE-855 (the FieldCacheRangeFilter part) is pretty much identical in purpose and design, down to the name. The major implementation differences are that it overloaded BitSet which was necessary prior to the addition of DocIdSetIterator. Thus my implementation looks significantly cleaner even though it is basically functionally identical.\n\nI think this shows that any decent idea will be repeatedly reinvented until it is widely enough known. I personally would have saved some time both in conceptualization and implementation had I been aware of this. \n\nI would very much like to credit Matt in CHANGES.txt for this as well; it seems like an accident of fate that I'm not using his implementation today.\n",
            "date": "2008-12-04T18:59:03.301+0000",
            "id": 44
        },
        {
            "author": "Uwe Schindler",
            "body": "I reopened the wrong issue :-(\n\nThe class to handle is FieldCacheRangeFilter! Here, why reopen:\n\nThis Filter is really cool on iterating on the FieldCache for StringIndex and can be even faster for ranges, that are int/float/double/... - so why not retrofit to our new naming-convention and extend:\n\n- FieldCacheRangeFilter.newTermRange()\n- FieldCacheRangeFilter.newByteRange()\n- FieldCacheRangeFilter.newShortRange()\n- FieldCacheRangeFilter.newIntRange()\n- ...\n\nIt could because of that also be used on \"old\" int/long fields of dates, if a good parser is given (parser that does SimpleDateFormat -> long -> FieldCache -> direct comparison on this raw numbers). I would try to extend this to all types and it can be faster than TrieRange, if the range is already in FieldCache!",
            "date": "2009-06-23T21:07:31.520+0000",
            "id": 45
        },
        {
            "author": "Uwe Schindler",
            "body": "Here is an first version of retrofitted FieldCacheRangeFilter. It supports StringIndex like before for normal string ranges and as an example Byte ranges on FieldCache.getBytes(). Optional a parser can be given, passed to getBytes(). The internal code structure was changed to make it easier to implement the iterator for each data type.\n\nThe test currently only checks StringIndex (it is the original test), other datatypes beyond byte are not implemented until now (it's mostly copy'n'paste...)",
            "date": "2009-06-24T18:30:14.797+0000",
            "id": 46
        },
        {
            "author": "Uwe Schindler",
            "body": "Patch that implements all FieldCache data types. The Double/Float includesXxxx code is a littly bit a hack, I will think about it again.\n\nWhat is currently not consistent for all types of range queries (RangeQuery, NumericRangeQuery, FieldCacheRangeFilter) is, if it is allowed to have one bound null and include it. In my opinion, this should always be allowed and should deliver same results for inclusive or not.\n\nThere is a first test together with TrieRangeQuery (for ints) in TestNumericRangeQuery32.java - which passes.\n\nAn important restricion for this type with numeric field caches is: There must be exactly one value per document. If value is missing, 0 is assumed. For Strings, no value is allowed, not for numbers (because arrays like int[] cannot contain null values).",
            "date": "2009-06-25T00:14:28.238+0000",
            "id": 47
        },
        {
            "author": "Uwe Schindler",
            "body": "Updated patch. It now changes the includeUpper/Lower behaviour to be consistent with other range query types (RangeQuery, NumericRangeQuery). It also has updates to hashCode and equals (missing parser).\n\nAfter I wrote some additional tests (possibly inside TestNumericRangeQuery32/64), I think it is ready to commit.\n\nI found no better solution to remove the large dupplicate code parts, but this is not possible because of different array data types.",
            "date": "2009-06-25T09:50:29.684+0000",
            "id": 48
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good, Uwe!  The only issue I found was you're using includeUpper where you should be using includeLower.",
            "date": "2009-06-25T14:07:58.551+0000",
            "id": 49
        },
        {
            "author": "Uwe Schindler",
            "body": "where? these are typical copy'n'paste errors and missing tests...",
            "date": "2009-06-25T14:09:55.664+0000",
            "id": 50
        },
        {
            "author": "Uwe Schindler",
            "body": "oh ja, found it - was copy'n'paste",
            "date": "2009-06-25T14:13:04.860+0000",
            "id": 51
        },
        {
            "author": "Uwe Schindler",
            "body": "Mike: There is one thing, I wanted to know, as I am not so familar with the whole internal query handling:\n\nThe DocIdSet for the native numeric types may return document ids, that are deleted (because for native types there is no possibility to find out if there was no term saved, null-fields or deleted docs would simply have 0 in the field cache array). A range covering 0 always returns all doc ids of deleted docs or docs without a numeric field (this can be noted in the javadocs: \"for numeric queries every document must have exact *one* numeric term per field\"). Is this a problem, will the false-hits in the iterator be filted because doc is deleted?\n\nIn the case of an real filter the query, that is filtered will not return deleted docs, but a ConstantScoreQuery on this filter would return deleted docs?",
            "date": "2009-06-25T22:17:27.509+0000",
            "id": 52
        },
        {
            "author": "Michael McCandless",
            "body": "bq.  but a ConstantScoreQuery on this filter would return deleted docs?\n\nYou are right!  I guess one workaround is to AND it with a MatchAllDocsQuery, if you are using ConstantScoreQuery w/o already ANDing it with a query that takes deletions into account.\n\nSo there are two issues:\n\n  * This filter returns deleted docs\n\n  * This filter \"pretends\" deleted docs had a value of zero\n\nThis is then only a problem if nothing else in the query applies deletions.  Other FieldCache driven filters have challenges here, too; eg we just fixed LUCENE-1571 where local lucene tripped up on deleted docs (because it's using the Strings FieldCache, and hit nulls for deleted docs)\n\nI'm not sure how we should fix this... (and I think we should open a new issue to do so).  I don't want to force this filter to always take deletions into account (since for many queries the filter is \"and'd\" on, deletions are already factored in).  More generally, we need to think about what's the \"right\" top-down way to ask a scorer to take deletions and filters into account.  Eg, LUCENE-1536 is looking at sizable performance improvements for the \"relatively dense and supports random access\" type of filters.\n",
            "date": "2009-06-26T09:20:48.467+0000",
            "id": 53
        },
        {
            "author": "Uwe Schindler",
            "body": "Hey Mike, same time... :-)\n\nI did some recherche and also found out, that a filter's DocIdSet should not list deleted documents.\n\nBecause of that, I changed the non-StringIndex (which will never contain strings of deleted docs because it has a order[]->0 mapping) to use IndexReader.termDocs(null) to lists the docIds (which is no real problem, as it is just an iterator an a bitset, the additional cost is low, tested with 10 Mio index).\n\nI also created a superclass for all the iterators working on numbers, to get the termDocs handled easily. The type-specific iterators ony override a matchDoc() method. StringIndex iterator stays separate, because it is optimized and has no deleted docs problem as described before.\n\nThis patch also contains tests for all (except byte) types.\n\nI will commit in a day or two.\n\n(an other solution for future would be to have an additional bitset for numeric values in addition to the native type array (in FieldCache), that holds the information, if the document had a term available. This would also cover the deleted docs)",
            "date": "2009-06-26T09:28:36.170+0000",
            "id": 54
        },
        {
            "author": "Michael McCandless",
            "body": "OK, patch looks good Uwe!\n\nHaving this filter just always take deletions into account is the safe solution; presumably the added performance cost is OK since this filter is so fast to begin with.\n\nLonger term I think we need a cleaner way to ask a Scorer to \"carry out\" deletions & filtering, and have it more optimally delegate that request to its sub-scorers as needed.\n\nOne corner case issue: if I eg make a newShortRange w/ lowerVal == Short.MAX_VALUE and includeLower=false, which should match no docs, I think in this case you overflow short in computing inclusiveLowerPoint and thus match possibly many docs incorrectly?  (Same for byte/int/long).",
            "date": "2009-06-26T09:55:30.075+0000",
            "id": 55
        },
        {
            "author": "Uwe Schindler",
            "body": "I did some performance tests and compared this filter with TrieRange (precStep 8) on an 5 Mio index with homegenous distributed int values from Integer.MIN_VALUE to Integer.MAX_VALUE and 200 queries with random bounds in same range. Platform was Win32 with 1.5 GIG RAM on my Thinkpad T60 Core Duo (not 2 Duo!), Java 1.5:\n\nloading field cache\ntime: 11826.602264 ms\nWarming searcher...\navg number of terms: 414.365\nTRIE: best time=4.51482 ms; worst time=1560.544985 ms; avg=470.56886981499997 ms; sum=323328111\nFIELDCACHE: best time=314.611773 ms; worst time=878.438461 ms; avg=511.93189495499996 ms; sum=323328111\n\nThis test shows, that with a good warmed searcher and the whole index in OS cache is the same in speed. A constant score convential range query is far out (about 10 to 1000 times slower dependent on how far the random range bounds are away).\n\nThe same with the old patch (using no TermDocs) and a completely separate loop (not matchDoc() method call), the FieldCache filter only hits the trie filter here:\n\nloading field cache\ntime: 12134.143027 ms\nWarming searcher...\navg number of terms: 403.785\nTRIE: best time=3.890159 ms; worst time=1266.979462 ms; avg=453.553236545ms; sum=308154314\nFIELDCACHE: best time=84.019897 ms; worst time=434.558023 ms; avg=235.91554798500002 ms; sum=308154314\n\nBoth test runs show, that the queries work correct (sum is identical, it shows that both returned exact the same hits).\n\nIn all cases I would still prefer TrieRange (hihi), especially because of the long warming time for the field cache. And TrieRange gets even better with lower precSteps, but not really (in constant score mode the bits sets are the bigger problem)",
            "date": "2009-06-26T10:40:23.134+0000",
            "id": 56
        },
        {
            "author": "Uwe Schindler",
            "body": "Attached my performance test program for reference.",
            "date": "2009-06-26T10:43:35.125+0000",
            "id": 57
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. One corner case issue: if I eg make a newShortRange w/ lowerVal == Short.MAX_VALUE and includeLower=false, which should match no docs, I think in this case you overflow short in computing inclusiveLowerPoint and thus match possibly many docs incorrectly? (Same for byte/int/long).\n\nThis problem has also NumericRangeQuery (see the TermEnum impl there). I could change both queries to simply return the empty iterator (like when upper<lower)",
            "date": "2009-06-26T11:46:17.362+0000",
            "id": 58
        },
        {
            "author": "Michael McCandless",
            "body": "bq. This problem has also NumericRangeQuery (see the TermEnum impl there). I could change both queries to simply return the empty iterator (like when upper<lower)\n\nRight, and I see you've already fixed it!\n\nFrom your performance runs, looking at the average times, forcing this\nfilter to take deletions into account made it ~2X slower.  That's\nquite costly.\n\n(Though, you really should seed the Random() so the two tests run\nprecisely the same set of queries against precisely the same index).\n\nI would imagine that for most usage of this filter, taking deletes\ninto account is not necessary, because it's being used as a filter\nwith a query whose scorer won't return deleted docs.  Then we've taken\nthis perf hit for nothing...\n\nSomehow, we really need better control, when creating scorers, on just\nwhen we need and don't need deletions / filters to be \"AND'd\" in.\n\nAlso, this filter isn't good when not many docs pass the filter, since\nit's an O(N) scan through the index.  Trie should do much better in\nthose cases.\n\nI wonder, if we could make a hybrid approach that eg loads the trie\nfields into a fast in-memory postings format (simple int arrays), just\nhow much faster it'd be.  Ie, if you want to spend memory, spending it\non trie's postings would presumably net the best performance.  Once we\nhave flexible indexing we could presumably \"swap in\" an in-RAM\npostings impl and then run trie against that.\n",
            "date": "2009-06-26T15:23:49.548+0000",
            "id": 59
        },
        {
            "author": "Uwe Schindler",
            "body": "Here an updated patch with the corner cases fixed (incl tests) for this filter (TrieRange is already fixed in trunk).",
            "date": "2009-06-26T15:31:36.637+0000",
            "id": 60
        },
        {
            "author": "Uwe Schindler",
            "body": "{quote}\nFrom your performance runs, looking at the average times, forcing this\nfilter to take deletions into account made it ~2X slower. That's\nquite costly.\n{quote}\n\nThis is not only because of the deleted docs. Its also because the while loops are no longer hard coded with bounds, so there is an additional method call to check if something is a hit. The linear scan makes this also very costly. But without it, the code is unmaintainable (with all problems like copy/paste errors) and so on. It is almost 6 times the same code :(\n\n{quote}\nI would imagine that for most usage of this filter, taking deletes\ninto account is not necessary, because it's being used as a filter\nwith a query whose scorer won't return deleted docs. Then we've taken\nthis perf hit for nothing...\n{quote}\n\nI could put in a switch, that uses another iterator, if 0 is not inside the range (because then all deleted docs would never be hits) or the IR has no deletions. But I think this optimization is something for later.\n\nIn my opinion, in most cases TrieRange is better (and with Payloads, too). So I keeps this filter how it is for the beginning.",
            "date": "2009-06-26T15:38:04.168+0000",
            "id": 61
        },
        {
            "author": "Michael McCandless",
            "body": "bq. This is not only because of the deleted docs. Its also because the while loops are no longer hard coded with bounds, so there is an additional method call to check if something is a hit. The linear scan makes this also very costly. But without it, the code is unmaintainable (with all problems like copy/paste errors) and so on. It is almost 6 times the same code.\n\nThe tradeoff of code ugliness vs performance is always an \"interesting\" one, but here we lost 2X performance :(  Maybe we leave this to future source code specialization.\n\nbq. In my opinion, in most cases TrieRange is better (and with Payloads, too).\n\nSince, with the 2X slowdown, and with the linear-scan impl, this filter is not faster than trie... should we even bother?  It's going to confuse users having two ways to do numeric filtering, and, trie seems to be best across the board anyway?  Are there any times that this filter is better than trie?\n\nbq.   So I keeps this filter how it is for the beginning.\n\nOK, you mean leave this filter as only doing text (term) ranges?  OK, that sounds good.",
            "date": "2009-06-26T16:26:23.147+0000",
            "id": 62
        },
        {
            "author": "Uwe Schindler",
            "body": "I wanted to inform, that the latest patch (and the ones before) are still buggy. During changing the upper/lower bound settings I missed the case that StringIndex.binarySearch can return negative values, if the exact key was not found. See javadocs of Arrays.binarySearch. The problem is that the testcase did not found that.\n\nI will fix that and do some further tests later, but I am away now.",
            "date": "2009-06-26T17:07:49.828+0000",
            "id": 63
        },
        {
            "author": "Uwe Schindler",
            "body": "Attached is a new patch, that has 2 DocIdSetIterator implementations, one with TermDocs, one without. The TermDocs one is for numeric types only choosen, if the reader contains deletions *and* 0 is inside the range. For all other cases (also StringIndex) the simple DocIdSetIterator using the counter is used.\n\nFor more code-reuse, all range implementations now use the same abstract DocIdSet implementation and only override matchDoc(). My tests showed, that use of this method does not affect performance (method is inlined), the original stringindex impl is as fast as the new one with matchDoc().\n\nThis patch also restores the original handling of the return value of binarySearch (which can be negative).\n\nHere again the comparison:\n\n*Version with TermDocs:*\nloading field cache\ntime: 6767.23131 ms\nWarming searcher...\navg number of terms: 378.75\nTRIE: best time=5.232229 ms; worst time=553.791334 ms; avg=250.4418579 ms; sum=31996909\nFIELDCACHE: best time=212.763912 ms; worst time=357.100414 ms; avg=279.75582110000005 ms; sum=31996909\n\n*Version without (because index in testcase has no deletions):*\nloading field cache\ntime: 6463.311678 ms\nWarming searcher...\navg number of terms: 378.75\nTRIE: best time=4.539963 ms; worst time=581.657446 ms; avg=246.58688465 ms; sum=31996909\nFIELDCACHE: best time=64.747614 ms; worst time=211.557335 ms; avg=139.16517340000001 ms; sum=31996909\n\n(my T60 was not on battery, because of this the measurement with TermDocs and FieldCache loading was faster that before). But both tests before and after optimization were done with same settings. The randseed was identical (0L)",
            "date": "2009-06-26T22:22:49.159+0000",
            "id": 64
        },
        {
            "author": "Uwe Schindler",
            "body": "It seems that the latest patch has no problems anymore. Without deletions or if 0 is not inside the range it seems to be faster than trie range, with the problem of long first-time searches (cache loading). But if you e.g. search on this field or use the cache for something other, it may not be a problem.\n\nThe biggest advantage of this is, that you do not need to index the values in a special way, you can simply use your old Number.toString() formatted fields and do range queries on them. For term/string ranges it works better than RangeFilter, but the memory usage is much higher (if you have lots of distinct terms) with StringIndex.\n\nMaybe for the future, there would also be a possibility to implement a TrieRangeQuery for Strings (the precisionStep would there be the number of chars per precision, so e.g. precStep=2 would be to index for \"lucene\" the tokens \"lu\", \"luce\", \"lucene\"). The same here like with TrieRange: a TokenStream that does this would be good.\n\nIn my opinion, this class is a good approach for range queries, if you have enough RAM and warm your searchers correctly, but do not want to change you index structure to use the new TrieRange. This class is not good for indexes where you will hit only few documents per range, as the cost of the linear scan for all data types then overweight.\n\nI think I will commit this later, if nobody objects. If you think, that only StringIndex and not numeric values should be handled by this class (throw away the new code), I tend to rename this class before release according to LUCENE-1713.",
            "date": "2009-06-30T08:03:58.363+0000",
            "id": 65
        },
        {
            "author": "Uwe Schindler",
            "body": "I forgot: Here the latest optimizations and corrections in the corner cases for StringIndex.",
            "date": "2009-06-30T08:08:22.000+0000",
            "id": 66
        },
        {
            "author": "Michael McCandless",
            "body": "OK let's keep the new code, and keep the current name; it's great that it can now be faster than trie in some cases.  I'd love to eventually see an in-memory trie structure; seems like this'd be the fastest of all ;)\n\nI only briefly skimmed the patch (I'm on \"vacation\" until Jul 6) and it looks good!",
            "date": "2009-06-30T09:40:47.659+0000",
            "id": 67
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. I only briefly skimmed the patch (I'm on \"vacation\" until Jul 6) and it looks good!\n\nHappy holidays! (my holiday is later). I commit shortly.",
            "date": "2009-06-30T09:48:27.463+0000",
            "id": 68
        },
        {
            "author": "Uwe Schindler",
            "body": "Added CHANGES.txt & committed revision 789682.",
            "date": "2009-06-30T11:18:17.428+0000",
            "id": 69
        }
    ],
    "component": "",
    "description": "These classes implement inexpensive range filtering over a field containing a single term. They do this by building an integer array of term numbers (storing the term->number mapping in a TreeMap) and then implementing a fast integer comparison based DocSetIdIterator.\n\nThis code is currently being used to do age range filtering, but could also be used to do other date filtering or in any application where there need to be multiple filters based on the same single term field. I have an untested implementation of single term filtering and have considered but not yet implemented term set filtering (useful for location based searches) as well. \n\nThe code here is fairly rough; it works but lacks javadocs and toString() and hashCode() methods etc. I'm posting it here to discover if there is other interest in this feature; I don't mind fixing it up but would hate to go to the effort if it's not going to make it into Lucene.\n\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1461",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Cached filter for a single term field",
    "systemSpecification": true,
    "version": ""
}