{
    "comments": [
        {
            "author": "Ryan McKinley",
            "body": "See some discussion here:\nhttp://search.lucidimagination.com/search/document/b6a531f7b73621f1/trie_fields_and_sortmissinglast",
            "date": "2010-09-17T04:49:31.812+0000",
            "id": 0
        },
        {
            "author": "Ryan McKinley",
            "body": "This patch replaces the cached primitive[] with a CachedObject.  The object heiarch looks like this:\n\n{code:java}\n\n    public abstract static class CachedObject { \n    \n  }\n\n  public abstract static class CachedArray extends CachedObject {\n    public final Bits valid;\n    public CachedArray( Bits valid ) {\n      this.valid = valid;\n    }\n  };\n\n  public static final class ByteValues extends CachedArray {\n    public final byte[] values;\n    public ByteValues( byte[] values, Bits valid ) {\n      super( valid );\n      this.values = values;\n    }\n  };\n  ...\n{code}\n\nThen this @deprecates the getBytes() classes and replaces them with getByteValues()\n\n{code:java}\n\n  public ByteValues getByteValues(IndexReader reader, String field)\n  throws IOException;\n\n  public ByteValues getByteValues(IndexReader reader, String field, ByteParser parser)\n  throws IOException;\n  \n{code}\n\nthen repeat for all the other types!\n\nAll tests pass with this patch, but i have not added any tests for the BitSet (yet)\n\nIf people like the general look of this approach, I will clean it up and add some tests, javadoc cleanup etc\n",
            "date": "2010-09-17T04:56:10.213+0000",
            "id": 1
        },
        {
            "author": "Ryan McKinley",
            "body": "A slightly simplified version",
            "date": "2010-09-17T05:20:55.920+0000",
            "id": 2
        },
        {
            "author": "Uwe Schindler",
            "body": "That looks exactly like I proposed it!\n\nThe only thing: For DocTerms the approach is not needed? You can check for null, so the Bits interface is not needed. As the OpenBitSets are created with the exact size and don't need to grow, you can use fastSet to speed up creation by doing no bounds checks.",
            "date": "2010-09-17T05:25:10.877+0000",
            "id": 3
        },
        {
            "author": "Uwe Schindler",
            "body": "When this is committed, we can improve also some Lucene parts: FieldCacheRangeFilter does not need to do extra deletion checks and instead use the Bits interface to find missing/non-valued documents. Lucene's sorting Collectors can be improved to have a consistent behaviour for missing values (like Solr's sortMissingFirst/Last).",
            "date": "2010-09-17T05:30:13.648+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "Looks great!\n\nShould we make it optional, whether the valid bitset should be computed?  Many apps wouldn't need it, so it just ties up (admittedly smallish amounts of) RAM unnecessarily?\n\nbq. Lucene's sorting Collectors can be improved to have a consistent behaviour for missing values (like Solr's sortMissingFirst/Last).\n\n+1\n\nShouldn't we pull Solr's sortMissingFirst/Last down into Lucene?",
            "date": "2010-09-17T09:20:37.165+0000",
            "id": 5
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. Should we make it optional, whether the valid bitset should be computed? Many apps wouldn't need it, so it just ties up (admittedly smallish amounts of) RAM unnecessarily?\n+1 we can save that overhead and high level apps can enable it by default if needed.\n\n",
            "date": "2010-09-17T11:09:02.128+0000",
            "id": 6
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. Should we make it optional, whether the valid bitset should be computed?\n\nThe trick is how to implement that (unless you mean just set it to true/false for all fields at once).  Putting a flag on the FieldCache.getXXX methods is insufficient.\nOnly the application knows if some of it's future uses of that field will require the bitset for matching docs, but it's Lucene that's often making the calls to the field cache.\n\nPerhaps FieldCache.Parser was originally just too narrow in scope - it should have been a factory method for handling all decisions about creating and populating a field cache entry?",
            "date": "2010-09-17T12:47:07.527+0000",
            "id": 7
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. Perhaps FieldCache.Parser was originally just too narrow in scope - it should have been a factory method for handling all decisions about creating and populating a field cache entry?\nI guess we need to be able to manually configure FieldCache with some kind of FieldType. There have been several issues mentioning this and it keeps coming up again and again. I think it is just time to rethink Fieldable / Field and move towards some kind of flexible type definition for Fields in Lucene. A FieldType could then have a FieldCache Attribute which contains all necessary info including the parser and flags like the one we are talking about. Yet, before I get too excieted about FieldType, yeah something with a wider scope than FieldCache.Parser would work in this case. I don't know how far the FieldType is away but it can eventually replace whatever is going to be implemented here in regards to that flag. \n\nI think by default we should not enable the Bits feature but it must be explicitly set via whatever mechanism we gonna use.\n\n",
            "date": "2010-09-17T13:24:43.365+0000",
            "id": 8
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. I guess we need to be able to manually configure FieldCache with some kind of FieldType.\n\nI don't know how well that would work.  For one, there's only one FieldCache, so configuring it with anything seems problematic.\nAlso, if I have to list out all the fields I'm going to use, that's another big step backwards.\n\nA factory would be a pretty straightforward way to increase the power, by allowing users to populate the entry through any mechanism, and optionally do extra calculations when the entry is populated (think statistics, sum-of-squares, etc).",
            "date": "2010-09-17T13:40:03.082+0000",
            "id": 9
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. Also, if I have to list out all the fields I'm going to use, that's another big step backwards.\nI don't think that this is needed at all neither would it be a step backwards - not even near to that. But since we aren't on an issue about FieldType lets just drop it...\n\nbq. A factory would be a pretty straightforward way to increase the power, by allowing users to populate the entry through any mechanism, and optionally do extra calculations when the entry is populated (think statistics, sum-of-squares, etc).\nWhatever you call it (using Factory is fine) but isn't that what you mentioned to be insufficient? I mean this is something you would pass to a FieldCache.getXXX, right? ",
            "date": "2010-09-17T13:59:13.285+0000",
            "id": 10
        },
        {
            "author": "Shai Erera",
            "body": "One thing I've wanted to do for a long time, but didn't get to doing it, is open up FieldCache to allow the application to populate the entries from other sources - specifically pyloads. I wrote a sorting solution which relies solely on payloads, and wanted to contribute it to Lucene, but due to lack's of FieldCache hook points, I didn't find the time to do the necessary refactoring.\n\nSorting based on payloads-data has several advantages:\n# It's much faster to read than iterating on the lexicon and parsing the term values into sortable values.\n# If your application needs to cater sort over 10s of millions of documents, or if it needs to keep its RAM usage low, you can do the sort while reading the payload data as the search happens. It's faster than if it was in RAM, but the current FieldCache does not allow you to sort w/o RAM consumption.\n# You don't inflate your lexicon w/ sort values, affecting other searches. In some situations, you can add a unique term per document for the sort values (such as when sorting by date and require up to a millisecond precision).\n\nI'm bringing it up so that if you consider any refactoring to FieldCache, I'd appreciate if you can keep that in mind. If the right hooks will open up, I'll make time to contribute my sort-by-payload package. If you don't, then it'll need to wait until I can find the time to do the refactoring.\n\n",
            "date": "2010-09-17T14:01:30.731+0000",
            "id": 11
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. Whatever you call it (using Factory is fine) but isn't that what you mentioned to be insufficient? I mean this is something you would pass to a FieldCache.getXXX, right? \n\nI was suggesting handling it the same way as FieldCache.Parser - it's set on the SortField.  But instead of just being able to control parsing of a term (which is too limited), it needs to be able to control everything. (This would solve Shai's needs too)",
            "date": "2010-09-17T14:09:48.458+0000",
            "id": 12
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. open up FieldCache to allow the application to populate the entries from other sources\n\n+1\n\nbq. specifically payloads\n\nIf CSF did not exist, I'd be totally on board with this...  but it looks to be right around the corner now.  Are there any advantages to using payloads over CSF for fieldcache population?",
            "date": "2010-09-17T14:14:19.612+0000",
            "id": 13
        },
        {
            "author": "Ryan McKinley",
            "body": "This is a band-aid, but we could consider adding something like:\n{code:java}\n  public void setCacheValidBitsForFields( Set<String> names );\n{code}\non FieldCache, then checking if the field is in that set before making the BitSet\n\nWhen solr reads the schema, it could look for any fields have sortMissingLast and then call:\n{code:java}\n  FieldCache.DEFAULT.setCacheValidBitsForFields()\n{code}\n\nThe factory idea also sounds good, but i don't see how would work without big big changes",
            "date": "2010-09-17T15:12:52.990+0000",
            "id": 14
        },
        {
            "author": "Shai Erera",
            "body": "bq. Are there any advantages to using payloads over CSF for fieldcache population?\n\nWell .. payloads already exist (in my code :)), while CSF is \"just around the corner\" for a long time. While the two ultimately achieve the same goal, CSF is more generic than just payloads, and if we'd want to take advantage of it w/ FieldCache, I assume we'll need to make more changes to FieldCache, because w/ CSF, people can store arbitrary byte[] and request to cache them. So sorting data is a subset of CSF indeed, but I think the road to CSF + CSF-FieldCache integration is long. But perhaps I'm not up-to-date and there is progress / someone actually working on CSF?\n\nAnyway, opening up FC to read from payloads seems to me a much easier solution, because besides reading the stuff from the payload, the rest of the classes continue to work the same (TopFieldCollector, Comparators etc.).\n\nMaybe a slight change to SortField will be required as well though, not sure yet.",
            "date": "2010-09-17T15:20:01.335+0000",
            "id": 15
        },
        {
            "author": "Ryan McKinley",
            "body": "Uwe: \"For DocTerms the approach is not needed...\"\n\nYa I realized this after looking at the patch I first submitted.  In the first patch, the cache holds a CachedObject rather then just an Object.  In the second, I changed back to just an Object so it does not need to wrap the DocTerms or DocTermsIndex\n\nFor the RangeFilter, with optional Bits calculation, that could would look somethign like:\n{code:java}\n\n        LongValues cached = FieldCache.DEFAULT.getLongValues(reader, field, (FieldCache.LongParser) parser);\n        final long[] values = cached.values;\n        if( cached.valid == null ) {\n          // ignore deleted docs if range doesn't contain 0\n          return new FieldCacheDocIdSet(reader, !(inclusiveLowerPoint <= 0L && inclusiveUpperPoint >= 0L)) {\n            @Override\n            boolean matchDoc(int doc) {\n              return values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;\n            }\n          };\n        }\n        else {\n          final Bits valid = cached.valid;\n          return new FieldCacheDocIdSet(reader, true) {\n            @Override\n            boolean matchDoc(int doc) {\n              return valid.get(doc) && values[doc] >= inclusiveLowerPoint && values[doc] <= inclusiveUpperPoint;\n            }\n          };\n        }\n{code}",
            "date": "2010-09-17T15:23:12.267+0000",
            "id": 16
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. public void setCacheValidBitsForFields( Set<String> names );\n\nSolr doesn't even know all of the fields at the time it reads it's schema.  And even if it did... this would seem to break multi-core or anything trying to have more than one index where the fields are different.  Seems like this needs to be passed down via SortField, just like FieldCache.Parser.  A factory makes this a more generic method than adding additional params to SortField every time we think of something like this... then we can add stuff like getFieldCacheParser() and other stuff to the factory.\n",
            "date": "2010-09-17T15:41:21.029+0000",
            "id": 17
        },
        {
            "author": "Ryan McKinley",
            "body": "oh right -- thats true.  Is a global flag sufficient?  \n\nIn lucene it could default to false and in solr default to true.\n\nI know we don't want to just keep adding more things to memory, but I'm not sure there is a huge win by selectively enabling and disabling some fields.",
            "date": "2010-09-17T15:52:38.961+0000",
            "id": 18
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. oh right - thats true. Is a global flag sufficient? \n\nYeah, solr could just always default it to on.  We don't know what kind of ad-hoc queries people will throw at solr and the 3% size increase (general case 1/32) seems completely worth it.",
            "date": "2010-09-17T16:00:49.580+0000",
            "id": 19
        },
        {
            "author": "Ryan McKinley",
            "body": "I added a static flag to CachedArray:\n{code:java}\n  public abstract static class CachedArray {\n    public static boolean CACHE_VALID_ARRAY_BITS = false;\n    \n    public final Bits valid;\n    public CachedArray( Bits valid ) {\n      this.valid = valid;\n    }\n  };\n{code}\nand then set it to true in the SolrCore static initalizer.\n\nIf folks are ok with this approach, I'll clean up the javadocs etc",
            "date": "2010-09-17T16:31:54.667+0000",
            "id": 20
        },
        {
            "author": "Ryan McKinley",
            "body": "FYI, I like the idea of revisiting the FieldCache, but i don't see a straightforward path.",
            "date": "2010-09-17T16:33:19.998+0000",
            "id": 21
        },
        {
            "author": "Uwe Schindler",
            "body": "I am against the configuration option to enable the additional BitSet. The problem is that you cannot control it for each usage for the FieldCache, as it is a static flag. We agreed in the past that we will remove all static defaults from Lucene (e.g. BQ.maxClauseCount) together with sytem properties. This flag can cause strange problems with 3rd party code (like when you lower the BQ maxClauseCount and suddenly your queries fail).\n\nThe overhead by the OpenBitSet is very marginal (for integers only 1/32, as Yonik said). If you have memory problems with the FieldCache, these 1/32 would not hurt you, as you should think about your whole configuration then (liek moving from ints to shorts or something like that).\n\nSo: Please don't add any static defaults or sysprops! Please, please, please!",
            "date": "2010-09-17T16:43:55.389+0000",
            "id": 22
        },
        {
            "author": "Mark Miller",
            "body": "bq. I was suggesting handling it the same way as FieldCache.Parser - it's set on the SortField. But instead of just being able to control parsing of a term (which is too limited), it needs to be able to control everything. (This would solve Shai's needs too)\n\nWe started down this path with LUCENE-831 - you could pass some *UnInverter on the sort field if i remember right, so that pretty much everything could be overridden. It has come up a lot - we really need this level of customizability eventually.",
            "date": "2010-09-17T16:56:44.354+0000",
            "id": 23
        },
        {
            "author": "Ryan McKinley",
            "body": "I'm all for dropping the static flag and always calculating the valid bits -- it makes things accurate with minimal cost.  \n\nI am sympathetic to folks who don't want this, and I'm not sure the cleanest way to support both options, or even if it is actually worthwhile.\n\nDo people see this 'option' as a showstopper?  If so, is there an easy way to configure?  without statics, the flag would need to be fetched from each parser, and the parser does not know what FieldCache it is used from (using FieldCache.DEFAULT is just as bad as the static flag IIUC)\n\n",
            "date": "2010-09-17T17:00:46.732+0000",
            "id": 24
        },
        {
            "author": "Marvin Humphrey",
            "body": "> So: Please don't add any static defaults or sysprops! Please, please, please!\n\n+1\n\nNo global variables which control behavior, please.",
            "date": "2010-09-17T17:11:47.165+0000",
            "id": 25
        },
        {
            "author": "Michael McCandless",
            "body": "I know it's only 3% (for ints... 12.5% for bytes), but, 3% here, 3% there and suddenly we're talking real money.\n\nLucene can only stay lean and mean if we don't allow these little 3% losses here and there!!\n\nLet's try to find some baby-step (even if not clean -- we know FieldCache, somehow, needs to be fixed more generally) for today?\n",
            "date": "2010-09-17T17:28:26.541+0000",
            "id": 26
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. Let's try to find some baby-step (even if not clean - we know FieldCache, somehow, needs to be fixed more generally) for today?\n\nThe cheapest option might be:\n\n{code}\n  public interface Parser extends Serializable {\n    public boolean recordMissing();\n  }\n{code}\n\nA better option is to replace FieldCache.Parser in SortField to be FieldCache.EntryCreator.\n\nOh, and if we're recording all the set bits, it would be really nice to record both\n- the number of values set\n- the number of unique values encountered\n\nBoth should be zero or non-measurable cost (a counter++ that does not produce a data dependency can be executed in parallel on a free int execution unit)\n",
            "date": "2010-09-17T18:01:14.145+0000",
            "id": 27
        },
        {
            "author": "Ryan McKinley",
            "body": "Are people generally ok with the idea of global on/off?  I think that is a reasonable approach... I agree that we should avoid static fields to control behavior.  But do we avoid it at the cost of not allowing the option, or waiting till we rework FieldCache?\n\nIf the consensus is that FieldCache needs to be reworked *before* somethign like this could be added, that's fine... i'll move on to other things.  Any relatively easy suggestions for how to enable the option without a global static?  (Note that FieldCache is already a global static -- at leaset FieldCache.DEFAULT is referenced a lot)\n\nPerhaps this could/should live in /trunk until a cleaner solution is viable?\n\n\n\n",
            "date": "2010-09-17T18:02:10.863+0000",
            "id": 28
        },
        {
            "author": "Uwe Schindler",
            "body": "I am against that option! No static defaults! (and if it *must* be there - default it to true on Lucene, too).\n\nbq. the number of values set \n\nThis is OpenBitSet.cardinality() ? I dont think we should add this extra cost during creation, as it can be retrieved quite easy if really needed.",
            "date": "2010-09-17T18:11:14.044+0000",
            "id": 29
        },
        {
            "author": "Ryan McKinley",
            "body": "I like the idea of optionally caching the numdocs and unique values -- that would make sorting by this field faster -- the ArrayValues class could be easily augmented with this.\n\nThe problem with augmenting the Parser class as you suggest is that we would have to rejiggy everything that touches parser.  We would need different default classes for things that want or don't want the missing records.  How do we handle this big:\n{code:java}\nif (parser == null) {\n        try {\n          return wrapper.getIntValues(reader, field, DEFAULT_INT_PARSER);\n        } catch (NumberFormatException ne) {\n          return wrapper.getIntValues(reader, field, NUMERIC_UTILS_INT_PARSER);      \n        }\n      }\n{code}\nyuck\n",
            "date": "2010-09-17T18:15:49.343+0000",
            "id": 30
        },
        {
            "author": "Yonik Seeley",
            "body": "If ya care - don't pass a null parser!  Otherwise you get the default.\n\nbq. This is OpenBitSet.cardinality()\n\nWhich isn't free... and calculating it over and over again is silly if you care about those numbers.\n\nbq. I dont think we should add this extra cost during creation,\n\nI don't think it will add extra cost.  I could be wrong, but I don't think it will be measurable.",
            "date": "2010-09-17T18:26:58.783+0000",
            "id": 31
        },
        {
            "author": "Ryan McKinley",
            "body": "bq. If ya care - don't pass a null parser! Otherwise you get the default. \n\nWhat if I care, but somethign else (that does not care) asks for the value first?  Seems odd to have so much depend on *who* asks for the value first\n\nbq. A better option is to replace FieldCache.Parser in SortField to be FieldCache.EntryCreator.\n\nHow would that work?  What if a filter creates the cache before the SortField?  ",
            "date": "2010-09-17T18:49:44.483+0000",
            "id": 32
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. What if I care, but somethign else (that does not care) asks for the value first? Seems odd to have so much depend on who asks for the value first\n\nAs long as it *can* be passed everywhere that matters, then it's up to the application - which knows if it ever needs the missing values or not for that field.  For solr, we could make it configurable per-field... but I'd prob default it to ON to avoid unpredictable weirdness.\n\nbq. What if a filter creates the cache before the SortField?\n\nIf we have a filter that uses the field cache, then it should also be able to specify the same stuff that SortField can.",
            "date": "2010-09-17T19:04:18.903+0000",
            "id": 33
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. I agree that we should avoid static fields to control behavior. But do we avoid it at the cost of not allowing the option, or waiting till we rework FieldCache?\n\nI agree with this sentiment - progress, not perfection.  Being able to turn it on or off for everything in the process is better than nothing at all.",
            "date": "2010-09-17T19:17:06.593+0000",
            "id": 34
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. How would that work?\n\nWe could start off simple - add only recordMissing functionallity and punt on the rest, while still leaving a place to add it.\n\n{code}\nclass FieldCache {\n\n  public static class EntryCreator {\n    public boolean recordMissing() {\n      return false;\n    }\n    \n    public abstract Parser getParser();\n  }\n{code}\n\nNot even sure if a whole hierarchy is needed at this point... in the future, we'd prob want\n\n{code}\n  public static EntryCreatorInt extends EntryCreator {\n    public IntValues getIntValues(IndexReader reader, String field) {... code currently in FieldCacheImpl that fills the fieldCahe...}\n    ...\n  }\n{code}\n",
            "date": "2010-09-17T19:34:39.449+0000",
            "id": 35
        },
        {
            "author": "Ryan McKinley",
            "body": "Maybe, but I'm still not sure this cleans things up enough to be worth the trouble -- ideally the API should be easy to have consistent results.  I don't like that it would be too easy to mess things up if you the application does not use the same parser from various components (that may be in different libraries etc).  Conceptually it makes sense to have settings about what is or is not cached attached to the FieldCache itself, not to the things that ask the FieldCache for its values -- and letting whoever asks first set the behavior for the next guy who asks (regardless of what they ask for!).  \n\nIf we are going to make it essentially required to always pass in the right Parser/EntryCreator, we should at least remove all the ways of not passing one in -- since that call is saying \"use what ever is there, and the next guy who asks should be ok with it too\"\n\nDoes something like the EntryCreator idea fix -- or at least begin to fix -- the other FieldCache issues?  If not, is it really worth introducing just to avoid a static variable?\n\nI think the best near term option is live with the static initializer, and fix it when the we rework the FieldCache to fix a host of other issues.  For solr the default will be set to always calculate, for lucene... we will let Mike and Uwe duke it out :)\n\n\n\n\n\n\n\n",
            "date": "2010-09-17T21:45:15.755+0000",
            "id": 36
        },
        {
            "author": "Uwe Schindler",
            "body": "Supporting different parsers is not an issue at all. You can call getBytes() with different parsers, you simply create two entries in the cache, as each parser produces a different cache instance. And getBytes() without parser is also fine, as then you get the default parser from the cache (which would not create a third instance!). - [Parser is part of the cache key]",
            "date": "2010-09-17T21:51:07.017+0000",
            "id": 37
        },
        {
            "author": "Ryan McKinley",
            "body": "I thought of  an optimization that could reduce memory usage...  \n\nIf all non-deleted documents have a value, we don't need a real BitSet -- just a Bits implementation that always returns true.\n\nThat should save 3% (or 12.5%) here and there.\n\n- - - - - -\n\nOn other thing to consider... do we want to remove the getXXXX functions that do not pass in a Parser?  passing in null, is equivalent?",
            "date": "2010-09-19T00:21:34.363+0000",
            "id": 38
        },
        {
            "author": "Ryan McKinley",
            "body": "Here is the code for ByteValues that:\n# optionally stores the BitSet via static config\n# does not cache a real BitSet unless only some docs match\n# calculates numDocs/numTerms\n\n{code:java}\n\n    @Override\n    protected ByteValues createValue(IndexReader reader, Entry entryKey) throws IOException {\n      Entry entry = entryKey;\n      String field = entry.field;\n      ByteParser parser = (ByteParser) entry.custom;\n      if (parser == null) {\n        return wrapper.getByteValues(reader, field, FieldCache.DEFAULT_BYTE_PARSER);\n      }\n      int numDocs = 0;\n      int numTerms = 0;\n      int maxDoc = reader.maxDoc();\n      final byte[] retArray = new byte[maxDoc];\n      Bits valid = null;\n      Terms terms = MultiFields.getTerms(reader, field);\n      if (terms != null) {\n        final TermsEnum termsEnum = terms.iterator();\n        final Bits delDocs = MultiFields.getDeletedDocs(reader);\n        final OpenBitSet validBits = new OpenBitSet( maxDoc );\n        DocsEnum docs = null;\n        try {\n          while(true) {\n            final BytesRef term = termsEnum.next();\n            if (term == null) {\n              break;\n            }\n            final byte termval = parser.parseByte(term);\n            docs = termsEnum.docs(delDocs, docs);\n            while (true) {\n              final int docID = docs.nextDoc();\n              if (docID == DocsEnum.NO_MORE_DOCS) {\n                break;\n              }\n              retArray[docID] = termval;\n              validBits.set( docID );\n              numDocs++;\n            }\n            numTerms++;\n          }\n        } catch (StopFillCacheException stop) {}\n        \n        // If all non-deleted docs are valid we don't need the bitset in memory\n        if( numDocs > 0 && CachedArray.CACHE_VALID_ARRAY_BITS ) {\n          boolean matchesAllDocs = true;\n          for( int i=0; i<maxDoc; i++ ) {\n            if( !delDocs.get(i) && !validBits.get(i) ) {\n              matchesAllDocs = false;\n              break;\n            }\n          }\n          if( matchesAllDocs ) {\n            valid = new Bits.MatchAllBits( maxDoc );\n          }\n          else {\n            valid = validBits;\n          }\n        }\n      }\n      if( numDocs < 1 ) {\n        valid = new Bits.MatchNoBits( maxDoc );\n      }\n      return new ByteValues( retArray, valid, numDocs, numTerms );\n    }\n{code}",
            "date": "2010-09-19T03:44:12.603+0000",
            "id": 39
        },
        {
            "author": "Ryan McKinley",
            "body": "Any thoughts on this?\n\nI think the best move forward is to:\na. optimize as much as possible\nb. drop the no-parser function option\nc. optionally store the bitset via static config (ugly, but lesser of many ugly options)\nd. set lucene default=false (actually I don't care)\ne. set solr default=true\n\nUnless there are objections, I will clean up the patch, fix javadoc, tests, etc",
            "date": "2010-09-22T02:02:08.369+0000",
            "id": 40
        },
        {
            "author": "Uwe Schindler",
            "body": "Also set the Lucene default to true, as I want to improve sorting and FCRF.",
            "date": "2010-09-22T02:15:17.304+0000",
            "id": 41
        },
        {
            "author": "Ryan McKinley",
            "body": "Here is a (hopefully) final patch that adds a bunch of tests to exercise the the 'valid' bits (and check that MatchAll is used when appropriate)",
            "date": "2010-09-22T06:36:01.150+0000",
            "id": 42
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Ryan,\n\nfew comments:\n- the tests if all/no bits are set and so the special Bits implementations are returned are fine, but the special case for all bits are valid may be a little bit ineffective and seldom\n- please use the correct Java code style (\"{\" should be at the end of previous line and not in separate line for method declarations), the Eclipse code style is available in Wiki",
            "date": "2010-09-22T06:59:47.374+0000",
            "id": 43
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Also set the Lucene default to true\n\nPlease don't!\n\nbq. as I want to improve sorting and FCRF.\n\nBut: sorting, FCRF must continue to work if the app chooses not to\nload valid bits, right?\n\nOther feedback on current patch:\n\n  * We don't have to @Deprecate for 4.0 -- just remove it, and note\n    this in MIGRATE.txt.  (Though for 3.x we need the deprecation, so\n    maybe do 3.x patch first, then remove deprecations for 4.0?).\n\n  * FieldCache.EntryCreator looks orphan'd?\n\nIt looks like the valid bits will not reflect deletions (by design),\nright?  Ie caller cannot rely on valid always incorporating deleted\ndocs.  (Eg the MatchAll opto disregards deletions, and, a reopened\nsegment can have new deletions yet shares the FC entry).\n\nThe static config still also bothers me... and, going that route means\nwe must agree on a default (which is looking hard!).\n\nWhat if we:\n\n  * Allow specifying \"loadValidBits\" on each load (eg via Parser or\n    separate arg to FC.getXXValues), but,\n\n  * We separately cache the valid bits (we'd still have the XXXValues\n    returned, to include the valid bits & values).\n\nThis way if an app \"messes up\", they do not end up double-storing the\nactual values, ie the worst that happens is they have to re-invert\njust to generate the valid bits.  Even that should be fairly rare, ie,\nif they use MissingStringLastComparator it'll init both values & valid\nbits entries in the cache on the first go.\n",
            "date": "2010-09-22T10:18:03.538+0000",
            "id": 44
        },
        {
            "author": "Robert Muir",
            "body": "bq. Also set the Lucene default to true, as I want to improve sorting and FCRF.\n\nbq. I know it's only 3% (for ints... 12.5% for bytes), but, 3% here, 3% there and suddenly we're talking real money.\n\nI'm having trouble understanding the use case for this bitset.\n\nThe jira issue says to add a bitset, but doesnt explain why.\n\nThe linked thread talks about this being useful for sorting missing values last, but I don't think this justifies\nincreasing the size of fieldcache by default.\n",
            "date": "2010-09-22T11:29:21.687+0000",
            "id": 45
        },
        {
            "author": "Ryan McKinley",
            "body": "Here is a new patch that removes the static config.  Rather then put a property on Parser class, I added a class:\n{code:java}\n  public abstract static class CacheConfig {\n    public abstract boolean cacheValidBits();\n  }\n{code}\nand this gets passed to the getXXXValues function:\n{code:java}\nByteValues getByteValues(IndexReader reader, String field, ByteParser parser, CacheConfig config)\n{code}\n\nI think this is a better option then adding a parameter to Parser since we can have an easy upgrade path.  Parser is an interface, so we can not just add to it without breaking compatibility.  To change things in 4.x, 3.x should have an upgrade path.\n\nI took Mike's suggestion and include the CacheConfig hashcode in the Cache key -- however, I don't cache the Bits separately since this is an edge case that *should* be avoided, but at least does not fail if you are not consistent.\n\nThis does cache a MatchAllBits even when 'cacheValidBits' is false, since that is small (a small class with one int)\n\n-----------\n\nbq.     *  We don't have to @Deprecate for 4.0 - just remove it, and note this in MIGRATE.txt. (Though for 3.x we need the deprecation, so maybe do 3.x patch first, then remove deprecations for 4.0?).\n\nMy plan was to apply with deprecations to 4.x, then merge with 3.x.  Then replace the calls in 4.x, then remove the old functions.  Does this sound reasonable?\n\nI would like this to get in 3.x since we could then remove many solr types in 4.x and have a 3.x migration path.\n\nbq.  * FieldCache.EntryCreator looks orphan'd?\n\ndooh, thanks\n\n\nbq. It looks like the valid bits will not reflect deletions (by design), right? Ie caller cannot rely on valid always incorporating deleted docs. (Eg the MatchAll opto disregards deletions, and, a reopened segment can have new deletions yet shares the FC entry).\n\nRight, the ValidBits are only checked for docs that exists (and the FC values are only set for docs that exists -- this has not changed), and may contain false positives for deleted docs.  I think this is OK since most use cases (i can think of) deal with deletions anyway.   Any ideas how/if we should change this?  (I did not realize that the FC is reused after deletions -- so clever)\n\n----------------\n\nbq. I'm having trouble understanding the use case for this bitset.\n\nMy motivation is for supporting the supportMissingLast feature in solr sorting (that could now be pushed to lucene).  For example if I have a bunch of documents and only some have the field \"bytes\" -- sorting 'bytes desc' works great, but sorting 'bytes asc' puts all the documents that do not have the field 'bytes' first since the FieldCache thinks they are all zero.\n\nIf we get this working in solr, we can deprecate and delete all the \"sortable\" number fields and have that same functionality on Trie* fields.\n\n\n\n\n\n",
            "date": "2010-09-22T15:36:35.430+0000",
            "id": 46
        },
        {
            "author": "Yonik Seeley",
            "body": "If folks think that being able to tell a real \"0\" from a missing value is not useful for Lucene, we\ncould extend Ryan's CacheConfig to include a factory method that creates / populates ByteValues, IntValues, etc.\nThen all the bitset stuff could be kept in Solr only.  I'm sensitive about pushing stuff into Lucene that is *only* useful for Solr.\n\n",
            "date": "2010-09-22T16:00:44.241+0000",
            "id": 47
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. I think this is a better option then adding a parameter to Parser since we can have an easy upgrade path. Parser is an interface, so we can not just add to it without breaking compatibility. To change things in 4.x, 3.x should have an upgrade path.\n\nHmm... I'd rather make an exception to 3.x, ie, allow the addition of\nthis method to the interface, than confuse the 4.x API, going forward,\nwith 2 classes?\n\nCreating a custom FieldCache parser is an extremely advanced use\ncase... very few users do this, and those that do will grok this\nmethod?\n\nbq. However, I don't cache the Bits separately since this is an edge case that should be avoided, but at least does not fail if you are not consistent.\n\nThis makes me nervous since it can now lead to further cases of field\ncache insanity, ie, you loaded it once w/o the valid bits, and again\nw/ the valid bits, and now your values array is taking up 2X the RAM.\n\nIt's already bad enough that FC allows one kind of insanity :)\n\nbq. This does cache a MatchAllBits even when 'cacheValidBits' is false, since that is small (a small class with one int)\n\nHmm... but if I pass false here, it shouldn't spend any time\nallocating the bit set, building it, checking the bit set for \"all\nbits set\", etc.?\n\n{quote}\nbq.     *  We don't have to @Deprecate for 4.0 - just remove it, and note this in MIGRATE.txt. (Though for 3.x we need the deprecation, so maybe do 3.x patch first, then remove deprecations for 4.0?).\n\nMy plan was to apply with deprecations to 4.x, then merge with 3.x.  Then replace the calls in 4.x, then remove the old functions.  Does this sound reasonable?\n{quote}\n\nOK that sounds like a good plan!\n\nbq. Right, the ValidBits are only checked for docs that exists (and the FC values are only set for docs that exists -- this has not changed), and may contain false positives for deleted docs.  I think this is OK since most use cases (i can think of) deal with deletions anyway.   Any ideas how/if we should change this?\n\nI think this is the right approach -- expecting FC's valid bits to\ntake deletions into account is too much.  We have IR.getDeletedDocs\nfor this.\n\nBut, eg this means classes like FCRF will still have to consult\ndeleted docs.\n\nReally, \"in general\" we need a better way for the query execution path\nto enforce deleted docs.  Eg if the FCRF will be AND'd w/ a query\nthat's already excluding del docs then it need not be careful about\ndeletions...\n\nbq.  (I did not realize that the FC is reused after deletions -- so clever)\n\nHa!  There was a time when it didn't ;)\n",
            "date": "2010-09-22T16:23:22.068+0000",
            "id": 48
        },
        {
            "author": "Michael McCandless",
            "body": "bq. My motivation is for supporting the supportMissingLast feature in solr sorting (that could now be pushed to lucene).\n\n{quote}\nIf folks think that being able to tell a real \"0\" from a missing value is not useful for Lucene, we\ncould extend Ryan's CacheConfig to include a factory method that creates / populates ByteValues, IntValues, etc.\nThen all the bitset stuff could be kept in Solr only.  I'm sensitive about pushing stuff into Lucene that is *only* useful for Solr.\n{quote}\n\nI'm very much +1 for making this (exposing thea valid bitset) possible\nin Lucene.\n\nUsers have asked over time how they can tell if a given doc has a field value.\n\nAnd being able to distinguish missing values, eg to sort them last, or\nto do something else, is useful.  Once we do this we should also\n[eventually] move \"sort missing last\" capability into Lucene's\ncomparators.\n",
            "date": "2010-09-22T16:24:51.761+0000",
            "id": 49
        },
        {
            "author": "Uwe Schindler",
            "body": "I am also strongly +1 for the additional Bits interface (as Ryan did, it does not always need to be a real OpenBitSet, so when no deletions and all things set, we can use a dummy one).\nI had often use cases where i needed the information, if this document really has a value set or not, and i don't use Solr so much.\n\n{quote}\nAnd being able to distinguish missing values, eg to sort them last, or\nto do something else, is useful. Once we do this we should also\n[eventually] move \"sort missing last\" capability into Lucene's\ncomparators.\n{quote}\n\n+1\n\n{quote}\nI think this is the right approach - expecting FC's valid bits to\ntake deletions into account is too much. We have IR.getDeletedDocs\nfor this.\n{quote}\n\nWe don't need to AND them together, maybe simply wrap the OpenBitset by a custom Bits impl, that ands in the getter? But as deletions are separated in IndexReader and the cache can reuse the cache even when new deletions are added, i think keeping it separate is fine.\n\nAbout the whole bit set: Do we really need to couple the Bits interface to the type? Because if you exchange the parser/native type (e.g. parse ints as byte), the valid docs are still the same, only the native type representation is different. So how about we add a getBits(field) method to FieldCache that returns the valid docs. If field was not yet retrieved as a native type it could throw IllegalStateEx, else it would return the Bits interface (globally, but per field, but not per parser/datatype) created during the last FC polulation run? We have then also the possibility to disable the default generation of Bits and do it lazily (which should run faster, as it does not need to parse the values, only enumerate terms and termdocs).\n\n{quote}\nReally, \"in general\" we need a better way for the query execution path\nto enforce deleted docs. Eg if the FCRF will be AND'd w/ a query\nthat's already excluding del docs then it need not be careful about\ndeletions...\n{quote}\n\nThats another thing, but maybe we remove deleted docs completely from query processing and simply apply it like a filter before the collector. Not sure about the implications and performance.",
            "date": "2010-09-22T16:47:55.350+0000",
            "id": 50
        },
        {
            "author": "Ryan McKinley",
            "body": "bq. Hmm... I'd rather make an exception to 3.x, ie, allow the addition of this method to the interface, than confuse the 4.x API, going forward, with 2 classes?\n\nThat is OK with me.  Would be cleaner and simpler.  (though semantically it does not make sense to me -- why ask the parser what to cache?)\n\n\nbq. >>    This does cache a MatchAllBits even when 'cacheValidBits' is false, since that is small (a small class with one int)\nbq. Hmm... but if I pass false here, it shouldn't spend any time allocating the bit set, building it, checking the bit set for \"all bits set\", etc.?\n\nWell it does not try *hard*, only if numDocs==maxDocs, it does not look at anything.  If the cost of caching new MatchAllBits( maxDocs ) isn't worth occasional win by knowing all the values are valid, then I will remove it.\n\n\nbq. So how about we add a getBits(field)\n\nInteresting... i'll mess for a bit and let you know what I think :)\n\nrather then throwing an exception, that might be a flag, since I could imagin many thigns would use the Bits if they exist and do something else if they dont\n\n\n\n\n",
            "date": "2010-09-22T17:29:07.192+0000",
            "id": 51
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Hmm... I'd rather make an exception to 3.x, ie, allow the addition of this method to the interface, than confuse the 4.x API, going forward, with 2 classes?\n\nSame here, we already defined the FieldCache \"interface\" as subject to change. Mabye we should simply remove it in trunk and only have a class? This interface was never of any use, because you were not able to supply any other field cache implementation (the DEFAULT field is *final* because all fields in interfaces are defined as *final* by the Java Language Spec.",
            "date": "2010-09-22T17:34:37.581+0000",
            "id": 52
        },
        {
            "author": "Yonik Seeley",
            "body": "Regardless of if there is a separate getBits(field), I think we should add/use ByteValues, IntValues, etc.  It's just so much more extensible going forward.\n",
            "date": "2010-09-22T17:35:27.117+0000",
            "id": 53
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Regardless of if there is a separate getBits(field), I think we should add/use ByteValues, IntValues, etc. It's just so much more extensible going forward.\n\n+1",
            "date": "2010-09-22T17:39:35.687+0000",
            "id": 54
        },
        {
            "author": "Michael McCandless",
            "body": "bq.  So how about we add a getBits(field) method to FieldCache that returns the valid docs. \n\nThis would be great!\n\nAssuming we separately cache the valid docs, we could then allow caching *only* valid docs, for apps that want to know if a doc has a value but do not need the full array of values RAM resident.",
            "date": "2010-09-22T17:41:30.294+0000",
            "id": 55
        },
        {
            "author": "Michael McCandless",
            "body": "bq. That is OK with me. Would be cleaner and simpler. (though semantically it does not make sense to me - why ask the parser what to cache?)\n\nYeah that is weird.  Maybe we rename Parser -> EntryCreator?\n",
            "date": "2010-09-22T17:49:20.723+0000",
            "id": 56
        },
        {
            "author": "Uwe Schindler",
            "body": "We should also allow the parser to stop iterating term without the strange StopFillCacheException (needed for Numeric fields).",
            "date": "2010-09-22T17:52:18.251+0000",
            "id": 57
        },
        {
            "author": "Ryan McKinley",
            "body": "bq. Regardless of if there is a separate getBits(field), I think we should add/use ByteValues, IntValues, etc. It's just so much more extensible going forward.\n\nIf we have a separate getBits( field ) call, should the Bits be added to the XxxValues class?  I suspect not.\n\n \n",
            "date": "2010-09-22T18:11:58.899+0000",
            "id": 58
        },
        {
            "author": "Michael McCandless",
            "body": "bq. If we have a separate getBits( field ) call, should the Bits be added to the XxxValues class? I suspect not.\n\nYeah I think it need not be.\n\nBut, EntryCreator still should be able to state that it'd like the bits computed & cached as a side effect?  And, if the bits wasn't already computed, then they'd be computed on-demand?  (This enables caching only valid bits and not the values array).\n\nIf we do this then we can leave cutover to XXXValues (still a good idea) as a separate issue?",
            "date": "2010-09-22T18:19:09.276+0000",
            "id": 59
        },
        {
            "author": "Uwe Schindler",
            "body": "I think it should be a separate entry in the cache. Only that its only regenerated, if it does not already exist for the field/IR combination. So there are these combinations:\n\n- cache is purged (inititially)\n- user calls getBytes(field) -> bytes[] is filled, also the Bits\n- user calls additionally getInts(field) for some reason -> int[] is filled, but as Bits already exit they dont need to be filled\n- user calls getBits(field) -> returns the pre-filled bits from one of the previous calls\n\nAlternatively:\n\n- Cache is purged (initially)\n- User calls getBits(field), there are no bits available until now, bits are populated from TermEnum/TermDocs, but no byte[] or int[], as not requested\n\nWith a config option, one could switch automatic polulation of Bits off, so in the first combination, only the last call to getBits() would populate bit set.\n\nI all cases this is easily possible by having a separate cache with separate population method for the bits. If some method like getBytes() also populates the Bits, it should simply add the created Bits manually to the cache.\n\nInstead of a global config option, we could simply add a Boolean to the getXxx methods, to tell the cache if it should also populate Bits (if not already done?). The default would maybe false for Lucene, but solr would always pass true.\n\nDoes this sound like a plan?",
            "date": "2010-09-22T18:22:20.071+0000",
            "id": 60
        },
        {
            "author": "Ryan McKinley",
            "body": "bq. If we do this then we can leave cutover to XXXValues (still a good idea) as a separate issue?\n\nI'm already in deep... I'd like to keep the XxxValues in this patch, and use a different one to bring everything up-to-date with the new API in 3.x/4.x\n\n",
            "date": "2010-09-22T18:23:32.140+0000",
            "id": 61
        },
        {
            "author": "Yonik Seeley",
            "body": "bq.  If we have a separate getBits( field ) call, should the Bits be added to the XxxValues class? I suspect not.\n\nSeems a lot easier just to put all the optional stuff on the *Values class - more performant too (and avoid more synchronizing lookups)\n",
            "date": "2010-09-22T18:24:47.237+0000",
            "id": 62
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. I all cases this is easily possible by having a separate cache with separate population method for the bits.\n\nThis seems more complex, and less extensible.  What's the issue with just putting the bits reference down on CachedArray?",
            "date": "2010-09-22T18:27:19.746+0000",
            "id": 63
        },
        {
            "author": "Uwe Schindler",
            "body": "It's not more complicated its easier. The Bits are a real separate thing, its just a cache of the information, if values are assigned or not. It does not depend on the data type like byte, int,... Its just separate. And as said before, if somebody requests the same field in different types, it would only have one bits. Also one could request the Bits alone, or could first request a field without bits (using a boolean) and later again with bits, in which case those are lazily loaded\n\nTh implementation would be simple, similar to the way, how the caches are filled for NumericFields (it adds the values two times to the cache, with the null parser and the real used parser). I this case youl would simply request the bits cache on e.g. the int[] creation, if not availabe, also build the bits in parallel and add to bits cache.",
            "date": "2010-09-22T18:34:38.870+0000",
            "id": 64
        },
        {
            "author": "Michael McCandless",
            "body": "bq. What's the issue with just putting the bits reference down on CachedArray?\n\nOne risk was added insanity cases, ie you looked up once w/ the bits and later w/o and it double-stores the values array.\n\nAnother gain of separating the bits retrieval is it becomes possible to get only the valid bits (ie, w/o a value array), for apps that just want to know if a given doc had a field.\n\nBut we could probably still achieve these two benefits while using a single class for looking up everything \"cached\" about a field?  Ie, the CachedArray could return non-null bits but null values?",
            "date": "2010-09-22T18:37:46.357+0000",
            "id": 65
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. But we could probably still achieve these two benefits while using a single class for looking up everything \"cached\" about a field? Ie, the CachedArray could return non-null bits but null values?\n\nExactly.\nAnd with NRT and increasing number of segments, the number of synchronized lookups per segment could really start to matter.",
            "date": "2010-09-22T18:41:02.594+0000",
            "id": 66
        },
        {
            "author": "Ryan McKinley",
            "body": "bq. more performant too (and avoid more synchronizing lookups)\n\nThis was my motivation for putting it all together and using the options as part of the key.  But yes, inconsistent usage will eat up RAM.  That is better then my original fear that inconsistent usage would give you unpredictable results!\n\nAlso, with the current Cache implementaion, we would need to somehow be able to add two cache entries from within Cache.createEntry() -- either rewrite all that or hack in some way to pass the FieldCache to the createEntry method.\n\nKeeping the values and bits in different cache entries is pretty ugly (especially for the normal case where we want them both all the time)\n{code:java}\n    return new ByteValues() {\n      (byte[])caches.get(Byte.TYPE).get(reader, new Entry(field, parser??)),\n      (Bits)caches.get(Byte.TYPE).get(reader, new Entry(field, parser??)),\n    };\n{code}\n\n\nbq. But we could probably still achieve these two benefits while using a single class for looking up everything \"cached\" about a field? Ie, the CachedArray could return non-null bits but null values?\n\nBrainstorming here...  if Parser -> EnteryCreator and the 'EntryCreator.hashCode()' is used as the map key (as it is now)\n\n{code:java}\n  public abstract class EntryCreator implements Serializable {\n    public boolean cacheValidBits() {\n      return false;\n    }\n    public boolean cacheValues() {\n      return true;\n    }\n  }\n{code}\n\nThe trick would be to use the same *key* regardless of what we ask for (values but no bits - bits but no values - bits and values, etc) and then fill up whatever is missing if it is not in the existing cache.\n\nThat might suggest that the 'class' could be the key, but setting the cacheValidBits/values would need to get implemented by inheratance, so that is out.\n\nother ideas?  directions I am not thinking about?\n\n\n\n\n",
            "date": "2010-09-22T19:33:10.708+0000",
            "id": 67
        },
        {
            "author": "J.J. Larrea",
            "body": "I only just waded through this thread, so apologies in advance if this is redundant or off-topic...\n\nIt seems to me that there could and should be a standalone enhancement to FieldCache/FCImpl to support Boolean-valued fields. \n\nSince there is no native array-of-bits in Java, it could have the signature:\n\n    BitSet getBits(IndexReader reader, String field, BooleanParser parser)  [implementation returning an OpenBitSet for efficiency]\n\nA pre-supplied BooleanParser implementation StringMatchBooleanParser could map any of one of a set of uncased strings to true, and a default subclass eg. DefaultStringMatchBooleanParser could supply { \"T\", \"TRUE\", \"1\", \"Y\", \"YES\" } for the set of strings.  So the defaulted and typical case getBits( ir, \"field\" ) would do what one typically expects of boolean-valued fields.\n\nWith that in place, then couldn't one simply define a parser that indicates value present for a docID regardless of what the term value is:\n\n    public static BooleanParser AlwaysReturnTrueBooleanParser = new BooleanParser() { public boolean parseByte(BytesRef term) { return true; } }\n\n    BitSet getValueExists(IndexReader reader, String field) {\n       return  getBits( ir, field, AlwaysReturnTrueBooleanParser );\n    }\n \nThen a client (e.g. FieldComparator implementation) interested in ValueExists values could ask for them, and they would be independently cached from whatever other field type cache(s) were requested on that field by the same or different clients.  The only cost would be iterating the Term/docID iterators a second time (as for additional cache variants on the same field) - minor.\n\nDoes this make sense?",
            "date": "2010-09-22T19:41:27.600+0000",
            "id": 68
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. The only cost would be iterating the Term/docID iterators a second time\n\nActually, that's major - otherwise I would have already just done that and stored it in a separate cache for Solr's needs.",
            "date": "2010-09-22T19:49:59.537+0000",
            "id": 69
        },
        {
            "author": "J.J. Larrea",
            "body": "{quote}\nActually, that's major - otherwise I would have already just done that and stored it in a separate cache for Solr's needs.\n{quote}\n\nIs the one-time-per-IndexReader-lifecycle cost of multiplying the cache load time by some factor < 2.0 (since the term values don't need to be decoded), really so terrible that one has to contemplate global state variables, or a constant increase in cache memory, or significant API changes, or the potential for double-allocation (with then an additional 1x cache load time), or increased code complexity, ...?  Even with all the lovely Solr support for parallel pre-warming?",
            "date": "2010-09-22T21:13:37.962+0000",
            "id": 70
        },
        {
            "author": "Ryan McKinley",
            "body": "bq Is the one-time-per-IndexReader-lifecycle cost of multiplying the cache load time by some factor < 2.0 ... really so terrible \n\nit can be... on a big index just iterating all the terms/docs take a long time.  Try the LukeRequestHandler on an index with a million+ docs!\n\n-------------\n\nHere is different variation, it changes *lots* but if we are talking about changing Parser from interface to class, then I guess the cat can be out of the bag.\n\nWhat about something like: \n{code:java|title=FieldCache.java}\n  ...\n\n  \n  public class EntryConfig implements Serializable \n  {\n    public Parser getParser() {\n      return null;\n    }\n    public boolean cacheValidBits() {\n      return false;\n    }\n    public boolean cacheValues() {\n      return true;\n    }\n    \n    /**\n     * The HashCode is used as part of the Cache Key (along with the field name).  \n     * To allow multiple calls with different parameters, make sure the hashCode \n     * does not include the specific instance and parameters.\n     */\n    public int hashCode()\n    {\n      return EntryConfig.class.hashCode();\n    }\n  }\n  \n  \n  public abstract class CachePopulator \n  {\n    public abstract void fillValidBits(  CachedArray vals, IndexReader reader, String field, EntryConfig creator ) throws IOException;\n    public abstract void fillByteValues( CachedArray vals, IndexReader reader, String field, EntryConfig creator ) throws IOException;\n    ...\n  }\n\n  public abstract CachePopulator getCachePopulator();\n\n...\n\n  public ByteValues getByteValues(IndexReader reader, String field, EntryConfig creator )\n\n...\n\n{code}\n\n\nThe field cache implementation would make sure what you asked for is filled up before passing it back (though i think this has some concurrency issue)\n{code:java}\n\n  public ByteValues getByteValues(IndexReader reader, String field, EntryConfig config) throws IOException\n  {\n    ByteValues vals = (ByteValues) caches.get(Byte.TYPE).get(reader, new Entry(field, config));\n    if( vals.values == null && config.cacheValues() ) {\n      populator.fillByteValues(vals, reader, field, config);\n    }\n    if( vals.valid == null && config.cacheValidBits() ) {\n      populator.fillValidBits(vals, reader, field, config);\n    }\n    return vals;\n  }\n{code}\n\nThe Cache would then delegate the creation to the populator:\n{code:java}\n\n    @Override\n    protected final ByteValues createValue(IndexReader reader, Entry entry, CachePopulator populator) throws IOException {\n      String field = entry.field;\n      EntryConfig config = (EntryConfig)entry.custom;\n      if (config == null) {\n        return wrapper.getByteValues(reader, field, new EntryConfig() );\n      }\n      ByteValues vals = new ByteValues();\n      if( config.cacheValues() ) {\n        populator.fillByteValues(vals, reader, field, config);\n      }\n      else if( config.cacheValidBits() ) {\n        populator.fillValidBits(vals, reader, field, config);\n      }\n      else {\n        throw new RuntimeException( \"the config must cache values and/or bits\" );\n      }\n      return vals;\n    }\n{code}\n\nThe fillByteValues would be the same code as always, but I think the CachedArray should make sure the same parser is used everytime\n{code:java}\n\n    @Override\n    public void fillByteValues( CachedArray vals, IndexReader reader, String field, EntryConfig config ) throws IOException\n    {\n      ByteParser parser = (ByteParser) config.getParser();\n      if( parser == null ) {\n        parser = FieldCache.DEFAULT_BYTE_PARSER;\n      }\n      // Make sure it is the same parser\n      int parserHashCode = parser.hashCode();\n      if( vals.parserHashCode != null && vals.parserHashCode != parserHashCode ) {\n        throw new RuntimeException( \"Subsequent calls with different parser!\" );\n      }\n      vals.parserHashCode = parserHashCode;\n     ...\n{code}\n\nThis is different then the current code where asking for the cached values with two different parsers (that return different hashcodes) will make two entries in the cache.\n\nThis approach would let us:\n* cache values and bits independently or together\n* subsequent calls with different parameters should behave reasonably\n* If CachePopulator is pluggable/extendable that may make some other issues easier\n* lets us use CachePopulator outside of the cache context (perhaps useful)\n\n\n\n",
            "date": "2010-09-23T00:02:20.790+0000",
            "id": 71
        },
        {
            "author": "Yonik Seeley",
            "body": "Now we're talking!\n\nQ: why aren't the CachePopulator methods just directly on EntryConfig - was it easier to share implementations that way or something?\n\nAlso:\n- It doesn't seem like we need two methods fillValidBits , fillByteValues - shouldn't it just be one method that looks at the config and fills in the appropriate entries based on cacheValidBits() and cacheValues()?\n- We should allow an implementation to create subclasses of ByteValues, etc...  what about this method:\n   public abstract CachedArray  fillEntry( CachedArray vals, IndexReader reader, String field, EntryConfig creator )\nThat way, an existing entry can be filled in (i.e. vals != null) or a new entry can be created.\nOh, wait, I see further down a \"ByteValues createValue()\" - if that's meant to be a method on CachePopulator, I guess it's all good - my main concern was being able to create subclasses of ByteValues and frields.\n\nAnyway, all that's off the top of my head - I'm sure you've thought about it more at this point.",
            "date": "2010-09-23T14:28:59.646+0000",
            "id": 72
        },
        {
            "author": "Ryan McKinley",
            "body": "bq. Q: why aren't the CachePopulator methods just directly on EntryConfig - was it easier to share implementations that way or something?\n\nTwo reasons (but I can be talked out of it)\n1. this approach separates what you are asking for (bits/values/etc) from how they are actually generated (the \"populator\").  Something makes me uncomfortable about the caller asking for Values needing to also know how they are generated.  Seems easy to mess up.  With this approach the 'populator' is attached to the field cache, and defines how stuff is read, vs the 'EntryConfig' that defines what the user is asking for (particulary since they may change what they are asking for in subsequent calls)\n\n2. The 'populator' is attached to the FieldCache so it has consistent behavior across subsequet calls to getXxxxValues().  Note that with this approach, if you ask the field cache for just the 'values' then later want the 'bits' it uses the same populator and adds the results to the existing CachedArray value.\n\n\nbq. It doesn't seem like we need two methods fillValidBits , fillByteValues\n\nThe 'fillValidBits' just fills up the valid bits w/o actually parsing (or caching) the values.  This is useful when:\n1. you only want the ValidBits, but not the values (Mike seems to want this)\n2. you first ask for just values, then later want the bits.  \n\nThinking some more, I think the populator should look like this:\n{code:java}\npublic abstract class CachePopulator \n  {\n    public abstract ByteValues   createByteValues(   IndexReader reader, String field, EntryConfig config ) throws IOException;\n    public abstract ShortValues  createShortValues(  IndexReader reader, String field, EntryConfig config ) throws IOException;\n    public abstract IntValues    createIntValues(    IndexReader reader, String field, EntryConfig config ) throws IOException;\n    public abstract FloatValues  createFloatValues(  IndexReader reader, String field, EntryConfig config ) throws IOException;\n    public abstract DoubleValues createDoubleValues( IndexReader reader, String field, EntryConfig config ) throws IOException;\n    \n    public abstract void fillByteValues(   ByteValues   vals, IndexReader reader, String field, EntryConfig config ) throws IOException;\n    public abstract void fillShortValues(  ShortValues  vals, IndexReader reader, String field, EntryConfig config ) throws IOException;\n    public abstract void fillIntValues(    IntValues    vals, IndexReader reader, String field, EntryConfig config ) throws IOException;\n    public abstract void fillFloatValues(  FloatValues  vals, IndexReader reader, String field, EntryConfig config ) throws IOException;\n    public abstract void fillDoubleValues( DoubleValues vals, IndexReader reader, String field, EntryConfig config ) throws IOException;\n\n    // This will only fill in the ValidBits w/o parsing any actual values\n    public abstract void fillValidBits( CachedArray  vals, IndexReader reader, String field, EntryConfig config ) throws IOException;\n  }\n{code}\n\nThe default 'create' implementation could look something like this:\n\n{code:java}\n\n    @Override\n    public ShortValues createShortValues( IndexReader reader, String field, EntryConfig config ) throws IOException \n    {\n      if( config == null ) {\n        config = new SimpleEntryConfig();\n      }\n      ShortValues vals = new ShortValues();\n      if( config.cacheValues() ) {\n        this.fillShortValues(vals, reader, field, config);\n      }\n      else if( config.cacheValidBits() ) {\n        this.fillValidBits(vals, reader, field, config);\n      }\n      else {\n        throw new RuntimeException( \"the config must cache values and/or bits\" );\n      }\n      return vals;\n    }\n{code}\n\nAnd the Cache 'createValue' would looks somethign like this:\n{code:java}\n\n  static final class ByteCache extends Cache {\n    ByteCache(FieldCache wrapper) {\n      super(wrapper);\n    }\n    \n    @Override\n    protected final ByteValues createValue(IndexReader reader, Entry entry, CachePopulator populator) throws IOException {\n      String field = entry.field;\n      EntryConfig config = (EntryConfig)entry.custom;\n      if (config == null) {\n        return wrapper.getByteValues(reader, field, new SimpleEntryConfig() );\n      }\n      return populator.createByteValues(reader, field, config);\n    }\n  }\n{code}\n\nthoughts?  This would open up lots more of the field cache... so if we go this route, lets make sure it addresses the other issues people have with FieldCache.  IIUC, the other big request is to load the values from an external source -- that should be possible with this approach.",
            "date": "2010-09-23T16:19:53.271+0000",
            "id": 73
        },
        {
            "author": "Yonik Seeley",
            "body": "Oh... I mis-matched the parens when I was looking at your proposal (hence the confusion).\n\nI think getCachePopulator() should be under EntryConfig - that way people can provide their own (and extend ByteValues to include more info)\nOtherwise, we'll forever be locked into a lowest common denominator of only adding info that everyone can agree on.\n",
            "date": "2010-09-23T16:29:19.319+0000",
            "id": 74
        },
        {
            "author": "Ryan McKinley",
            "body": "{quote}\nI think getCachePopulator() should be under EntryConfig - that way people can provide their own (and extend ByteValues to include more info)\n{quote}\n\nSo you think it is better for *each* call to define how the cache works rather then having that as an attribute of the FieldCache (that could be extended).  The on thing that concerns me is that that forces all users of the FieldCache to be in sync.\n\nIn this proposal, you could set the CachePopulator on the FieldCache. \n\n{quote}\nOtherwise, we'll forever be locked into a lowest common denominator of only adding info that everyone can agree on.\n{quote}\n\nThis is why I just added the 'createXxxxValues' functions on CachePopulator -- a subclass could add other values.\n\n------------\n\nIt looks like the basic difference between what we are thinking is that the Populator is attached to the FieldCache rather then each call to the FieldCache.  From my point of view, this would make it easier for system with a schema (like solr) have consistent results across all calls, rather then making each request to the FieldCache need to know about the schema -> parsers -> populator\n\nbut I can always be convinced ;)",
            "date": "2010-09-23T16:43:13.164+0000",
            "id": 75
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. In this proposal, you could set the CachePopulator on the FieldCache. \n\nHmmm, OK, as long as it's possible.\n\nbq. From my point of view, this would make it easier for system with a schema (like solr) have consistent results across all calls, rather then making each request to the FieldCache need to know about the schema -> parsers -> populator\n\nI think this may make it a lot harder from Solr's point of view.\n- it's essentially a static... so it had better not ever be configurable from the schema or solrconfig, or it will break multi-core.\n- if we ever *did* want to treat fields differently (load some values from a DB, etc), we'd want to look that up in the schema - but we don't have a reference to the scema in the populator, and we wouldn't want to store one there (again, we have multiple schemas).  So... we could essentially create custom EntryConfig object and then our custom CachePopulator could delegate to the entry config  (and we've essentially re-invented a way to be able to specify the populator on a per-field basis).\n\nAre EntryConfig objects stored as keys anywhere?   We need to be very careful about memory leaks.",
            "date": "2010-09-23T17:07:23.545+0000",
            "id": 76
        },
        {
            "author": "Yonik Seeley",
            "body": "It's all doable though I guess - even if EntryConfig objects are used as cache keys, we could store a weak reference to the solr core.\nSo I say, proceed with what you think will make it easy for Lucene users - and don't focus on what will be easy for Solr.",
            "date": "2010-09-23T17:19:34.618+0000",
            "id": 77
        },
        {
            "author": "Ryan McKinley",
            "body": "preface, I don't really know how FieldCache is used, so my assumptions could be way off...\n\nIn solr, is there one FieldCache for all all cores, or does each core get its own FieldCache?  \n\nI figured each core would create a single CachePopulator (with a reference to the schema) and attach it to the FieldCache.  If that is not possible, then ya, it will be better to put that in the request.\n\nbq. Are EntryConfig objects stored as keys anywhere? We need to be very careful about memory leaks.\n\nYes, the EntryConfig is part of the 'Entry' and gets stored as a key.  \n",
            "date": "2010-09-23T17:37:49.317+0000",
            "id": 78
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. In solr, is there one FieldCache for all all cores, or does each core get its own FieldCache? \n\nThere is a single FieldCache for all cores (same as in Lucene).",
            "date": "2010-09-23T17:52:32.933+0000",
            "id": 79
        },
        {
            "author": "Yonik Seeley",
            "body": "Passing it in would also allow a way to get rid of the StopFillCacheException hack for NumericField in the future.",
            "date": "2010-09-23T18:10:23.669+0000",
            "id": 80
        },
        {
            "author": "Ryan McKinley",
            "body": "Ok, as I look more, I think it may be worth some even bigger changes!  \n\nIs there any advantage to having a different map for each Type?  The double (and triple) cache can get a bit crazy and lead to so much duplication\n\nWhat about moving to a FieldCache that is centered around the very basic API:\n\n{code:java}\npublic <T> T get(IndexReader reader, String field, EntryCreator<T> creator)\n{code}\n\nEntry creator would be something like\n{code:java}\npublic abstract static class EntryCreator<T> implements Serializable \n  {\n    public abstract T create( IndexReader reader, String field );\n    public abstract void validate( T entry, IndexReader reader, String field );\n    \n    /**\n     * NOTE: the hashCode is used as part of the cache Key, so make sure it \n     * only changes if you want different entries for the same field\n     */\n    @Override\n    public int hashCode()\n    {\n      return EntryCreator.class.hashCode();\n    }\n  }\n{code}\n\nWe could add all the utility functions that cast stuff to ByteValues etc.  We would also make sure that the Map does not use the EntryCreator as a key, but uses it to generate a key.\n\nA sample EntryCreator would look like this:\n{code:java}\n\nclass BytesEntryCreator extends FieldCache.EntryCreator<ByteValues> {\n\n  @Override\n  public ByteValues create(IndexReader reader, String field) \n  {\n    // all the normal walking stuff using whatever parameters we have specified\n  }\n\n  @Override\n  public void validate(ByteValues entry, IndexReader reader, String field) \n  {\n    // all the normal walking stuff using whatever parameters we have specified\n  }  \n}\n{code}\n\nThoughts on this approach?  \n\n\nCrazy how a seemingly simple issue just explodes :(",
            "date": "2010-09-23T18:55:19.352+0000",
            "id": 81
        },
        {
            "author": "Yonik Seeley",
            "body": "Hmmm, that would also seem to transform the FieldCache into a more generic index reader cache - not a bad idea!",
            "date": "2010-09-23T19:05:29.215+0000",
            "id": 82
        },
        {
            "author": "Uwe Schindler",
            "body": "+1\n\nMaybe we should also look at CSF again and what Simon did (LUCENE-2186). In my opinion, the field cache's public API should not look different from CSF, so one can simply also sort against a CSF.\n\nI know, some people here will hurt me if I suggest to remove tha native arrays and instead provide getter methods like in the ValueSource approach. The native arrays are unflexible and Java 6 will hopefully optimize away the additional method call (at least I have seen no speed penalty when trying with CSF's getter API). Cool things could be done like materializing the FieldCache to disk using mmap by e.g. FileChannel.map(...).order(ByteOrder.BIG_ENDIAN).asFloatBuffer() which is then accessible using get(docId). I tested this and works very fine for sorting in Lucene! Java uses internally source code specialization to return different classes depending on native byte order that access the underlying ByteBuffer directly (not manually combining 4 bytes into a float). So the get(docId) call is only bounds checks and *one* mmaped memory access.",
            "date": "2010-09-23T19:16:13.827+0000",
            "id": 83
        },
        {
            "author": "Ryan McKinley",
            "body": "bq. Hmmm, that would also seem to transform the FieldCache into a more generic index reader cache - not a bad idea! \n\nBut is it a good one?!  This would let the FieldCache just focus on the synchronized cache mechenism, and the each EntryCreator would need to do its own Parsing etc\n\n\nAnyone know what the deal is with IndexReader:\n{code:java}\n  /** Expert */\n  public Object getCoreCacheKey() {\n    return this;\n  }\n{code}\n\n",
            "date": "2010-09-23T19:17:53.271+0000",
            "id": 84
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Anyone know what the deal is with IndexReader:\n\nIt is to share the cache for clones of IndexReaders or when SegmentReaders are reopended with different deleted docs. In this case the underlying Reader is the same, so it should use its cache (e.g. when deleted docs are added, you dont need to invalidate the cache).\n\nFor more info, ask Mike McCandless!",
            "date": "2010-09-23T19:22:13.206+0000",
            "id": 85
        },
        {
            "author": "Yonik Seeley",
            "body": "Uwe, I think we need to keep the native arrays.\n\nbq. Java 6 will hopefully optimize away the additional method call\n\nIt only does if you have *one* implementation at the point where it is used.  We just got done specializing the ord sorting code with native arrays because of this - the speed hit was really non-trivial, and it happened with the latest versions of all of the JVMs I tested (oracle 1.6, oracle 1.7, ibm 1.6).  I see no relief for this issue on the horizon.",
            "date": "2010-09-23T19:23:02.465+0000",
            "id": 86
        },
        {
            "author": "Ryan McKinley",
            "body": "bq. remove tha native arrays and instead provide getter methods like in the ValueSource approach\n\nWhat would this look like?  Are you suggesting rather then having:\n{code:java}\nclass ByteValues {\n  public byte[] values;\n};\n{code}\n\nwe have:\n{code:java}\nclass ByteValues {\n  public byte getValue( int doc )\n};\n{code}\n\nor are you suggesting like DocValues that has, intVal, longVal, floatVal, even though only one *really* makes sense?\n\nIs this something that wold need the proposed FieldCache API to change?  or could it be implemented via EntryCreator?\n\nIf we like the more general cache, that probably needs its own issue (wow scope creep!)\n\n\n\n\n\n\n\n",
            "date": "2010-09-23T19:34:54.088+0000",
            "id": 87
        },
        {
            "author": "Uwe Schindler",
            "body": "Yonik: I was expecting this answer...\n\nThe reason is that my current contact (it's also your's) has exactly that problem also with norms (but also FC), that they want to lazily load values for sorting/norms (see the very old issue LUCENE-505). At least we should have a TopFieldDocCollector that can alternatively to native arrays also use a ValueSource-like aproach with getter methods - so you could sort against a CSF. Even if it is 20% slower, in some cases thats the only way to get a suitable search experience. Not always speed is the most important thing, sometimes also space requirements or warmup times. I would have no problem with providing both and chosing the implementation that is most speed-effective. So if no native arrays are provided by the FieldCache use getter methods.",
            "date": "2010-09-23T19:35:29.217+0000",
            "id": 88
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I see no relief for this issue on the horizon.\n\nWe need to specialize the code... either manually or automatically...",
            "date": "2010-09-23T19:40:58.671+0000",
            "id": 89
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. If we like the more general cache, that probably needs its own issue\n\nCorrect - with a more general fieldCache, one could implement alternatives that MMAP files, etc.  But those alternative implementations certainly should not be included in this issue.",
            "date": "2010-09-23T19:43:41.595+0000",
            "id": 90
        },
        {
            "author": "Uwe Schindler",
            "body": "I just wanted to mention that, so the design of the new FC is more flexible in that case. I am just pissed of because of these arrays and no flexibility :-(\n\nThe FC impl should be in line with the CSF aproach from LUCENE-2186.",
            "date": "2010-09-23T19:48:43.507+0000",
            "id": 91
        },
        {
            "author": "Michael McCandless",
            "body": "I like where this all is going!!  We can finally fix FC!!\n\nbq. In my opinion, the field cache's public API should not look different from CSF, so one can simply also sort against a CSF.\n\n+1\n\nFrom the consumption standpoint, they (FC and CSF) really ought to be\none and the same.\n\nWhat's \"unique\" about FieldCache is that it derives its values via\nuninversion... this is nice because there's no index change, but it's\nslow at reader open time.  It's also error-proned (you may hit\nmultiple values per doc, these values may have gone through analysis,\netc.)\n\nCSF, instead, actually computes the values during indexing and stores\nthe raw, pre-computed array, in the index.\n\nThey are both just different sources for the same thing.\n\nAlso, an app should be free to plugin its own external source, and it\nshould present this same \"values source\" API.\n\nbq. Uwe, I think we need to keep the native arrays.\n\nI think the API should allow for optional retrieval of the backing\narray (and we should [manually, for today] specialize the sort\ncomparators), but primary access should be a method call eg\nByteValues.getValue(int docID).\n",
            "date": "2010-09-24T10:10:34.301+0000",
            "id": 92
        },
        {
            "author": "Uwe Schindler",
            "body": "{quote}\nbq. Uwe, I think we need to keep the native arrays.\n\nI think the API should allow for optional retrieval of the backing\narray (and we should [manually, for today] specialize the sort\ncomparators), but primary access should be a method call eg\nByteValues.getValue(int docID).\n{quote}\n\nExactly. Maybe do it like with NIO buffers: they have methods hasArray(), array() and arrayOffset(), the two last ones throw UnsupportedOp, if first is false. We already have quite a lot TopFieldDocCollectors impls as inner classes, a few more choosen by hasArray()... haha :-)",
            "date": "2010-09-24T15:56:54.117+0000",
            "id": 93
        },
        {
            "author": "Yonik Seeley",
            "body": "It feels like incremental improvement keeps being held hostage...\nCan't we first just allow the retrieval of ByteValues, etc, that also have Bits on it?\nChanging everything to go through getValue() should be a separate issue (and CSF isn't even finalized/committed yet).",
            "date": "2010-09-24T16:21:29.725+0000",
            "id": 94
        },
        {
            "author": "Ryan McKinley",
            "body": "bq. Can't we first just allow the retrieval of ByteValues, etc, that also have Bits on it?\n\nI sure *hope* so... otherwise, I doubt anything will move forward.  \n\nMy hope with LUCENE-2665, is to fix the basic problem -- in a way that does not close the door to the improvements we want to make in the future.  I think/hope we are close.  \n\nIf I'm missing something let me know, so I can stop wasting my time.\n\n(I don't even know what CSF is, and do not want to learn right now)\n",
            "date": "2010-09-24T17:28:48.140+0000",
            "id": 95
        },
        {
            "author": "Ryan McKinley",
            "body": "Here is a new patch that incorporates most of the ideas we have discussed.  I tried addressing the larger issue of the FieldCache mess in LUCENE-2665, but that is too big to tackle in one go.  After getting something to *almost* work for LUCENE-2665, I then just took the EntryCreator stuff and am adding that to 'LUCENE-2649'\n\nAs such, some of the choices about how EntryCreator works are based on future plans, and my feel akward today.  Speciffically:\n\n1. In the future, a Cache on the IndexReader should not necessarily be tied to a field name.  To do this, the field name parameter should be part of the EntryCreator.  In this first pass, we will need to pass the field name twice:\n{code:java}\nIntValues  vals = cache.getInts(reader, \"fieldName\", new IntValuesCreator( \"fieldName\", parser, flags ) )\n{code}\nI think the tradeoff is OK, and makes fewer changes in the future.\n\n2. In the future, the EntryCreator.getCacheKey() should be the only key stored.  To fall within the existing structure, the entire EntryCreator is stored on the 'custom' field on the internal cache, but the equals and hashCode values are bubbled up.  This makes more sense for LUCENE-2665.  For now we need to be careful that the EntryCreator classes are reasonable things to store as Keys (it includes the Parser, etc)\n\n-----------------------------\n\nI added a bunch of tests to exercise how sequential calls with different options behave.\n\nI think this patch is ready to commit to /trunk -- when it is in, I'll make a patch for LUCENE-2665.\n\nSince /trunk is now a bit more experimental, and I feel pretty good about the feedback this has had I will probably jump the gun and commit soon",
            "date": "2010-09-25T19:10:11.349+0000",
            "id": 96
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Apologies for 'CTR' rather then 'RTC' -- we can always revert if I jumped the gun!\n\nBetter to ask forgiveness than permission :)\n\nIn fact I'm +1 on switching Lucene's trunk to CTR model instead, now\nthat we have 3.x as the stable branch.  We have enough \"policemen\"\naround here that I think this'd work well.\n\nThe changes look great Ryan -- nice work!\n\nSome smallish feedback:\n\n  * I see some windows eol's snuck in... can you change the\n    svn:eol-style of all the new sources to \"native\"?\n\n  * Some classes are missing copyright header (at least EntryKey,\n    SimpleEntryKey)\n\n  * Shouldn't we only incr .numDocs if the bit wasn't already set?\n    (To be robust if docs have more than one value).  Ie we can use\n    OpenBits.getAndSet.  Maybe then add and assert that numDoc <=\n    maxDoc in the end...\n\n  * Then, we can pass null for the delDocs to the enums, and then we\n    don't need a 2nd pass to detect matchAllDocs (just test if\n    .numDocs == maxDoc())?\n\nI think we should hold off on backport to 3.x until we stabilize\nLUCENE-2665?\n\nIt looks like you've also fixed LUCENE-2527 with this?  Ie the\nfasterButMoreRAM=true|false now cache to the same key?  It's just that\nperhaps we should \"upgrade\" the entry, if it was first created w/\nfalse and then the current call passes true?\n",
            "date": "2010-09-26T10:49:00.383+0000",
            "id": 97
        },
        {
            "author": "Michael McCandless",
            "body": "I think we have a synchronization issue on the call to validate?  Ie, more than 1 thread can enter validate, and eg compute the valid bits (if they weren't computed on the first create())?",
            "date": "2010-09-26T11:01:52.934+0000",
            "id": 98
        },
        {
            "author": "Yonik Seeley",
            "body": "bq.  It's just that perhaps we should \"upgrade\" the entry, if it was first created w/false and then the current call passes true?\n\nWe just need to watch the thread safety of stuff like this.\nFor Bits, it should be trivial... if you didn't ask for bits, you shouldn't be looking at it.\nfasterButMoreRAM is different... upgrading an existing entry could be tricky.",
            "date": "2010-09-26T13:30:09.897+0000",
            "id": 99
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nWe just need to watch the thread safety of stuff like this.\nFor Bits, it should be trivial... if you didn't ask for bits, you shouldn't be looking at it.\nfasterButMoreRAM is different... upgrading an existing entry could be tricky.\n{quote}\n\nAnd, also, we need to decide the policy.  Ie I think it's best if whoever gets there first, wins, for this case.  Ie we should not in fact upgrade to fasterButMoreRAM if the existing entry isn't...",
            "date": "2010-09-26T16:08:12.931+0000",
            "id": 100
        },
        {
            "author": "Ryan McKinley",
            "body": "Thanks Mike, I'll take a look and fix the small issues later tonight.\n\nbq. I think we should hold off on backport to 3.x until we stabilize LUCENE-2665?\n\n+1, I think it makes sense to backport to 3.x when there is a clear upgrade path.  \n\n\n\nbq. I think we have a synchronization issue on the call to validate?\n\nLooks like it.  There are two approaches we could take.  \n\n1. synchronize from the cache:\n{code:java}\n      if( key.creator.shouldValidate() ) {\n        synchronized( key ) {\n          key.creator.validate( (T)value, reader);\n        }\n      }\n{code}\n\n2. make each creator responcible for validation.  For example, the DoubleValuesEntryCreator would look like:\n{code:java}\n\n  @Override\n  public DoubleValues validate(DoubleValues entry, IndexReader reader) throws IOException {\n    boolean ok = false;\n    if( hasOption(OPTION_CACHE_VALUES) ) {\n      ok = true;\n      if( entry.values == null ) {\n        synchronized( this ) {\n          if( entry.values == null ) {\n            fillDoubleValues(entry, reader, field);\n          }\n        }\n      }\n    }\n    if( hasOption(OPTION_CACHE_BITS) ) {\n      ok = true;\n      if( entry.valid == null ) {\n        synchronized( this ) {\n          if( entry.valid == null ) {\n            fillValidBits(entry, reader, field);\n          }\n        }\n      }\n    }\n    if( !ok ) {\n      throw new RuntimeException( \"the config must cache values and/or bits\" );\n    }\n    return entry;\n  }\n{code}\nThat is a bit more complicated, but avoids synchonization when things are valid.  Thoughts?\n\n\nbq. I think it's best if whoever gets there first, wins, for this case\n\nYes, this is the current behavior -- the validate method does not do anything:\n{code:java}\n  public T validate(T entry, IndexReader reader) throws IOException {\n    // TODO? nothing? perhaps subsequent call with FASTER_BUT_MORE_RAM?\n    return entry;\n  }\n{code}\n\n\n- - - - - - - -\n\nThe other key behavior that we should note is that with CachedArrayEntryCreators, if you pass a different parser for the same field, it will give you an error.  Previously this created two cache entries (and then added something to the insanity log).  If someone wants to do that, they could override the XxxEntryCreator.getCacheKey() to return key that includes the Parser.  By default this seems like an error to me.\n\n\n\n",
            "date": "2010-09-26T21:17:05.501+0000",
            "id": 101
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. if you pass a different parser for the same field, it will give you an error.\n\nIf that were true, it would have broken solr in multiple places.  It may not be ideal - but it's not an error.\nSo I'm a little confused - why does solr still work fine?",
            "date": "2010-09-26T23:21:25.794+0000",
            "id": 102
        },
        {
            "author": "Ryan McKinley",
            "body": "bq. If that were true, it would have broken solr in multiple places\n\nWhat is an example?  The only thing I have seen is stuff that tries one parser and if it throws an exception, tries another (that is supported)\n\nAs is, the cached value keeps the hash code for the parser created the values.  If we validate (true by default) then it checks that the passed in hash code matches the one that is stored.  Perhaps different Parsers are returning the same hashCode?    \n\nDoes it make sense to create a different key depending on what Parser we use?  Is it normal to interpret the same field multiple different ways?",
            "date": "2010-09-26T23:33:45.064+0000",
            "id": 103
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. What is an example?\n\nFacet component, query elevation component, stats component, and sorting.\nFor example, if you facet on the popularity field, it will currently use strings - but if you sort on the popularity field, it will ask the FieldCache for ints.\n\nBut all of these instances in Solr involve treating the same field as both a string and as a number - perhaps you actually need to ask for numbers, but with different parsers, to trigger the exception?\n\n",
            "date": "2010-09-26T23:47:14.192+0000",
            "id": 104
        },
        {
            "author": "Ryan McKinley",
            "body": "Ok, that makes sense -- what I mean is that you can not ask for the same *type* with different parsers.  Bytes, vs Strings, vs Ints all have different keys and within the FieldCache, they are even stored in different Maps\n\nWhat you can not do is as for a IntValues using: FieldCache.DEFAULT_INT_PARSER, then switch to FieldCache.NUMERIC_UTILS_INT_PARSER for the same field.\n\nmake sense?",
            "date": "2010-09-26T23:59:25.790+0000",
            "id": 105
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. Ok, that makes sense - what I mean is that you can not ask for the same type with different parsers.\n\nYep, seems fine since people can work around it if they need to.",
            "date": "2010-09-27T00:32:48.956+0000",
            "id": 106
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. What you can not do is as for a IntValues using: FieldCache.DEFAULT_INT_PARSER, then switch to FieldCache.NUMERIC_UTILS_INT_PARSER for the same field.\n\nJust to note, my previous FieldCache impl did this correctly. When you requested a FieldCache without parser it created the parser=null entry and additionally the real used parser as two separate entries. When you later use directly (instead of automatics) passing in the trie parser, it returns the additionally created entry in the cache. If you request using another parser, it creates a new entry. So i was (theoretically) possible to parse the same field as int or long and additionally as trie or plain number (but the latter throwed of couse some NFE), so not really useful.",
            "date": "2010-09-27T03:46:37.958+0000",
            "id": 107
        },
        {
            "author": "Yonik Seeley",
            "body": "Prompted by Robert's pointer to non-thread-safety of parser changing in numeric classes, I did a quick review of IntValuesCreator.  It isn't thread safe if you don't specify a parser yourself and let it auto-detect, but I'll leave that part for another time.\n\nI was going to point out that using deletedDocs when enumerating docs is probably a waste, since all readers that only differ by deletedDocs (generated by reopen, etc) will share the same cache entry.  But then I realized that this is perhaps a bug - not just in this issue, but in all FieldCache implementations since we went away from using \"reader\" as the key?\n\n\nOther stuff we can do:\n- use fastSet (as uwe said)\n- use bulk API\n- we instantiate vals.values lazily for some reason... and then at the end, if it still hasn't been instantiated, we do it anyway?\n- I'm still trying to grok the logic of calling checkMatchAllBits only if vals.valid == null... seems like it will always return null in that case?",
            "date": "2010-11-15T22:42:41.203+0000",
            "id": 108
        },
        {
            "author": "Yonik Seeley",
            "body": "For the sort-missing-last type of functionality, the current comparator code looks like this (see IntComparator for more context):\n{code}\nfinal int v2 = (checkMissing && !cached.valid.get(doc)) \n   ? missingValue : cached.values[doc];\n{code}\nAnd I was thinking of changing it to this:\n{code}\nint v2 = cached.values[doc];\nif (valid != null && v2==0 && !valid.get(doc))\n  v2 = missingValue;\n{code}\n\nThis should make the common case faster by both eliminating an unneeded variable (checkMissing)\nand checking that the value is the Java default value before checking the bitset.\n\nThoughts?",
            "date": "2010-12-06T22:20:55.300+0000",
            "id": 109
        },
        {
            "author": "Ryan McKinley",
            "body": "looks good to me\n\nbq. we instantiate vals.values lazily for some reason... and then at the end, if it still hasn't been instantiated, we do it anyway?\n\nI don't know about this, I just copied from the existing code...  \n\nWe could make the case where Bits.MatchNoBits( maxDoc ), have a null array.  This would make your proposed change invalid though since it checks the array first.\n\n\nbq.  I'm still trying to grok the logic of calling checkMatchAllBits only if vals.valid == null... seems like it will always return null in that case?\n\nThe assumption is that once vals.valid is set, it should not be recalculated.\n\nThe reasons for the if vals.valie == null in the validate function are:\n - the vals.valid Bits may have been set in fillXXValues\n - the first call may have excluded checkMatchAllBits, and  a subsequet call has it set\n\nAre you asking about in the validate function?  If so, fillXXXValues can set the vals.valid, so it does not do it again.  \n",
            "date": "2010-12-06T22:36:10.063+0000",
            "id": 110
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. looks good to me \n\nCool - I'll work up a patch.\n\nbq.  The assumption is that once vals.valid is set, it should not be recalculated.\n \nOh, tricky.  We should comment that.\n\nAnd we can then also change this:\n     OpenBitSet validBits = (hasOption(OPTION_CACHE_BITS)) ? new OpenBitSet( maxDoc ) : null;\nTo this: \n    OpenBitSet validBits = (hasOption(OPTION_CACHE_BITS) && valid.vals==null) ? new OpenBitSet( maxDoc ) : null;",
            "date": "2010-12-08T16:29:33.309+0000",
            "id": 111
        },
        {
            "author": "Yonik Seeley",
            "body": "Here's a draft patch.\nedit: move to LUCENE-2671",
            "date": "2010-12-08T17:16:44.547+0000",
            "id": 112
        },
        {
            "author": "Ryan McKinley",
            "body": "This has been done for a while... any problems should now get their own issue",
            "date": "2011-03-25T19:48:13.869+0000",
            "id": 113
        }
    ],
    "component": "core/search",
    "description": "The FieldCache returns an array representing the values for each doc.  However there is no way to know if the doc actually has a value.\n\nThis should be changed to return an object representing the values *and* a BitSet for all valid docs.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2649",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "FieldCache should include a BitSet for matching docs",
    "systemSpecification": true,
    "version": ""
}