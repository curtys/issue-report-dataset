{
    "comments": [
        {
            "author": "Martijn van Groningen",
            "body": "Attached initial version of the JoinQuery which based on the one in Solr.",
            "date": "2011-11-27T20:34:18.556+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "Awesome to finally bring JoinQuery to pure Lucene apps!\n\nCan we cut back to normal ctor (not builder API) to create the\nJoinQuery?  One can always create a builder API layer on top if\nnecessary.\n\nHow does the preComputedFromDocs work?  It's not per-segment?  Like\nit's a bitset across entire toplevel doc space?\n\nHmm we are also using MultiFields.getLiveDocs, which is quite slow to\nuse (must do binary search on each doc lookup).\n\nI wonder if we can make this work per-segment... but that can be a 2nd\nphase.\n\nI think you can use seekExact instead of seekCeil?  Better\nperformance...\n\nWhat is the AdjustedDISI for (and when would Weight.scorer get a\ntop-level context...)?\n",
            "date": "2011-11-29T01:12:35.233+0000",
            "id": 1
        },
        {
            "author": "Martijn van Groningen",
            "body": "I'll remove the builder api. This was just my sugar api. I'll change that in a constructor where the toSearcher and preComputedFromDocs are optional.\n\nbq. How does the preComputedFromDocs work? It's not per-segment? Like it's a bitset across entire toplevel doc space?\nYes, it is a bitset across all segments, so toplevel. The whole query implementation is top level. People can use this the execute the real query outside the JoinQuery (if this isn't specified the query is executed when the Weight is created for the JoinQuery). I think this would be nice if people want the for example cache the encapsulated query.\n\nbq. I wonder if we can make this work per-segment... but that can be a 2nd phase.\nI thought about that. But this would mean we need to execute the join query in two phases. The first phase would collect all the \"from\" values from the documents matching the encapsulated query. The second phase would match the documents that have matching on the \"to\" side with the \"from\" values collected in the first phase. The will work per segment and I think would also make the JoinQuery work in distributed environment. We can do this in a second development phase.\n\nbq. I think you can use seekExact instead of seekCeil? Better performance...\nI'll change that.\n\nbq. What is the AdjustedDISI for (and when would Weight.scorer get a top-level context...)? \nActually that is only used to map top level docids to segment docids (Basically doing: toplevel_docid - base). This is necessary because Weight.scorer works per segment and the query implementation not.",
            "date": "2011-11-29T06:54:55.892+0000",
            "id": 2
        },
        {
            "author": "Jason Rutherglen",
            "body": "Great to see this moving out of Solr and getting new eyes on it (with added improvements)!",
            "date": "2011-11-29T16:08:50.270+0000",
            "id": 3
        },
        {
            "author": "Martijn van Groningen",
            "body": "Updated patch.\n* Removed builder api.\n* Updated to latest Lucene api changes.\n* Use seekExact instead of seekCeil method.\n* Added random test.",
            "date": "2011-12-08T17:53:19.785+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good!\n\n  * I like the test...\n\n  * Maybe rename actualQuery to fromQuery?  (So it's clear that\n    JoinQuery runs fromQuery using fromSearcher, joining on\n    fromSearcher.fromField to toSearcher.toField).\n\n  * Why preComputedFromDocs...?  Like if you were to cache something,\n    wouldn't you want cache the toSearcher's bitset instead?\n\n  * Maybe rename JoinQueryWeight.joinResult to topLevelJoinResult, to\n    contrast it w/ the per-segment scoring?  And add a comment\n    explaining that we compute it once (on first segment) and all\n    later segments then reuse it?\n\n  * I wonder if we could make this a Filter instead, somehow?  Ie, at\n    its core it converts a top-level bitset in the fromSearcher doc\n    space into the joined bitset in the toSearcher doc space.  It\n    could even maybe just be a static method taking in fromBitset and\n    returning toBitset, which could operate per-segment on the\n    toSearcher side?  (Separately: I wonder if JoinQuery should do\n    something with the scores of the fromQuery....?  Not right now but\n    maybe later...).\n\n  * Why does the JoinQuery javadoc say \"The downside of this\n    is that in a sharded environment not all documents might get\n    joined / linked.\" as a downside to the top-level approach?  Maybe\n    reword that to state that all joined to/from docs must reside in\n    the same shard?  In theory we could (later) make a shard friendly\n    approach?  Eg, first pass builds up all unique Terms in the\n    fromSearcher.fromField for docs matching the query (across all\n    shards) and 2nd pass is basically a TermFilter on those...\n\n  * Not sure it matters, but... including the preComputedFromDocs in\n    hashCode/equals is quite costly (it goes bit by bit...).  Maybe it\n    shouldn't be included, since it contains details about the\n    particular searcher that query had been run against?  (In theory\n    Query instances are searcher independent.)\n\nIn general I think this approach is somewhat inefficient, because it\nalways iterates over every possible term in fromSearcher.fromField,\nchecking the docs for each to see if there is a match in the query.\nIe, it's like FieldCache, in that it un-inverts, but it's uninverting\non every query.\n\nI wonder if we could DocTermOrds instead?  (Or,\nFieldCache.DocTermsIndex or DocValues.BYTES_*, if we know fromSearcher.fromField is\nsingle-valued).  This way we uninvert once (on init), and then doing\nthe join should be much faster since for each fromDocID we can lookup\nthe term(s) to join on.\n\nLikewise on the toSearcher side, if we had doc <-> ord/term loaded we\ncould do the forward (term -> ord) lookup quickly (in memory binary\nsearch).\n\nBut then this will obviously use RAM... so we should have the choice\n(and start w/ the current patch!).\n",
            "date": "2011-12-12T13:53:13.900+0000",
            "id": 5
        },
        {
            "author": "Martijn van Groningen",
            "body": "bq. Maybe rename actualQuery to fromQuery?\nYes, fromQuery makes more sense than actualQuery.\n\n{quote}\nWhy preComputedFromDocs...? Like if you were to cache something,\nwouldn't you want cache the toSearcher's bitset instead?\n{quote}\nThis is in the case if your from query was cached and your toSearch's\nbitset isn't, which is a likely scenario.\nBut caching the toSearcher's bitset is better off course when\npossible. But this should be happen outside the JoinQuery, right?\n\nbq. Maybe rename JoinQueryWeight.joinResult to topLevelJoinResult,\nI agree a much more descriptive name.\n\n{quote}\nI wonder if we could make this a Filter instead, somehow? Ie, at\nits core it converts a top-level bitset in the fromSearcher doc\nspace into the joined bitset in the toSearcher doc space. It\ncould even maybe just be a static method taking in fromBitset and\nreturning toBitset, which could operate per-segment on the\ntoSearcher side? (Separately: I wonder if JoinQuery should do\nsomething with the scores of the fromQuery....? Not right now but\nmaybe later...).\n{quote}\nIt just matches docs from one side to the to side. That is all... So static method / filter should be able to do the job.\nI'm not sure, but if it is a query it might be able to one day encapsulate the joining in the Lucene query language?\n\nbq. Maybe reword that to state that all joined to/from docs must reside in the same shard?\n+1\n\n{quote}\nI wonder if we could DocTermOrds instead? (Or,\nFieldCache.DocTermsIndex or DocValues.BYTES_*, if we know\nfromSearcher.fromField is\nsingle-valued). This way we uninvert once (on init), and then doing\nthe join should be much faster since for each fromDocID we can lookup\nthe term(s) to join on.\n{quote}\nI really like that idea! This already crossed my mind a few days ago\nas an improvement to speedup the joining. Would be nice if the user can \nchoose between a more ram but faster variant and a less ram but slower variant.\nI think we can just make two concrete JoinQuery impl that both have a different\njoinResult(...) impl.",
            "date": "2011-12-12T16:12:13.916+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nThis is in the case if your from query was cached and your toSearch's\nbitset isn't, which is a likely scenario.\n{quote}\n\nHmm can you describe this?  You mean the app sometimes actually uses the fromSearcher.fromQuery's results, directly, without joining?\n\n{quote}\nIt just matches docs from one side to the to side. That is all... So static method / filter should be able to do the job.\nI'm not sure, but if it is a query it might be able to one day encapsulate the joining in the Lucene query language?\n{quote}\n\nYeah... the core API is really the join method, to translate top-level docIDs in fromSearcher over to toSearcher's top-level docIDs.\n\nThe AdjustedDISI (maybe rename SliceDISI?  SubReaderDISI?  ie, something to indicate it \"slices\" a sub-reader's portion of the top-level docID space) can then be used to translate back into a per-segment Filter.\n\nI think it would be cleaner as a Filter...?  This is actually similar to DuplicateFilter, which also must operate on top-level docIDs (since dups can happen in different segments).\n\nbq.  Would be nice if the user can choose between a more ram but faster variant and a less ram but slower variant.\n\nYeah I agree... but what worries me is just how slow this non-RAM version will be.  Ie, it must do the full join and uninvert every time; so even if your fromQuery only matches a tiny number of docs... you pay massive cost of the full join.  Even better than using FC/DV/DTO to map docID -> term(s) per query, we could hold in RAM the join result itself (docID -> docID(s)) in some form, then the query just directly maps the docIDs w/o having to lookup terms again.\n\nStepping back a bit... do we know how this impl compares to how ElasticSearch does joins?  And to how Solr does...?",
            "date": "2011-12-12T19:10:51.210+0000",
            "id": 7
        },
        {
            "author": "Martijn van Groningen",
            "body": "bq. You mean the app sometimes actually uses the fromSearcher.fromQuery's results, directly, without joining?\nYes. In the case of Solr it is checking the filter cache.\n\nbq. but what worries me is just how slow this non-RAM version will be.\nI have been running the JoinQuery on my test data set (10.1 M docs) and it isn't as bad as I expect it would be. This data set contains 100000 products each product having 100 offers. The JoinQuery with a *:* query as fromQuery takes about 900 ms and a fromQuery selecting all products with a specific keyword takes about 350 ms. I think this specific query implementation is suitable for environments where RAM might be scarce. The RAM version should be the default.\n\nbq. Stepping back a bit... do we know how this impl compares to how ElasticSearch does joins? And to how Solr does...?\nES only has index time joining, right? Solr basically uses the same mechanism as the JoinQuery in this patch, but a bit smarter. It tries to cache the from term to to term lookup (see JoinQParserPlugin.java line 367). I couldn't port this part to joining module since this caching relies heavily on the SolrIndexSearcher.",
            "date": "2011-12-12T22:18:03.472+0000",
            "id": 8
        },
        {
            "author": "Jason Rutherglen",
            "body": "Maybe we can (in another issue) move bit set filter caching into SearchManager, for use by Lucene Join (here), and others.  At the same time making bitset filtering per-segment, a fundamental improvement from the existing (old) Solr code.",
            "date": "2011-12-13T00:00:53.396+0000",
            "id": 9
        },
        {
            "author": "Martijn van Groningen",
            "body": "Attached a new patch with a completely different joining implementation. Basically the joining happens inside a static method, works per segment and uses FieldCache.\n\nThis is just a small try-out and is lacking any tests.",
            "date": "2012-01-07T19:11:34.767+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "Wow new patch is tiny -- just 2 static methods!\n\nRight now you do 3 passes -- 1st pass records fromSearcher's docIDs\nmatching fromQuery; 2nd pass translates those matching docIDs into\nthe joinable terms in fromSearcher.fromField; 3rd pass then records\ntoSearcher docIDs matching those terms in toField.\n\nBut I think the first 2 passes could be combined?  Ie, as you collect\neach hit from fromQuery, instead of recording the docID, go and look up\nthe term in fromField for that doc and save it away?  Then you don't\nneed to save the fromSearcher docIDs?  (3rd pass would then be the\nsame).\n\nAlso, instead of making a toplevel bit set as the return\nresult... could it be an ordinary filter?  Then the 3rd pass would be\nimplemented in Filter.getDocIDSet, and the Filter instance would hold\nonto these terms computed by the combined 1st/2nd pass?\n\nI think this is a great step forward over previous patch... so tiny\ntoo :)\n\nThe 1st/2nd pass would have \"expected\" cost, ie on the order of how\nmany hits matched in fromQuery.  But the 3rd pass has a high cost even\nfor tiny queries since it visits every doc, checking whether its terms\nare in the set.  We might be able to improve on that somehow... eg if\nthe number of terms is small, really you want to invert that process\n(ie visit the postings and gather all matching docs), either with an\nOR query or with TermsFilter (in modules/queries)?  Hmm, in fact,\nisn't this just a MultiTermQuery?  We can then use auto rewrite mode\nto rewrite as filter or small BooleanQuery?\n",
            "date": "2012-01-08T19:05:32.892+0000",
            "id": 11
        },
        {
            "author": "Martijn van Groningen",
            "body": "bq. But I think the first 2 passes could be combined?\nThat idea also crossed my mind. We should definitely do this.\n\nbq. Also, instead of making a toplevel bit set as the return result... could it be an ordinary filter? \nI like that approach. It can just return a TermsFilter instance, right?\n\nMaybe we should have to variants one returns a MTQ and one a TermsFilter? So users can choose whether to want a filter or a query.\n",
            "date": "2012-01-08T23:57:27.795+0000",
            "id": 12
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nMaybe we should have to variants one returns a MTQ and one a TermsFilter? So users can choose whether to want a filter or a query.\n{quote}\n\nBut they can always choose the rewriteMode to not be AUTO and explicitly choose filter if they want this.",
            "date": "2012-01-09T00:14:55.159+0000",
            "id": 13
        },
        {
            "author": "Martijn van Groningen",
            "body": "Attached a new patch. Changes:\n* Everything happens now in two passes.\n* The second pass is in the custom MTQ impl.\n* JoinUtil contains one method that returns a MTQ impl.\n* Reused the tests from previous patch for the JoinUtil.\n\nbq. But they can always choose the rewriteMode to not be AUTO and explicitly choose filter if they want this.\nYes makes sense. First time I used the MTQ directly. So feedback on my on MTQ impl is welcome :)\n\nI think that this patch can easily be backported to 3x.",
            "date": "2012-01-10T23:01:58.459+0000",
            "id": 14
        },
        {
            "author": "Michael McCandless",
            "body": "Looking great!  I think we are getting close :)\n\nMaybe use BytesRefHash (oal.util) to gather the terms and sort\nthem in the end?  Then hold onto the BytesRef[] (sorted).  It's\ndesigned for exactly this usage...\n\nAlso, I think you should always seek in your FilteredTermsEnum, and\nsimply iterate yourself through the BytesRef[]?  Probably perf wasn't\ngreat before because you were calling TreeSet.higher on each term?\n\nI think you should call setRewriteMethod in your TermSetQuery ctor, to\nCONSTANT_SCORE_AUTO_REWRITE_DEFAULT, since we have no scores here.\nThis rewrite method uses BQ if number of terms/docs is \"smallish\" else\ncreates a filter, so it should give good perf.\n",
            "date": "2012-01-11T16:56:46.236+0000",
            "id": 15
        },
        {
            "author": "Martijn van Groningen",
            "body": "Cool. I will use the BytesRefHash in the patch and see how this pans out.\n\n{quote}\nAlso, I think you should always seek in your FilteredTermsEnum, and\nsimply iterate yourself through the BytesRef[]? Probably perf wasn't\ngreat before because you were calling TreeSet.higher on each term?\n{quote}\nI've been testing this out with treeSet.higher(). In the case when there're gaps between the terms the performance is much better with seeking then not seeking (I picked random terms in a small test). In the terms are next / near to each other it seems it is better use a non seeking implementation. I think we should have a seeking and non seeking impl available. Maybe there should should be an option named useSeekingTermsEnum which decides what impl to us?\n\nbq. I think you should call setRewriteMethod in your TermSetQuery ctor, to CONSTANT_SCORE_AUTO_REWRITE_DEFAULT\nCONSTANT_SCORE_AUTO_REWRITE_DEFAULT is already the default in MTQ, right?",
            "date": "2012-01-13T09:59:37.448+0000",
            "id": 16
        },
        {
            "author": "Michael McCandless",
            "body": "Oh yeah MTQ already defaults to that rewrite method :)\n\nReally, since you know the exact number of terms up front... you could specialize it to constant BQ or filter yourself... but that can be later.\n\nI don't think you should be calling TreeSet.higher on every term check -- that's a relatively costly binary search through the TreeSet each time.  Really you should be able to just walk through (with your own upto++) the pre-sorted BytesRef[] you pulled from the BytesRefHash.  Then we should retest seek vs next...",
            "date": "2012-01-13T18:04:04.838+0000",
            "id": 17
        },
        {
            "author": "Martijn van Groningen",
            "body": "Attached new patch.\n* Extract collector from JoinUtil and named it TermsCollector.\n* TermsCollector uses BytesRefHash.\n* Renamed TermSetQuery to TermsQuery.\n* TermsQuery uses sorted ByteRefs[]\n* TermsCollector has two concrete impl. One for single value per field and one for multiple values per field.\n* Moved the classes for query time joining under the package o.a.l.search.join.query\n* Added documentation.\n\nThe JoinUtil is now just a convience class.\n\nShould we add the existing block joining classes under o.a.l.search.join.index or o.a.l.search.join.block?\n\nI think this patch is ready to get committed.",
            "date": "2012-01-15T16:19:37.432+0000",
            "id": 18
        },
        {
            "author": "Martijn van Groningen",
            "body": "Oops... I found out that the assertion in FilteredTermsEnum line 207 was failing. Before I added the previous patch I ran the tests from my IDE and assertions weren't enabled...\n\nIn some cases I was seeking backwards which isn't good. The attached patch fixes this issue. All the tests in the join module pass now.",
            "date": "2012-01-15T20:42:45.585+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "Looking great!\n\n  * Can't TermsCollector be package private?  Like it's only used\n    privately in JoinUtil.createJoinQuery?\n\n  * Can JoinUtil.createJoinQuery return Query not MultiTermQuery...?\n    Just gives us freedom in the future to impl a different Query if\n    we want/need to...\n\n  * Typo: collecter -> collector\n\n  * Can TermsQuery be package private?  Also, we can save a pass over\n    the sorted terms by having TermsQuery hold the int[] ord array and\n    then just do the lookup (against BytesRefHash) as it\n    goes... really TermsQuery could just take the BytesRefHash.  Then\n    we wouldn't have to materialize a new BytesRef for each matched\n    term... just reuse a single scratch BytesRef inside TermsQuery.\n\n  * In the TermsQuery.accept... should that {{return AcceptStatus.YES}}\n    in the {{if (cmp == 0)}} be a YES_AND_SEEK\n    (after setting the next term as the seekTerm)?\n\n  * Hmm we sort by unicode order in TermsCollector.getCollectedTerms,\n    but then by the term dict's comparator in TermsQuery; maybe just\n    use the UTF8AsUnicode comparator in TermsQuery too?  And note in\n    jdocs that this is required?\n",
            "date": "2012-01-16T00:25:26.699+0000",
            "id": 20
        },
        {
            "author": "Michael McCandless",
            "body": "Also: I wonder if we should use DocValues in addition (instead of?) FieldCache.getTerms...",
            "date": "2012-01-16T00:44:15.176+0000",
            "id": 21
        },
        {
            "author": "Martijn van Groningen",
            "body": "I think that joining by docvalues would be great addition! I do think that joining by indexed terms should be possible to.\nTo support joining by docvalues we can make a different TermsCollector implementation, but I'm unsure how the TermsQuery can be used. Can A MTQ also iterate over docvalues? \n\nI think we should commit what we currently have first and then address joining by docvalues in a new issue.",
            "date": "2012-01-16T10:53:12.162+0000",
            "id": 22
        },
        {
            "author": "Michael McCandless",
            "body": "bq. To support joining by docvalues we can make a different TermsCollector implementation, but I'm unsure how the TermsQuery can be used. \n\nActually I think TermsQuery (once we change it to just take the BytesRefHash and iterate over the int[] ord instead of BytesRef[]) can be re-used.\n\nIe, when collecting terms from DocValues, you'd just stuff them into the BytesRefHash like you do now... so supporting terms from DocValues should simply be another private impl inside TermsCollector (just like you can now pull from DocTermOrds or FieldCache).\n\nBut I agree: let's do this (enable JoinQuery to use DocValues) in a follow-on issue!",
            "date": "2012-01-16T12:20:17.842+0000",
            "id": 23
        },
        {
            "author": "Robert Muir",
            "body": "Hi Martijn: the patch is looking really nice! \n\nJust a trivial thing, can we swap ctor arguments of\n{code}\npublic TermsQuery(BytesRef[] terms, String field) {\n{code}\n\nto\n\n{code}\npublic TermsQuery(String field, BytesRef[] terms) {\n{code}\n\nMost of the apis on indexreader etc take field,term so I think it looks more consistent.",
            "date": "2012-01-16T13:54:47.739+0000",
            "id": 24
        },
        {
            "author": "Martijn van Groningen",
            "body": "Attached a new patch!\n* The static method now has Query as return type.\n* TermsCollector and TermsQuery are package protected.\n* TermsQuery now holds an int[] ords instead of an array of ByteRefs. This is a really nice improvement :)\n* Swapped the const args of TermQuery. Consistency is important!\n* After [some discussion|http://colabti.org/irclogger/irclogger_log/lucene-dev?date=2012-01-16#l231] moved the classes back to o.a.l.search.join package.  \n* The query time joining uses the dict comparator on all places now (instead of ByteRef#UTF8AsUnicode).\n\nIn the that cmp==0 in TermsQuery at line 121; YES_AND_SEEK can't be returned. The FilteredTermsEnum prohibits seeking to a term smaller or equal then the current term. ",
            "date": "2012-01-16T21:46:14.953+0000",
            "id": 25
        },
        {
            "author": "Martijn van Groningen",
            "body": "Small update. Removed unused method.",
            "date": "2012-01-16T21:49:21.960+0000",
            "id": 26
        },
        {
            "author": "Martijn van Groningen",
            "body": "Fixed the YES_AND_SEEK issue. In the previously described case it uses YES_AND_SEEK now without seeking to the current term (that triggered the assertion error).",
            "date": "2012-01-16T22:33:22.653+0000",
            "id": 27
        },
        {
            "author": "Michael McCandless",
            "body": "+1 it looks great!  Good work :)",
            "date": "2012-01-16T22:51:08.425+0000",
            "id": 28
        },
        {
            "author": "Martijn van Groningen",
            "body": "Attached a new patch.\n* Fixed some small comment typos.\n* Removed the reuse field in the TermsQuery. Since it doesn't make sense to reuse b/c it assigned null! Also I cannot know if the caller is done with the TermsEnum.\n\nIt is ready. I'll commit this asap.",
            "date": "2012-01-16T22:59:05.973+0000",
            "id": 29
        },
        {
            "author": "Martijn van Groningen",
            "body": "Committed to trunk. I'll leave this issue open for the backport to 3.6",
            "date": "2012-01-16T23:18:12.508+0000",
            "id": 30
        },
        {
            "author": "Jason Rutherglen",
            "body": "Sweet!  How join would work in distributed mode, that would be very useful for BigData projects.",
            "date": "2012-01-17T01:40:22.412+0000",
            "id": 31
        },
        {
            "author": "Robert Muir",
            "body": "Just a few ideas about 3.x backport:\n* MultiTermQuery doesn't have a real 'seek' there... you have to open up and swap in a new 'actualEnum' (does this clone the indexinput etc?) starting at the new seek position... I think its tricky and ugly. Maybe we can/should seek to 3.x TermEnum/FilteredTermEnum so this will work nicer...? I havent looked at how hairy this would be though.\n* The rewrites are not always per-segment like they are in trunk, and it could be bad to \"seek\" a lot when MTQ is in 'auto mode' because if I recall its expensive (creating lots of multitermsenums). Maybe since Join knows up-front how many terms are in the hash it should determine which method to use itself (setRewriteMethod)... maybe on 3.x just set filter rewrite since you know will be \"seeking\"? Since you return the query the user could still always override the rewrite method if they really care, it would just be a default.\n",
            "date": "2012-01-17T05:20:11.220+0000",
            "id": 32
        },
        {
            "author": "Martijn van Groningen",
            "body": "bq. Sweet! How join would work in distributed mode, that would be very useful for BigData projects.\nThe join is actually executed in a two pass search. During the first pass search all the terms are gathered for the matching documents based on the fromQuery. In the second pass search all documents are collected that match with the gather terms for a specific field. To only way I currently see how this can work in a distributed environment is that all machines in the cluster execute the first pass search and then copy the collected terms between machines. After this is done each machine can execute the second pass. If your data allows it, you can partition data in your cluster this allows you to skip the copying of terms. \n\nCurrently the api is just one static method and assumes that the joining happens locally. I think we need to have two more methods. One method that returns the first pass terms and one method that constructs a query based on terms from the first pass.\n\nRobert: Yes I see that 3.x MTQ isn't as great as MTQ in trunk. Maybe we need a different approach (not use MTQ)? The api is clean for users, and allows us to do joining different in 3x. I'll start backporting and see how well it goes.",
            "date": "2012-01-19T19:26:51.531+0000",
            "id": 33
        },
        {
            "author": "Jason Rutherglen",
            "body": "I was reviewing this issue to use where Solr's join implementation may not be the right choice.\n\nIn this Lucene Join implementation, a new BytesRefHash is built per query (and cannot be reused).  This could generate a fair amount of garbage quickly.  \n\nAlso the sort compare using BRH is per byte (not as cheap as an ord compare).  We can probably use DocTermsIndex to replace the use of BytesRefHash by comparing DTI's ords.  Then we are saving off the bytes into BRH per query, and the comparison would be faster.\n\nAdditionally, for a join with many terms, the number of postings could become a factor in performance.  Because we are not caching bitsets like Solr does, it seems like an excellent occasion for a faster less-compressed codec.\n\nFurther, to save on term seeking, if the term state was cached (eg, the file pointers into the posting list), the iteration on the terms dict would be removed.\n\nGranted all this requires more RAM, however in many cases (eg, mine), that would be acceptable.",
            "date": "2012-01-19T20:52:33.669+0000",
            "id": 34
        },
        {
            "author": "Martijn van Groningen",
            "body": "I'm not sure how you plan to sort by DTI ords. The terms collected in the first phase are from many segments. The ords from DTI are only valid inside a segment. You can create a toplevel DTI but that is expensive... \n\nCurrently caching is minimal and can be improved at the cost of more RAM. The TermsCollector caches the from terms via DocTerms in the FC (per segment). Caching can be improved in the second phase as you described, by saving a bitset per fromTerm?. But I think we first need to tackle how bitsets are cached in general. Solr caches (which the Solr JoinQuery uses) are top level (one commit and you lose it all). I'm unsure how to cache the posting list file pointers with the current Lucene api... I think this (joining) caching should be addressed in a new issue.\n\nPerformance of the JoinUtil looks actual quite good from what I have measured. I have a test data set containing 100000 products and 100 offers per product (10100000 docs in total). The JoinUtil is between 2 till 3 times faster than Solr's JoinQuery with this data set on my dev machine.",
            "date": "2012-01-19T23:01:56.422+0000",
            "id": 35
        },
        {
            "author": "Martijn van Groningen",
            "body": "Attached initial patch for 3x branch.\n* I replaced the BytesRefHash with a Set for now. FC in 3x is String based.\n* TermsQuery doesn't seek, but does a binary search to find the index in the String[] (created from the Set).... Not ideal. So we need some seeking.\n\nI ran some quick tests and performance is 3 till 4 times worse then the JoinUtil committed to trunk.",
            "date": "2012-01-19T23:08:52.922+0000",
            "id": 36
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}The terms collected in the first phase are from many segments{quote}\n\nWhy is that necessary?\n\n{quote}Caching can be improved in the second phase as you described, by saving a bitset per fromTerm?{quote}\n\nPossibly, only for terms with a high number of documents.  Or we can use a faster to decode (less compressed) posting codec.\n\nbq. The JoinUtil is between 2 till 3 times faster than Solr's JoinQuery with this data set on my dev machine\n\nInteresting, thanks for sharing.\n\n",
            "date": "2012-01-19T23:46:49.931+0000",
            "id": 37
        },
        {
            "author": "Jason Rutherglen",
            "body": "Just following up on the per-segment terms collection.  Join is going to be used as a filter in most cases (?).  Filters can be applied per-segment (unlike scoring queries).  So it seems possible to avoid the BRH creation by using the DTI?",
            "date": "2012-01-19T23:56:19.311+0000",
            "id": 38
        },
        {
            "author": "Martijn van Groningen",
            "body": "Jason: Better late then never... BRH is used to collect the matching from terms. The DTI just contains all terms / ords for a field. Comparing DTI ords isn't going to work when a term is in more than one segment or appears in a different field (fromField / toField). So I think the BRH can't be replaced by the DTI. The BRH could be cached per query.",
            "date": "2012-02-05T21:29:36.110+0000",
            "id": 39
        },
        {
            "author": "Martijn van Groningen",
            "body": "Attached updated version of query time joining for 3x branch. Instead of doing a binary search for each term comparison it seeks / iterates forward. It can't do seeking like we do in trunk, so it isn't as fast as in trunk. However I do think this can be committed to at least have query time join support in 3x. Back porting per segment filtering and the MTQ that is in trunk is quite some work...",
            "date": "2012-02-05T21:37:52.716+0000",
            "id": 40
        },
        {
            "author": "Martijn van Groningen",
            "body": "Committed latest 3x patch to branch3x.",
            "date": "2012-02-07T21:23:42.410+0000",
            "id": 41
        },
        {
            "author": "Martijn van Groningen",
            "body": "The joining in 3x is ~3 times slower than the joining committed to trunk. This is mainly caused by the fact that the MTQ in trunk can do seeking while the the MTQ in 3x can only increment to the next term.\n\n",
            "date": "2012-02-07T21:27:50.651+0000",
            "id": 42
        }
    ],
    "component": "modules/join",
    "description": "Solr has (psuedo) join query for a while now. I think this should also be available in Lucene.  ",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-3602",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "RFE",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Add join query to Lucene",
    "systemSpecification": true,
    "version": ""
}