{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "> - It seems that DocumentsWriter does not obey the maxMergeDocs\n>   parameter. If I don't flush manually, then the index only contains\n>   one segment at the end and the test fails.\n\nThis bug actually predates DocumentsWriter: the flushing logic has\nnever respected maxMergeDocs.  I think normally maxMergeDocs is far\nlarger than maxBufferedDocs.\n\nTo fix this we could change the flushing logic to include \"# buffered\ndocs > maxMergeDocs\" as one of its flush criteria, if the current\nmerge policy is a LogMergePolicy.\n\n> - If I flush manually after each addDocument() call, then the index\n>   contains more segments. But still, there are segments that contain \n>   more docs than maxMergeDocs, e. g. 55 vs. 50.\n\nThis behavior also predates the recent changes (MergePolicy, etc.), eg\nthe test fails on 2.1 if you flush every 6 docs (whenever \"0 == i%6\").\n\nReally the current approach is better described as \"any segment with\ndoc count greater than maxMergeDocs will not be merged\".\n\nWe could just fix the javadocs to match the current approach?\n\nOr, we could change the code to actually work the way the current\njavadoc says, ie \"no segment with > maxMergeDocs will ever be\ncreated\".\n\nThough, changing the code is somewhat tricky: in order to know whether\na segment will have > maxMergeDocs after the merge is done, you must\nknow the delete count against each of the segments, which is somewhat\ncostly to compute now (you have to read the current _X_N.del file for that\nsegment).\n\nMaybe we should store the deleteCount in the SegmentInfo (and save it\nto segments_N); we've discussed this in the past, eg, you would also\nwant to do this when making a merge policy that takes deletes into\naccount (favors merging segments that have many deletes).\n\nNote also that making the similar change for \"maxMergeMB\" is not\nreally feasible: you can't really compute how many MB a merged segment\nwill be from the input segments without just doing the merge and then\nchecking the resulting size.  Maybe we could make a coarse\napproximation by summing input sizes of the segments (usually this is\nan upper bound on final segment ssize), maybe doing proportional\nreduction of this size based on delete count.  Still it would be\napproaximate and you could wind up with a segment larger than\nmaxMergeMB.\n",
            "date": "2007-10-01T08:51:16.685+0000",
            "id": 0
        },
        {
            "author": "Yonik Seeley",
            "body": "> We could just fix the javadocs to match the current approach?\nThat sounds like the right approach.",
            "date": "2007-10-01T13:25:44.184+0000",
            "id": 1
        },
        {
            "author": "Michael McCandless",
            "body": "OK I will commit a fix to the javadocs.",
            "date": "2007-10-16T20:12:48.787+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "Corrected the javadocs.",
            "date": "2007-10-17T16:54:44.256+0000",
            "id": 3
        }
    ],
    "component": "core/index",
    "description": "I found two possible problems regarding IndexWriter's maxMergeDocs value. I'm using the following code to test maxMergeDocs:\n\n{code:java} \n  public void testMaxMergeDocs() throws IOException {\n    final int maxMergeDocs = 50;\n    final int numSegments = 40;\n    \n    MockRAMDirectory dir = new MockRAMDirectory();\n    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);      \n    writer.setMergePolicy(new LogDocMergePolicy());\n    writer.setMaxMergeDocs(maxMergeDocs);\n\n    Document doc = new Document();\n    doc.add(new Field(\"field\", \"aaa\", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));\n    for (int i = 0; i < numSegments * maxMergeDocs; i++) {\n      writer.addDocument(doc);\n      //writer.flush();      // uncomment to avoid the DocumentsWriter bug\n    }\n    writer.close();\n    \n    new SegmentInfos.FindSegmentsFile(dir) {\n\n      protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {\n\n        SegmentInfos infos = new SegmentInfos();\n        infos.read(directory, segmentFileName);\n        for (int i = 0; i < infos.size(); i++) {\n          assertTrue(infos.info(i).docCount <= maxMergeDocs);\n        }\n        return null;\n      }\n    }.run();\n  }\n{code} \n  \n- It seems that DocumentsWriter does not obey the maxMergeDocs parameter. If I don't flush manually, then the index only contains one segment at the end and the test fails.\n\n- If I flush manually after each addDocument() call, then the index contains more segments. But still, there are segments that contain more docs than maxMergeDocs, e. g. 55 vs. 50. The javadoc in IndexWriter says:\n{code:java}\n   /**\n   * Returns the largest number of documents allowed in a\n   * single segment.\n   *\n   * @see #setMaxMergeDocs\n   */\n  public int getMaxMergeDocs() {\n    return getLogDocMergePolicy().getMaxMergeDocs();\n  }\n{code}",
    "hasPatch": false,
    "hasScreenshot": false,
    "id": "LUCENE-1012",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Problems with maxMergeDocs parameter",
    "systemSpecification": true,
    "version": ""
}