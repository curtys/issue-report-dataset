{
    "comments": [
        {
            "author": "Ahmet Arslan",
            "body": "TurkishLowerCaseFilter that lowercases character 'I' correctly is added.",
            "date": "2009-12-01T20:29:36.006+0000",
            "id": 0
        },
        {
            "author": "Robert Muir",
            "body": "Hi Ahmet, this patch is looking very nice, thank you!\n\nI have some minor suggestions:\n* can we use hex notation (maybe also constants too) for the special case?\n* you can use assertTokenStreamContents here (it is in the base test case) to simplify your test, it works like assertAnalyzesTo but on tokenstream\n\nI will let others comment on where this belongs (maybe contrib?)\nWherever it is, I would like to use it in snowball contrib also.\n",
            "date": "2009-12-01T20:39:20.678+0000",
            "id": 1
        },
        {
            "author": "Uwe Schindler",
            "body": "Looks cool, it even uses the new CharUtils API.\n+1 for using assertTokenStreamContents.",
            "date": "2009-12-01T20:45:35.906+0000",
            "id": 2
        },
        {
            "author": "Robert Muir",
            "body": "I have one comment, that this will not work correctly on text that is not NFC. This is because uppercase I with dot can be represented as \\u0130 (as you handle it), but also decomposed as \\u0049 + \\u0307. There can also be stuff in between technically...\n\nafter finding a regular I (\\u0049) we could search ahead for COMBINING DOT ABOVE (ignoring any nonspacing marks and format and such along the way), and handle this differently.\n\nbut non-NFC text doesn't work correctly throughout most of lucene's analysis components as it is now anyway, so I don't think we should worry about it right now. Maybe we could add a comment for the future though.\n",
            "date": "2009-12-01T20:50:49.371+0000",
            "id": 3
        },
        {
            "author": "Uwe Schindler",
            "body": "As this is a new lowercasefilter, shouldn't be the default matchVersion  completely removed? Other filters have deprecated the no-matchVersion filter and this one also. A new class should not have deprecated parts. -> remove",
            "date": "2009-12-01T20:56:44.419+0000",
            "id": 4
        },
        {
            "author": "DM Smith",
            "body": "bq. but non-NFC text doesn't work correctly throughout most of lucene's analysis components as it is now anyway, so I don't think we should worry about it right now. Maybe we could add a comment for the future though.\n\nIt might be good to note the NFC (NFKC?) requirement in the JavaDoc.\n\nMaybe its just me, but I think it is critical to normalize the input to Lucene for both indexing and searching. Unless a NFCNormalizingFilter is added to Lucene, I think it is the responsibility of the caller.",
            "date": "2009-12-01T21:00:02.223+0000",
            "id": 5
        },
        {
            "author": "DM Smith",
            "body": "For new classes, would it be helpful to add @since to the class JavaDoc?",
            "date": "2009-12-01T21:03:27.988+0000",
            "id": 6
        },
        {
            "author": "Robert Muir",
            "body": "bq. Maybe its just me, but I think it is critical to normalize the input to Lucene for both indexing and searching. Unless a NFCNormalizingFilter is added to Lucene, I think it is the responsibility of the caller.\n\nyeah I think its critical too.\n\nbq. It might be good to note the NFC (NFKC?) requirement in the JavaDoc. \n\nyeah or maybe just a hint in the comments (because this is an exceptionally tricky case). \nthis same problem also applies to ASCIIFoldingFilter, pretty much all of the analyzers, etc too...\n",
            "date": "2009-12-01T21:03:39.051+0000",
            "id": 7
        },
        {
            "author": "Robert Muir",
            "body": "bq. Unless a NFCNormalizingFilter is added to Lucene, I think it is the responsibility of the caller.\n\nbtw DM, if you are interested, I inserted a long discussion about unicode normalization and how it interacts with Lucene tokenstreams in general in the javadoc header of ICUNormalizationFilter for LUCENE-1488. (please comment over there if you have suggestions or thoughts on it)\n",
            "date": "2009-12-01T21:07:25.251+0000",
            "id": 8
        },
        {
            "author": "Simon Willnauer",
            "body": "There is no need to use CharacterUtils in here. You can use Character.codePointAt() directly. This is a new class and does not need to preserve any bw. compatibility. I agree with uwe, the Version should go away in this patch.\n\nOnce more thing, this patch seems to be in core. I do not see any reason why this should be in core though. We should move it to contrib though as it serves such a specific usecase.\n\n",
            "date": "2009-12-01T21:20:29.957+0000",
            "id": 9
        },
        {
            "author": "Robert Muir",
            "body": "Simon, I would rather see this in contrib also. \n\nWould there be opposition to making contrib/snowball depend upon contrib/analyzers so the SnowballAnalyzer can use this filter instead of lowercase filter for the Turkish case? (based upon Version, of course)?\n",
            "date": "2009-12-01T21:22:47.611+0000",
            "id": 10
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. Would there be opposition to making contrib/snowball depend upon contrib/analyzers so the SnowballAnalyzer can use this filter instead of lowercase filter for the Turkish case? (based upon Version, of course)?\n\ni think we can arrange something like that. Since we factored out Smart-cn the jar has reasonable size so this won't be an issue. maybe we should think about moving snowball into analyzers/snowball - just an idea.\nAnyway, this is somewhat unrelated to this particular patch but still considerable.",
            "date": "2009-12-01T21:29:36.217+0000",
            "id": 11
        },
        {
            "author": "Robert Muir",
            "body": "I don't think its really unrelated, I think its a consideration towards where we put this.\n\nThe turkish analyzer happens to be in contrib/snowball, and thats what really needs this for turkish search. (Although I agree this filter could be useful on its own)",
            "date": "2009-12-01T21:33:08.442+0000",
            "id": 12
        },
        {
            "author": "Ahmet Arslan",
            "body": "assertTokenStreamContents, @since and hex constants are added.\ndeprecated constructor is removed.",
            "date": "2009-12-01T21:56:57.991+0000",
            "id": 13
        },
        {
            "author": "Robert Muir",
            "body": "Ahmet, hi I think you might have accidentally left the old (duplicate) test in there that does not use assertTokenStreamContents?\n",
            "date": "2009-12-01T22:06:43.123+0000",
            "id": 14
        },
        {
            "author": "Ahmet Arslan",
            "body": "I kept the old test method and added a new one. Should i remove old one?",
            "date": "2009-12-01T22:10:26.942+0000",
            "id": 15
        },
        {
            "author": "Robert Muir",
            "body": "Ahmet, I think so. they both test the same functionality, but the second test is less code, and in my opinion, better. assertTokenStreamContents does some additional checks, it clears attributes in between, it calls .end(), things like that.\n",
            "date": "2009-12-01T22:16:17.216+0000",
            "id": 16
        },
        {
            "author": "Uwe Schindler",
            "body": "One othe possibility to resolve the problem in a completely different way: You could wrap a MappingCharFilter on top of the input reader in Analyzer and just add a replacement for this one char:\n[http://lucene.apache.org/java/3_0_0/api/all/org/apache/lucene/analysis/MappingCharFilter.html]\n\nThis would be a very easy fix without code duplication. You just change the input before tokenization. And its already in Lucene core, just plug it into the analyzer's tokenStream() or reusableTokenStream() method as a wrapper around the Reader param.\n\nThis would be very easy also for the other analyzers having problem with seldom chars. It can also be used to remove chars at all or replace them by longer sequences like \u00e4 -> ae (for german).",
            "date": "2009-12-01T22:18:05.362+0000",
            "id": 17
        },
        {
            "author": "Ahmet Arslan",
            "body": "test that does not use assertTokenStreamContents is removed.",
            "date": "2009-12-01T22:18:57.311+0000",
            "id": 18
        },
        {
            "author": "Robert Muir",
            "body": "bq. One othe possibility to resolve the problem in a completely different way: You could wrap a MappingCharFilter on top of the input reader in Analyzer and just add a replacement for this one char:\n\nUwe, but this is inflexible. If we want to make this filter support turkish lowercasing in the future for all of unicode, not just NFC composed form, we cannot do it with MappingCharFilter. Again I don't think we should fix this now, but in the future I think we might want to. ",
            "date": "2009-12-01T22:22:31.327+0000",
            "id": 19
        },
        {
            "author": "Uwe Schindler",
            "body": "The patch's TurkishLowerCaseFilter is as unflexible as that. The idea is just a replacement for the current patch (and it is even a little bit more universal, because you can change the chars to map).",
            "date": "2009-12-01T22:27:15.900+0000",
            "id": 20
        },
        {
            "author": "Robert Muir",
            "body": "bq. test that does not use assertTokenStreamContents is removed. \n\nThanks Ahmet, in my opinion this is good, we just have to figure out where to place it. \n\nMy vote is for contrib/analyzers/common/tr for now.",
            "date": "2009-12-01T22:28:00.007+0000",
            "id": 21
        },
        {
            "author": "Robert Muir",
            "body": "bq. The patch's TurkishLowerCaseFilter is as unflexible as that. The idea is just a replacement for the current patch (and it is even a little bit more universal, because you can change the chars to map).\n\nUwe this is not true. With a tokenfilter, I can use Version that will apply the logic i mentioned above:\nbq. after finding a regular I (\\u0049) we could search ahead for COMBINING DOT ABOVE (ignoring any nonspacing marks and format and such along the way), and handle this differently.\n\nyou cannot do this with mappingchar filter, or rather, you could, but there would be millions of mappings for this one character. I could later patch this filter with Version and some lookahead based on unicode properties if i wanted to improve it.",
            "date": "2009-12-01T22:29:39.782+0000",
            "id": 22
        },
        {
            "author": "Uwe Schindler",
            "body": "if I replace this code from Ahmet's test\n\n{code}\npublic class TestTurkishLowerCaseFilter extends BaseTokenStreamTestCase {\n\n  public void testTurkishLowerCaseFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(\n        new StringReader(\"\\u0130STANBUL \\u0130ZM\\u0130R ISPARTA\"));\n    TokenStream filter = new TurkishLowerCaseFilter(Version.LUCENE_30, stream);\n\tassertTokenStreamContents(filter, new String[] {\"istanbul\", \"izmir\", \"\\u0131sparta\",});      \n  }\n\n}\n{code}\n\nby that, there is not even a new class or anything needed:\n\n{code}\npublic class TestTurkishLowerCaseFilter extends BaseTokenStreamTestCase {\n\n  static final NormalizeCharMap map = new NormalizeCharMap();\n  static {\n   map.add(\"\\u0049\", \"0x0131\");\n  }\n\n  public void testTurkishLowerCaseFilter() throws Exception {\n    TokenStream stream = new WhitespaceTokenizer(\n        new MappingCharFilter(map,\n        new StringReader(\"\\u0130STANBUL \\u0130ZM\\u0130R ISPARTA\")));\n    TokenStream filter = new LowerCaseFilter(Version.LUCENE_30, stream);\n\tassertTokenStreamContents(filter, new String[] {\"istanbul\", \"izmir\", \"\\u0131sparta\",});      \n  }\n\n}\n{code}\n\nIt just works.",
            "date": "2009-12-01T22:34:07.662+0000",
            "id": 23
        },
        {
            "author": "Robert Muir",
            "body": "Uwe I don't think you understand what I am saying.\n\nif my text is instead I\u0307STANBUL versus your \u0130STANBUL, it will not work.",
            "date": "2009-12-01T22:37:43.607+0000",
            "id": 24
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. Uwe this is not true. With a tokenfilter, I can use Version that will apply the logic i mentioned above\n\nI am talking about *this* patch. Not any later version! I suggest to not apply this patch at all and for now tell the user to use above helper construct until we have ICU in core or whatever (sorry for the missing  \\u, I do not want to edit again...)",
            "date": "2009-12-01T22:39:27.267+0000",
            "id": 25
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, I am talking about this patch too. it is simple and can be extended to the future to handle such things.\nyour mappingcharfilter approach cannot, and I don't see us having ICU in core ever, even though I would love such a thing.\n\nAdditionally, it will make it easier to fix SnowballAnalyzer, which is currently *broken for turkish language* because it uses the wrong lowercase.\n",
            "date": "2009-12-01T22:41:39.519+0000",
            "id": 26
        },
        {
            "author": "Uwe Schindler",
            "body": "Robert: I understand your problem, but it affects LowerCaseFilter at all and is not special to the Turkish lower filter. If you have decomposed characters even LowerCaseFilter would fail for *all* languages (even German if you compose \u00e4 out of a and two dots). In germany really nobody uses de-composed chars, I do not know how this is in Turkey, but the last time I was there, they just used the simpliest composed chars (like germans), they even have the umlauts which they use from the basic latin1 range. And for that this filter works and is a quick fix.\n\nBut I give up now.\n\n*EDIT* Good night, I cannot hack my keys anymore :-) Sorry for heavy issue editing.",
            "date": "2009-12-01T22:51:58.967+0000",
            "id": 27
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, it *is* specific to the turkish case.\nbecause for german, whether you have A, umlaut or A+umlaut as one character, it works regardless.\nturkish is the only case where its more complex, because the casing of the character actually depends upon a diacritic that may not be composed, and may have other diacritics in between.\n\nthis is what makes it such a bear to support in case folding:\n\n{noformat}\n#      Note that the Turkic mappings do not maintain canonical equivalence without additional processing.\n#      See the discussions of case mapping in the Unicode Standard for more information.\n{noformat}\n\nThe problem is that context is required, and sometimes marks must actually be deleted for proper casing.\n\n{noformat}\n# When lowercasing, remove dot_above in the sequence I + dot_above, which will turn into i.\n# This matches the behavior of the canonically equivalent I-dot_above\n\n0307; ; 0307; 0307; tr After_I; # COMBINING DOT ABOVE\n0307; ; 0307; 0307; az After_I; # COMBINING DOT ABOVE\n\n# When lowercasing, unless an I is before a dot_above, it turns into a dotless i.\n\n0049; 0131; 0049; 0049; tr Not_Before_Dot; # LATIN CAPITAL LETTER I\n0049; 0131; 0049; 0049; az Not_Before_Dot; # LATIN CAPITAL LETTER I\n{noformat}\n\nbq. but the last time I was there, they just used the simpliest composed chars (like germans).\n\nThis is why i recommended we not go crazy and only work on the composed form. But in the future we might want to correct this.\nthis is *impossible* to do with mappingcharfilter, that is my only point.",
            "date": "2009-12-01T22:56:45.866+0000",
            "id": 28
        },
        {
            "author": "Robert Muir",
            "body": "attached is a slight modification, to support proper lowercasing for decomposed forms as well.\nwhen an uppercase I is encountered, some lookahead is necessary (as long as characters are nonspacing marks they can be in between), to see if there is later a COMBINING_DOT_ABOVE.\n\nin this case, the uppercase I must be lowercased to a regular 'i', and this COMBINING_DOT_ABOVE later removed.\n",
            "date": "2009-12-02T00:32:21.910+0000",
            "id": 29
        },
        {
            "author": "Robert Muir",
            "body": "Hi Uwe, now are you ok with this being a TokenFilter? :)\n\nCan we discuss where to put it, maybe contrib/analyzers under \"tr\" ?",
            "date": "2009-12-02T00:42:58.053+0000",
            "id": 30
        },
        {
            "author": "Uwe Schindler",
            "body": "Robert: Looks crazy but seems to work, great work! Small comments about the patch:\nThe int codepoint constants should be *static* final not just final, termAtt should also be final [and optionally can be initialized directly in the declaration which I prefer, but that's your turn]. In the test, the Version.LUCENE_CURRENT should be used (which has no effect on the tests, but maybe in future).",
            "date": "2009-12-02T06:57:05.493+0000",
            "id": 31
        },
        {
            "author": "Robert Muir",
            "body": "Uwe, it is kinda crazy. but I think worth the effort not to have such wierd behavior.\n\nI didn't optimize it yet in general either. In reality, on composed text this should not slow you down.\nit should be one additional getType() check after any uppercase \"I\". (its 2 right now, I will fix)\nI'll also add in your suggestions (and move the patch to contrib/analyzers/tr for now), and upload a new patch.\n ",
            "date": "2009-12-02T07:08:30.870+0000",
            "id": 32
        },
        {
            "author": "Robert Muir",
            "body": "updated patch for contrib, with optimization i mentioned earlier, some cleanups from Uwe, and some more docs.\n",
            "date": "2009-12-02T07:31:03.196+0000",
            "id": 33
        },
        {
            "author": "Robert Muir",
            "body": "Sorry, new patch, forgot to change tests to LUCENE_CURRENT as Uwe mentioned.",
            "date": "2009-12-02T07:43:08.384+0000",
            "id": 34
        },
        {
            "author": "Simon Willnauer",
            "body": "Maybe I miss something but what is the reason to use Version in here. This is new code and it does not have to maintain any bw compatibility. There is also no need to use CharacterUtils you can use Character directly, can't you?!\n\n",
            "date": "2009-12-02T09:40:18.764+0000",
            "id": 35
        },
        {
            "author": "Uwe Schindler",
            "body": "We should use version from the beginning of new tokenfilters to be prepared for the future. In this case the version could be ignored until we have a new version of this filter. But to be consistent with LowerCaseFilter I would prefer to also use the matchVersion to select the right utils class.",
            "date": "2009-12-02T09:49:19.524+0000",
            "id": 36
        },
        {
            "author": "Simon Willnauer",
            "body": "bq.  But to be consistent with LowerCaseFilter I would prefer to also use the matchVersion to select the right utils class.\n\nI agree with the consistency ctor but I don't with the CharacterUtils. I do not see why a new class should yield buggy behavior in any case. If somebody uses Version < 3.1 this filter will not work correctly. We should prevent this in any case, another possibility would be enforcing a version >=3.1\nThen I would be ok with it.",
            "date": "2009-12-02T10:04:00.044+0000",
            "id": 37
        },
        {
            "author": "Simon Willnauer",
            "body": "I think we should have a consistent behaviour for new classes taking version. It would make sense to have a method in Version like:\n{code}\npublic enum Version {\n...\n public void ensureOnOrAfter(Version other){\n    if(!onOrAfter(other))\n      throw new IllegalArgumentException(\"Version < \" +this.name() + \" are not supported\");\n  }\n{code}\n\nThat enforces the version for new classes. This would also help to get rid of old behaviour in 4.0\nWe should open another issue for this I guess. I see many usecases where this could be very useful.\n\n\n",
            "date": "2009-12-02T10:13:02.315+0000",
            "id": 38
        },
        {
            "author": "Robert Muir",
            "body": "bq. If somebody uses Version < 3.1 this filter will not work correctly. \n\nSimon, all the tests pass with Version < 3.1 (I tried LUCENE_20). So its just consistency with the other code.\nI think we should do the same in general, just my opinion.\n\nFor example, if I write a new analyzer in 3.1, I'm just gonna take Version and pass it to my components.\nIts true I do not even need Version. Maybe by 3.2 comes out there is only 1 thing affected by Version.\nAnd yes, if its based on StandardTokenizer someone can set version LUCENE_24 and it will parse acronyms in an invalid way,\neven though the analyzer wasn't around at the time of Lucene 2.4\n\nBut this is easy and consistent, to just pass along version, we already do this in other places, I think its not worth changing.\n",
            "date": "2009-12-02T15:26:01.790+0000",
            "id": 39
        },
        {
            "author": "Simon Willnauer",
            "body": "Robert, I see your point. The root cause why this bugs me is that this TokenFilter changes his behavior (at least if you index deseret with this analyzer) :) depending on the passed version. I don't think that new code should try do anything based on version. The Version ctor it totally ok for me in this case but we should really use Integer.CodePointAt() instead of CharacterUtil. Once I think about the mess ensureOnOrAfter would create throughout all the code I doubt it would to any good in the end.\n\n",
            "date": "2009-12-02T17:22:42.264+0000",
            "id": 40
        },
        {
            "author": "Robert Muir",
            "body": "bq. I don't think that new code should try do anything based on version.\n\nok, I will change this. You are right, I would look at this problem differently if we didnt have CharacterUtil which makes it just so easy to support the old and new behavior.\n",
            "date": "2009-12-02T17:34:36.563+0000",
            "id": 41
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. ok, I will change this. You are right, I would look at this problem differently if we didnt have CharacterUtil which makes it just so easy to support the old and new behavior.\nActually, a unused Version argument is silly. If we have to add it in the future because of some change, you WANT to deprecate the ctor to make users aware of it. that is what deprecations are made for. I would not argue about consistency as not every TokenFilter has a Version ctor. (EdgeNGramTokenFilter for instance - this is just first coming to my mind). I would remove it completely! Use Character.codePointAt() and you are good to go.\n\n",
            "date": "2009-12-02T17:44:24.553+0000",
            "id": 42
        },
        {
            "author": "Robert Muir",
            "body": "patch with Simon's suggestions.",
            "date": "2009-12-02T17:46:05.923+0000",
            "id": 43
        },
        {
            "author": "Simon Willnauer",
            "body": "Robert, this patch looks good! \nMuch cleaner now - version is not appropriate here!\n\nThanks for updating.",
            "date": "2009-12-02T18:24:34.199+0000",
            "id": 44
        },
        {
            "author": "Robert Muir",
            "body": "trivial update with comment for the test and better example explaining what this is all about.",
            "date": "2009-12-02T18:40:36.434+0000",
            "id": 45
        },
        {
            "author": "Simon Willnauer",
            "body": "I will take over for Robert - go ahead and get automation done robert!",
            "date": "2009-12-02T18:48:35.583+0000",
            "id": 46
        },
        {
            "author": "Simon Willnauer",
            "body": "I plan to commit the latest patch until 12/06/09 if nobody objects.",
            "date": "2009-12-03T10:31:38.977+0000",
            "id": 47
        },
        {
            "author": "Robert Muir",
            "body": "Hello Simon, if this issue is resolved (so we do not forget), can we open a separate issue to fix the SnowballAnalyzer when using Turkish language?\nI also think we should add some javadocs to the snowball stem filter that explain you need to use this filter beforehand for it to work.\n\nI already have some unit tests produced showing it doesn't work correctly with LowerCaseFilter and that it also does not handle uppercase.\n",
            "date": "2009-12-03T16:14:16.037+0000",
            "id": 48
        },
        {
            "author": "Simon Willnauer",
            "body": "I will commit this tomorrow if nobody objects.",
            "date": "2009-12-04T23:42:58.615+0000",
            "id": 49
        },
        {
            "author": "Simon Willnauer",
            "body": "Committed in revision 887535\n\nThanks Ahmet / Robert!",
            "date": "2009-12-05T12:47:06.163+0000",
            "id": 50
        }
    ],
    "component": "modules/analysis",
    "description": "java.lang.Character.toLowerCase() converts 'I' to 'i' however in Turkish alphabet lowercase of 'I' is not 'i'. It is LATIN SMALL LETTER DOTLESS I.\n\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2102",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "LowerCaseFilter for Turkish language",
    "systemSpecification": true,
    "version": "3.0"
}