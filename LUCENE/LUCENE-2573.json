{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "I think just keep it simple?  User sets RAM buffer size, and we compute fixed high/low watermarks from there?",
            "date": "2010-07-28T17:01:09.897+0000",
            "id": 0
        },
        {
            "author": "Michael Busch",
            "body": "Yeah I like that better too.  Will implement that approach.",
            "date": "2010-07-28T17:10:01.549+0000",
            "id": 1
        },
        {
            "author": "Jason Rutherglen",
            "body": "Users probably won't customize to that level of detail, and RAM usage isn't entirely accurate in Java anyways.  Lets keep it as is (ie, setting the ram buffer size).",
            "date": "2010-07-28T17:22:42.797+0000",
            "id": 2
        },
        {
            "author": "Michael Busch",
            "body": "Jason, are you still up for working on a patch for this one?\n\nWe should probably get the realtime branch in a healthy state first and run some performance tests before we start working on all the fun stuff.\nAlmost there!",
            "date": "2010-07-29T16:14:11.384+0000",
            "id": 3
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael, I could, I was working on the terms dictionary, reading\npostings etc, so that deletes will operate correctly. It'd be\ngreat to nail down the concurrency of the *BlockPools first LUCENE-2575, as\nso much uses them, then test performance etc, otherwise we'll\nhave a lot of code relying on something that could be changing?",
            "date": "2010-07-29T17:03:48.247+0000",
            "id": 4
        },
        {
            "author": "Jason Rutherglen",
            "body": "So yeah I'll work on a patch for this issue.",
            "date": "2010-07-29T17:13:51.711+0000",
            "id": 5
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael, DWPT.numBytesUsed isn't currently being updated?",
            "date": "2010-07-29T18:29:40.559+0000",
            "id": 6
        },
        {
            "author": "Michael Busch",
            "body": "bq. Michael, DWPT.numBytesUsed isn't currently being updated? \n\nYou can delete that one.  I factored all the memory allocation/tracking into DocumentsWriterRAMAllocator.  You might have to get some memory related stuff from trunk, e.g. the balanceRAM() code and adapt it.",
            "date": "2010-07-29T19:28:56.430+0000",
            "id": 7
        },
        {
            "author": "Jason Rutherglen",
            "body": "Is DocumentsWriterRAMAllocator.numBytesUsed accurate then?  It seems like some things are being recorded into it, however I'm not immediately sure everything is...",
            "date": "2010-07-30T01:17:43.140+0000",
            "id": 8
        },
        {
            "author": "Michael Busch",
            "body": "I'm not 100% sure, I need to review the code to refresh my memory...",
            "date": "2010-07-30T04:32:42.299+0000",
            "id": 9
        },
        {
            "author": "Michael Busch",
            "body": "Hi Jason, are you still working on the patch here?",
            "date": "2010-08-15T09:52:03.295+0000",
            "id": 10
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael, I've been on holiday in Europe for the last 2 weeks.  When I'm back tomorrow I'll resume work on it.",
            "date": "2010-08-15T20:06:34.387+0000",
            "id": 11
        },
        {
            "author": "Jason Rutherglen",
            "body": "I'm not sure if after a DWPT is flushing we need to decrement what would effectively be a \"projected RAM usage post current DWPT flush completion\".  Otherwise we could in many cases, start the flush of most/all of the DWPTs.  ",
            "date": "2010-09-06T02:48:01.296+0000",
            "id": 12
        },
        {
            "author": "Jason Rutherglen",
            "body": "It looks like StoredFieldsWriter is reused after flushing a DWPT, however we're not resetting isClosed.",
            "date": "2010-09-06T03:45:43.102+0000",
            "id": 13
        },
        {
            "author": "Jason Rutherglen",
            "body": "We probably need a test that delays the flush process, otherwise flushing to RAM occurs too fast to proceed to the next tier.",
            "date": "2010-09-06T05:13:14.597+0000",
            "id": 14
        },
        {
            "author": "Jason Rutherglen",
            "body": "Here's a first cut...\n\nThe TestDWPTFlushByRAM doesn't do much at this point.  It adds documents in 2 threads, and prints the RAM usage to stdout.  It more or less shows the tiered flushing working.  \n\nI don't think we're tracking all of the RAM usage yet, maybe just the terms?  I need to review.  ",
            "date": "2010-09-06T19:00:57.424+0000",
            "id": 15
        },
        {
            "author": "Jason Rutherglen",
            "body": "The DWPT that happens to exceed the first tier, is flushed out.  This was easier to implement than finding the highest RAM consuming DWPT and flushing it, from a different thread.",
            "date": "2010-09-06T19:08:17.332+0000",
            "id": 16
        },
        {
            "author": "Jason Rutherglen",
            "body": "I did a search through the code and ByteBlockAllocator.perDocAllocator has no references, it can probably be removed, unless there was some other intention for it.",
            "date": "2010-09-06T19:21:16.278+0000",
            "id": 17
        },
        {
            "author": "Jason Rutherglen",
            "body": "I think we can/should track the RAM consumption directly in IntBlockPool and ByteBlockPool.  I'm not sure if we're tracking those allocations right now, however if we are or are not, it'd be clearer to add a getBytesUsed to these pool classes.",
            "date": "2010-09-06T19:36:39.775+0000",
            "id": 18
        },
        {
            "author": "Jason Rutherglen",
            "body": "The last comment is incorrect, DocumentsWriterRAMAllocator does keep track of all allocations of int[] and byte[], and the number of bytes used.  I'm not sure why the test is reporting zero bytes used after documents have been successfully added.  ",
            "date": "2010-09-06T20:15:08.574+0000",
            "id": 19
        },
        {
            "author": "Jason Rutherglen",
            "body": "In DocumentsWriterRAMAllocator, we're only recording the addition of more bytes when a new block is created, however because previous blocks may be recycled, it is the recycled blocks that are not being recorded as bytes used.  Should we record all allocated blocks as \"in use\" ie, count them as bytes used, or wait until they are \"in use\" again to be counted as consuming RAM?",
            "date": "2010-09-07T00:12:35.811+0000",
            "id": 20
        },
        {
            "author": "Michael McCandless",
            "body": "bq. We probably need a test that delays the flush process, otherwise flushing to RAM occurs too fast to proceed to the next tier.\n\nWe can modify MockRAMDir to optionally \"take its sweet time\" when writing certain files?\n\n{quote}\nI'm not sure if after a DWPT is flushing we need to decrement what would effectively be a \"projected RAM usage post current DWPT flush completion\". Otherwise we could in many cases, start the flush of most/all of the DWPTs.\n{quote}\n\nBut shouldn't tiered flushing take care of this?  Ie you only decr RAM consumed when the flush of the DWPT finishes, not before?\n\nbq. The DWPT that happens to exceed the first tier, is flushed out. This was easier to implement than finding the highest RAM consuming DWPT and flushing it, from a different thread.\n\nHmm but this won't be most efficient, in general?  Ie we could end up creating tiny segments depending on luck-of-the-thread-scheduling?\n\nbq. I did a search through the code and ByteBlockAllocator.perDocAllocator has no references, it can probably be removed, unless there was some other intention for it.\n\nI think this makes sense -- each DWPT now immediately flushes to its private doc store files, so there's no longer a need to track per-doc pending RAM?\n\n{quote}\nIn DocumentsWriterRAMAllocator, we're only recording the addition of more bytes when a new block is created, however because previous blocks may be recycled, it is the recycled blocks that are not being recorded as bytes used. Should we record all allocated blocks as \"in use\" ie, count them as bytes used, or wait until they are \"in use\" again to be counted as consuming RAM?\n{quote}\n\nI think we have to track both.  If a buffer is not in the pool (ie not free), then it's in use and we count that as RAM used, and that counter is used to trigger tiered flushing.  Separately we have to track net allocated, in order to trim the buffers (drop them, so GC can reclaim) when we are over the .setRAMBufferSizeMB.",
            "date": "2010-09-07T09:34:13.906+0000",
            "id": 21
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. shouldn't tiered flushing take care of this\n\nFaulty thinking for a few minutes.\n\n{quote}but this won't be most efficient, in general? Ie we could end up creating tiny segments depending on luck-of-the-thread-scheduling?{quote}\n\nTrue.  Instead, we may want to simply not-flush the current DWPT if it is in fact not the highest RAM user.  When addDoc is called on the thread with the highest RAM usage, we can then flush it.\n\nbq. there's no longer a need to track per-doc pending RAM\n\nI'll remove it from the code.\n\n{quote}If a buffer is not in the pool (ie not free), then it's in use and we count that as RAM used{quote}\n\nOk, I'll make the change.  \n\n{quote}we have to track net allocated, in order to trim the buffers (drop them, so GC can reclaim) when we are over the .setRAMBufferSizeMB{quote}\n\nI haven't seen this in the realtime branch.  Reclamation of extra allocated free blocks may need to be reimplemented.  \n\nI'll increment num bytes used when a block is returned for use.\n\nOn this topic, do you have any thoughts yet about how to make the block pools concurrent?  I'm still leaning towards a random access file (seek style) interface because this is easy to make concurrent, and hides the underlying block management mechanism, rather than directly exposes it like today, which can lend itself to problematic usage in the future.",
            "date": "2010-09-07T13:46:51.707+0000",
            "id": 22
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. We can modify MockRAMDir to optionally \"take its sweet time\" when writing certain files?\n\nYes, I think we need to implement something of this nature.  We *could* even randomly assign a different delay value per flush.  Of course how the test would instigate this from outside of DW, is somewhat of a different issue.",
            "date": "2010-09-07T13:49:53.242+0000",
            "id": 23
        },
        {
            "author": "Jason Rutherglen",
            "body": "* perDocAllocator is removed from DocumentsWriterRAMAllocator\n\n* getByteBlock and getIntBlock always increments the numBytesUsed\n\nThe test that simply prints out debugging messages looks better.  I need to figure out unit tests.",
            "date": "2010-09-07T18:09:42.666+0000",
            "id": 24
        },
        {
            "author": "Jason Rutherglen",
            "body": "The last patch also only flushes a DWPT if it's the highest RAM consumer.",
            "date": "2010-09-07T18:17:22.020+0000",
            "id": 25
        },
        {
            "author": "Jason Rutherglen",
            "body": "There was a small bug in the choice of the max DWPT, in that all DWPTs, including ones that were scheduled to flush were being compared against the current DWPT (ie the one being examined for possible flushing).",
            "date": "2010-09-07T23:42:33.423+0000",
            "id": 26
        },
        {
            "author": "Jason Rutherglen",
            "body": "I was hoping something clever would come to me about how to unit test this, nothing has.  We can do the slowdown of writes to the file(s) via a Thread.sleep, however this will only emulate a real file system in RAM, what then?  I thought about testing the percentage however is it going to be exact?  We could test a percentage range of each of the segments flushed?  I guess I just need to run the all of the unit tests, however some of those will fail because deletes aren't working properly yet.  ",
            "date": "2010-09-24T00:44:18.743+0000",
            "id": 27
        },
        {
            "author": "Simon Willnauer",
            "body": "Here is my first cut / current status on this issue. First of all I have a couple of failures related to deletes but they seem not to be related (directly) to this patch since I can reproduce them even without the patch. \nall of the failures are related to deletes in some way so I suspect that there is another issue for that, no?\n\nThis patch implements a tiered flush strategy combined with a concurrent flush approach. \n\n* All decisions are based on a FlushPolicy which operates on a DocumentsWriterSession (does the ram tracking and housekeeping), once the flush policy encounters a transition to the next tier it marks the \"largest\" ram consuming thread \nas flushPending if we transition from a lower level and all threads if we transition from the upper watermark (level). DocumentsWriterSession shifts the memory of a pending thread to a new memory \"level\" (pendingBytes) and marks the thread as pending. \n\n* Once FlushPolicy#findFlushes(..) returns the caller tries to check if itself needs to flush and if so it \"checks-out\" its DWPT and replaces it with a complete new instance. Releases the lock on the ThreadState and continues to flush the \"checked-out\" DWPT. After this is done or if the current DWPT doesn't need flushing the indexing thread checks if there are any other pending flushes and tries to (non-blocking) obtain their lock. It only tries to get the lock and only tries once since if the lock is taken another thread is already holding it and will see the flushPending once finished adding the document.\n\n\nThis approach tries to utilize as much conccurrency as possible while flushing the DWPT and releaseing its ThreadState with an entirely new DWPT. Yet, this might also have problems especially if IO is slow and we filling up indexing RAM too fast. To prevent us from bloating up the memory too much I introduced a notation of \"healtiness\" which operates on the net-bytes used in the DocumentsWriterSession (flushBytes + pendingBytes + activeBytes) -- (flushBytes - mem consumption of currently flushing DWPT, pendingBytes - mem consumption of marked as pending ThreadStates / DWPT, activeBytes mem consuption of the indexing DWPT). If net-bytes reach a certain threshold (2*maxRam currently) I stop incoming threads until the session becomes healty again.\n\nI run luceneutil with trunk vs. LUCENE-2573 indexing 300k wikipedia docs with 1GB MaxRamBuffer and 4 Threads. Searches on both indexes yield identical results (Phew!) \nIndexing time in ms look promising\n||trunk||patch|| diff ||\n|134129 ms|102932 ms|{color:green}23.25%{color}| \n\nThis patch is still kind of rough and needs iterations so reviews and questions are very much welcome.\n\n\n",
            "date": "2011-03-07T16:51:07.785+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "OK I tested trunk vs RT branch + patch, indexing 10M Wikipedia docs\nwith 1 GB RAM buffer and 8 threads.\n\nTrunk took 298 sec for initial indexing, then 198 sec to wait for\nmerges to catch up, and 24 sec to commit.\n\nRT+patch took 289 sec for initial indexing, then 225 sec to wait for\nmerges, then 26 sec to commit.\n\nNot sure yet why I'm not seeing real concurrency gains here... is\nthere anything printed to infoStream if the safety net (hijack app\nthreads) kicks in?\n",
            "date": "2011-03-07T22:13:20.489+0000",
            "id": 29
        },
        {
            "author": "Michael McCandless",
            "body": "Woops -- I screwed up the above test!\n\nCorrected numbers: trunk takes 439 sec to index 10M docs, 202 sec to waitForMerges, 17 sec to commit.\n\nRT + patch: 289 sec to index 10M docs, 225 sec to waitForMerges, 26 sec to commit.\n\nThis is w/ 8 threads (machine has 24 cores), writing to Intel X25M G2 ssd.\n\nAwesome speedup!!  Nice work everyone ;)  Looking forward to making a new blog post soon!",
            "date": "2011-03-08T09:50:30.859+0000",
            "id": 30
        },
        {
            "author": "Simon Willnauer",
            "body": "here is an updated patch that writes more stuff to the infostream including if we are unhealthy and block threads.\nI also fixed some issues with TestIndexWriterExceptions.\n\nAnother idea we should follow IMO is to see if we can biggypack the indexing threads on commit / flushAll instead of waiting to be able to lock each DWPT and flush it sequentially. This should be fairly easy since we can simply mark them as flushPending and let incoming indexing thread do the flush in parallel. Depending on how we index and how big the DWPTs are this could give us another sizable gain. For instance if  you index and frequently commit, lets say every 10k docs (so many folks do stuff like that) but keep on indexing we should see concurrency helping us a lot since commit is not blocking all incoming indexing threads. I think we should spinoff another issues once this is ready",
            "date": "2011-03-08T11:39:03.267+0000",
            "id": 31
        },
        {
            "author": "Michael McCandless",
            "body": "Not done reviewing the patch but here's some initial feedback:\n\n\nVery cool (and super advanced) that this adds a FlushPolicy!  But for\n\"normal\" usage we go and make either DocCountFP or TieredFP, depending\non whether IWC is flushing by docCount, RAM or both right?  Ie one\nnormally need not make their own FlushPolicy.\n\nMaybe rename TieredFP -> ByRAMFP?  Also, I'm not sure we need the N\ntiers?  I suspect that may flush too heavily?  Can we instead simplify\nit and have only the low and high water marks?  So we flush when\nactive RAM is over low water mark?  (And we stall if active + flushing\nRAM exceeds high water mark).\n\nCan we rename isHealthy to isStalled (ie, invert it)?\n\nI'm still unsure we should even include any healthy check APIs.  This\nis an exceptional situation and I don't think we need API exposure for\nit?  If apps really want to, they can turn on infoStream (we should\nmake sure \"stalling\" is logged, just like it is for merging) and\ndebug from there?\n\nMaybe rename pendingBytes to flushingBytes?  Or maybe\nflushPendingBytes?  (Just to make it clear what we are pending on...).\n\nMaybe rename FP.printInfo(String msg) --> FP.message?  (Consistent w/\nour other classes).\n\nI wonder if FP.findFlushes should be renamed to something like\nFP.visit, and return void?  Ie, it's called for its side effects of\nmarking DWPTs for flushing, right?  Separately, whether or not this\nthread will go and flush a DWPT is for IW to decide?  (Like it could\nbe this thread didn't mark any new flush required, but it should go\noff and pull a DWPT previously marked by another thread).  So then IW\nwould have a private volatile boolean recording whether any active\nDWPTs have flushPending.\n",
            "date": "2011-03-08T11:57:37.075+0000",
            "id": 32
        },
        {
            "author": "Simon Willnauer",
            "body": "{quote}\nMaybe rename TieredFP -> ByRAMFP? Also, I'm not sure we need the N\ntiers? I suspect that may flush too heavily? Can we instead simplify\nit and have only the low and high water marks? So we flush when\nactive RAM is over low water mark? (And we stall if active + flushing\nRAM exceeds high water mark).\n{quote}\nthis really sounds like a different FlushPolicy to me. But is worth a try - should be easy to add with this patch. so you mean we always flush ALL DWPT once we reached the low watermark? I don't think this is a good idea. And I wonder if that is a bit too aggressive to say you put DW into stalled mode if we exceed the high watermark. Anyway we can try and see what works better right?\n\nbq. Can we rename isHealthy to isStalled (ie, invert it)?\nsure isStalled sounds fine\n\nbq. I'm still unsure we should even include any healthy check APIs.\nfor now this is internal only so even if we decide to I would shift that to a different issue.\n\n{quote}\nMaybe rename pendingBytes to flushingBytes? Or maybe\nflushPendingBytes? (Just to make it clear what we are pending on...).\n{quote}\n\nyeah that is true - flushPendingBytes to make it consistent - my fault...\n\n{quote}\nI wonder if FP.findFlushes should be renamed to something like\nFP.visit, and return void? Ie, it's called for its side effects of\nmarking DWPTs for flushing, right? Separately, whether or not this\nthread will go and flush a DWPT is for IW to decide? (Like it could\nbe this thread didn't mark any new flush required, but it should go\noff and pull a DWPT previously marked by another thread). So then IW\nwould have a private volatile boolean recording whether any active\nDWPTs have flushPending.\n{quote}\nI was unsure about the name too so I just made it consistent with MergePolicy. Visit is ok I think.\nthe return value is maybe a relict from earlier version where I haven't had the DocWriterSession#hasPendingFlushes() yeah I think we can make that void and simply check if there are any. I think I do that today already ",
            "date": "2011-03-08T15:57:28.667+0000",
            "id": 33
        },
        {
            "author": "Michael Busch",
            "body": "bq. Awesome speedup!!\n\nYAY! Glad the branch is actually faster :)\n\nThanks for helping out with this patch, Simon. I'll try to look at the patch soon. My last week was super busy.",
            "date": "2011-03-08T18:02:18.484+0000",
            "id": 34
        },
        {
            "author": "Michael McCandless",
            "body": "bq. so you mean we always flush ALL DWPT once we reached the low watermark? \n\nNo, I mean: as soon as we pass low wm, pick biggest DWPT and flush it.\nAs soon as you mark that DWPT as flushPending, its RAM used is removed\nfrom active pool and added to flushPending pool.\n\nThen, if the active pool again crosses low wm, pick the biggest and\nmark as flush pending, etc.\n\nBut if the flushing cannot keep up, and the sum of active +\nflushPending pools crosses high wm, you hijack (stall) incoming\nthreads.\n\nI think this may make a good \"flush by RAM\" policy, but I agree we should\ntest.  I think the fully tiered approach may be overly complex...\n\n\nbq. for now this is internal only so even if we decide to I would shift that to a different issue.\n\nOK sounds good.\n\nAlso, if the app really cares about this (I suspect none will) they\ncould make a custom FlushPolicy that they could directly query to find\nout when threads get stalled.\n\nBesides this, is it only getting flushing of deletes working\ncorrectly that remains, before landing RT?\n",
            "date": "2011-03-08T20:27:15.069+0000",
            "id": 35
        },
        {
            "author": "Simon Willnauer",
            "body": "{quote}\nI think this may make a good \"flush by RAM\" policy, but I agree we should\ntest. I think the fully tiered approach may be overly complex...\n{quote}\n\nyeah possibly, I think simplifying this is easy now though...\n\n{quote}\nAlso, if the app really cares about this (I suspect none will) they\ncould make a custom FlushPolicy that they could directly query to find\nout when threads get stalled.\n{quote}\n\nyeah I think we don't need to expose that through IW.\n\n{quote}\nBesides this, is it only getting flushing of deletes working\ncorrectly that remains, before landing RT?\n{quote}\n\nwe need to fix LUCENE-2881 first too.\n\n",
            "date": "2011-03-08T20:35:13.319+0000",
            "id": 36
        },
        {
            "author": "Michael Busch",
            "body": "bq. we need to fix LUCENE-2881 first too.\n\nYeah, I haven't merged with trunk since we rolled back 2881, so we should fix it first, catch up with trunk, and then make deletes work.  I might have a bit time tonight to work on 2881.",
            "date": "2011-03-08T23:49:42.822+0000",
            "id": 37
        },
        {
            "author": "Simon Willnauer",
            "body": "next iteration on this patch. I changed some naming issues and separated a ByRAMFlushPolicy as an abstract base class. This patch contains the original MultiTierFlushPolicy and a SingleTierFlushPolicy that only has a low and a high watermark. This policy tries to flush the biggest DWPT once LW is crossed and flushed all DWPT once HW is crossed. \n\nThis patch also adds a \"flush if stalled\" control that hijacks indexing threads if the DW is stalled and there are still pending flushes. If so the incoming thread tries to check out a pending DWPT and flushes it before it adds the actual document.\n\nI didn't benchmark the more complex MultiTierFP vs. SingleTierFP yet. I hope I get to this soon.",
            "date": "2011-03-14T15:02:36.417+0000",
            "id": 38
        },
        {
            "author": "Michael McCandless",
            "body": "I still see a healtiness (mis-spelled) in DW.\n\nI'd rather not have the stalling/healthiness be baked into the API, at\nall.  Can we put the hijack logic entirely private in the flush-by-ram\npolicies?  (Ie remove isStalled()/hijackThreadsForFlush()).\n\nInstead of\n\n{noformat}\n+    synchronized (docWriter.docWriterSession) {\n+      netBytes = docWriter.docWriterSession.netBytes();\n+    }\n{noformat}\n\n, shouldn't we just make that method sync'd?\n\n\nBe careful defaulting TermsHash.trackAllocations to true -- eg term\nvectors wants this to be false.\n\nCan we move FlushSpecification out of FlushPolicy?  Ie, it's a private\nimpl detail of DW right?  (Not part of FlushPolicy's API).  Actually\nwhy do we need it?  Can't we just return the DWPT?\n\nWhy do we have a separate DocWriterSession?  Can't this be absorbed\ninto DocWriter?\n\nInstead of FlushPolicy.message, can't the policy call DW.message?\n\nOn the by-RAM flush policies... when you hit the high water mark, we\nshould 1) flush all DWPTs and 2) stall any other threads.\n\nWhy do we dereference the DWPTs with their ord?  EG, can't we just\nstore their \"state\" (active or flushPending) on the DWPT instead of in\na separate states[]?\n\nDo we really need FlushState.Aborted?  And if not... do we really need\nFlushState (since it just becomes 2 states, ie, Active or Flushing,\nwhich I think is then redundant w/ flushPending boolean?).\n\nI think the default low water should be 1X of your RAM buffer?  And\nhigh water maybe 2X?  (For both flush-by-RAM policies).\n",
            "date": "2011-03-14T19:06:29.950+0000",
            "id": 39
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. I still see a healtiness (mis-spelled) in DW.\nugh. I will fix\n{quote}\nI'd rather not have the stalling/healthiness be baked into the API, at\nall. Can we put the hijack logic entirely private in the flush-by-ram\npolicies? (Ie remove isStalled()/hijackThreadsForFlush()).\n{quote}\n\nI agree for the hijack part but the isStalled is something I might want to control. I mean we can still open it up eventually so rather make it private for now but keep a not on in. \n\n{quote}\nCan we move FlushSpecification out of FlushPolicy? Ie, it's a private\nimpl detail of DW right? (Not part of FlushPolicy's API). Actually\nwhy do we need it? Can't we just return the DWPT?\n{quote}\n\nit currently holds the ram usage for that DWPT when it was checked out so that I can reduce the flushBytes accordingly. We can maybe get rid of it entirely but I don't want to rely on the DWPT bytesUsed() though.\nWe can certainly move it out - this inner class is a relict though.\n\nbq. Why do we have a separate DocWriterSession? Can't this be absorbed\ninto DocWriter?\n\nI generally don't like cluttering DocWriter and let it grow like IW. DocWriterSession might not be the ideal name for this class but its really a ram tracker for this DW. Yet, we can move out some parts that do not directly relate to mem tracking. Maybe DocWriterBytes?\n\nbq. Be careful defaulting TermsHash.trackAllocations to true \u2013 eg term\nvectors wants this to be false.\n\nI need to go through the IndexingChain and check carefully where to track memory anyway. I haven't got to that yet but good that you mention it that one could easily get lost.\n\n\n\n\n\nbq. Instead of FlushPolicy.message, can't the policy call DW.message?\nI don't want to couple that API to DW. What would be the benefit beside from saving a single method?\n{quote}\nOn the by-RAM flush policies... when you hit the high water mark, we\u2028should 1) flush all DWPTs and 2) stall any other threads.\n{quote}\nWell I am not sure if we should do that. I don't really see why we should forcefully stop the world here. Incoming threads will pick up a flush immediately and if we have enough resources to index further why should we wait until all DWPT are flushed. if we stall I fear that we could queue up threads that could help flushing while stalling would simply stop them doing anything, right? You can still control this with the healthiness though. We currently do flush all DWPT btw. once we hit the HW. \n\n{quote}\nWhy do we dereference the DWPTs with their ord? EG, can't we just\u2028store their \"state\" (active or flushPending) on the DWPT instead of in\u2028a separate states[]?\n{quote}\nThat is definitely an option. I will give that a go.\n{quote}\nDo we really need FlushState.Aborted? And if not... do we really need\u2028FlushState (since it just becomes 2 states, ie, Active or Flushing,\u2028which I think is then redundant w/ flushPending boolean?).\n{quote}\nthis needs some more refactoring I will attach another iteration\n{quote}\nI think the default low water should be 1X of your RAM buffer? And\u2028high water maybe 2X? (For both flush-by-RAM policies).\n{quote}\nhmm, I think we need to revise the maxRAMBufferMB Javadoc anyway so we have all the freedom to do whatever we want. yet, I think we should try to keep the RAM consumption similar to what it would have used in a previous release. So if we say HW is 2x then suddenly some apps might run out of memory. I am not sure if we should do that or rather stick to the 90% to 110% for now.  We need to find good defaults for this anyway.\n",
            "date": "2011-03-15T10:48:32.240+0000",
            "id": 40
        },
        {
            "author": "Michael McCandless",
            "body": "bq. it currently holds the ram usage for that DWPT when it was checked out so that I can reduce the flushBytes accordingly. We can maybe get rid of it entirely but I don't want to rely on the DWPT bytesUsed() though.\n\nHmm, but, once a DWPT is pulled from production, its bytesUsed()\nshould not be changing anymore?  Like why can't we use it to hold its\nbytesUsed?\n\nbq. I generally don't like cluttering DocWriter and let it grow like IW. DocWriterSession might not be the ideal name for this class but its really a ram tracker for this DW. Yet, we can move out some parts that do not directly relate to mem tracking. Maybe DocWriterBytes?\n\nWell DocWriter is quite small now :) (On RT branch).  And adding\nanother class means we have to be careful about proper sync'ing (lock\norder, to avoid deadlock)... and I think it should get smaller if we\ncan remove state[] array, FlushState enum, etc. but, OK I guess we can\nleave it as separate for now.  How about DocumentsWriterRAMUsage?\nRAMTracker?\n\n{quote}\nbq. Instead of FlushPolicy.message, can't the policy call DW.message?\n\nI don't want to couple that API to DW. What would be the benefit beside from saving a single method?\n{quote}\n\nHmm, good point.  Though, it already has a SetOnce<DocumentsWriter> --\nhow come?  Can the policy call IW.message?  I just think FlushPolicy\nought to be very lean, ie show you exactly what you need to\nimplement...\n\n{quote}\nbq. On the by-RAM flush policies... when you hit the high water mark, we\u2028should 1) flush all DWPTs and 2) stall any other threads.\n\nWell I am not sure if we should do that. I don't really see why we should forcefully stop the world here. Incoming threads will pick up a flush immediately and if we have enough resources to index further why should we wait until all DWPT are flushed. if we stall I fear that we could queue up threads that could help flushing while stalling would simply stop them doing anything, right? You can still control this with the healthiness though. We currently do flush all DWPT btw. once we hit the HW.\n{quote}\n\nAs long as we default the high mark to something \"generous\" (2X low\nmark), I think this approach should work well.\n\nIe, we \"begin\" flushing as soon as low mark is crossed on active RAM.\nWe pick the biggest DWPT and take it of rotation, and immediately\ndeduct its RAM usage from the active pool.  If, while we are still\nflushing, active RAM again grows above the low mark, then we pull\nanother DWPT, etc.  But then if ever the total flushing + active\nexceeds the high mark, we stall.\n\nBTW why do we track flushPending RAM vs flushing RAM?  Is that\ndistinction necessary?  (Can't we just track \"flushing\" RAM?).\n",
            "date": "2011-03-15T16:42:15.482+0000",
            "id": 41
        },
        {
            "author": "Simon Willnauer",
            "body": "next iteration containing a large number of refactorings.\n\n* I moved all responsibilities related to flushing including synchronization into the DocsWriterSession and renamed it to DocumentsWriterFlushControl.\n\n* DWFC now only tracks active and flush bytes since the relict from my initial patch where pending memory was tracked is not needed anymore. \n\n* DWFC took over all synchronization so there is not synchronized (flushControl) {...} in DocumentsWriter anymore. Seem way cleaner too though.\n\n* Healthiness now blocks once we reach 2x maxMemory and SingleTierFlushPolicy uses 0.9 maxRam as low watermark and 2x low watermark as its HW to flush all threads. The multi tier one is still unchanged and flushes in linear steps from 0.9 to 1.10 x maxRam. We should actually test if this does better worse than the single tier FP.\n\n* FlushPolicy now has only a visit method and uses the IW.message to write to info stream.\n\n* ThreadState now holds a boolean flag that indicates if a flush is pending which is synced and written by DWFC. States[] is gone in DWFC.\n\n* FlushSpecification is gone and DWFC returns DWPT upon checkoutForFlush. Yet, I still track the mem for the flushing DWPT seperatly since the DWPT#bytesUsed() changes during flush and I don't want to rely on that this doesn't change. As a nice side-effect I can check if a checked out DWPT is passed to doAfterFlush and assert on that.\n\nnext steps here are benchmarking and getting good defaults for the flush policies. I think we are close though.",
            "date": "2011-03-18T15:36:40.825+0000",
            "id": 42
        },
        {
            "author": "Michael McCandless",
            "body": "  * I think once we sync up to trunk again, the FP should hold the\n    IW's config instance, and pull settings \"live\" from it?  Ie this\n    way we keep our live changes to flush-by-RAM.  Also, Healthiness\n    (it won't get updates to RAM buffer now).\n\n  * Should we rename *ByRAMFP --> *ByRAMOrDocCountFP?  Since it \"ors\"\n    docCount and RAM usage trigger right?  Oh, I see, not quite -- it\n    requires RAM buffer be set.  I think we should relax that?  Ie a\n    single flush policy (the default) flushes by either/or?\n\n  * Shouldn't these flush policies also trigger by\n    maxBufferedDelCount?\n\n  * Maybe FP.init should throw IllegalStateExc not IllegalArgExc?\n    (Because, no arg is allowed once the \"state\" of FP has already\n    been init'ed).\n\n  * Probably FP.writer should be a SetOnce?\n\n  * Hmm we still have a FlushPolicy.message?  Can't we just make IW\n    protected and then FlushPolicy impl can call IW.message?  (And\n    also remove FP.setInfoStream).\n\n  * Is IW.FlushControl not really used anymore?  We should remove it?\n\n  * I still think LW should be 1.0 of your RAM buffer.  Ie, IW will\n    start flushing once that much RAM is in use.\n\n  * I still see \"synchronized (docWriter.flushControl) {\" in\n    IndexWriter\n\n  * We should jdoc that IWC.setFlushPolicy takes effect only on init\n    of IW?\n\n  * Add \"for testing only\" comment to IW.getDocsWriter?\n\n  * I wonder whether we should convey \"what changed\" to the FP?  EG,\n    we can 1) buffer a new del term, 2) add a new doc, 3) both\n    (updateDocument).  It could be we have onUpdate, onAdd, onDelete?\n    Or maybe we keep single method but rename to onChange?  Ie, it's\n    called because *something* about the incoming DWPT has changed.\n\n  * The flush policy shouldn't have to compute \"delta\" RAM like it\n    does now?  Actually why can't it just call\n    flushControl.activeBytes(), and we ensure the delta was already\n    folded into that?  Ie we'd call commmitPerThreadBytes before\n    FP.visit.  (Then commitPerThreadBytes wouldn't ever add to\n    flushBytes, which is sort of spooky -- like flushBytes should get\n    incr'd only when we pull a DWPT out for flushing).\n\n  * I don't think we should ever markAllWritersPending, ie, that's\n    not the right \"reaction\" when flushing is too slow (eg you're on a\n    slow hard drive) since over time this will result in flushing lots\n    of tiny segments unnecessarily.  A better reaction is to stall the\n    incoming threads; this way the flusher threads catch up, and once\n    you resume, then the small DPWTs have a chance to get big before\n    they are flushed.\n\n  * Misspelled: markLargesWriterPending -> markLargestWriterPending\n",
            "date": "2011-03-19T14:29:09.956+0000",
            "id": 43
        },
        {
            "author": "Simon Willnauer",
            "body": "here is my current state on this issue. I did't add all JDocs needed (by far) and I will wait until we settled on the API for FlushPolicy.\n\n* I removed the complex TieredFlushPolicy entirely and added one DefaultFlushPolicy that flushes at IWC.getRAMBufferSizeMB() / sets biggest DWPT pending.\n* DW will stall threads if we reach 2 x maxNetRam which is retrieved from FlushPolicy so folks can lower that depending on their env.\n\n* DWFlushControl checks if a single DWPT grows too large and sets it forcefully pending once its ram consumption is > 1.9 GB. That should be enough buffer to not reach the 2048MB limit. We should consider making this configurable.\n\n* FlushPolicy has now three methods onInsert, onUpdate and onDelete while DefaultFlushPolicy only implements onInsert and onDelete, the Abstract base class just calls those on an update.\n\n* I removed FlushControl from IW\n* added documentation on IWC for FlushPolicy and removed the jdocs for the RAM limit. I think we should add some lines about how RAM is now used and that users should balance the RAM with the number of threads they are using. Will do that later on though.\n\n* For testing I added a ThrottledIndexOutput that makes flushing slow so I can test if we are stalled and / or blocked. This is passed to MockDirectoryWrapper. Its currently under util but it rather should go under store, no?\n\n* byte consumption is now committed before FlushPolicy is called since we don't have the multitier flush which required that to reliably proceed across tier boundaries (not required but it was easier to test really). So FP doesn't need to take care of the delta\n\n* FlushPolicy now also flushes on maxBufferedDeleteTerms while the buffered delete terms is not yet connected to the DW#getNumBufferedDeleteTerms() which causes some failures though. I added //nocommit & @Ignore to those tests.\n\n* this patch also contains a @Ignore on TestPersistentSnapshotDeletionPolicy which I couldn't figure out why it is failing but it could be due to an old version of LUCENE-2881 on this branch. I will see if it still fails once we merged.\n\n* Healthiness now doesn't stall if we are not flushing on RAM consumption to ensure we don't lock in threads. \n\n\nover all this seems much closer now. I will start writing jdocs. Flush on buffered delete terms might need some tests and I should also write a more reliable test for Healthiness... current it relies on that the ThrottledIndexOutput is slowing down indexing enough to block which might not be true all the time. It didn't fail yet. \n\n",
            "date": "2011-03-23T15:28:18.826+0000",
            "id": 44
        },
        {
            "author": "Michael McCandless",
            "body": "Patch is looking better!  I love how simple DefaultFP is now :)\n\n  * 1900 * 1024 * 1024 is actually 1.86 GB; maybe just change comment\n    to 1900 MB?  Or we could really make the limit 1.9 GB (= 1945.6\n    MB)\n\n  * I think we should make the 1.9 GB changeable\n    (setRAMPerThreadHardLimitMB?)\n\n  * How come we lost 'assert !bufferedDeletesStream.any();' in\n    IndexWriter.java?\n\n  * Why default trackAllocations to true when ctor always sets it (in\n    TermsHash.java)?\n\n  * Can we not simply not invoke the FP if flushPending is already set\n    for the given DWPT?  (So that every FP need not check that).\n\n  * In DefaultFP.onDelete -- we shouldn't just return if numDocsInRAM\n    is 0?  Ie an app could open IW, delete 2M terms, close, and we\n    need to flush several times due to RAM usage or del term count...\n\n  * Maybe rename DefaultFP --> FlushByRAMOrCounts?\n\n  * Won't the new do/while loop added to ThreadAffinityDWThreadPool\n    run hot, if minThreadState is constantly null...?  (Separately\n    that source needs a header)\n\n  * I love the ThrottledIndexOutput!\n\n  * For the InterruptedException in ThrottledIndexOutput.sleep, we\n    should rethrow w/ oal.util.ThreadInterruptedException (best\n    practice... probably doesn't really matter here)\n\n  * We should fix DefaultFlushPolicy to first pull the relevant config\n    from IWC (eg maxBufferedDocs), then check if that config is -1 or\n    not, etc., because IWC's config can be changed at any time (live)\n    so we may read eg 10000 at first and then -1 the second time.\n\nMaybe, for stalling, instead of triggering by max RAM, we can take\nthis simple approach: if the number of flushing DWPTs ever exceeds one\nplus number of active DWPTs, then we stall (and resume once it's below\nagain).\n\nThis approach would then work for flush-by-docCount policies too, and\nwould still roughly equate to up to 2X RAM usage for flush-by.\n\nIt's really odd that TestPersistentSDP fails now... this should be\nunrelated to the (admittedly, major) changes we're making here...\n\nHmm.... deletes are actually tricky, because somehow the FlushPolicy\nneeds access to the \"global\" deletes count (and also the to per-DWPT\ndeletes count).  If a given DWPT has 0 buffered docs, then indeed the\nbuffered deletes in its pool doesn't matter.  But, we do need to respect\nthe buffered deletes in the global pool...",
            "date": "2011-03-25T18:27:59.269+0000",
            "id": 45
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. How come we lost 'assert !bufferedDeletesStream.any();' inIndexWriter.java?\nso this is tricky here. Since we are flushing concurrently this could false fail. The same assertion is in bufferedDeletesStream.prune(segmentInfos); which is synced. But another thread could sneak in between the prune and the any() check updating / deleting a document this could false fail. Or do I miss something here?\n\nbq. Maybe, for stalling, instead of triggering by max RAM, we can take\nthis simple approach: if the number of flushing DWPTs ever exceeds one\nplus number of active DWPTs, then we stall (and resume once it's below\nagain).\n\nAwesome idea mike! I will do that!\n\n{quote}\nWe should fix DefaultFlushPolicy to first pull the relevant config\nfrom IWC (eg maxBufferedDocs), then check if that config is -1 or\nnot, etc., because IWC's config can be changed at any time (live)\nso we may read eg 10000 at first and then -1 the second time.\n{quote}\n\nWhat you mean is we should check if we flush by Ram, DocCount etc. and only if so we check the live values for Ram, DocCount etc.?\n\n{quote}\nHmm.... deletes are actually tricky, because somehow the FlushPolicy\nneeds access to the \"global\" deletes count (and also the to per-DWPT\ndeletes count). If a given DWPT has 0 buffered docs, then indeed the\nbuffered deletes in its pool doesn't matter. But, we do need to respect\nthe buffered deletes in the global pool...\n{quote}\nI think it does not make sense to check both the global count and the DWPT count against the same value. If we have a DWPT that exceeds it we also exceed globally or could it happen that a DWPT has more deletes than the global pool? Further if we observe the global pool and we exceed the limit do we flush all as written on the IWC documentation?\n\nonce we sort this out I upload a new patch with javadoc etc for flush policy. we seem to be close here man! \n",
            "date": "2011-03-28T14:11:34.956+0000",
            "id": 46
        },
        {
            "author": "Simon Willnauer",
            "body": "next iterations.\n* added JavaDoc to all classes and interfaces.\n* fixed the possible hot loop in DWPTThreadPool\n* changed stalling logic to block if more flushing DWPT are around than active ones.\n* check IWC setting on init and listen to live changes for those who have not been disabled.\n* made the hard per thread RAM limit configurable and added DEFAULT_RAM_PER_THREAD_HARD_LIMIT set to 1945MB\n\n* renamed  DefaultFP --> FlushByRAMOrCounts \n* added setFlushDeletes to DWFlushControl that is checked each time we add a delete and if set we flush the global deletes.\n\nthis seems somewhat close though. its time to benchmark it again.\n\n\n\n\n",
            "date": "2011-03-29T14:46:29.097+0000",
            "id": 47
        },
        {
            "author": "Simon Willnauer",
            "body": "Today I merged with trunk and updated my patch. I fixed the testcases that had an @Ignore on them and run tests. 1 out of 5 tests fails on TestStressIndexing and TestNRTThreads which is due to the update issues which should be addressed in LUCENE-2956. All other tests pass including all RAM / NumDoc / BufferedDeleteTerms related tests. As a consequence I committed the current state of this issue to the RT branch too. I will keep this issue open for now.",
            "date": "2011-03-30T13:36:27.705+0000",
            "id": 48
        },
        {
            "author": "Michael Busch",
            "body": "Thanks Simon!\nI'll work on LUCENE-2956 next.",
            "date": "2011-03-30T17:53:53.737+0000",
            "id": 49
        },
        {
            "author": "Simon Willnauer",
            "body": "I run a couple of benchmarks with interesting results the graph below show documents per second for the RT branch with DWPT yielding a very good IO/CPU utilization and overall throughput is much better than trunks.\n!http://people.apache.org/~simonw/DocumentsWriterPerThread_dps.png! \nYet, when we look at trunk the peak performance is much better on trunk than on DWPT. The reason for that I think is that we flush concurrently which takes at most one thread out of the loop, those are the little drops in docs/sec. This does not yet explain the reason for the constantly lower max indexing rate, I suspect that this is at least influenced due to the fact that flushing is very very CPU intensive. At the same time CMS might kick in way more often since we are writing more segments which are also smaller compared to trunk. Eventually, I need to run a profiler and see what is going on.\n!http://people.apache.org/~simonw/Trunk_dps.png! \n\nInteresting is that beside the nice CPU utilization we also have an nearly perfect IO utilization. The graph below shows that we are consistently using IO to flush segments. the width of the bars show the time it took to flush a single DWPT, there is almost no overlap.\n!http://people.apache.org/~simonw/DocumentsWriterPerThread_flush.png! \n\nOverall those are super results! Good job everybody!\n\nsimon",
            "date": "2011-03-31T13:42:55.321+0000",
            "id": 50
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. influenced due to the fact that flushing is very very CPU intensive\n\nDo you think this is due mostly to the vint decoding?  We're not interleaving postings on flush with this patch so the CPU consumption should be somewhat lower.\n\nbq. At the same time CMS might kick in way more often since we are writing more segments which are also smaller compared to trunk\n\nThis's probably the more likely case.  In general, we may be able to default to a higher overall RAM buffer size, and perhaps there won't be degradation in indexing performance like there is with trunk?  In the future with RT we could get fancy and selectively merge segments as we're flushing, if writing larger segments is important.  \n\nI'd personally prefer to write out 1-2 GB segments, and limit the number of DWPTs to 2-3, mainly for servers that are concurrently indexing and searching (eg, the RT use case).  I think the current default number of thread states is a bit high.  ",
            "date": "2011-03-31T15:26:46.628+0000",
            "id": 51
        },
        {
            "author": "Michael Busch",
            "body": "Thanks, Simon, for running the benchmarks! Good results overall, even though it's puzzling why flushing would be CPU intensive.\n\nWe should probably do some profiling to figure out where the time is spent. I can probably do that Sunday, but feel free to beat me :)",
            "date": "2011-03-31T16:07:54.810+0000",
            "id": 52
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. Thanks, Simon, for running the benchmarks! Good results overall, even though it's puzzling why flushing would be CPU intensive.\nwell during flush we are encoding lots of VInts thats making it cpu intensive.\n\nI actually run the benchmark through a profiler and found out what the problem was with my benchmarks.\nWhen I indexed with DWPT my HDD was soo busy flushing segments concurrently that the read performance suffered and my indexing threads blocked on the line doc file where I read the records from. This explains the large amounts of spikes towards 0 doc/sec. The profiler also showed that we are waiting on ThreadState#lock() constantly with at least 3 threads. I changed the current behavior of the threadpool to not clear the thread bindings when I replace a DWPT for flushing an voila! we have comparable peak ingest rate. \n\n!http://people.apache.org/~simonw/DocumentsWriterPerThread_dps_01.png! \n\nNote the difference DWPT indexes the documents in 6 min 15 seconds!\n\n!http://people.apache.org/~simonw/Trunk_dps_01.png! \n\nHere we have 13 min 40 seconds! NICE!\n\n!http://people.apache.org/~simonw/DocumentsWriterPerThread_flush_01.png! \n",
            "date": "2011-04-01T12:35:47.045+0000",
            "id": 53
        },
        {
            "author": "Michael Busch",
            "body": "Awesome speedup! Finally all this work shows great results!!\n\nWhat's surprising is that the merge time is lower with DWPT. How can that be, considering we're doing more merges?",
            "date": "2011-04-01T16:32:19.246+0000",
            "id": 54
        },
        {
            "author": "Simon Willnauer",
            "body": "this is committed to branch reviews should go through LUCENE-3023",
            "date": "2011-04-14T12:19:57.993+0000",
            "id": 55
        }
    ],
    "component": "core/index",
    "description": "Now that we have DocumentsWriterPerThreads we need to track total consumed RAM across all DWPTs.\n\nA flushing strategy idea that was discussed in LUCENE-2324 was to use a tiered approach:  \n- Flush the first DWPT at a low water mark (e.g. at 90% of allowed RAM)\n- Flush all DWPTs at a high water mark (e.g. at 110%)\n- Use linear steps in between high and low watermark:  E.g. when 5 DWPTs are used, flush at 90%, 95%, 100%, 105% and 110%.\n\nShould we allow the user to configure the low and high water mark values explicitly using total values (e.g. low water mark at 120MB, high water mark at 140MB)?  Or shall we keep for simplicity the single setRAMBufferSizeMB() config method and use something like 90% and 110% for the water marks?",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2573",
    "issuetypeClassified": "IMPROVEMENT",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Tiered flushing of DWPTs by RAM with low/high water marks",
    "systemSpecification": true,
    "version": ""
}