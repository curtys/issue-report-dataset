{
    "comments": [
        {
            "author": "Robert Muir",
            "body": "reusableTokenStream + tests for the contrib analyzers. only a few are non-final and require the back compat code.",
            "date": "2009-08-10T11:51:35.001+0000",
            "id": 0
        },
        {
            "author": "Robert Muir",
            "body": "no code changes, but improve the reusableTokenStream tests for cn and smartcn to also test offsets, testing that reset() is working correctly.\n",
            "date": "2009-08-10T13:03:34.211+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "I moved this to 2.9, if there are concerns please feel free to change this, but I think it is ready.",
            "date": "2009-08-10T13:14:07.053+0000",
            "id": 2
        },
        {
            "author": "Shai Erera",
            "body": "Robert - wouldn't it make sense to pull SavedStreams (maybe call it ReusableStreams?) up to Analyzer, and have all the extensions use it? I couldn't help but notice that this code is duplicated in all the Analyzers.\n\nAlso, and I don't know if it's a matter for a different issue - the fact that reusableTokenStream accepts a field name is misleading. On one hand, it makes you think you can ask for a.rts(\"a) and a.rts(\"b\") safely, but on the other it is documented to be not that safe (i.e., don't call this method if you need more than one token stream from an analyzer at the same time).\n\nI don't know how to solve it best - I'd like to have a tokenStream method that accepts the field name, and that I can get a reused token stream, for that field name. But I also would like to have a method that I can call \"get a reusable token stream\" and \"I don't care which field it is\". So maybe have two variants:\n# reusableTokenStream(Reader reader)\n# reusableTokenStream(String field, Reader reader)\nThis is kind of related to LUCENE-1678, as I think we'd like tokenStream to return a reused one, but maybe having a tokenStream which always returns a new one, and a reusableTokenStream (w/o a field) which reuses a stream (maybe the 'default' stream), would be good.\n\nWhat do you think?",
            "date": "2009-08-10T13:49:39.778+0000",
            "id": 3
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nRobert - wouldn't it make sense to pull SavedStreams (maybe call it ReusableStreams?) up to Analyzer, and have all the extensions use it? I couldn't help but notice that this code is duplicated in all the Analyzers. \n{quote}\n\nShai, it would be great if somehow this could be factored. its not complete duplication: different things need to happen here: for example Thai and Smart Chinese have filters that keep state and require a reset(). But i don't know, seems like it could be factored into Analyzer and reset() called on both tokenizer and filters... \n\nI am trying to imagine a situation where refactoring this kind of thing would prevent some flexibility, but i think if tokenstreams keep state in some wierd way they should implement reset() for this purpose.\n\n{quote}\nAlso, and I don't know if it's a matter for a different issue - the fact that reusableTokenStream accepts a field name is misleading.\n{quote}\n\nprobably to support PerFieldAnalyzerWrapper is my first thought. how would PerFieldAnalyzerWrapper work correctly if field is not supplied???",
            "date": "2009-08-10T14:09:54.659+0000",
            "id": 4
        },
        {
            "author": "Uwe Schindler",
            "body": "{quote}\ndifferent things need to happen here: for example Thai and Smart Chinese have filters that keep state and require a reset(). But i don't know, seems like it could be factored into Analyzer and reset() called on both tokenizer and filters... \n\nI am trying to imagine a situation where refactoring this kind of thing would prevent some flexibility, but i think if tokenstreams keep state in some wierd way they should implement reset() for this purpose.\n{quote}\n\nreset() is always called by IndexWriter before consuming the TokenStream; end() is called as last operation on the TokenStream.\n\nAnd each TokenFilter should for sure pass the call also to the input TokenStream... The default impl does this.",
            "date": "2009-08-10T14:16:01.007+0000",
            "id": 5
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nreset() is always called by IndexWriter before consuming the TokenStream; end() is called as last operation on the TokenStream.\n{quote}\n\nUwe, this may be the case for IndexWriter, but if I do not explicitly call it like so in ThaiAnalyzer, then ThaiWordFilter's reset() is not invoked:\n{noformat}\n  streams.source.reset(reader);\n  streams.result.reset(); // reset the ThaiWordFilter's state <-- right here\n{noformat}\n\nBy calling reset I can ensure it happens regardless of what is consuming the tokenstream... (such as my tests!) maybe this is overkill?\n",
            "date": "2009-08-10T14:21:58.477+0000",
            "id": 6
        },
        {
            "author": "Shai Erera",
            "body": "I actually meant to pull the class SavedStreams up to Analyzer, and leave the rest of the logic in each Analyzer impl (I don't think there can be a default impl for all). SavedStreams have a Tokenizer and TokenFilter, which is what every reusable token stream needs.\n\nAs for PerFieldAnalyzerWrapper --> it will still have a reusableTS(field) version to call. But when you call this method, you guarantee you reuse a TS for that field only.\n\nBut if IW needs to call this method for every field it parses, then it can only call reusableTokenStream(field), and therefore I wonder how that optimization work ...",
            "date": "2009-08-10T15:50:46.280+0000",
            "id": 7
        },
        {
            "author": "Robert Muir",
            "body": "Shai: I see what you are saying wrt SavedStreams... \nbut in my opinion the biggest problem with the current setup is that it feels fragile... forcing me to create separate tests for reusableTS() to ensure its doing what TS() does.\n\nIf you can think of a better way to implement this patch, i'd love to hear it because I'm not thrilled with what I had to do either, I just didnt see a better way.\nbut really its implemented the same way the core analyzers (Standard/Simple/Stop) are done.\n",
            "date": "2009-08-10T16:41:50.200+0000",
            "id": 8
        },
        {
            "author": "Robert Muir",
            "body": "I am thinking of expanding this patch to include reset() impls for state-keeping tokenizers/filters that do not currently have an analyzer...\nnot really part of this issue but it presents an issue for someone trying to create a custom analyzer (with reusableTS) based on these components.",
            "date": "2009-08-10T22:39:58.631+0000",
            "id": 9
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. I am thinking of expanding this patch to include reset() impls for state-keeping tokenizers/filters that do not currently have an analyzer...\n\n+1\n\nIn the past we've always encouraged people to create their own analyzers by plugging together the provided filters - we should keep this simple to do.\n\nNow the problem:  TokenStream.reset() has different semantics than what we are using it for here.  CachingTokenFilter uses it to start the replay of the last string of tokens it saw (definitely bad for reuse).\n\nSo.... do we redefine reset()?  Something like CachingTokenFilter is very special case, and I don't feel like it should have it's own method.  There are other ways it could be implemented now anyway.\n ",
            "date": "2009-08-11T10:59:11.194+0000",
            "id": 10
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nNow the problem: TokenStream.reset() has different semantics than what we are using it for here. CachingTokenFilter uses it to start the replay of the last string of tokens it saw (definitely bad for reuse).\n{quote}\n\nHmm, I have not looked at CachingTokenFilter, sounds like an issue there.\nIn this case I was referring to state-keeping tokenizers/filters without reset() impls in contrib: Ngram, shingles, things like that.\nI looked at the core tokenstreams and most of those seemed to properly support it... I guess we have one exception\n",
            "date": "2009-08-11T11:08:17.082+0000",
            "id": 11
        },
        {
            "author": "Yonik Seeley",
            "body": "For something like CachingTokenFilter to work, the implementation of all other TokenFilters.reset() is still what we want - \"get rid of your state because we are starting over\".  So it's really only the caching-type filters that have a semantic clash... (i.e. does reset() mean replay what I've seen before, or does reset mean start over with new input) and perhaps we can start off by saying \"don't use these in conjunction with reusable token streams\".\n",
            "date": "2009-08-11T11:29:22.957+0000",
            "id": 12
        },
        {
            "author": "Robert Muir",
            "body": "yonik, I see your point. \n\n\"get rid of your state because we are starting over\" just happens to equal \"Resets this stream to the beginning.\" 90% of the time,\nbut sometimes these things are different...\n\nI would really like to see it easier to have reusableStreams in the future, both for a person writing a custom analyzer and in an \"automatic\" case like TokenizerChain\n",
            "date": "2009-08-11T11:36:57.306+0000",
            "id": 13
        },
        {
            "author": "Robert Muir",
            "body": "add reset() impls for ngram/* and compound/*. These still need tests, and still a few more to be done.",
            "date": "2009-08-11T12:01:31.796+0000",
            "id": 14
        },
        {
            "author": "DM Smith",
            "body": "If CachingTokenFilter.reset() means rewind, then how does one reset it?\n\nWithout that, it is not reusable.\n\nWhen I did the prior token/filter changes, IIRC, some were not reusable. Maybe reset needs to be documented that for reuse, the common case, it means reset (or no-op) and otherwise means rewind. And then document which classes are not reusable.\n\nOr should these single use classes be made reusable? This would argue for a rewind() method. IMHO it is a bug that reset does not follow the contract.  ",
            "date": "2009-08-11T12:34:13.680+0000",
            "id": 15
        },
        {
            "author": "Mark Miller",
            "body": "bq. This would argue for a rewind() method.\n\nIs that a crazy idea? It almost seems like we should have reset,  requiring an impl now, and add rewind, making it optional as reset used to be?\n\nBarring the back compat issues, isn't that how this should be setup?",
            "date": "2009-08-11T12:43:34.514+0000",
            "id": 16
        },
        {
            "author": "Robert Muir",
            "body": "personally I like this idea. CachingTokenFilter is not final though, so you are right back compat will need some thought, but it makes sense.",
            "date": "2009-08-11T12:53:40.631+0000",
            "id": 17
        },
        {
            "author": "Yonik Seeley",
            "body": "I don't think we need a rewind() at all on TokenStream - it's too special-case.",
            "date": "2009-08-11T13:02:28.061+0000",
            "id": 18
        },
        {
            "author": "Mark Miller",
            "body": "So what do you propose  ? You would just cast to CachingTokenFilter if you want rewind?\n\nI've never really been a fan of these methods that are optionally implemented ... and you must know the type if you know it can rewind.\n\nSo what about making reset required (and work the same across all TokenStreams) and adding rewind to CachingTokenFilter then?",
            "date": "2009-08-11T13:05:41.731+0000",
            "id": 19
        },
        {
            "author": "Mark Miller",
            "body": "Or even a Rewindable interface that can implemented if a TokenStream supports Rewind?\n\nThen, if you really needed it, you could use InstanceOf to check for support.",
            "date": "2009-08-11T13:21:10.877+0000",
            "id": 20
        },
        {
            "author": "Yonik Seeley",
            "body": "It depends on the use case for CachingTokenFilter.\nWhen it's used in places like QueryParser.getFieldQuery(), the consumer creates the CachingTokenFilter and can rewind it too.\n\nIf one has managed to use the same instance more than once in the same document, other tricks could be used such as resetting to the beginning after false is returned from incrementToken() or implementing rewind in end().  Seems like either would work.\n\nBut in reality, the concept of CachingTokenFilter isn't really compatible with the concept of reuse at all... so I don't think we necessarily need to do anything except document that it's not reusable.  Adding rewind() to TokenStream won't solve this semantic problem.\n",
            "date": "2009-08-11T13:23:32.978+0000",
            "id": 21
        },
        {
            "author": "Robert Muir",
            "body": "add reusable/reset impls for shingles, snowball, and memory/synonym.\nmemory/synonym had no previous tests afaik.\ntests are still needed for compound,ngram, and shingles reset()\nmemory/PatternAnalyzer still does not use reusableTS\nand there are two wrappers: shingle/ShingleAnalyzerWrapper and query/QueryAutoStopWordAnalyzer that should be fixed and tested.\n\nunfortunately something came up at work, so I may be slow on this, if you want to jump in, please help!\nand let me know what you are tackling, I will do my best to work this issue late night to get it resolved.\n",
            "date": "2009-08-12T23:35:24.795+0000",
            "id": 22
        },
        {
            "author": "Robert Muir",
            "body": "with ShingleAnalyzerWrapper and tests, plan to do QueryAutoStopWord the same way... off to work",
            "date": "2009-08-13T11:19:30.717+0000",
            "id": 23
        },
        {
            "author": "Shai Erera",
            "body": "Robert, what I meant about pulling SavedStreams up to Analyzer (few comments above) was to do something like this:\n{code}\nclass Analyzer {\n  protected static class Streams {\n    public Tokenizer tokenizer;\n    public TokenStream tokenStream;\n  }\n  ...\n}\n\nclass MyAnalyzer extends Analyzer {\n  public reusableTokenStream() {\n    Streams streams = getPrevTS();\n    if (streams == null) {\n      streams = new Streams();\n      streams.tokenizer = new Tokenizer();\n      streams.tokenStream = new TokenStream();\n      setPrevTS(streams);\n   } else {\n      streams.tokenizer.reset(reader);\n      streams.tokenStream.reset();\n   }\n   return streams.tokenStream;\n}\n{code}\n\nThis will just save the declaration of SavedStreams or Streams in all sub-classes. In addition we can do the following:\n# Define reset(String, Reader) on Streams, so that everyone just calls streams.reset(), instead of resetting tokenizer and tokenStream. Streams will do that internally.\n# Define a protected abstract getTokenizer() on Analyzer that all Analyzers implement. (due to back-compat, this can throw UOE - let's leave it for now).\n# Have Analyzer's reusableTokenStream look like the following:\n{code}\npublic TokenStream reusableTokenStream(String field, Reader reader) {\n  Streams streams = getPreviousTokenStream();\n  if (streams == null) {\n    streams = new Streams();\n    streams.tokenizer = getTokenizer(field, reader);\n    streams.tokenStream = tokenStream();\n    setPrevTS(streams);\n  } else {\n    streams.reset(field, reader);\n  }\n  return streams.tokenStream;\n}\n{code}\n\nAnd that can be even more simplified, by having Streams define a ctor which accepts Tokenizer and TokenStream. We can also instead of doing \"new Streams()\" call a method newStreams() so that sublcasses can override if they want to provide a different Streams impl. Not a must, and we might even consider the whole thing final (Streams, reusableTokenStream ? etc.)\n\nThat will save some code in that patch I believe. What do you think?\n\nI haven't touched the back-compat issues yet - let's discuss the idea first.",
            "date": "2009-08-13T21:18:29.380+0000",
            "id": 24
        },
        {
            "author": "Shai Erera",
            "body": "Also Robert, I've looked at the patch and I see that you don't call reset() on streams.result, just on source. I think you should call that on streams.result too. You do it in TestSynonymTokenFilter.\n\nIf we go w/ my proposal above, such issues will not happen, since there will be only one copy of reusableTokenStreams (at least for the majority of analyzers).",
            "date": "2009-08-13T21:22:44.566+0000",
            "id": 25
        },
        {
            "author": "Robert Muir",
            "body": "Shai, first of all let me address your first comment... for the saved streams thing.\n\none issue would be how to implement AnalyzerWrappers with that? with the existing functionality I am able to make this work. See my impl for ShingleAnalyzerWrapper for an example.\n\nyour second comment, I only call reset() on streams.result when there is a state-keeping TokenFilter on that chain. it is not necessary to invoke it if reset() is a no-op...\n",
            "date": "2009-08-13T21:34:19.177+0000",
            "id": 26
        },
        {
            "author": "Robert Muir",
            "body": "Shai thinking about this some more, as long as the behavior could be overridden for the extreme case\ni think it would make some sense to somehow have a 'default' implementation (what you are proposing does make sense)\nand it could be overridden in the strange cases if the thing is non-final.\n\nbut now my problem child is memory/PatternAnalyzer, its source of tokens is not a Tokenizer.\nSo the getTokenizer() method does not make sense in that case... (I guess it could throw UOE there but it starts to sound like we are dancing around the problem)",
            "date": "2009-08-13T21:47:52.434+0000",
            "id": 27
        },
        {
            "author": "Shai Erera",
            "body": "bq. I only call reset() on streams.result when there is a state-keeping TokenFilter on that chain. it is not necessary to invoke it if reset() is a no-op...\n\nWhat if one of those streams will become state-keeper some day? I don't think that calling reset() and have it done nothing will be expensive, no?\n\nbq. but now my problem child is memory/PatternAnalyzer, its source of tokens is not a Tokenizer.\n\nIt could just override reusableTokenStream and do what it wants, no?\n\nI must admit that I still have in mind the current TokenStream and Analyzer API. Therefore my suggestion may not be 100% compatible w/ AttributeSource and the new stuff. But my gut feeling tells me there has to be a way to remove all those unnecessary impls in all Analyzers. The following default impl seems too obvious than to say we cannot do it:\n{code}\nprotected TokenStream internalTokenStream() {\n  // do something\n}\n\nprotected Tokenizer getTokenizer() {\n  // do something\n}\n\npublic TokenStream tokenStream() {\n  TokenStream result = getTokenizer();\n  result = internalTokenStream(result);\n  return result;\n}\n\npublic TokenStream reusableTokenStream() {\n  Streams streams = getPrevTS();\n  if (streams == null) {\n    streams = new Streams();\n    streams.tokenizer = getTokenizer();\n    streams.tokenStream = internalTokenStream(streams.tokenizer);\n    setPrevTS(streams);\n  } else {\n    streams.reset();\n  } \n  return streams.tokenStream;\n}\n{code}\n\nI'll try to impl it tomorrow, using the new API and see how it goes.",
            "date": "2009-08-13T21:58:57.987+0000",
            "id": 28
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nWhat if one of those streams will become state-keeper some day? I don't think that calling reset() and have it done nothing will be expensive, no?\n{quote}\nit could also become completely incompatible with reuse in some way, (example: CachingTokenFilter). in that case reset will not help either :)\n\n{quote}\nIt could just override reusableTokenStream and do what it wants, no?\n{quote}\nOh, definitely, my concern was what should getTokenizer() do in that case? I guess as long as getTokenizer() is not public and is documented as optional operation, then I would not have a problem with this case...\n\n{quote}\nI'll try to impl it tomorrow, using the new API and see how it goes.\n{quote}\nplease do, I think its a great idea! I am just bringing up the ridiculous cases in contrib, as long as its flexible enough that things can be overridden in those cases, it could save a lot of heartache and maintenance hassle.\n\ndo you have any ideas on the back compat issues?\n",
            "date": "2009-08-13T22:15:02.277+0000",
            "id": 29
        },
        {
            "author": "Shai Erera",
            "body": "We only need getTokenizer because TokenStream.reset() does not accept a Reader. If we could introduce such method on TokenStream, we wouldn't need to refer to Tokenizer directly.\n\nbq. do you have any ideas on the back compat issues?\n\nWell it's a bit trickier ... today we call reusableTokenStream in our indexing code, and either get a new instance, or a reused instance. We cannot change Analyzer's default behavior, which returns a new instance (unless we're willing to break back-compat), because Analyzers that did not override reusableTokenStream, may break if we start reusing the instance by default (for example if they add two fields to a document w/ reusableTokenStream called twice).\n\nAlso, deprecate reusableTokenStream and define a new one (say reuseTokenStream), and move to use it is not good either, since we want its default impl to reuse the token stream, and impls that did not override it may break.\n\nSo how about if we create a new abstract ReusingAnalyzer which impls reusableTokenStream to always reuse it. And we add Streams to Analyzer as a protected static class. That way, Analyzers that don't care about reuse, can still extend Analyzer. Analyzers which care about reuse and are fine w/ ReusingAnalyzer's impl, can move to extend it. And Analyzers that care about reuse but want their reuse to be done differently can choose to extend ReusingAnalyzer, or Analyzer.\n\nBack-compat wise, we're safe since:\n# Existing Lucene Analyzers that reuse can be changed to extend ReusingAnalyzer.\n# Existing Analyzers (outside Lucene code) either override or not reusableTokenStream, and therefore won't break.\n# Our indexing code will still call reusableTokenStream, no change here.\n# Any code out there which traverses an Analyzer by calling reusableTokenStream does not need to change anything.\n\nI think that'd work?",
            "date": "2009-08-14T12:12:47.077+0000",
            "id": 30
        },
        {
            "author": "Robert Muir",
            "body": "Shai, works for me.\n\ni will keep working on this patch, but if you get ReusingAnalyzer together, i can easily move 95% of the code to it :)\nit is doing the tests that take forever anyway and they will not change.\n\nthank you!",
            "date": "2009-08-14T12:22:13.723+0000",
            "id": 31
        },
        {
            "author": "Robert Muir",
            "body": "the tests will not pass for upcoming patch unless LUCENE-1801 is applied first.\n\nthis is because i test that if you interrupt a shinglefilter, the type is correct (and it is currently not being cleared correctly).",
            "date": "2009-08-14T18:27:07.725+0000",
            "id": 32
        },
        {
            "author": "Robert Muir",
            "body": "rest of the analyzers and tests, except memory/PatternAnalyzer. The design of the tokenstreams used by this analyzer does not support reusableTS and it will need to be redesigned to make this happen.\n\n",
            "date": "2009-08-14T18:30:01.579+0000",
            "id": 33
        },
        {
            "author": "Yonik Seeley",
            "body": "Patch looks good - do you plan on committing soon Robert?",
            "date": "2009-08-15T14:46:08.578+0000",
            "id": 34
        },
        {
            "author": "Robert Muir",
            "body": "Yonik, thanks for reviewing it. \nI wanted to wait a bit and see if Shai wanted to give a crack at ReusingAnalyzer, but we could do that as a separate issue and then refactor code to use it?\n",
            "date": "2009-08-15T15:37:12.178+0000",
            "id": 35
        },
        {
            "author": "Yonik Seeley",
            "body": "Yes, I think we should just commit this now - the most important part is that people can create their own reusable tokenstreams from Lucene's tokenizers and token filters.  Making an easier to use ReusingAnalyzer can be a separate issue.",
            "date": "2009-08-15T15:43:25.685+0000",
            "id": 36
        },
        {
            "author": "Robert Muir",
            "body": "Yonik, ok, I will look over the patch again, but I plan on committing this tonight or tomorrow if nothing comes up.",
            "date": "2009-08-15T15:53:11.208+0000",
            "id": 37
        },
        {
            "author": "Shai Erera",
            "body": "Apologies for the late post, I had a busy weekend. Attached patch includes ReusingAnalyzer, Streams in Analyzer and javadocs.\n\nRobert, please have a look. I think extending it should be fairly straightforward and we can probably finish the integration in a couple of days. However if you discover it isn't the case, we can separate it into a different issue.\n\nAlso, I did not include a note in CHANGES. Once you're done merging it into the larger patch, I can help w/ the javadocs and CHANGES if required.",
            "date": "2009-08-15T19:55:35.062+0000",
            "id": 38
        },
        {
            "author": "Yonik Seeley",
            "body": "Perhaps the Streams class should be part of ReusingAnalyzer and not Analyzer?  It's a specific implementation of a reusable token stream, not part of the Analyzer interface.",
            "date": "2009-08-15T20:13:33.162+0000",
            "id": 39
        },
        {
            "author": "Shai Erera",
            "body": "Well ... it's true and false at the same time. On one hand, I think Analyzer should impl reusableTokenStream just like ReusingAnalyzer, but we can't do that because of back-compat. On the other hand, Streams does belong to ReusingAnalyzer because it makes use of it.\n\nWhat I thought was that maybe someone would want to make use of Streams w/o extending Analyzer. And ... we may want to constraint setPreviousTokenStream to Streams, or TokenStream or a generic type of thing, to avoid casting and be more type-safe.\n\nI wonder if we'll stay w/ Analyzer.reusableTS as it is forever, or will we break it one day to be like ReusingAnalyzer (and by that deprecate ReusingAnalyzer?).\n\nI guess that if we think for the long term that ReusingAnalyzer will stay, and hence most Analyzers will actually be ReusingAnalyzer extension, then I'm ok w/ moving Streams into ReusingAnalyzer. But keeping it in Analyzer will allow us in the future to constrain prevTokenStream to be of that type and not a generic Object.",
            "date": "2009-08-15T20:42:31.510+0000",
            "id": 40
        },
        {
            "author": "Robert Muir",
            "body": "Shai, I will take a look at your patch as soon as I am at a real computer. thanks for your work in advance, we maybe should put it on another issue though just to keep the scope of this one reasonably contained.\n\n{quote}\nAnd ... we may want to constraint setPreviousTokenStream to Streams, or TokenStream or a generic type of thing, to avoid casting and be more type-safe.\n{quote}\n\nsee QueryAutoStopWordAnalyzer in my patch for a counter-example to this. in this case, it is a Set, because it is dependent upon field.",
            "date": "2009-08-15T21:21:40.019+0000",
            "id": 41
        },
        {
            "author": "Yonik Seeley",
            "body": "In general, we should strive to treat our base abstract classes like interfaces, with the ability to provide default implementations to avoid back compatibility breaks (while avoiding adding members or non-overrideable methods).  One could make the case that the ClosableThreadLocal should not be in Analyzer either, but it's been there long enough now, it would break back compat to move it.\n\nbq. What I thought was that maybe someone would want to make use of Streams w/o extending Analyzer.\n\nThey still can - ReusableAnalyzer.Streams.\n\nbq. But keeping it in Analyzer will allow us in the future to constrain prevTokenStream to be of that type and not a generic Object.\n\nDoesn't seem like we should force all tokenstreams to be reusable, or constrain the exact form of how a reusable token stream is obtained.\n",
            "date": "2009-08-15T21:49:20.728+0000",
            "id": 42
        },
        {
            "author": "Shai Erera",
            "body": "I guess you're both right. I thought that one day we'll cancel ReusingAnalyzer and pull it up to Analyzer, but it looks like ReusingAnalyzer makes sense to stay, and so we can move Streams to it.\n\nRobert, if possible, I'd like to get this one in as part of this issue. The reason is that you already modified all Analyzers to impl reusableTokenStream. I'm afraid that if we'll do it in another issue, some Analyzers will be skipped over. If you want, I can apply this to your patch and post pack an updated one tomorrow.",
            "date": "2009-08-15T22:03:18.586+0000",
            "id": 43
        },
        {
            "author": "Mark Miller",
            "body": "To not break back compat, everything has got to work even if they don't yet move from the deprecated method.\n\n",
            "date": "2009-08-15T23:28:04.155+0000",
            "id": 44
        },
        {
            "author": "Robert Muir",
            "body": "{quote}\nRobert, if possible, I'd like to get this one in as part of this issue. The reason is that you already modified all Analyzers to impl reusableTokenStream. I'm afraid that if we'll do it in another issue, some Analyzers will be skipped over. If you want, I can apply this to your patch and post pack an updated one tomorrow.\n{quote}\n\nShai, this is a valid concern. But also lets not forget analyzers that already implement reusableTS that are not a part of this patch (yet should be changed to extend ReusingAnalyzer)... examples include collation/* analyzers/fa, etc.\n\nBut even before this I think we should make sure everyone is happy with ReusingAnalyzer itself... this is the only reason I think it might merit another issue... this patch is already a little unwieldy because I crept the scope to include reset(Reader) and reset() methods for tokenstreams that keep state...\n",
            "date": "2009-08-15T23:47:00.014+0000",
            "id": 45
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. But even before this I think we should make sure everyone is happy with ReusingAnalyzer itself... this is the only reason I think it might merit another issue\n\n+1\n\nThe ReusingAnalyzer brings up other issues of protocol - right now consumers like lucene indexing call reset() on the stream, but I see the prototype ReusingAnalyzer also calling reset() on the stream.",
            "date": "2009-08-16T00:24:48.704+0000",
            "id": 46
        },
        {
            "author": "Shai Erera",
            "body": "bq. right now consumers like lucene indexing call reset() on the stream, but I see the prototype ReusingAnalyzer also calling reset() on the stream.\n\nI don't think that's a new problem - I simply coded what I think most Analyzers that do impl reusableTS do. And if there are reusableTS impls that don't call reset() on purpose, then we shouldn't call it.\n\nTherefore, I think that we should change our code to not call reset(). I don't think there's a reusableTS impl which does not call reset(), because it relies on the consumer to do it (nobody guarantees that anyway). We should simply note that on reusableTS javadoc (e.g., something like \"return an already reset token stream\"). I don't mind doing that in a separate issue if that's what you prefer.",
            "date": "2009-08-16T06:17:16.758+0000",
            "id": 47
        },
        {
            "author": "Robert Muir",
            "body": "i would like to commit this as is later today and we create a separate issue for improving reusability:  ReusingAnalyzer, figure out who calls reset(), that sort of thing? \n",
            "date": "2009-08-16T11:34:18.232+0000",
            "id": 48
        },
        {
            "author": "Shai Erera",
            "body": "Works for me. +1",
            "date": "2009-08-16T11:38:15.650+0000",
            "id": 49
        },
        {
            "author": "Robert Muir",
            "body": "Committed revision 804680.",
            "date": "2009-08-16T12:42:51.540+0000",
            "id": 50
        },
        {
            "author": "Robert Muir",
            "body": "there is a small backwards compatibility problem here.\nsome of the analyzers, such as GermanAnalyzer, have a setStemExclusionTable method.\n\nwith reusableTS, if someone changes the Stem Exclusion table, it will not take effect instantly like it did before.\nso for these, if someone changes it, we should blow away saved streams I think: setPreviousTokenStream(null)\n\npatch and tests coming.",
            "date": "2009-08-18T13:17:19.509+0000",
            "id": 51
        },
        {
            "author": "Robert Muir",
            "body": "fix + tests for br, de, fr, and nl analyzers.",
            "date": "2009-08-18T14:22:38.019+0000",
            "id": 52
        },
        {
            "author": "Robert Muir",
            "body": "i plan to commit this fix at the end of the day if nobody objects to it.",
            "date": "2009-08-18T15:19:04.087+0000",
            "id": 53
        },
        {
            "author": "Michael Busch",
            "body": "Patch looks good!",
            "date": "2009-08-18T23:47:39.735+0000",
            "id": 54
        },
        {
            "author": "Robert Muir",
            "body": "Michael, thanks for the review. \n\nBut I am looking over all the analyzers again one last time, and I think i found another devious one:\nCzechAnalyzer has a loadStopWords() method which is not a utility method, it replaces the stoptable.\n\nSo I will go thru these again (very slowly), and upload another patch.\n\nSorry for missing these the first time.",
            "date": "2009-08-19T00:00:56.663+0000",
            "id": 55
        },
        {
            "author": "Robert Muir",
            "body": "null out saved streams in the case someone calls CzechAnalyzer loadStopWords(),\nso that new stopwords are applied immediately with reusableTS.",
            "date": "2009-08-19T01:07:28.164+0000",
            "id": 56
        },
        {
            "author": "Robert Muir",
            "body": "Committed revision 805766.",
            "date": "2009-08-19T11:58:36.872+0000",
            "id": 57
        }
    ],
    "component": "modules/analysis",
    "description": "most contrib analyzers do not have an impl for reusableTokenStream\n\nregardless of how expensive the back compat reflection is for indexing speed, I think we should do this to mitigate any performance costs. hey, overall it might even be an improvement!\n\nthe back compat code for non-final analyzers is already in place so this is easy money in my opinion.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1794",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "implement reusableTokenStream for all contrib analyzers",
    "systemSpecification": true,
    "version": ""
}