{
    "comments": [
        {
            "author": "Doron Cohen",
            "body": "Simple patch fixing this.\nPlaning to commit this shortly if no objections.",
            "date": "2007-12-26T22:07:39.812+0000",
            "id": 0
        },
        {
            "author": "Yonik Seeley",
            "body": "Resetting tokens (including the position) is currently handled via Token.clear() before the consumer reuses a token.\n",
            "date": "2007-12-26T22:14:51.278+0000",
            "id": 1
        },
        {
            "author": "Doron Cohen",
            "body": "I think I checked that and found that Token.clear() is too strong to be invoked in those cases, but let me check again...",
            "date": "2007-12-26T22:20:11.385+0000",
            "id": 2
        },
        {
            "author": "Yonik Seeley",
            "body": "In what cases?  The protocol we are currently using requires that the caller of next(token) provide a suitably initialized token (positionIncrement defaults to 1, payload removed, etc).",
            "date": "2007-12-26T22:43:05.360+0000",
            "id": 3
        },
        {
            "author": "Doron Cohen",
            "body": "Currently Token.clear() is used only for un-tokenized fields in DocmentsWriter - Tokenizer implementations of next(Token) do not call it.\nI think they can be modified to call it (instead of explicitly reseting just the pos-incr).\nBut since these methods already set the value for start-offset, calling these method might eat the speed-up gained by reusing tokens.\nBut then again, shouldn't tokenizers also reset the payload info? (seems wrong to assume there there's no payload in the input reusable token.)\nSo I guess the right thing to do is to call clear() in all toknizers (3 actually) - will work that path.\n\n",
            "date": "2007-12-26T22:44:15.830+0000",
            "id": 4
        },
        {
            "author": "Doron Cohen",
            "body": "Updated patch - Tokenizers now calling clear() as suggested.\nThis seems cleaner - Token manages its data members that should get cleared, so if more data members are added to Token in the future only Token.clear() needs to be updated - thanks Yonik!\nAll core tests pass.",
            "date": "2007-12-26T23:06:51.534+0000",
            "id": 5
        },
        {
            "author": "Yonik Seeley",
            "body": "{quote}Currently Token.clear() is used only for un-tokenized fields in DocmentsWriter{quote}\n\nI think it's used for both tokenized and un-tokenized.... see line1319.\nIt seems redundant to call clear() in both the consumer (DocumentsWriter) and producer (Tokenizer).\n\n",
            "date": "2007-12-27T04:53:28.280+0000",
            "id": 6
        },
        {
            "author": "Doron Cohen",
            "body": "{quote}\nI think it's used for both tokenized and un-tokenized.... see line1319.\nIt seems redundant to call clear() in both the consumer (DocumentsWriter) and producer (Tokenizer).\n{quote}\n\nYou're right again Yonik, I missed line 1319.\n\nBut I think it would be cleaner/safer to move the responsibility to clear() from consumers to producers.\n(Producer being the deepest tokenstream in the call sequence, the one that would instantiate a new Token if it implemented next()).\n\nOtherwise you get bugs like the one I had in testStopPositons() in the patch for LUCENE-1095: \nThe test chains two stop filters:\n* a = WhiltSpaceAnalyzer().\n* f1 = StopFilter(a)\n* f2 = StopFilter(f1)\n\nNow the test itself calls next().\nStopFilter implements only next(Token).\nSo this is the sequence we get:\n* test call f2.next()\n* TokenSteam next() calls t2.next(new Token())\n* t2.next(r) calls t1.next(r) repeatedly (until r not stopped).\n* t1.next(r) calls a.ts.next(r) repeatedly (until r not stopped).\n\nThe cause for the bug was that when t1 returns a token r, it may have set r's pos_incr to something other than1. But when t2 calls t1 again (because t2 stopped r), that pos_incr should have bean cleared to 1. Now this can also be fixed by changing StopFilter to clear() before every call to t1.next(r), except for the first time it calls ts.next(), because for the first call it can assume that its consumer already cleared r. Since most words are not stopped, this distinction between first call to successive calls is important, performance wise.\n\nNow, this is a little complicated (and not only because of my writing style : - ) ), \nand so I think moving the clear() responsibility to the (deep most) producer would make things more simple and safe. (?)\n",
            "date": "2007-12-27T06:37:46.839+0000",
            "id": 7
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nBut I think it would be cleaner/safer to move the responsibility to clear() from consumers to producers.\n{quote}\n\n+1\n\nSo, the contract of next(Token) is: if you are the source of Tokens, you must call Token.clear() before setting the fields in it & returning it.",
            "date": "2007-12-29T11:07:06.392+0000",
            "id": 8
        },
        {
            "author": "Doron Cohen",
            "body": "Thanks Mike, I'll remove the call from DocumentsWriter and updated the javadocs accordingly.",
            "date": "2007-12-30T06:25:45.750+0000",
            "id": 9
        },
        {
            "author": "Doron Cohen",
            "body": "Updated patch calling Token.clear() only by producers and javadocs updated as discussed.",
            "date": "2007-12-30T06:27:20.003+0000",
            "id": 10
        },
        {
            "author": "Doron Cohen",
            "body": "Committed, thanks for the reviews Yonik & Mike!",
            "date": "2007-12-30T07:36:47.554+0000",
            "id": 11
        }
    ],
    "component": "",
    "description": "Tokenizers which implement the reuse form of the next method:\n    next(Token result) \nshould reset the postionIncrement of the returned token to 1.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1101",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "TokenStream.next(Token) reuse 'policy': calling Token.clear() should be responsibility of producer.",
    "systemSpecification": true,
    "version": "2.3"
}