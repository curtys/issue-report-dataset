{
    "comments": [
        {
            "author": "Spyros Kapnissis",
            "body": "I am attaching a patch that uses the TokenStream API instead of Token for the decompounders (CompoundWordTokenFilterBase, DictionaryCompoundWordTokenFilter, HyphenationCompoundWordTokenFilter).Non-common attributes are now being passed on correctly through the analyzer chain.  unit test of the functionality is included.\n\nHope it helps..any improvements are of course welcome.",
            "date": "2011-10-23T00:40:18.440+0000",
            "id": 0
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi Spyros,\nthanks for taking care. The patch looks correct, I have some minor recommendations (I will post them later).\nI will report back soon.",
            "date": "2011-10-23T11:20:21.943+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "I agree, lets fix this! \n\nAre there any other tokenstreams that don't yet work correctly with the attributes API? \nI was surprised there was no warning in the javadocs for this one.\nI thought I remembered us adding a warning, maybe that was ShingleMatrix though.\n",
            "date": "2011-10-23T15:52:01.620+0000",
            "id": 2
        },
        {
            "author": "Uwe Schindler",
            "body": "ShingleMatrix is gone in 4.0 and deprecated in 3.x. There may be other filters with the same problem, maybe we should look for uses of Token class (which MUST DIE DIE DIE!) in the analyzers.",
            "date": "2011-10-23T17:26:49.845+0000",
            "id": 3
        },
        {
            "author": "Uwe Schindler",
            "body": "Attached you will find a new patch for trunk. I made some improvements to the copy operations and CompoundTokenClass:\n- copy operations no longer create useless String objects or clones of String's internal char[] (this slows down indexing a lot)\n- the algorithmic hyphenator uses CTA's char[] directly as it did for Token before (see above) and uses optimized append()\n- the broken non-unicode-conform lowercasing was removed, instead, the CharArraySet is created case insensitive. If you pass in an own CharArraySet, it has to be case insensitive, if not, filter will fail (what to do? Robert, how do we handle that otherwise?)\n- As all tokens are again CTAs, the CAS lookup is fast again.\n- Some whitespace cleanup in the test and relicts in base source file (Lucene requires 2 spaces, no tabs)\n\nRobert, if you could look into it, it would be great. I did not test it with Solr, but for me it looks correct.\n\nUwe",
            "date": "2011-10-24T20:38:57.636+0000",
            "id": 4
        },
        {
            "author": "Uwe Schindler",
            "body": "More cleanup:\n- As original token is always preserved, is not put into the list at all and returned without modifying (no extra copy operations)\n- removed deprecated makeDictionary method and corrected matchVersion usage.",
            "date": "2011-10-24T21:13:44.255+0000",
            "id": 5
        },
        {
            "author": "Uwe Schindler",
            "body": "One more time the filter was revisited and partly rewritten:\n- it no longer clones the orginal token, as decompounding is done when TokenStream is on this token, which does not change. The decompounder simply takes termAtt/offsetAtt and produces new CompoundToken instances out of it, added to the LinkedList. The original is returned unmodified by a simple \"return true\". This filter actually only creates new opjects when compounds are found, all other tokens are passed as is.\n- CompoundToken is now a simple wrapper around the characters and the offsets, copied out of the unmodified original termAtt.\n\nI think thats the most effective implementation of this filters. I think it's ready to commit.",
            "date": "2011-10-24T21:30:36.882+0000",
            "id": 6
        },
        {
            "author": "Robert Muir",
            "body": "Just one idea: if the base has makeDictionary(String[]), then maybe deprecate-3x-remove-trunk the stupid string[] ctors and just take the CharArraySet?\n\nI think this would remove about half the ctors in both base and subclasses, and I think these ctors are stupid myself.\n\nOtherwise, looks great!",
            "date": "2011-10-25T01:08:00.901+0000",
            "id": 7
        },
        {
            "author": "Uwe Schindler",
            "body": "Robert: I agree. I think, I will do this in a separate issue, to get those changes first into 3.x (by merging) and then deprecate/remove all this later. Do we have other StopFilters and similar classes that have those String[] ctors?\n\nI will also add a javadocs info to the ctor: \"This filter does many lookups on the dictionary, so it is recommended for optimal performance to add a LowercaseFilter *before* this filter and make the CharArraySet passed as dictionary case insensitive.\"\n\nI would like to add this as a hint, as this filter does per term lots of lookups in the Set.",
            "date": "2011-10-25T06:49:33.788+0000",
            "id": 8
        },
        {
            "author": "Uwe Schindler",
            "body": "Final trunk patch, which gets committed now:\n- Changed the CompoundToken class to use CharSequence for holding the slices.\n- Added javadocs with recommendations for performance.\n- Deprecated all String[] methods.\n\nOnce this is committed, I will backport and then in a second step remove the deprecated methods in trunk.",
            "date": "2011-10-25T10:40:55.034+0000",
            "id": 9
        },
        {
            "author": "Uwe Schindler",
            "body": "Committed trunk revision: 1188597",
            "date": "2011-10-25T10:43:09.588+0000",
            "id": 10
        },
        {
            "author": "Uwe Schindler",
            "body": "Committed 3.x revision: 1188604",
            "date": "2011-10-25T11:10:36.177+0000",
            "id": 11
        },
        {
            "author": "Uwe Schindler",
            "body": "Finally also removed deprecations, so this issue is resolved.\n\nThanks Spyros for reporting this and making the initial patch!",
            "date": "2011-10-25T11:30:59.167+0000",
            "id": 12
        },
        {
            "author": "Spyros Kapnissis",
            "body": "Looks great now, thanks a lot guys!",
            "date": "2011-10-25T14:40:09.747+0000",
            "id": 13
        },
        {
            "author": "Uwe Schindler",
            "body": "Bulk close after release of 3.5",
            "date": "2011-11-27T12:29:27.919+0000",
            "id": 14
        }
    ],
    "component": "modules/analysis",
    "description": "The CompoundWordTokenFilterBase.setToken method will call clearAttributes() and then will reset only the default Token attributes (term, position, flags, etc) resulting in any custom attributes losing their value. Commenting out clearAttributes() seems to do the trick, but will fail the TestCompoundWordTokenFilter tests..",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-3508",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Decompounders based on CompoundWordTokenFilterBase cannot be used with custom attributes",
    "systemSpecification": true,
    "version": "3.4, 4.0-ALPHA"
}