{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "+1 for omitNorms to be viral.",
            "date": "2011-01-03T20:38:28.913+0000",
            "id": 0
        },
        {
            "author": "Robert Muir",
            "body": "here's an initial patch hacked up by mike and I... also removed the \"multireader norms\" method that \ntakes a byte[]+offset from IndexReader.\n\none oddity is that MultiNorms.norms() always returns a filled byte[] here for non-atomic readers (never null).\nBut i think this is ok for MultiNorms, its not used in searching (only for SlowMultiReaderWrapper etc)\n\ni think somehow it would be good to have more tests that test \"doesnt have field\" versus \"omits norms\",\nand also (likely not in this is issue) we should think about IR's norm-setting methods.\n\nI don't like that these use Similarity.getDefault(): it seems we could require you to pass in the Sim for the float case.\nI also don't like that we expose a public setNorm that takes a byte value either!\n\nLong-term we should look at pulling this norm-encoding stuff out of Sim... the Sim should just be dealing with floats,\nthis encoding stuff belongs somewhere else.\n",
            "date": "2011-01-09T12:51:52.242+0000",
            "id": 1
        },
        {
            "author": "Robert Muir",
            "body": "an alternative to totally clear up the faking here that mike thought of:\n\nIf we can somehow differentiate between omitNorms (null), and 'doesnt have field' (say, exception),\nwe wouldn't need to fake. In multinorms we could then safely return null if any reader returns null,\nbut throw an exception if all readers throw an exception.\n",
            "date": "2011-01-09T14:03:10.877+0000",
            "id": 2
        },
        {
            "author": "Robert Muir",
            "body": "here's an updated patch:\n* The IR.setNorm(float) is also removed, forcing the user to use the correct similarity versus us using the wrong one (the static)\n* MultiNorms doesn't fake norms anymore, instead it handles the case of non-existent field versus omitted norms.\n* When a document doesnt have a field, its (undefined) norms are written as zero bytes instead of Similarity.getDefault().encodeNorm(1f). \n* All uses of Similarity.get/setDefault are now gone in lucene core, except for in IndexSearcher and IndexWriterConfig.\n",
            "date": "2011-01-09T17:21:08.302+0000",
            "id": 3
        },
        {
            "author": "Robert Muir",
            "body": "sorry i had a piece of backwards logic in MultiNorms.\n\nof course all tests pass either way, which is why we need a good mixed-schema test (with RIW) \nfor this issue before it can go in (no matter what we do)",
            "date": "2011-01-09T17:38:17.480+0000",
            "id": 4
        },
        {
            "author": "Robert Muir",
            "body": "ok here's the final patch: with a reasonably good test and docs.\n\nI think this one is ready to commit: gets rid of fake norms and use of the static Similarity.getDefault()\n",
            "date": "2011-01-11T15:42:18.856+0000",
            "id": 5
        },
        {
            "author": "Robert Muir",
            "body": "For branch 3.x, i would like to deprecate the IndexReader.setNorm(float) based method, so its no surprise when its removed here.\n\nhere was the changes entry (so i would insert some text like this in the deprecation): \n{noformat}\nLUCENE-2846: Remove the deprecated IndexReader.setNorm(int, String, float).\n  This method was only syntactic sugar for setNorm(int, String, byte), but\n  using the global Similarity.getDefault().encodeNormValue.  Use the byte-based\n  method instead to ensure that the norm is encoded with your Similarity.\n{noformat}\n",
            "date": "2011-01-11T15:46:12.308+0000",
            "id": 6
        },
        {
            "author": "Grant Ingersoll",
            "body": "I get the comparison to omitTF, but this functionality has been around a long time.  Why does it have to be all or nothing?  Couldn't we investigate a sparse data structure to be used instead?  We use the current dense approach when a high percentage contain norms and the sparse when less have that amount?  I'm not sure what that data structure is just yet, but over in Mahout we have sparse and dense vectors and we have primitive collections that could be useful.",
            "date": "2011-01-11T16:09:33.807+0000",
            "id": 7
        },
        {
            "author": "Robert Muir",
            "body": "bq. I get the comparison to omitTF, but this functionality has been around a long time.\n\nCan you explain what functionality you are losing with this patch?",
            "date": "2011-01-11T16:24:37.947+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Couldn't we investigate a sparse data structure to be used instead?\n\nThis would be very interesting to explore.  The fact that norms are dense, on disk and in memory, can cause horrific problems like your index taking much much more disk space and RAM on optimize/big merge finishing.\n\nBut I think that's orthogonal to the improvements here?  Ie this issue removes fake norms, invalid uses of default Sim, etc.\n\nAlso, I would worry about the lookup cost of sparse vectors in RAM -- looking up the norm per doc is a severe hotspot on Lucene.",
            "date": "2011-01-11T16:39:12.190+0000",
            "id": 9
        },
        {
            "author": "Robert Muir",
            "body": "bq. But I think that's orthogonal to the improvements here? Ie this issue removes fake norms, invalid uses of default Sim, etc.\n\nRight, after talking to Grant, I think his comments are on the larger issues of norms... I am totally not trying to address\nthis here. This one is just about cleaning up things that are \"buggy\", especially using Similarity.getDefault when we shouldn't.\n\nThe scope of this one (even though its just cleanup) is large enough I think... I'd like to proceed with this unless there are\nany objections. We can address real improvements to norms somewhere else, and hopefully this will help make that easier.\n",
            "date": "2011-01-11T18:35:09.161+0000",
            "id": 10
        },
        {
            "author": "Robert Muir",
            "body": "Committed revision 1058367.\n\nI deprecated the dangerous setNorm(float) method in 3.x in revision 1058370,\ninstead pointing at setNorm(byte) and using Similarity.encodeNormValue(),\nso you can ensure your Similarity is always used (not Similarity.getDefault)\n",
            "date": "2011-01-13T00:20:11.004+0000",
            "id": 11
        }
    ],
    "component": "",
    "description": "omitTF is viral. if you add document 1 with field \"foo\" as omitTF, then document 2 has field \"foo\" without omitTF, they are both treated as omitTF.\n\nbut omitNorms is the opposite. if you have a million documents with field \"foo\" with omitNorms, then you add just one document without omitting norms, \nnow you suddenly have a million 'real norms'.\n\nI think it would be good for omitNorms to be viral too, just for consistency, and also to prevent huge byte[]'s.\nbut another option is to make omitTF anti-viral, which is more \"schemaless\" i guess.\n",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2846",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "omitTF is viral, but omitNorms is anti-viral.",
    "systemSpecification": true,
    "version": ""
}