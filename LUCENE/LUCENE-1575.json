{
    "comments": [
        {
            "author": "Michael McCandless",
            "body": "Looks good!  Thanks Shai.  Some responses:\n\nbq. Create a final class HitCollectorWrapper, and use it in the deprecated methods in IndexSearcher, wrapping the given HitCollector.\n\nThis turns deprecated HitCollector into a Collector?  Seems like it\nshould be package private?\n\nbq. Change TopFieldDocCollector (deprecated) to extend TopDocsCollector, instead of TopScoreDocCollector. Implement topDocs(start, howMany)\n\nThis is deprecated, so we shouldn't add topDocs(start, howMany)?  I\nthink just switch it back to extending the deprecated TopDocCollector\n(like it does in 2.4)?\n\nbq. What if during collect() Scorer is null? (i.e., not set) - is it even possible?\n\nI think Lucene should guarantee not to do that?\n\nbq. I noticed that many (if not all) of the collect() implementations discard the document if its score is not greater than 0. Doesn't it mean that score is needed in collect() always?\n\nHmmmm good point.  I would love to stop screening for 0 score in the\ncore collectors (like Solr).  Maybe we fix the core collectors to not\nscreen by zero score, but we add a new \"only keep positive scores\"\ncollector chain/wrapper class that does the filtering and the forwards\ncollection to another collector?  This way there's a migration path if\nsomehow users are relying on this.\n\nAnd we should note this difference clearly in the javadocs for the new\nhierarchy.\n\nbq. There might be even a 3rd patch which handles the setScorer thing in Collector (maybe even a different issue?) \n\nI think it's fine if it's the same issue, though doing it as 2 patches\nis going to make life difficult.  I think a single patch covering\nchanges to src/java, and one to src/test is OK, though I'd personally\nprefer just one patch overall.\n",
            "date": "2009-03-27T20:43:51.609+0000",
            "id": 0
        },
        {
            "author": "Shai Erera",
            "body": "{quote}\nThis turns deprecated HitCollector into a Collector? Seems like it\nshould be package private?\n{quote}\n\nInitially I wrote it but then deleted. I decided to make the decision as I create the patch. If this will be used only in IndexSearcher, then it should be a private static final class in IndexSearcher, otherwise a package private one. However, if it turns out we'd want to use it for now in other places too where we deprecate the HitCollector methods, then it will be public.\nAnyway, it will be marked deprecated, and I have the intention to make it as 'invisible' as possible.\n\n{quote}\nThis is deprecated, so we shouldn't add topDocs(start, howMany)? I\nthink just switch it back to extending the deprecated TopDocCollector\n(like it does in 2.4)?\n{quote}\n\nThat's a good idea.\n\n{quote}\nHmmmm good point. I would love to stop screening for 0 score in the\ncore collectors (like Solr). Maybe we fix the core collectors to not\nscreen by zero score, but we add a new \"only keep positive scores\"\ncollector chain/wrapper class that does the filtering and the forwards\ncollection to another collector? This way there's a migration path if\nsomehow users are relying on this.\n{quote}\n\nI can do that. Create a FilterZeroScoresCollector which wraps a Collector and passes forward only documents with score > 0. BTW, how can a document get a zero score?\n\nI thought to split patches to code and test since I believe the code patch can be ready sooner for review. The test patch will just fix test cases. If that matters so much, I can create a final patch in the end which contains all the changes for easier commit?",
            "date": "2009-03-28T14:17:12.137+0000",
            "id": 1
        },
        {
            "author": "Marvin Humphrey",
            "body": "> BTW, how can a document get a zero score?\n\nAny number of ways, since Query and Scorer are extensible.  How about a RandomScoreQuery that uses floor(rand(1.9))?  Or say that you have a bitset of docs which should match and you use that to feed a scorer.  What score should you assign?  Why not 0?  Why not -1?  Should it matter?",
            "date": "2009-03-28T14:51:00.072+0000",
            "id": 2
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I thought to split patches to code and test since I believe the code patch can be ready sooner for review. The test patch will just fix test cases. If that matters so much, I can create a final patch in the end which contains all the changes for easier commit?\n\nOK that sounds great.  The back-compat tests will also assert nothing broke.\n\nbq. Anyway, it will be marked deprecated, and I have the intention to make it as 'invisible' as possible.\n\nOK.\n\nbq. BTW, how can a document get a zero score?\n\nI've wondered the same thing.  There was this thread recently:\n\n   http://www.nabble.com/TopDocCollector-td22244245.html",
            "date": "2009-03-28T15:59:45.425+0000",
            "id": 3
        },
        {
            "author": "Shai Erera",
            "body": "After I posted the question on how can a document get a 0 score, I realized that it's possible due to extensions of Similarity for example. Thanks Marvin for clearing that up. I guess though that the Lucene core classes will not assign <= 0 score to a document?\n\nAnyway, whether it's true or not, I think I agree with Mike saying we should remove this screening from the core collectors. If my application extends Lucene in a way that it can assign <= 0 scores to documents, and it has the intention of screening those documents, it should use the new FilterZeroScoresCollector (maybe call it OnlyPositiveScoresCollector?)\n\nI don't think that assigning <= 0 score to a document necessarily means it should be removed from the result set. \n\nHowever, Mike (and others) - isn't there a back-compatibility issue with changing the core collectors to not screen on <=0 score documents? I mean, what if my application relies on that and extended Lucene in a way that it sometimes assigns 0 scores to documents? Now when I'll switch to 2.9, those documents won't be filtered. I will be able to use the new FilterZeroScoresCollector, but that'll require me to change my app's code.\n\nMaybe just do it for the new collectors (TopScoreDocCollector and TopFieldCollector)? I need to change my app's code anyway if I want to use them, so as long as we document this fact in their javadocs, we should be fine?",
            "date": "2009-03-28T16:30:22.317+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. However, Mike (and others) - isn't there a back-compatibility issue with changing the core collectors to not screen on <=0 score documents? \n\nHmm right there is, because the search methods will use the new collectors.\n\nbq. I need to change my app's code anyway if I want to use them, so as long as we document this fact in their javadocs, we should be fine?\n\nActually there's no change to your code required (the search methods should use the new collectors).  So we do have a back-compat difference.\n\nWe could make the change (turn off filtering), but put a setter on IndexSearcher to have it insert the \"PositiveScoresOnlyCollector\" wrapper?  I think the vast majority of users are not relying on <= 0 scoring docs to be filtered out.",
            "date": "2009-03-28T17:07:10.374+0000",
            "id": 5
        },
        {
            "author": "Shai Erera",
            "body": "bq. We could make the change (turn off filtering), but put a setter on IndexSearcher to have it insert the \"PositiveScoresOnlyCollector\" wrapper?\n\nThen why do that at all? If I need to call searcher.setKeepOnlyPositiveScores, then it means a change to my code. I could then just pass in the PositiveScoresOnlyCollector to the search methods instead, right?\n\nI guess you are referring to the methods which don't take a collector as a parameter and instantiate a new TopScoreDocCollector internally? I tend to think that if someone uses those, it is just because they are simple, and I find it very hard to imagine that that someone relies on the filtering. So perhaps we can get away with just documenting the change in behavior?\n\nbq. I think the vast majority of users are not relying on <= 0 scoring docs to be filtered out.\n\nI tend to agree. This has been around for quite some time. I checked my custom collectors, and they do the same check. I only now realize I just followed the code practice I saw in Lucene's code, never giving it much thought of whether this can actually happen. I believe that if I'd have extended Lucene in a way such that it returns <=0 scores, I'd be aware of that and probably won't use the built-in collectors. I see no reason to filter <= 0 scored docs anyway, and if I wanted that, I'd probably write my own filtering collector ...\n\nI think that if we don't believe people rely on the <= 0 filtering, let's just document it. I'd hate to add a setter method to IndexSearcher, and a unit test, and check where else it should be added (i.e., in extending searcher classes) and introduce a new API which we might need to deprecate some day ...\nPeople who'll need that functionality can move to use the methods that accept a Collector, and pass in the PositiveScoresOnlyCollector. That way we also keep the 'fast and easy' search methods really simple, fast and easy.\n\nIs that acceptable?",
            "date": "2009-03-28T18:22:18.381+0000",
            "id": 6
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Then why do that at all? If I need to call searcher.setKeepOnlyPositiveScores, then it means a change to my code. I could then just pass in the PositiveScoresOnlyCollector to the search methods instead, right?\n\nOK, I agree.  Let's add an entry to the top of CHANGES.txt that states this [minor] break in back compatibility, as well as the code fragment showing how to use that filter to get back to the pre-2.9 way?",
            "date": "2009-03-28T18:49:20.824+0000",
            "id": 7
        },
        {
            "author": "Shai Erera",
            "body": "Great !",
            "date": "2009-03-29T03:54:55.960+0000",
            "id": 8
        },
        {
            "author": "Shai Erera",
            "body": "BooleanScorer defines an internal package private static final Collector class. Two questions:\n# May I change it to BooleanCollector? (the name conflicts with the Collector name we want to give to all base collectors)\n# May I change it to private static final? It is used only in BooleanScorer's newCollector() method.\nI think the two are safe because it's already package-private and there's no other Lucene code which uses it.\n\nBTW, we might wanna review BooleanScorer's internal classes visibility. They are all package-private, with some public methods, however used by BooleanScorer only ... But that's something for a different issue.",
            "date": "2009-03-29T06:15:21.592+0000",
            "id": 9
        },
        {
            "author": "Michael McCandless",
            "body": "bq. May I change it to BooleanCollector? (the name conflicts with the Collector name we want to give to all base collectors)\n\nbq. May I change it to private static final? It is used only in BooleanScorer's newCollector() method.\n\nI think these are fine.",
            "date": "2009-03-29T09:31:07.091+0000",
            "id": 10
        },
        {
            "author": "Michael McCandless",
            "body": "I think as part of this we should allow TopFieldCollector to NOT get the score of each hit?  EG another boolean to the ctor?",
            "date": "2009-03-30T11:28:53.788+0000",
            "id": 11
        },
        {
            "author": "Shai Erera",
            "body": "I am not sure what you mean - score is used all over the place in collect() as well as other methods. updateBottom for example takes a score, updates bottom.score and then calls adjustTop(). Do you mean that if ignoreScore is true (in ctor), then setScorer should not save the Scorer and not call scorer.score()? If so, what should I do with all the methods that accept score? Create another code path in TopFieldCollector which ignore the score?\n\nAlso, what should the default value be? true (for ignoring scores)?",
            "date": "2009-03-30T11:37:20.126+0000",
            "id": 12
        },
        {
            "author": "Shai Erera",
            "body": "Ok I now understand better where score is used in TopFieldCollector ... It is used in a number of places, two important are:\n# Maintain maxScore for returning in TopFieldDocs.\n# Passed to FieldComparator.compareBottom which takes a doc and score parameters. There is one comparator RelevanceComparator which makes use of the score passed, however that's part of the method signature.\n# Passed to FieldComparator.copy - again used by RelevanceComparator only.\n# Passed to updateBottom, which updates the score of the least element in the queue and then calls adjustTop().\n\n* Number 2, 3 and 4 can be resolved by adding a setScorer to FieldComparator (as empty implementation) which TopFieldCollector will call in each collect() call, passing the Scorer that was given to it in its setScorer.\n* Then, we override that in RelevanceComparator, saving the Scorer and using it whenever the score is needed. Of course we'll need to save the current score, so that we don't call score() too many times for the same document.\n* This eliminates the need to define on TopFieldCollector whether scores should be saved. The reason is that the Sort parameter may include a SortField.SCORE field, which will invoke the RelevanceComparator.\n\nThe question is what to do with maxScore? It is needed for TopDocs / TopFieldDocs. It may also be important to know the maxScore of a query, even if you sort it by something which is not a score.\n\nQuestion is - if the steps above make sense, why should we do them at all? :)\nNow the score is computed and passed on to every FieldComparator we received in Sort. Cleaning the method signature means additional code overhead in RelevanceComparator. If we want to compute maxScore as well, it means the score will be computed twice, once in collect() and once in RelevanceComparator.\n\nWe can solve the double score() computation by using an internal ScoreCacheScorer which keeps the score of the current document and returns it whenever score() is called, unless it's a new document and then it delegates the call to the wrapped Scorer. TopFieldCollector can instantiate it in setScorer.\n\nBut this looks quite a lot for cleaning a method signature, don't you think? Of course if you can suggest how we somehow remove the maxScore computation, then it might be a good change, since only if SortField.SCORE is used, will the score be computed.",
            "date": "2009-03-30T12:06:38.351+0000",
            "id": 13
        },
        {
            "author": "Michael McCandless",
            "body": "bq. The question is what to do with maxScore? It is needed for TopDocs / TopFieldDocs. \n\nThis is why I was thinking you'd have to tell TopFieldCollector whether or not it should track scores.  Furthermore, even if SortField.SCORE is not in your SortFields, an app may still want the scores to be enrolled in the TopFieldDocs, for presentation.\n\nTurning off scoring in TopFieldCollector's ctor just means 1) TopFieldCollector won't track max score, and 2) TopFieldCollector will leave score at 0 in the returned ScoreDoc array.\n\nbq. Number 2, 3 and 4 can be resolved by adding a setScorer to FieldComparator (as empty implementation) which TopFieldCollector will call in each collect() call, passing the Scorer that was given to it in its setScorer.\n\n+1\n\nIt makes sense to push the same improvement (not always passing a score; instead, you ask the scorer for score if you need it) down into the FieldCollector API.\n\nbq. We can solve the double score() computation by using an internal ScoreCacheScorer which keeps the score of the current document and returns it whenever score() is called, unless it's a new document and then it delegates the call to the wrapped Scorer. TopFieldCollector can instantiate it in setScorer.\n\n+1\n",
            "date": "2009-03-30T14:12:43.726+0000",
            "id": 14
        },
        {
            "author": "Shai Erera",
            "body": "bq. Turning off scoring in TopFieldCollector's ctor just means 1) TopFieldCollector won't track max score, and 2) TopFieldCollector will leave score at 0 in the returned ScoreDoc array.\n\nJust to be clear - TopDocs as well as TopFieldDocs require a maxScore parameter in their ctor. So are you suggesting to pass something like Float.NaN as maxScore if scoring is turned off? Or introducing a new ctor which does not require maxScore, and defaults to Float.NaN? (or both?)\n\nbq. Furthermore, even if SortField.SCORE is not in your SortFields, an app may still want the scores to be enrolled in the TopFieldDocs, for presentation.\n\nRight - we should separate between getting score out of the FieldComparator API and tracking scores in TFC. If I don't have SortField.SCORE in my list of sort fields, then scorer.score() will not be called at all from the FieldComparators layer.\n\nTracking scores in TFC is what I'm having troubles with. Turning it off does not necessarily improve anything .. Might be, and might not. In setScorer() I'd still need to register Scorer for passing on to FieldComparator. In collect() I'd still need to check whether score tracking is on, and if so, call scorer.score() and track maxScore. Note that if ScoreCacheScorer is used, then calling scorer.score() in collect does not have too much overhead.\n\nAlso, what will be the default from a Lucene perspective? true - i.e., always keep track of scores?",
            "date": "2009-03-30T14:30:34.541+0000",
            "id": 15
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Or introducing a new ctor which does not require maxScore, and defaults to Float.NaN?\n\nI think that's a good approach, though for TopDocs the ctor should be package private I think (only called from TopFieldDocs' ctor)?  And the javadocs should clearly spell out that this could happen (so people don't get scared on seeing Float.NaN coming back).\n\nbq. Turning it off does not necessarily improve anything .. Might be, and might not. In setScorer() I'd still need to register Scorer for passing on to FieldComparator. In collect() I'd still need to check whether score tracking is on, and if so, call scorer.score() and track maxScore. Note that if ScoreCacheScorer is used, then calling scorer.score() in collect does not have too much overhead.\n\nI think this is an improvement?  (Scorer.score() will not have been called... that's the goal here).\n\nI guess we could also consider making a separate TopFieldCollector (NonScoringTopFieldCollector or some such), instead of sprinkling if statements all over the place.\n\nbq. Also, what will be the default from a Lucene perspective? true - i.e., always keep track of scores?\n\nGood question... we have the freedom to choose.  Perhaps default to off?  But say clearly in the migration javadocs that you have to set that to true to get same behavior as TSDC?",
            "date": "2009-03-30T15:04:23.749+0000",
            "id": 16
        },
        {
            "author": "Shai Erera",
            "body": "ok I'll add another package-private ctor to TopDocs which does not get maxScore and defaults to NaN, as well as update the javadocs. No back-compat here since the only code that will use it is TFC, which is new.\n\nbq. I think this is an improvement? (Scorer.score() will not have been called... that's the goal here).\n\nWell, if we use ScoreCacheScorer, then this call is really fast, returning immediately and w/o computing the score.\n\nbq. I guess we could also consider making a separate TopFieldCollector (NonScoringTopFieldCollector or some such), instead of sprinkling if statements all over the place.\n\nI always like such approaches. How's that sound:\n# Create a base NSTFC, which has a protected score member, initialized to 0, and is exactly like the current TFC, only w/o maxScore.\n# Have TFC extend NSTFC, override collect() and:\n## Set super.score = scorer.score(). That is required for updateBottom which updates the score on the ScoreDoc in pq.\n## Compute maxScore.\n## Call super.collect().\n## Override topDocs(start, howMany) to provide one with maxScore.\n\nbq. Good question... we have the freedom to choose. Perhaps default to off? But say clearly in the migration javadocs that you have to set that to true to get same behavior as TSDC?\n\nSo you suggest the methods on IndexSearcher today that take a Sort as parameter will default to NSTFC? As long as we document it it's ok? Are all of these new?",
            "date": "2009-03-30T15:52:38.075+0000",
            "id": 17
        },
        {
            "author": "Michael McCandless",
            "body": "bq. How's that sound:\n\nThat sounds good!  So to be consistent maybe we create ScoringTopFieldCollector and NonScoringTopFieldCollector?\n\nThis means we don't need ScoreCacheScorer?  (because ScoringTopFieldCollector will always grab the score).  Though how do we change FieldComparator API so as to not pass score around?  All comparators except RelevanceComparator don't use it.\n\nbq. Well, if we use ScoreCacheScorer, then this call is really fast, returning immediately and w/o computing the score.\n\nI'm actually torn on how fast this will be: I think that will be an if statement that's hard for the CPU to predict, which is costly.\n\nbq. So you suggest the methods on IndexSearcher today that take a Sort as parameter will default to NSTFC? As long as we document it it's ok? Are all of these new?\n\nHmmm... actually, no, I think those must continue to use NSTFC for the existing methods (to remain back compatible), but add a new search method that takes a boolean trackScore?",
            "date": "2009-03-30T16:19:08.421+0000",
            "id": 18
        },
        {
            "author": "Shai Erera",
            "body": "bq. So to be consistent maybe we create ScoringTopFieldCollector and NonScoringTopFieldCollector?\n\nAnd have STFC extend NSTFC? I see no reason to create an abstract TopFieldCollector.\n\nbq. This means we don't need ScoreCacheScorer? (because ScoringTopFieldCollector will always grab the score). Though how do we change FieldComparator API so as to not pass score around? All comparators except RelevanceComparator don't use it.\n\nI was actually thinking of that class for RelevanceComparator. So perhaps I can implement the logic inside RelevanceComparator? Although this sounds like a nice utility class, now that we have a setScorer on Collector - others may find it useful too.\nRemember that score-tracking is done for maxScore and ScoreDoc purposes (inside STFC). The score in the FieldComparator API is used only in RelevanceComparator, whether it's STFC or NSTFC.\n\nbq. I think those must continue to use NSTFC for the existing methods (to remain back compatible)\n\nDid you mean continue to use STFC? The current behavior is that scoring is tracked, I think.\n\nbq. add a new search method that takes a boolean trackScore?\n\nI actually prefer not to expose any more methods. IndexSearcher already has plenty of them. Instead, one can use the very generic, simple and useful method search(Query, Collector) and pass in a NSTFC instance. Otherwise we'll end up adding many search() methods to IndexSearcher, if we continue with that approach going forward.",
            "date": "2009-03-30T16:59:41.419+0000",
            "id": 19
        },
        {
            "author": "Michael McCandless",
            "body": "bq. And have STFC extend NSTFC? I see no reason to create an abstract TopFieldCollector.\n\nYes.\n\nbq. Although this sounds like a nice utility class, now that we have a setScorer on Collector - others may find it useful too.\n\nOK I agree, it would be useful to have for general usage (eg chaining collectors).\n\nBut what is the plan now for the FieldComparator API?  We no longer pass score all around, but expose access to scorer, which only RelevanceComparator (in core) will use?\n\nbq. Did you mean continue to use STFC? The current behavior is that scoring is tracked, I think.\n\nSorry, yes, STFC.  Beginning to lose mind...\n\nbq. I actually prefer not to expose any more methods. IndexSearcher already has plenty of them. Instead, one can use the very generic, simple and useful method search(Query, Collector) and pass in a NSTFC instance. Otherwise we'll end up adding many search() methods to IndexSearcher, if we continue with that approach going forward.\n\nI think that'd be OK... the only thing that bothers me is I think the natural default when sorting by field is to not gather the score.  Ie I don't want someone evaluating Lucene in the future to say our field sort is too slow when they didn't realize they had to go use this advanced API that turns off scoring.\n\nWhat if we add a new method, and deprecate the old one?  This way come 3.0 we will not have added any methods, and then when sorting by field you see that you have to choose with or without scores.",
            "date": "2009-03-30T17:16:42.995+0000",
            "id": 20
        },
        {
            "author": "Shai Erera",
            "body": "bq. But what is the plan now for the FieldComparator API? We no longer pass score all around, but expose access to scorer, which only RelevanceComparator (in core) will use?\n\nYes. FieldComparator will have a default empty setScorer() method, which will be overridden by RelevanceComparator. In TopFieldCollector (forget the final name now) setScorer() method we set the Scorer on all FieldComparators. During collect(), only RelevanceComparator, if it exists, will compute the score.\n\nbq. What if we add a new method, and deprecate the old one?\n\nThe current methods are:\n* Searchable - search(Weight, Filter, int, Sort)\n* Searcher - search(Query, Filter, int, Sort)\n* IndexSearcher - search(Weight, Filter, int, Sort, boolean /* fillFields */)\n\nDeprecate all three, and add the same but take another boolean as a parameter? I have two comments regarding that:\n# The current methods need to call the new ones with trackScores = true since that's the current behavior.\n# When we are left with only the new versions, I'm afraid those methods will not look 'simple fast' to a user - I now have to decide whether I want to track scores or not, something I haven't given much thought to before. I kind of like the current signature, but I understand your concern regarding defaults.\n\nBTW, Searchable is an interface, so we cannot add it there. Searcher is an abstract class and we cannot add the method to it with default implementation (as I believe the other search methods will call the new one with default=true). So it only leaves IndexSearcher as an option. But then what if someone uses MultiSearcher? ParallelMultiSearcher? etc.\n\nIs it possible to deprecate a method, documenting that its runtime behavior will change in 3.0 and then in 3.0 change to not track scores?\n\nIf we're touching TopFieldCollector in such a way, I'd like to propose the following refactoring. It stems from the current complex implementation in collect() which checks in every collect call if we have just one Comparator or Multi, and we're talking about having two versions w/ and w/o score tracking:\n* Keep TopFieldCollector as abstract class with a static create() factory method, and an abstract updateBottom() method. It will still implement topDocs(start, howMany).\n* Have the factory method create one of 4 instances, all extend TFC, but are private internal classes, which do not concern the user:\n*# OneComparatorNonScoringTopFieldCollector - assumes just one FieldComparator exists as well as scoring should not be tracked.\n*# OneComparatorScoringTopFieldCollector - assumes just one FieldComparator exists as well as scoring should be tracked.\n*# MultiComparatorNonScoringTopFieldCollector - assumes more than one FieldComparator exists, as well as scoring should not be tracked.\n*# MultiComparatorScoringTopFieldCollector - assumes more than one FieldComparator exists, as well as scoring should be tracked.\n\nThe advantages are:\n* We simplify the API - the user is only aware of TopFieldCollector, and instead of calling a ctor it calls a static create method, which takes all the arguments as the ctor takes.\n* We are free to create whatever instance is the right and most optimized one given the input parameters. The user does not care how the instance is called. Hence the long names - they are internal anyway.\n* The code is much cleaner and easy to understand. It also does not need to check if we have just one comparator or more in every call to collect.\n* We don't need to add a protected score to a NonScoring collector (read above about code readability) just because a Scoring one extends it and will make use of it.\n\nSince TopFieldCollector is new, we have the freedom to do it right w/o deprecating anything. I think it's a much cleaner design. It is orthogonal to the discussion we're having regarding the search methods and parameters. They will use the create() factory method instead of creating a collector, passing whatever arguments they have. So let's not confuse the two.\n\nThe patch for this issue is ready. As soon as we agree on how to proceed with TFC, I'll add the changes and submit the patch.",
            "date": "2009-03-31T05:57:49.473+0000",
            "id": 21
        },
        {
            "author": "Michael McCandless",
            "body": "bq. If we're touching TopFieldCollector in such a way, I'd like to propose the following refactoring\n\n+1.  That looks great.\n\nbq. When we are left with only the new versions, I'm afraid those methods will not look 'simple fast' to a user \n\nI agree, which is why I'd like in 3.0 for the default to be \"don't score when sorting by fields\".\n\nbq. Is it possible to deprecate a method, documenting that its runtime behavior will change in 3.0 and then in 3.0 change to not track scores?\n\nI think this may in fact be our best option: don't deprecate the method, but document that in 3.0 this method will no longer do scoring.  There is a precedent here: in 3.0, IndexReader.open is going to return readOnly readers by default (vs read/write today).  We have also done similar fixes within a minor release, eg fixes to StandardAnalyzer.  I think there are other things we should do (eg, StopFilter should enable position increment by default, which it doesn't today -- LUCENE-1258).  If we do this approach, on committing this issue you should open a new one w/ fix version 3.0 to switch up the default.\n\nI think, with 3.0, if we clearly document in CHANGES, as well as on the particular APIs, the changes to Lucene's defaults, that's sufficient?",
            "date": "2009-03-31T11:17:10.429+0000",
            "id": 22
        },
        {
            "author": "Shai Erera",
            "body": "bq. If we do this approach, on committing this issue you should open a new one w/ fix version 3.0 to switch up the default.\n\nOk then let's do that. I'll add a TODO to these methods that it should be changed in 3.0, and also open another issue. (The TODO is in case I forget to open the issue).\n\nI'll also add documentation to the CHANGES file as well as the API.\n\nOk, I think the patch should be ready soon then. Just need to complete the refactoring to TopFieldCollector.",
            "date": "2009-03-31T11:59:26.245+0000",
            "id": 23
        },
        {
            "author": "Shai Erera",
            "body": "When I was about to make the changes to FieldComparator (add setScorer and calling scorer.score() when necessary) I noticed that scorer.score() declares it throws IOException, while the FieldComparatro methods don't. So two ways to handle it:\n# catch the IOException and ignore it, assuming a 0 score. This is if we think no Scorer will actually throw an IOException.\n# Change FieldComparator APIs to declare throwing IOE, which will give us flexibility in the future. Since the Lucene code throws IOE in many places, I don't think it's a problem.\n\nI'm in favor of (2) (I had to add IOE to Collector.collect() for that reason).",
            "date": "2009-03-31T15:26:20.232+0000",
            "id": 24
        },
        {
            "author": "Michael McCandless",
            "body": "I like 2 as well.  I find reserving that freedom to be very helpful, while preventing it to be a real hassle later on...",
            "date": "2009-03-31T15:31:03.499+0000",
            "id": 25
        },
        {
            "author": "Shai Erera",
            "body": "Eventually I decided to include just one patch file (instead of code and test) since it was simpler after all. Please be sure to review the following:\n# Collector class and documentation.\n# New TopDocsCollector class.\n# TopFieldCollector refactoring.\n# Methods deprecation.\n# New TestTopDocsCollector as well as test cases in TestSort.",
            "date": "2009-04-01T08:49:56.193+0000",
            "id": 26
        },
        {
            "author": "Uwe Schindler",
            "body": "I just wonder, why HitCollectorWrapper implements:\n{code}\npublic void collect(int doc, float score) {\n    collector.collect(doc + base, score);\n}\n{code}\nThis is not needed by Collector abstract class and never called.",
            "date": "2009-04-01T09:09:45.506+0000",
            "id": 27
        },
        {
            "author": "Shai Erera",
            "body": "oops :) leftovers from when it extended MultiReaderHitCollector (now called Collector)",
            "date": "2009-04-01T09:15:22.886+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "Shai, it looks like you \"svn copy\"'d MultiReaderHitCollector.java --> Collector.java?  It causes \"patch\" to be confused when applying the patch.  The simple workaround is to pre-copy that file yourself, manually, before appying the patch.",
            "date": "2009-04-01T09:17:19.732+0000",
            "id": 29
        },
        {
            "author": "Uwe Schindler",
            "body": "bq. oops  leftovers from when it extended MultiReaderHitCollector (now called Collector)\nThis is why we really should move to Java 1.5 soon and its @Override annotation...",
            "date": "2009-04-01T09:25:01.808+0000",
            "id": 30
        },
        {
            "author": "Shai Erera",
            "body": "I did not do any \"svn copy\", just used Eclipse refactoring to change the name of the class to Collector. I did not understand though from your comment if I should do it differently and post another patch, or is that a hint to how someone can still apply the patch?",
            "date": "2009-04-01T09:27:21.867+0000",
            "id": 31
        },
        {
            "author": "Uwe Schindler",
            "body": "JavaDoc errors:\n- The Collector javadoc example still contains the score in its collect() method.\n- The javadocs of ParallelMultiSearcher's new Collector's {code}public void search(Weight weight, Filter filter, final Collector collector){code} has still HitCollector in its JavaDocs.\n\nThis is what I found out when reading the new generated Javadocs.",
            "date": "2009-04-01T09:34:19.617+0000",
            "id": 32
        },
        {
            "author": "Shai Erera",
            "body": "Thanks Mike. I ran the javadocs task and found other mentions of MultiReaderHitCollector as well as fixed some more javadocs. BTW, the javadoc Ant task outputs many errors on missing files/names, but that something for another issue.",
            "date": "2009-04-01T09:47:02.479+0000",
            "id": 33
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "I'm late to this discussion, so I may have missed something. Is there any provision in the new API for the early termination of hit collection? Currently the (time | count)-limited collectors that are used in Nutch and Solr have to throw RuntimeException to break the loop. It would be much more elegant if the new Collector.collect() had a way to signal the caller that it should stop looping without incurring the cost of throwing an Exception. E.g. by returning a boolean, or setting a flag in the caller.",
            "date": "2009-04-01T09:56:08.972+0000",
            "id": 34
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Thanks Mike.\n\nThat was Uwe ;)",
            "date": "2009-04-01T10:01:11.066+0000",
            "id": 35
        },
        {
            "author": "Michael McCandless",
            "body": "Looks great!  Thanks Shai.\n\nMy biggest question/issue is how TopDocsCollector now (still?)\nrequires that the PQ you give it is sorting primarily by score (eg\ngetMaxScore() assumes the max is in the PQ; topDocs() uses\nresults[0]'s score when start == 0).  We've sort of come full circle\n(a \"hidden assumption\" that you are sorting by score was what started\nthe whole thread in the beginning) ;)\n\nWe get away with that with TopFieldCollector because that overrides\nall the score-related processing.\n\nI'm not sure how to cleanly fix this... maybe TopDocsCollector should\nmake maxScore() abstract?  Or... it's as if we need the\nscoring/non-scoring bifurcation up higher (moved out of\nTopFieldCollector to above TopDocsCollector)?\n\nEG say I provide my own PQ that's sorting by date (by loading date\nfrom some external source, say); I may or may not care for score.\n\nMaybe we make a ScoringWrapperCollector that grabs score in its own\ncollect, then calls collect() on its child?\n\nA few other small things:\n\n  - In the private final static classes inside TopFieldCollector, you\n    can make the members final too (eg reverseMul, comparator), in\n    case it helps compiler.\n\n  - Can you add a paragraph @ top of CHANGES stating the pending\n    default swap in 3.0 (ie the same note you added to\n    IndexSearcher.search).  Add a new \"Changes in backwards\n    compatibility policy\" section at the very top (look at how 2.4.0\n    release did it).  And can you give explicit code fragment showing\n    how to get back to the old way (ie show that you must pass in\n    \"true\" for trackDocScores).\n",
            "date": "2009-04-01T10:02:08.787+0000",
            "id": 36
        },
        {
            "author": "Michael McCandless",
            "body": "bq. It would be much more elegant if the new Collector.collect() had a way to signal the caller that it should stop looping without incurring the cost of throwing an Exception. E.g. by returning a boolean, or setting a flag in the caller.\n\nI agree: we should work into this new collection API a way to stop early.\n\nBut I'm nervous about the cost of checking a returned boolean on every collect call vs the cost of throwing/catching an exception.  Adding the boolean check slows down every single collect() call (by just a bit, but bits really count here), even those that never use stop early (the majority of apps today).   Throwing an exception adds a clear cost when you actually throw & catch it, but presumably that cost is proportionally tiny because you only throw it on searches that have been running for a long time, anyway.\n\nWe could upgrade the exception to a checked exception; then we'd need to add \"throws XXX\" to the search methods in 3.0 (perhaps wrapping as RuntimeException until 3.0).  But then I also wonder if the checked exception logic would add instructions in the collect() path.",
            "date": "2009-04-01T10:12:07.524+0000",
            "id": 37
        },
        {
            "author": "Uwe Schindler",
            "body": "I think, a checked Exception to stop collecting would be the best. The \"cost\" of the exception is very minimal (it is only thrown once in the collector and catched somewhere at top level). So where would be the costly part? Microseconds per search?",
            "date": "2009-04-01T11:03:57.477+0000",
            "id": 38
        },
        {
            "author": "Shai Erera",
            "body": "Sorry  -- thanks Uwe :)\n\nOn Wed, Apr 1, 2009 at 1:01 PM, Michael McCandless (JIRA)\n\n",
            "date": "2009-04-01T11:23:15.050+0000",
            "id": 39
        },
        {
            "author": "Shai Erera",
            "body": "bq. I think, a checked Exception to stop collecting would be the best.\n\nI actually think a RuntimeException is better for the following reasons:\n* The effects of a checked exception added to collect() will not touch IndexSearcher only, but also some Scorer implementations.\n* That only exists in the TimeLimitedCollector, which is not instantiated by Lucene code, but rather by my application code. Therefore, reading its javadocs, I know I should wrap the searcher.search with a try-catch. On the other hand, the rest of the Lucene users will not need to do that since they don't use it.\n* Going forward, someone may come up with another limiting Collector implementation, for example \"I want to stop collecting and searching as soon as I hit 10 documents\" - what would we do then - add another checked exception?\n\nMy point is - since such Collectors (at least now) are now instantiated by the search application and not by the Lucene code itself, RuntimeException is as good as checked exception, only they don't require any changes to the methods signature.",
            "date": "2009-04-01T11:36:27.622+0000",
            "id": 40
        },
        {
            "author": "Shai Erera",
            "body": "bq. Maybe we make a ScoringWrapperCollector that grabs score in its own collect, then calls collect() on its child?\n\nHow about this:\n# I remove maxScore() completely from TopDocsCollector.\n# I define newTopDocs to return a TopDocs() w/o a maxScore (like we said, defaulting to Float.NaN). Also change the method signature to receive the start as well as ScoreDoc[] that was collected so far.\n# TopScoreDocCollector will override it, fetching maxScore and returning a TopDocs.\n#* TopFieldCollector will do the same, but returning TopFieldDocs.\n\nbq. Can you add a paragraph @ top of CHANGES stating the pending default swap in 3.0\n\nDone.\n\nbq. In the private final static classes inside TopFieldCollector, you can make the members final too\n\nDone.",
            "date": "2009-04-01T12:03:05.728+0000",
            "id": 41
        },
        {
            "author": "Shai Erera",
            "body": "Includes the latest comments from Mike.",
            "date": "2009-04-01T12:06:32.065+0000",
            "id": 42
        },
        {
            "author": "Michael McCandless",
            "body": "bq. How about this:\n\nOK that looks good!\n\nThough we now don't have a single shared ancestor class that can give\nyou a maxScore() when sorting by score or when sorting by fields.  Not\nsure how big a loss that is though...\n\nMore comments:\n\n  * Should we implement a default\n    TopDocsCollector.collect/setScorer/setNextReader?  Ie,\n    TopDocsCollector is supposed to be the \"you provide the PQ and we\n    do the rest\" collector, most closely matching TopDocCollector.\n    Or.... maybe we don't, because we don't know if we should compute\n    the score for you, you may want to put something other than\n    ScoreDoc into the queue, etc.  Who knows what your class\n    considers \"top\".\n\n  * What happened to OnlyPositiveScoresFilter?  (I don't see it).\n\n  * It's a bit of a trap on calling either topDocs method in\n    TopDocsCollector (or its subclasses) that it has popped everything\n    from your PQ as a side-effect.  This is technically a pre-existing\n    issue, but the new bracketed version makes the trap more \"trappy\".\n    For example, I can't call it N times once for each page -- I have\n    to re-run the search to get another page's worth of results.  Can\n    you update javadocs eg something like \"NOTE: you cannot call this\n    method more than once\" .\n",
            "date": "2009-04-01T12:37:23.656+0000",
            "id": 43
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I did not understand though from your comment if I should do it differently and post another patch, or is that a hint to how someone can still apply the patch?\n\nThe patch command gets confused because it sees set of diffs against what looks to be a pre-existing Collector.java, but of course I have no Collector.java locally.  \"svn diff\" did this because it knows you had renamed MRHC --> C.\n\nI think for now it's fine if those of us that need to apply the patch simply first copy MultiReaderHitCollector.java --> Collector.java.\n\nThis is yet another example of how \"patch\" needs to be better integrated with svn: there needs to be a mirror \"svn patch\" to \"svn diff\" that's able to properly carry over all svn changes, perhaps do a 3way merge, etc.",
            "date": "2009-04-01T12:44:24.081+0000",
            "id": 44
        },
        {
            "author": "Shai Erera",
            "body": "bq. Though we now don't have a single shared ancestor class that can give you a maxScore() when sorting by score or when sorting by fields\n\nI don't think it's a big issue. We have just two extensions to TopDocsCollector, each tracking maxScore differently. Perhaps later on if more extensions are created, or a demand comes from users, we add a ScoringTopDocsCollector class?\n\nbq. Should we implement a default TopDocsCollector.collect/setScorer/setNextReader?\n\nI actually think that TopDocsCollector gives you exactly what I had in mind. When I started it, there was only collect(doc, score) method, and I wanted to have a base class that will implement getTotalHits and topDocs for you, while you only need to provide an implementation to collect(). Now collect has really become setNextReader, setScorer and collect. Meaning, you know what you want to do, TDC just takes care of creating the TopDocs for you, and you can even override topDocs(start, howMany) if you want to return a different TopDocs instance.\n\nI think now it's clean and simple.\n\nbq. What happened to OnlyPositiveScoresFilter? (I don't see it).\n\nWell ... you don't see it because I forgot to implement it :).\nFunny thing - all the tests pass without it, meaning all that time, we were filtering <= 0 scored documents, but never really tested it ... I guess if it's because we never really believed it'll happen?\n\nAnyway, I'll add such a class and a suitable test class (make sure it fails w/o using that wrapper first).\n\nbq. Can you update javadocs eg something like \"NOTE: you cannot call this method more than once\" .\n\nThat has always been the case. Previously if you called topDocs() everything has been popped out of the queue for you. I understand what you say though, since we have the topDocs(start, howMany) API, nothing prevents a user from calling topDocs(0, 10) and topDocs(10, 10), only the second call will fail. However, I don't really think that's how people will use it ... and if they do, then perhaps they should just call topDocs() and do whatever they need on these ranges using the TopDocs object?\nI'll add that to the documentation as well.",
            "date": "2009-04-01T13:20:11.123+0000",
            "id": 45
        },
        {
            "author": "Shai Erera",
            "body": "Adds:\n* PositiveScoresOnlyCollector and TestPositiveScoresOnlyCollector.\n* Relevant comments in CHANGES\n* Comments to TopDocsCollector.topDocs(start, howMany) and topDocs(start)",
            "date": "2009-04-01T13:51:35.552+0000",
            "id": 46
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. Funny thing - all the tests pass without it, meaning all that time, we were filtering <= 0 scored documents, but never really tested it\n\nHmmm!\n\nbq. I actually think that TopDocsCollector gives you exactly what I had in mind.\n\nOK let's keep the current approach.\n\nbq. Perhaps later on if more extensions are created, or a demand comes from users, we add a ScoringTopDocsCollector class?\n\nOK.\n\nI think patch looks good -- I don't have any more comments now.\n",
            "date": "2009-04-01T15:59:03.116+0000",
            "id": 47
        },
        {
            "author": "Shai Erera",
            "body": "bq. I think patch looks good - I don't have any more comments now.\n\nGreat !\n\nWill you be committing it (perhaps after we give some more time for people to review it)?",
            "date": "2009-04-01T16:07:49.770+0000",
            "id": 48
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Will you be committing it (perhaps after we give some more time for people to review it)?\n\nYes... but let's let it age some so others can review.  Plus, I find coming back and looking at something again after just a couple of days results in a fresh perspective.",
            "date": "2009-04-01T16:17:40.139+0000",
            "id": 49
        },
        {
            "author": "Michael McCandless",
            "body": "Shai, some contrib tests fail to compile (still using MultiReaderHitCollector), eg:\n\n{code}\n[javac] /lucene/lucene.collector/contrib/miscellaneous/src/test/org/apache/lucene/index/TestFieldNormModifier.java:26: cannot find symbol\n[javac] symbol  : class MultiReaderHitCollector\n[javac] location: package org.apache.lucene.search\n[javac] import org.apache.lucene.search.MultiReaderHitCollector;\n{code}",
            "date": "2009-04-01T16:28:47.946+0000",
            "id": 50
        },
        {
            "author": "Shai Erera",
            "body": "Perhaps you can help me here - I tried to run the test Ant task and all\ntests passed. Then it got to building contrib's db and failed on sleepycat\nwith a major/minor version 49.0. Does this require Java 1.5? Should I run\nthe task with a 1.5 JDK?\nThe task stopped at this point with a BUILD FAILED message and did not\ncontinue to other modules.\n\nI don't have the entire contrib in my build path, only a handful of them\nwhich were not affected by these changes. Anyway, I'll take a look at it\ntomorrow and submit another patch.\n\nOn Wed, Apr 1, 2009 at 7:29 PM, Michael McCandless (JIRA)\n\n",
            "date": "2009-04-01T20:27:14.708+0000",
            "id": 51
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Does this require Java 1.5? Should I run the task with a 1.5 JDK?\n\nYes, likely contrib/db (and others) require 1.5.\n\nYou should be able to just do \"ant test-contrib\" in the toplevel dir.",
            "date": "2009-04-02T00:52:53.340+0000",
            "id": 52
        },
        {
            "author": "Shai Erera",
            "body": "Fixed TestFieldNormModifier and TestLengthNormModifier.\nAll tests pass now (including contrib)",
            "date": "2009-04-02T06:49:05.988+0000",
            "id": 53
        },
        {
            "author": "Michael McCandless",
            "body": "Could you also run \"ant test-tag\" (which tests JAR-drop-in back-compatibility)?  EG I'm getting this compilation error:\n{code}\n[javac] /lucene/src/lucene.collection/tags/lucene_2_4_back_compat_tests_20090320/src/test/org/apache/lucene/search/TestTimeLimitedCollector.java:136: incompatible types\n[javac] found   : org.apache.lucene.search.TimeLimitedCollector\n[javac] required: org.apache.lucene.search.HitCollector\n[javac]     return res;\n[javac]            ^\n{code}\n",
            "date": "2009-04-02T08:58:39.653+0000",
            "id": 54
        },
        {
            "author": "Shai Erera",
            "body": "I thought that ant test runs all tests. Thanks for the education.\n\nThe reason is that TimeLimitedCollector now extends Collector, which does not extend HitCollector. Therefore the method attempts to return an invalid type. I'm not sure how to fix it, because I cannot change the 2.4 test code, since Collector is not there.\n\nSo the only reasonable solution I see here is to:\n* Change TimeLimitedCollector to extend HitCollector, document that in 3.0 it will change to extend Collector and that in the meantime use HitCollectorWrapper if you want.\n* Comment out all the Collector related methods, including the new ctor, with a TODO to reenstate in 3.0.\n* Fix the TestTimeLimitedCollector wrap it with a HCW as well as using only HitCollector as the wrapped collector.\n\nOther solutions which I don't like are:\n* deprecate TLC and create a new one NewTimeLimitedCollector - I hate the name :)\n* Have Collector extend HitCollector - I hate to even consider that.\n\nWhat do you think?",
            "date": "2009-04-02T09:22:55.651+0000",
            "id": 55
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I thought that ant test runs all tests. Thanks for the education.\n\nProbably, it should.  I'll raise this on java-dev.\n\nbq. Change TimeLimitedCollector to extend HitCollector, document that in 3.0 it will change to extend Collector and that in the meantime use HitCollectorWrapper if you want.\n\nI think I like this solution best (though this is very much a lesser of all evils situation).\n\n<lament>\nAhh the contortions we must go through because of Lucene's success.  Marvin over on Lucy can happily make major changes without batting an eye. The sad reality is that the ongoing growth rate of a thing is inversely proportional to its popularity.\n</lament>",
            "date": "2009-04-02T09:43:36.840+0000",
            "id": 56
        },
        {
            "author": "Shai Erera",
            "body": "Changes:\n# TimeLimitedCollector, TestTimeLimitedCollector and CHANGES.\n# I also fixed a bug in TestTermScorer, which was discovered by the test-tag task, and existed since 1483 and propagated into HitCollectorWrapper as well: docBase was set to -1 by default, relying on setNextReader to be called. However if it's not called (as in TestTermScorer, or if someone called Scorer.score(Collector)), all document Ids are shifted backwards by 1. The test had a bug which asserted on the unshifted doc Id, and after I fixed the Ids to shift, it failed. Anyway, the test now works correctly, as well as HCW.\n# I checked all other Collector implementations and changed the default base to 0, unless in some test cases, where -1 had a meaning.\n\nAll tests (contrib, core and tags) pass.",
            "date": "2009-04-02T11:54:30.523+0000",
            "id": 57
        },
        {
            "author": "Michael McCandless",
            "body": "Super, all tests pass for me too...",
            "date": "2009-04-02T19:15:34.901+0000",
            "id": 58
        },
        {
            "author": "Shai Erera",
            "body": "I've been thinking about TimeLimitedCollector and the revert to extend HitCollector I had to do in the last patch - the main reason was that I couldn't find a better name and did not want to deprecate it. But then, I thought that perhaps the current name is not so good, and we can change it? Syntactically, it is not a 'limited' collector, but more of a 'limiting' collector (I think, not being a native English speaker I may be wrong).\nAlternative names I've been thinking about are TimeKeeperCollector, TimeLimitingCollector, TimingOutCollector.\nThe advantage is that we deprecate the current one and have a clear back-compat support, instead of changing it in 3.0 to extend Collector. If you agree with any of these names I can create a new class, deprecate the current one, change the tests back to use the new version (and remove all those comments about the changes in 3.0). What do you think?",
            "date": "2009-04-03T11:24:18.864+0000",
            "id": 59
        },
        {
            "author": "Michael McCandless",
            "body": "bq.  If you agree with any of these names I can create a new class, deprecate the current one, change the tests back to use the new version (and remove all those comments about the changes in 3.0). What do you think?\n\nI like this approach.  I like \"TimeLimitingCollector\", or maybe \"TimeoutCollector\"?",
            "date": "2009-04-03T12:12:00.750+0000",
            "id": 60
        },
        {
            "author": "Michael McCandless",
            "body": "\nOK, I attached a new patch with some minor changes:\n\n  * Beefed up javadocs in Collector.java; fixed other javadocs\n    warnings.  Tweaked CHANGES.txt.\n\n  * Renamed PositiveOnlyScoresCollector -->\n    PositiveScoresOnlyCollector\n\nAnd also came across these questions/issues:\n\n  * TopFieldCollector's updateBottom & add methods take score, and are\n    passed score from the non-scoring collectors, but shouldn't?\n\n  * TermScorer need not override score(HitCollector hc) (super does\n    the same thing).\n\n  * The changes to TermScorer make me a bit nervous.  EG, the new\n    InternalScorer: will it hurt performance?  Also this part:\n{code}\n+        // Set the Scorer doc and score before calling collect in case it will be\n+        // used in collect()\n+        s.d = doc;\n+        s.score = score;\n+        c.collect(doc);                      // collect score\n{code}\nis spooky: I don't like how we worry that one may call scorer.doc() (I\ndon't like the ambiguity in the API -- we both pass doc and fear you\nmay call scorer.doc()).  Not sure how to resolve it.\n\n  * Hmm -- we added a new abstract method to\n    src/java/org/apache/lucene/search/Searcher.java (that accepts\n    Collector).  Should that method be concrete (and throw UOE), for\n    back compat?\n\n  * We've also added a method to the \"Searchable\" interface, which is\n    a break in back-compat.  But my feeling is we should allow this\n    break (but Shai can you add another Note at the top of\n    CHANGES.txt, calling this out?).\n",
            "date": "2009-04-03T12:13:30.092+0000",
            "id": 61
        },
        {
            "author": "Shai Erera",
            "body": "bq. I like \"TimeLimitingCollector\", or maybe \"TimeoutCollector\"?\n\nI like TimeLimitingCollector better, as I think the name makes the class more self explanatory.\n\nbq. TopFieldCollector's updateBottom & add methods take score, and are passed score from the non-scoring collectors, but shouldn't?\n\nAt the end of the day, even the non-scoring collectors store a score in ScoreDoc, which is Float.NaN. So they should pass a score. Unlike the scoring ones, they always pass Float.NaN without ever calling scorer.score(). That's the cleanest way I've found I can make the changes to that class, w/o duplicating implementation all over the place. Notice that the scoring versions extend the non-scoring, and just add score computation, which resulted in a very clean implementation.\n\nbq. TermScorer need not override score(HitCollector hc) (super does the same thing).\n\nAgreed.\n\nbq. The changes to TermScorer make me a bit nervous.\n\nSince we pass Sorer to Collector, I thought we cannot really rely on anyone not calling scorer.doc() or getSimilarity ever - it is in the API. Since doc() is abstract, I had to implement it and just thought that retuning the current doc is better than -1 for example. There are some alternatives I see to resolve it:\n# Create an abstract ScoringOnlyScorer which extends Scorer and implements all methods to throw UOE (also as final), besides score() which it will define abstract. We then define a ScoringOnlyScorerWrapper which takes a Scorer and delegates the score() calls. We use SOSW in places where we can't extend SOS. Where we can, we just extend it directly and implement score(), like in the InternalScorer case.\n# Create a new class which implements just score() (I've yet to come with a good name since Scorer is already taken) and create a wrapper which takes a Scorer and delegates the score() calls to it. Then Collector will use that new class, and we're sure that only score() can be called.\n\nThe last two comments are completely an overlook by my side. I'm not so sure about your proposal though. If we add to Searcher a concrete impl which throws UOE, how would that work in 3.0? How would anyone who extends Searcher know that it has to extend this method? Maybe do it now, and document that in 3.0 it will become abstract again?\nAbout Searchable, I wonder how many do implement Searchable, rather than extend IndexSearcher. Perhaps instead of making any changes in back-compat and add documentation to CHANGES I'll just comment out this method with a TODO to re-enstate in 3.0?",
            "date": "2009-04-03T16:32:57.281+0000",
            "id": 62
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I like TimeLimitingCollector better, as I think the name makes the class more self explanatory.\n\nOK let's go with that!\n\n{quote}\nAt the end of the day, even the non-scoring collectors store a score in ScoreDoc, which is Float.NaN. So they should pass a score. Unlike the scoring ones, they always pass Float.NaN without ever calling scorer.score(). That's the cleanest way I've found I can make the changes to that class, w/o duplicating implementation all over the place. Notice that the scoring versions extend the non-scoring, and just add score computation, which resulted in a very clean implementation.\n{quote}\n\nOK... let's stick with this approach for now.  Since the impl is\nlocked down (ctor for TopFieldCollector is private) we can freely\nswitch up this API in the future without breaking back compat, if we\nwant to optimize not passing/copying around the unused score.\n\nCan't the scoring collector impls in TopFieldCollector be final?\n\nbq. Since we pass Sorer to Collector, I thought we cannot really rely on anyone not calling scorer.doc() or getSimilarity ever\n\nMaybe instead make InternalScorer non-static, and then doc() can\nreturn the doc from the TermScorer instance, instead of having to copy\n\"s.d = doc\" each time?  score can do a similar thing.\n\nActually, hang on: if I'm using a Collector that doesn't need the\nscore, TermScoring is still computing it?  We don't want that right?\nCan we simply pass \"this\" to setScorer(...)?\n\nbq. If we add to Searcher a concrete impl which throws UOE, how would that work in 3.0? How would anyone who extends Searcher know that it has to extend this method? Maybe do it now, and document that in 3.0 it will become abstract again?\n\nOK let's do that?\n\nbq. About Searchable, I wonder how many do implement Searchable, rather than extend IndexSearcher. Perhaps instead of making any changes in back-compat and add documentation to CHANGES I'll just comment out this method with a TODO to re-enstate in 3.0?\n\nOK.\n\nMake sure at the end of all of this, you open a new issue, marked as\nfix version 3.0, that has all the \"and then on 3.0 we do XYZ\"s from\nthis.",
            "date": "2009-04-03T17:38:07.852+0000",
            "id": 63
        },
        {
            "author": "Shai Erera",
            "body": "bq. Can't the scoring collector impls in TopFieldCollector be final?\n\nThey can, but they are private so they cannot be extended anyway. I can do that, but does it really matter?\n\nbq. We don't want that right? Can we simply pass \"this\" to setScorer(...)?\n\nThat's what I wanted to do, but then noticed that TermScorer.score() method is a bit different. However, now that I look at it again, I wonder if they are different. The difference is that in score(), it does at the end\n{code}\nreturn raw * Similarity.decodeNorm(norms[doc]);\n{code}\nand in score(Collector, int) it does\n{code}\nfloat[] normDecoder = Similarity.getNormDecoder();\n...\nscore *= normDecoder[norms[doc] & 0xFF];\n{code}\n\nLooking in Similarity.decodeNorm, it does exactly what's done in score(Collector, int). So I guess this code has been duplicated for no good reason? Please validate what I wrote and if you also agree, I can change the entire method (score(Collector, int)) to not compute any score and call c.setScorer(this). That will solve it.\n\nSo are you ok with passing Scorer to Collector, instead of just a class with a single score() method?\n\nI will open an issue w/ a fix version 3.0 and take care of all those TODOs. Should the issue also get rid of the deprecated methods? Or will we have a general issue in 3.0 that removes all deprecated methods?\n",
            "date": "2009-04-03T18:25:58.104+0000",
            "id": 64
        },
        {
            "author": "Shai Erera",
            "body": "BTW Mike - I think the accidental changes to Searchable and Searcher could have been easily detected by test-tags if we had classes in the back-compat tag which implemented interfaces / extended abstract classes with empty implementations. These are not really junit tests, but if someone would have changed an interface or abstract class, then attempting to compile the test package against the trunk would fail.\n\nIt is not so relevant now, since the next release is 2.9 following by a 3.0 and back-compat will completely go away in 3.0, but perhaps post 3.0? Also, it will prevent us from making changes to back-compat like we wanted to in this issue, but perhaps it's good?\n",
            "date": "2009-04-03T18:39:37.641+0000",
            "id": 65
        },
        {
            "author": "Michael McCandless",
            "body": "\nI ran a \"first do no harm\" perf test, comparing trunk with this patch:\n\n||query||sort||hits||qps||qpsnew||pctg||\n|147|score|   6953|3631.1|3641.8|  0.3%|\n|147|title|   6953|2916.7|2255.6|-22.7%|\n|147|doc|   6953|3251.2|2676.8|-17.7%|\n|text|score| 157101| 208.1| 202.1| -2.9%|\n|text|title| 157101|  96.7|  84.8|-12.3%|\n|text|doc| 157101| 174.0| 115.2|-33.8%|\n|1|score| 565452|  58.0|  56.4| -2.8%|\n|1|title| 565452|  44.5|  34.1|-23.4%|\n|1|doc| 565452|  49.2|  32.8|-33.3%|\n|1 OR 2|score| 784928|  14.1|  13.7| -2.8%|\n|1 OR 2|title| 784928|  12.5|  11.5| -8.0%|\n|1 OR 2|doc| 784928|  13.0|  11.9| -8.5%|\n|1 AND 2|score| 333153|  15.5|  15.5|  0.0%|\n|1 AND 2|title| 333153|  14.8|  13.7| -7.4%|\n|1 AND 2|doc| 333153|  15.2|  14.2| -6.6%|\n\nLooks like:\n \n  * Sort by relevance got maybe a tad slower (~3%)\n\n  * Sort by field is now quite a bit slower (23-33% on term query '1')\n\nThis was on a full wikipedia index, with 14 segments, Sun java\n1.6.0_07 on OS X Mac Pro quad core, on Intel X25M 160 GB\nSSD.\n\nI think we need to iterate some to try to get some performance back.\n",
            "date": "2009-04-03T19:01:12.562+0000",
            "id": 66
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\n> Can't the scoring collector impls in TopFieldCollector be final?\n\nThey can, but they are private so they cannot be extended anyway. I can do that, but does it really matter?\n{quote}\n\nI was thinking in case it eeks performance.\n\nbq. So I guess this code has been duplicated for no good reason?\n\nDuplicated for performance I think.\n\nbq. I can change the entire method (score(Collector, int)) to not compute any score and call c.setScorer(this). That will solve it.\n\nI think we should try this?\n\nbq. So are you ok with passing Scorer to Collector, instead of just a class with a single score() method?\n\nGood question... I'm not sure.  It would be cleaner to expose only\nscore() (and I think we could add methods over time), but then we'll\nbe creating new instance per segment per search which'll only slow\nthings down.\n\nbq. I will open an issue w/ a fix version 3.0 and take care of all those TODOs. Should the issue also get rid of the deprecated methods? Or will we have a general issue in 3.0 that removes all deprecated methods?\n\nYou don't need to enumerate deprecated methods to get rid of -- we\nwon't forget those ones :) It's these other \"special\" tasks that may\nslip through the cracks.\n",
            "date": "2009-04-03T19:21:03.675+0000",
            "id": 67
        },
        {
            "author": "Michael McCandless",
            "body": "bq. BTW Mike - I think the accidental changes to Searchable and Searcher could have been easily detected by test-tags if we had classes in the back-compat tag which implemented interfaces / extended abstract classes with empty implementations. These are not really junit tests, but if someone would have changed an interface or abstract class, then attempting to compile the test package against the trunk would fail.\n\nI think that's a great idea!  Every interface/abstract class should\nhave a \"just compile me\" subclass in the tests.\n\nbq. It is not so relevant now, since the next release is 2.9 following by a 3.0 and back-compat will completely go away in 3.0, but perhaps post 3.0?\n\nIt is relevant because neither Searchable nor Searcher are deprecated\n(yet)?  Ie during development of 2.9 and of 3.0 we have to ensure we\ndon't break back compat of non-deprecated APIs.\n\nSo maybe fold this in on the next patch iteration?\n\nbq. Also, it will prevent us from making changes to back-compat like we wanted to in this issue, but perhaps it's good?\n\nIt's good, because it'd raise the issue right way vs us catching it or\nnot by staring at the code :)\n",
            "date": "2009-04-03T19:24:57.607+0000",
            "id": 68
        },
        {
            "author": "Jason Rutherglen",
            "body": "Something related to time limiting collectors we may want to\nsolve (maybe not in this patch) is passing the time limiting to\nthe sub-scorers. At the hit collector level the sub-scorers of a\nmulti clause query could be busy exceeding the time limit before\nreturning the first doc hit?",
            "date": "2009-04-03T20:02:31.311+0000",
            "id": 69
        },
        {
            "author": "Shai Erera",
            "body": "\n\nHow do I run such a test? Is there an algorithm for that in the benchmark\npackage?\n\nI compared the new TSDC to the trunk's version and the new code does ('-'\nmeans a negative change, '+' means a positive change, '|' means\nneither/undetermined):\n* adds one collector.setScorer() call to each query. (-)\n* The scorer.score() call in collect() was just moved from whoever called\ncollect() to inside collect(), so I don't think there's a difference. (|)\n* Does not check if score > 0.0f in each collect (+)\n* implements the new topDocs() method. Previously, it just implemented\ntopDocs() which returned everything. Now, topDocs() calls topDocs(0,\npq.size()), which verifies parameters and such - since that's executed once\nat the end of the search, I doubt that it has any effect major effect on the\nresults.\n\nBTW, as I scanned through the code I noticed that previously TSDC returned\nmaxScore = Float.NEGATIVE_INFINITY in case there were 0 results to the\nquery, and now it returns Float.NaN. I'm not sure however if this breaks\nanything, since maxScore is probably used (if at all) for normalization of\nscores, and in case there are 0 results you don't really have anything to\nnormalize? However I'm not sure ...\n\nRegarding TopFieldDocs I am quite surprised. I assume the test uses the\nOneComparatorScoringCollector, which means scores are computed:\n* It has the same issue as in TSDC regarding topDocs(). So I think it should\nbe changed here as well, however I doubt that's the cause for the\nperformance hit.\n* It computes the score and then does super.collect(), which adds a method\ncall (-)\n* It doesn't check if the score is > 0 (+)\n* It calls comparator.setScorer, which is ignored in all comparators besides\nRelevanceComparator. Not sure if it has any performance effects (|)\nThe rest of the code in collect() is exactly the same.\n\nCan it be that super.collect() has such an effect? When I think on the\nresults of TSDC (-3%) vs. TFC (-28% on avg.), I think it might be since\nsetScorer() is called once before the series of collect() calls, however\nsuper.collect() is called for every document. Your index is large (>2M\ndocuments, right?) and I don't know how many results are for each query, if\nthey are in the range of 100Ks, then that could be the explanation.\n\nMike - in case it's faster for you to run it, can you try to run the test\nagain with a change in the code which inlines super.collect() into\nOneComparatorScoringCollector and compare the results again? I will run it\nalso after you tell me which algorithm you used, but only tomorrow morning,\nso if you get to do it before then, that'd be great.\n\nI doubt that the change in topDocs() affects the query time that much, since\nit's called at the end of the search, and doing 4-5 'if' statements is\nreally not that expensive (I mean once per the entire search), comparing to\nScoreDoc[] array allocation, fetching Stored fields from the index etc. So\nI'd hate to implement all 3 topDocs() in each of the TopDocsCollector\nextensions unless it proves to be a problem.\n\nShai\n\nOn Fri, Apr 3, 2009 at 10:02 PM, Michael McCandless (JIRA)\n<jira@apache.org>wrote:\n\n",
            "date": "2009-04-03T20:59:14.075+0000",
            "id": 70
        },
        {
            "author": "Shai Erera",
            "body": "BTW, I can change FieldValueHitQueue like I changed TopFieldCollector by\nintroducing a factory create() method which will return a\nOneComparaterFieldValueHitQueue and MultiComparatorsFieldValueHitQueue.\nToday, FVHQ.lessThan checks the numComparators in each call, which is\nredundant.\n\nAlso the class isn't final and I'm not sure if we want to change it.\n\nWhat do you think?\n\n\n",
            "date": "2009-04-03T21:31:14.536+0000",
            "id": 71
        },
        {
            "author": "Shai Erera",
            "body": "Mike - about your comments on the new Searcher and Searchable search(Weight, Filter, Collector). I think that best (if not only) option currently is to remove them from the interface (comment out I mean) with a TODO to add in 3.0.\n\nI tried to just comment out in Searchable, and empty impl in Searcher which throws UOE. However that caused a problem in in MultiSearcher, ParallelMultiSearcher and RemoteSearchable:\n* RemoteSearchable impls Searchable - commenting out the new impl method with a TODO for 3.0 will be fine, but\n* MS and PMS accept Searchable in their ctor and use them in search(W, F, C) which they extend from Searcher (they MUST extend it because Searcher's throws UOE). However they call searchable.search, which accepts just a HC, and we can't wrap a Collector with a HC.\n\nPreviously, MS and PMS implemented the HC version by always wrapping with a MRHC. I think we should just pass in the given HC to the Searchable.search method, and rely on its wrapping by a HCW later on. In 3.0 we'll delete it entirely and use the Collector implementation.\n\nDo you see any other way?",
            "date": "2009-04-04T06:07:27.453+0000",
            "id": 72
        },
        {
            "author": "Shai Erera",
            "body": "Oh wait .. I should have tried to implement it before I sent the last email.\n\nAfter I tried to implement it, I noticed that commenting out the\nSearcher.search(W, F, C) method creates a chain of compilation errors, since\nall the HC methods now call the Collector one (actually all the search()\nmethods call that one eventually). So I'm not sure it's a good idea to\ncomment it out. I thought to comment out all the Collector search methods in\nSearcher, but then it resulted in compilation errors in other places.\n\nHow problematic is this break in back-compat, given it will be documented in\nCHANGES?\n* Have search(W, F, C) on Searchable? I don't think it will have such a\ngreat impact as I don't believe too many actually implement Searchable.\n* Have search(W, F, C) on Searcher as abstract? I know you offered, Mike, to\ncreate an empty impl which throws UOE, but I'm not sure what's worse: having\na compilation error or UOE at runtime (which can happen at the customer's).\nAfter all, all the search methods call this one eventually, and if you did\nextend Searcher (rather than IndexSearcher), you'll get UOE on every search.\n\n\n",
            "date": "2009-04-04T06:51:14.076+0000",
            "id": 73
        },
        {
            "author": "Michael McCandless",
            "body": "I'm attaching the Python scripts I use to run the tests.  You also need this small mod:\n\n{code}\nIndex: contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java\n===================================================================\n--- contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java\t(revision 761709)\n+++ contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/tasks/ReadTask.java\t(working copy)\n@@ -63,6 +63,9 @@\n     super(runData);\n   }\n \n+  // nocommit\n+  static boolean first = true;\n+\n   public int doLogic() throws Exception {\n     int res = 0;\n     boolean closeReader = false;\n@@ -101,6 +104,11 @@\n         } else {\n           hits = searcher.search(q, numHits);\n         }\n+        // nocommit\n+        if (first) {\n+          System.out.println(\"NUMHITS=\" + hits.totalHits);\n+          first = false;\n+        }\n         //System.out.println(\"q=\" + q + \":\" + hits.totalHits + \" total hits\"); \n \n         if (withTraverse()) {\n{code}\n\nAll the python scripts do is write an alg, run it, gather the results, and collate in the end.  You run sortBench5.py once on trunk and once in a checkout with this patch, each time in the contrib/benchmark directory.  It saves a pickle file (results.pk) which sortCollate5.py then loads (you'll have to edit the hardwired paths in sortCollate5.py).",
            "date": "2009-04-04T09:40:58.689+0000",
            "id": 74
        },
        {
            "author": "Michael McCandless",
            "body": "bq. adds one collector.setScorer() call to each query. \n\nActually, it's one setScorer() call per segment (my index had 14\nsegments).  But I can't imagine this slows things down.\n\nbq. The scorer.score() call in collect() was just moved from whoever called collect() to inside collect(), so I don't think there's a difference. (|)\n\nI would think so, though, maybe the compiler can no longer inline the\ncall.  We may need to peek at the asm.\n\nbq. Does not check if score > 0.0f in each collect implements the new topDocs() method.\n\nYes, should be a speedup.\n\n{quote} \nPreviously, it just implemented topDocs() which returned everything. Now, topDocs() calls topDocs(0,\npq.size()), which verifies parameters and such - since that's executed once\nat the end of the search, I doubt that it has any effect major effect on the\nresults.\n{quote}\n\nI agree.\n\n{quote} \nBTW, as I scanned through the code I noticed that previously TSDC returned\nmaxScore = Float.NEGATIVE_INFINITY in case there were 0 results to the\nquery, and now it returns Float.NaN. I'm not sure however if this breaks\nanything, since maxScore is probably used (if at all) for normalization of\nscores, and in case there are 0 results you don't really have anything to\nnormalize? However I'm not sure ...\n{quote}\n\nHmm good question.  Float.NAN seems more correct, so let's stick with\nthat?\n\n{quote} \nRegarding TopFieldDocs I am quite surprised. I assume the test uses the\nOneComparatorScoringCollector, which means scores are computed:\n{quote}\n\nYes, and we need to test multiple field sorting too. \t\n\n{quote} \nIt has the same issue as in TSDC regarding topDocs(). So I think it should\nbe changed here as well, however I doubt that's the cause for the\nperformance hit.\n{quote}\n\nWhat should be changed here?\n\nbq. It computes the score and then does super.collect(), which adds a method call \n\nYes.\n\nbq. It doesn't check if the score is > 0 \n\nShould be faster.\n\nbq. It calls comparator.setScorer, which is ignored in all comparators besides RelevanceComparator. Not sure if it has any performance effects (|)\n\nShouldn't have an effect.\n\n{quote} \nThe rest of the code in collect() is exactly the same.\nCan it be that super.collect() has such an effect? When I think on the\nresults of TSDC (-3%) vs. TFC (-28% on avg.), I think it might be since\nsetScorer() is called once before the series of collect() calls, however\nsuper.collect() is called for every document. Your index is large (>2M\ndocuments, right?) and I don't know how many results are for each query, if\nthey are in the range of 100Ks, then that could be the explanation.\n{quote} \n\nThe table shows number of hits per query -- query \"1\" is large.\n\n{quote} \nMike - in case it's faster for you to run it, can you try to run the test\nagain with a change in the code which inlines super.collect() into\nOneComparatorScoringCollector and compare the results again? I will run it\nalso after you tell me which algorithm you used, but only tomorrow morning,\nso if you get to do it before then, that'd be great.\n{quote}\n\nI'll test this.\n\n{quote}\nI doubt that the change in topDocs() affects the query time that much, since\nit's called at the end of the search, and doing 4-5 'if' statements is\nreally not that expensive (I mean once per the entire search), comparing to\nScoreDoc[] array allocation, fetching Stored fields from the index etc. So\nI'd hate to implement all 3 topDocs() in each of the TopDocsCollector\nextensions unless it proves to be a problem.\n{quote}\n\nI also cannot imagine this is a problem.  It's the slowdown of big\nsearches that spooks me (it's fine if fast searches pick up some added\ncost).\n",
            "date": "2009-04-04T09:49:15.335+0000",
            "id": 75
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nBTW, I can change FieldValueHitQueue like I changed TopFieldCollector by\nintroducing a factory create() method which will return a\nOneComparaterFieldValueHitQueue and MultiComparatorsFieldValueHitQueue.\n\nToday, FVHQ.lessThan checks the numComparators in each call, which is\nredundant.\n{quote}\n\nSeems good, unless the extra subclassing (and additions of super.XXX()) is somehow cause our performance loss.\n\nbq. Also the class isn't final and I'm not sure if we want to change it.\n\nYes let's make it final.  We need to eek...\n",
            "date": "2009-04-04T09:51:50.469+0000",
            "id": 76
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nHow problematic is this break in back-compat, given it will be documented in\nCHANGES?\n\n* Have search(W, F, C) on Searchable? I don't think it will have such a\ngreat impact as I don't believe too many actually implement Searchable.\n\n* Have search(W, F, C) on Searcher as abstract? I know you offered, Mike, to\ncreate an empty impl which throws UOE, but I'm not sure what's worse: having\na compilation error or UOE at runtime (which can happen at the customer's).\nAfter all, all the search methods call this one eventually, and if you did\nextend Searcher (rather than IndexSearcher), you'll get UOE on every search.\n{quote}\n\nOK let's add both and call it out in CHANGES.txt?",
            "date": "2009-04-04T09:54:26.181+0000",
            "id": 77
        },
        {
            "author": "Shai Erera",
            "body": "There are no super.XXX calls. The two FVHQ implementations just implement\nlessThan according to whether it's a single comparator or muli case. This\nremoves the check of numComparators == 1.\n\n\nOn Sat, Apr 4, 2009 at 12:53 PM, Michael McCandless (JIRA)\n\n",
            "date": "2009-04-04T11:39:17.438+0000",
            "id": 78
        },
        {
            "author": "Shai Erera",
            "body": "bq. OK let's add both and call it out in CHANGES.txt?\n\ngreat. so I leave them as they are in the latest patch and add a note to CHANGES.\n\nbq. Yes let's make it final. We need to eek...\n\nThis isn't necessary after all, since the class is now abstract, with a private ctor and two private final internal classes, which will be the concrete objects returned by create().\n\nBefore submitting the next patch version, I'd like to verify if super.collect() in TFC is the cause of the perf. degradation. We should also perf. test sorting w/o score tracking and note if there is any improvement over trunk. I'm downloading the latest enwiki xml (20090306), so I hope that sometime tomorrow it will finish the download, extract, indexing and search tests.\n",
            "date": "2009-04-04T11:53:54.991+0000",
            "id": 79
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nThere are no super.XXX calls. The two FVHQ implementations just implement\nlessThan according to whether it's a single comparator or muli case. This\nremoves the check of numComparators == 1.\n{quote}\nExcellent!\n\nbq. Before submitting the next patch version, I'd like to verify if super.collect() in TFC is the cause of the perf. degradation\n\nI'll run this & post back.",
            "date": "2009-04-04T13:55:46.954+0000",
            "id": 80
        },
        {
            "author": "Michael McCandless",
            "body": "\nOdd -- inlining super.collect into OCSC, and making OCSC final, did not alter the numbers much (I re-ran trunk baseline to confirm its close to prior trunk baseline):\n\n||query||sort||hits||qps||qpsnew||pctg||\n|147|score|   6953|3635.8|3650.1|  0.4%|\n|147|title|   6953|2915.7|2297.6|-21.2%|\n|147|doc|   6953|3265.6|2665.8|-18.4%|\n|text|score| 157101| 208.5| 202.9| -2.7%|\n|text|title| 157101|  97.0|  85.4|-12.0%|\n|text|doc| 157101| 174.3| 125.0|-28.3%|\n|1|score| 565452|  58.2|  56.6| -2.7%|\n|1|title| 565452|  44.6|  34.6|-22.4%|\n|1|doc| 565452|  49.2|  35.2|-28.5%|\n|1 OR 2|score| 784928|  14.1|  13.7| -2.8%|\n|1 OR 2|title| 784928|  12.6|  11.5| -8.7%|\n|1 OR 2|doc| 784928|  13.0|  11.9| -8.5%|\n|1 AND 2|score| 333153|  15.6|  15.5| -0.6%|\n|1 AND 2|title| 333153|  14.8|  13.7| -7.4%|\n|1 AND 2|doc| 333153|  15.2|  14.2| -6.6%|\n",
            "date": "2009-04-04T14:35:03.675+0000",
            "id": 81
        },
        {
            "author": "Michael McCandless",
            "body": "bq. We should also perf. test sorting w/o score tracking and note if there is any improvement over trunk.\n\nLet's wait a bit until we sort things out (eg, w/ current patch, TermScorer will still compute its score even if I don't need it).",
            "date": "2009-04-04T14:38:02.001+0000",
            "id": 82
        },
        {
            "author": "Michael McCandless",
            "body": "Shai can you post your latest patch, where TermScorer itself is passed down to the collector?",
            "date": "2009-04-04T14:57:10.582+0000",
            "id": 83
        },
        {
            "author": "Shai Erera",
            "body": "- Changed TermScorer.score() method to not call Similarity.decodeNorm. If we can change Scorer.similarity to be protected, we can give up getSimilarity() call in score(). Also changed TermScorer.score(Collector) to set 'this' as the collector's scorer.\n- Deprecated TimeLimitedCollector, created new TimeLimitingCollector, renamed TestTimeLimitedCollector to TestTimeLimitingCollector and used the new TimeLimitingCollector.\n- Changed FVHQ to have a static create which returns One/MultiComparatorFieldValueHitQueue version.\n- Changed TopFieldCollector setNextReader versions to not call pq.size() but rather use numHits.\n",
            "date": "2009-04-04T17:29:36.977+0000",
            "id": 84
        },
        {
            "author": "Michael McCandless",
            "body": "OK thanks.  Numbers w/ new patch:\n\n||query||sort||hits||qps||qpsnew||pctg||\n|147|score|   6953|3635.8|3704.1|  1.9%|\n|147|title|   6953|2915.7|2262.9|-22.4%|\n|147|doc|   6953|3265.6|2655.1|-18.7%|\n|text|score| 157101| 208.5| 199.9| -4.1%|\n|text|title| 157101|  97.0|  87.1|-10.2%|\n|text|doc| 157101| 174.3| 134.6|-22.8%|\n|1|score| 565452|  58.2|  56.5| -2.9%|\n|1|title| 565452|  44.6|  35.3|-20.9%|\n|1|doc| 565452|  49.2|  38.0|-22.8%|\n|1 OR 2|score| 784928|  14.1|  13.8| -2.1%|\n|1 OR 2|title| 784928|  12.6|  11.6| -7.9%|\n|1 OR 2|doc| 784928|  13.0|  11.9| -8.5%|\n|1 AND 2|score| 333153|  15.6|  15.4| -1.3%|\n|1 AND 2|title| 333153|  14.8|  13.7| -7.4%|\n|1 AND 2|doc| 333153|  15.2|  14.2| -6.6%|\n",
            "date": "2009-04-04T19:24:33.583+0000",
            "id": 85
        },
        {
            "author": "Michael McCandless",
            "body": "Attached patch; only differences are:\n\n  * Under contrib/benchmark I made changes so you can specify non-scoring field sorting\n\n  * Fixed the rename of TestTimeLimitedCollector --> Limiting to be patch-friendly\n\nOK I ran performance with score tracking disabled during field sorted search:\n\n||query||sort||hits||qps||qpsnew||pctg||\n|147|title|   6953|2915.7|4043.3| 38.7%|\n|147|doc|   6953|3265.6|4840.1| 48.2%|\n|text|title| 157101|  97.0| 128.0| 32.0%|\n|text|doc| 157101| 174.3| 273.2| 56.7%|\n|1|title| 565452|  44.6|  60.2| 35.0%|\n|1|doc| 565452|  49.2|  75.3| 53.0%|\n|1 OR 2|title| 784928|  12.6|  14.8| 17.5%|\n|1 OR 2|doc| 784928|  13.0|  15.2| 16.9%|\n|1 AND 2|title| 333153|  14.8|  17.9| 20.9%|\n|1 AND 2|doc| 333153|  15.2|  18.9| 24.3%|\n\nVery nice speedups!  We just have to figure out why the score-tracking variant got slower...\n",
            "date": "2009-04-04T19:48:24.576+0000",
            "id": 86
        },
        {
            "author": "Shai Erera",
            "body": "Mike - I've been trying to see this performance hit, without indexing the full wikipedia content, but failed. What I did is:\n* Create an index with 10M documents with one term (all with the same term). The index had 51 segments (I deliberately aimed at a high number).\n* Run a sort-by-doc (in reverse order so that collect() will actually do some work) search using TermQuery (so that TermScorer will be used).\n* Measure the avg. time of 20 query executions (I deliberately omit the first execution because I've noticed that the first run sometimes take much longer, even after I kill the JVM. I guess it has to do with OS caches).\n\nThe time I measure between the trunk and the current version are very much comparable:\ntrunk-patched: (1) 1248.45 ms (2) 1255.45 ms\ntrunk-orig: (1) 1314.1 ms (2) 1265.65 ms\nIn both runs the patched trunk ran faster. Both did not have any large differences.\n\nHere's the code:\n{code}\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tFile indexDir = new File(args[0]);\n\t\tDirectory dir = new FSDirectory(indexDir, new NativeFSLockFactory(indexDir));\n\t\t\n\t\tif (!indexDir.exists() || indexDir.list().length == 0) {\n\t\t\tint numDocs = 10000000;\n\t\t\tIndexWriter writer = new IndexWriter(dir, null, MaxFieldLength.UNLIMITED);\n\t\t\twriter.setMergeFactor(50);\n\t\t\twriter.setMaxBufferedDocs(numDocs/100);\n\t\t\tSystem.out.println(\"populating index\");\n\t\t\tlong time = System.currentTimeMillis();\n\t\t\tfor (int i = 0; i < numDocs; i++) {\n\t\t\t\tDocument doc = new Document();\n\t\t\t\tdoc.add(new Field(\"c\", \"test\", Store.NO, Index.NOT_ANALYZED));\n\t\t\t\twriter.addDocument(doc);\n\t\t\t}\n\t\t\twriter.close(false);\n\t\t\tSystem.out.println(\"time=\" + (System.currentTimeMillis() - time));\n\t\t}\n\t\t\n\t\tSystem.out.println(\"searching\");\n\t\tIndexSearcher searcher = new IndexSearcher(dir);\n\t\tSystem.out.println(\"numSegments=\" + searcher.getIndexReader().getSequentialSubReaders().length);\n\t\tSort sort = new Sort(new SortField(null, SortField.DOC, true));\n\t\tsearcher.search(new TermQuery(new Term(\"c\", \"test\")), null, 10, sort);\n\t\tlong time = System.currentTimeMillis();\n\t\tdouble numQueries = 20;\n\t\tfor (int i = 0; i < numQueries; i++) {\n\t\t\tsearcher.search(new TermQuery(new Term(\"c\", \"test\")), null, 10, sort);\n\t\t}\n\t\ttime = System.currentTimeMillis() - time;\n\t\tSystem.out.println(\"avg. time=\" + (time / numQueries) + \" ms\");\n\t\tsearcher.close();\n\t}\n\n{code}\n\nCan you try running this code on your machine?",
            "date": "2009-04-05T12:44:10.262+0000",
            "id": 87
        },
        {
            "author": "Michael McCandless",
            "body": "OK I ran it like this:\n\n{code}\njava -Xms1024M -Xmx1024M -Xbatch -server -cp build/classes/java:../lucene.collection PerfTest /lucene/fake\n{code}\n\nAcross these OS/JRE versions:\n\n||OS||JRE||Trunk||Patch||%tg change||\n|Linux|1.6.0_10|1075 ms|1120 ms|-4.2%|\n|Linux|1.5.0_08|1156 ms|1199 ms|-3.7%|\n|OS X|1.6.0_07 (64bit)|774 ms|823 ms|-6.3%|\n|OS X|1.5.0_16|987 ms|936 ms|5.2%|\n|Win Svr 2003 (64bit)|1.6.0_11 (64bit)|1209 ms|1308 ms|-8.2%|\n|Win Svr 2003 (64bit)|1.5.0_14 (64bit)|1559 ms|1564 ms|-0.3%|\n\nShai which OS/JRE did you run on?\n\nExcept for the JRE 1.5 cases on OS X and Win Svr 2003, things seem\ngenerally slower with the patch, though I think the test is a little\ntoo synthetic (sorting by doc reversed is far worse than a \"normal\"\nfielded sort where I think the queue would typically fairly quickly\n\"converge\").  Also, I think it's better to take min rather than avg\nacross all the runs.\n",
            "date": "2009-04-05T16:04:59.916+0000",
            "id": 88
        },
        {
            "author": "Shai Erera",
            "body": "bq. Shai which OS/JRE did you run on?\n\nWindows XP, 1.5 (32 bit)\n\nI chose the sort by doc since I was very curious as to the major drop in performance your queries show. When you sort by doc, the queue converges exactly after 10 documents (assuming you ask for 10 results). After that, every document that's collected fails at the comparator level (first line in the \"if (queueFull)\" code) and the collector literally does nothing. So all that's left are the calls to scorer.score().\nI chose to sort by docs reversed so that every call will actually do something.\nI measured scorer.score() calls in OCSC and they were insignificant (~14 ms total on a single query run).\n\nbq. Also, I think it's better to take min rather than avg across all the runs.\n\nI don't know. Some runs showed 112 ms and some 256 ms ... Taking the minimum just measures \"when the OS cache is fully optimized for your run\" case, that's why I prefer to take avg. Anyway, I don't think it matters since we're taking the same for both runs.\n\nI reviewed the code in TopFieldCollector and it's almost identical to before the patch, besides I differences I noted above, which don't seem like to have an effect. That's why I wanted to have a synthetic test, which we know exactly what it'll do.\n\nBTW, this test shows far smaller drop in performance comparing to your previous sort-by-doc runs, and it does collect ~x20 times more documents ...",
            "date": "2009-04-05T17:21:56.444+0000",
            "id": 89
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\n> Shai which OS/JRE did you run on?\n\nWindows XP, 1.5 (32 bit)\n{quote}\n\nDid you run with -server, -Xbatch, -Xms/-Xmx set to same value, etc?\nThe goal is to minimize any source of noise in the measurement.\n\nI also got mixed results on 1.5...\n\nbq. I chose the sort by doc since I was very curious as to the major drop in performance your queries show. When you sort by doc, the queue converges exactly after 10 documents (assuming you ask for 10 results). After that, every document that's collected fails at the comparator level (first line in the \"if (queueFull)\" code) and the collector literally does nothing. So all that's left are the calls to scorer.score().\n\nI agree, I think sort by doc is overly easy, but sort by doc reversed\nis overly hard.  I think sort by title & docdate (in wikipedia) are\nreasonable.  Let's just leave doc entirely out of future wikipedia\ntests.\n\nbq. I measured scorer.score() calls in OCSC and they were insignificant (~14 ms total on a single query run).\n\nThat's very odd, because I see big speedups when we don't score.\n\n{quote}\n> Also, I think it's better to take min rather than avg across all the runs.\n\nI don't know. Some runs showed 112 ms and some 256 ms ... Taking the minimum just measures \"when the OS cache is fully optimized for your run\" case, that's why I prefer to take avg. Anyway, I don't think it matters since we're taking the same for both runs.\n{quote}\n\nIf you really had such wide variance on your runs (I didn't), it's\neven more important to use min not avg.  It means there's alot of\nnoise in your measurement.\n\nFor this test we do want full OS caching, no page misses, no daemon\nthat wakes up a steals a few cycles, etc.  This is why min, across\nmany iterations, is good: it makes the numbers less noisy and more\ncomparable.  A random unlucky outlier won't increase the time.\n\nTrue, min is not a good way to measure what you'll typically see when\nrunning such searches in your app (the real world has lots of noise),\nbut that's not what we're after here.\n\nbq. I reviewed the code in TopFieldCollector and it's almost identical to before the patch, besides I differences I noted above, which don't seem like to have an effect. That's why I wanted to have a synthetic test, which we know exactly what it'll do.\n\nI think it may be the switch to an abstract Scorer -- the JIT can no\nlonger inline that code (maybe).\n\nOr maybe it's the switch to subclassing-not-if for the 4 impls of\nTopFieldCollector?\n\nbq. BTW, this test shows far smaller drop in performance comparing to your previous sort-by-doc runs, and it does collect ~x20 times more documents ...\n\nYeah but we changed alot of things.  I think sort by title, docdate\nare more realistic tests.  It is nice that your test is ~20X\nlarger... I wish we had a real standard dataset that large, and real\nqueries instead of the few specific ones we are testing, would be\nbetter.\n\n",
            "date": "2009-04-05T19:26:56.509+0000",
            "id": 90
        },
        {
            "author": "Michael McCandless",
            "body": "OK I modified your test (attached):\n\n  * Make a random int field and sort on that\n\n  * Switch to System.nanoTime to measure time\n\n  * Print min time in addition to avg\n\n  * Gather silly sum just to make sure JRE can't optimize away things\n\nHere are the results:\n\n||OS||JRE||Trunk||Patch||%tg||\n|OS X|1.5.0_16|389|334|14.1%|\n|OS X|1.6.0_07 (64 bit)|312|430|-27.4%|\n|Linux|1.5.0_08|403|337|16.4%|\n|Linux|1.6.0_10|337|303|10.1%|\n|Win Svr 2003 (64 bit)|1.5.0_14 (64 bit)|535|727|-36.6%|\n|Win Svr 2003 (64 bit)|1.6.0_11 (64 bit)|477|682|-43.0%|\n\nNow I'm even more baffled.  The Win Svr 2003 times became especially\nawful... in fact it's as if 64-bit JREs don't like this change.\n\nOr we may simply be chasing Java ghosts at this point... though these\nare awfully big ghosts.\n",
            "date": "2009-04-05T19:30:38.788+0000",
            "id": 91
        },
        {
            "author": "Mark Miller",
            "body": "Gave the bench a try here on 64-bit linux with open-jdk 6 and sun-java 5.\n\nI did one throw away run with the new patch version, and then recorded the times for both - got 30% drop with the new patch.\n\nReran both many times after and got approx the same times for both every attempt, using both JRE's mentioned above.\n\nDunno know what that implies, but another data point.\n\n- Mark",
            "date": "2009-04-06T02:10:16.333+0000",
            "id": 92
        },
        {
            "author": "Shai Erera",
            "body": "bq. Reran both many times after and got approx the same times for both every attempt, using both JRE's mentioned above.\n\nJust for verification - by \"same time\" did you mean that your successive runs show ~X ms for both patched and non-patched trunk, or by \"same time\" you meant that the successive runs show -30% drop with the patch?",
            "date": "2009-04-06T02:42:57.888+0000",
            "id": 93
        },
        {
            "author": "Mark Miller",
            "body": "As in, the 30% drop appeared to be an aberration. Both patched and un-patched performed the same on subsequent runs. It appears there is not a loss on my setup - or its very small if there is.",
            "date": "2009-04-06T02:48:04.738+0000",
            "id": 94
        },
        {
            "author": "Shai Erera",
            "body": "Yes, that's what I saw too. I will run the test with both 1.5 and 1.6, 32/64 bit versions and post the results.\n\nBTW, if you look at Mike's table above, it's a black and white thing: the 1.5 JRE really like this patch and 1.6 really hate it. Maybe we should not move to 1.6 then? ;) (kidding)",
            "date": "2009-04-06T02:56:47.615+0000",
            "id": 95
        },
        {
            "author": "Shai Erera",
            "body": "I wasn't able to run the test on 64-bit JRE. Here are the results on 32-bit JREs:\n\n||OS||JRE||Trunk||Patch||%tg\n|XP|IBM 1.5| 573 | 571 | {color:green}0.34%{color}\n| XP | 1.6.07 (32 bit) | 752 | 804 | {color:red}-6.4 %{color}\n|SRV 2003| IBM 1.5 | 530/469 | 536/493 | {color:green}1%{color}/{color:red}-4.86%{color}\n|SRV 2003| 1.6.07 (32 bit) | 858 | 699 | {color:green}22.7%{color}\n\nI ran each twice, and just in the SRV-2003-1.5 case there were differences between the two runs. Also, it's important to notice that unlike Mike's results, the SRV2003-JRE1.6 run had 22.7% improvement with the patched version. I re-ran the 2003 runs a couple of times and the results were consistent.",
            "date": "2009-04-06T09:09:36.146+0000",
            "id": 96
        },
        {
            "author": "Michael McCandless",
            "body": "Mark and Shai, are you guys using the last version of the bench (that sorts by random int field)?  Are you using the \"best\" time for your results?  How are you launching the JRE?\n\nbq. BTW, if you look at Mike's table above, it's a black and white thing: the 1.5 JRE really like this patch and 1.6 really hate it. Maybe we should not move to 1.6 then?\n\nActually, for my run on Linux, the patch was faster for both 1.5 & 1.6 JREs.",
            "date": "2009-04-06T09:47:58.056+0000",
            "id": 97
        },
        {
            "author": "Shai Erera",
            "body": "Added JustCompileSearch, JustCompileSearchFunction and JustCompileSearchSpans that extend/implement all abstract classes/interfaces in o.a.l.s, o.a.l.s.s and o.a.l.s.f. Those are not unit tests per-sei, however if anyone will change the interfaces/abstract classes in a way that it breaks back-compat, we'll know it right away. I think that in general this is something good to have for Lucene overall, however I only took care of the search.* packages in this patch.",
            "date": "2009-04-06T09:53:03.296+0000",
            "id": 98
        },
        {
            "author": "Shai Erera",
            "body": "I'm using the latest version which sorts by that random field (the output includes the prints of best, avg. and sum, so I'm sure of that). Also, the times I reported are the 'best' time. I launch the JRE like you posted with those args: \"-Xms1024M -Xmx1024M -Xbatch -server\".\n\nI reran now, and the results are consistent.",
            "date": "2009-04-06T10:00:50.311+0000",
            "id": 99
        },
        {
            "author": "Mark Miller",
            "body": "I just used the defaults for cmd line - I can give it another go ensuring server and more RAM. I used the latest perf code provided by Mike and the latest patch.\n\nI didn't look at the numbers too closely - my plan was to do a quick profile with each, but eyeballing runs with each over and over, they were approx the same (both best and avg), so I skipped the profiling.\n",
            "date": "2009-04-06T13:47:04.302+0000",
            "id": 100
        },
        {
            "author": "Michael McCandless",
            "body": "\nI ran 2 more JREs under linux:\n\n||OS||JRE||Trunk||Patch||%tg||\n||Linux|1.7.0 ea|333 ms|320 ms|{color:green}3.9%{color}|\n||Linux|IBM JRE 1.5.0|401 ms|352 ms|{color:green}12.2%{color}|\n",
            "date": "2009-04-06T14:25:12.335+0000",
            "id": 101
        },
        {
            "author": "Shai Erera",
            "body": "So how do we proceed? It looks like we get inconsistent results, sometimes over the same OS and JRE, just different machine. Perhaps the test is too synthetic, although it does capture the essence of the changes. Mike, can you post your Wikipedia index somewhere so I can download and run your previous queries and compare the results?",
            "date": "2009-04-06T15:54:32.371+0000",
            "id": 102
        },
        {
            "author": "Michael McCandless",
            "body": "bq. So how do we proceed?\n\nThe results are definitely highly varying...\n\nIt seems like I'm the only one seeing sizable performance loss with the patch,\nand then only with 64bit JREs (on OS X and Windows Server 2004 x64).\n\nMark when you saw no performance loss on  64 bit linux, was the JRE\n64 bit?\n\nIf so, then maybe we should simply proceed with the patch as is.\nThese differences are clearly java ghosts and there's not much we can\ndo about that....\n\nThe index is a little too large (2.6G) to schlepp around -- instead,\nhere's the alg I used to create it:\n\n{code}\nanalyzer=org.apache.lucene.analysis.standard.StandardAnalyzer\n\ndoc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker\n\nmerge.policy=org.apache.lucene.index.LogDocMergePolicy\n\ndocs.file=/Volumes/External/lucene/wiki.txt\ndoc.stored = false\ndoc.term.vector = false\ndoc.add.log.step=1000\nmax.field.length=2147483647\n\ndirectory=FSDirectory\nautocommit=false\ncompound=false\nram.flush.mb = 128\ndoc.maker.forever = false\n\nwork.dir=/lucene/work\n\n{ \"Rounds\"\n  ResetSystemErase\n  { \"BuildIndex\"\n    - CreateIndex\n     { \"AddDocs\" AddDoc > : *\n    - CloseIndex\n  }\n  NewRound\n} : 1\n\nRepSumByPrefRound BuildIndex\n{code}\n",
            "date": "2009-04-06T16:27:33.992+0000",
            "id": 103
        },
        {
            "author": "Mark Miller",
            "body": "Yes, both 64-bit versions - openjdk 6 and sun java 1.5. I appeared to be getting the same results with both jvm's and patched or not. I figured I'd try a bit of profiling, since I have a 64-bit setup, but doesnt appear I'd learn much. I'm going to try a bit more testing tonight for the heck of it - I've got sun 1.6 and a 32-bit 1.5 I could check with as well.",
            "date": "2009-04-06T17:07:11.505+0000",
            "id": 104
        },
        {
            "author": "Michael McCandless",
            "body": "Attached new patch:\n\n  * Changed members & methods in TopFieldCollector from \"protected\" to\n    package-private.\n\n  * Tweaked javadocs, CHANGES.txt\n\n  * Removed some dead code, nocommits\n\n  * Re-added TestTimeLimitedCollector\n\nBesides the java ghosts, for which we will close our eyes and hope\nthey disappear, I think this is ready to go in!\n\nI'll way a few days and then commit.\n",
            "date": "2009-04-07T10:08:10.460+0000",
            "id": 105
        },
        {
            "author": "Michael McCandless",
            "body": "New patch which just fixes contrib/spatial's cutover to the new API to further cutover to the new new API.",
            "date": "2009-04-07T17:25:50.369+0000",
            "id": 106
        },
        {
            "author": "Michael McCandless",
            "body": "I wonder if we should break out tracking of max score, which is far more costly, from tracking scores of hits inserted into the queue?\n\nTypically the number of inserts is very low (ie, the queue \"converges\" quickly) and so only track scores of inserted hits would be much lower cost than also tracking max score.",
            "date": "2009-04-08T15:13:46.491+0000",
            "id": 107
        },
        {
            "author": "Shai Erera",
            "body": "That's actually what's done in TopScoreDocCollector. maxScore is not tracked at all, and only when topDocs() is called, maxScore is set to the largest element in the queue. It has some overhead in topDocs() (might require a pop of all elements, even if the range is \"give me the last 10\"), which I personally don't care about since it happens once per search.\nmaxScore is only tracked in TopFieldCollector, which indeed adds several instructions to each collect, including a call to Math.max.\nWere you thinking along those lines? If so, I can remove maxScore tracking from TFC and compute it in topDocs only.\n\nAnother idea, which will prevent popping out all the elements from the queue just to compare maxScore is to actually make it a method of TopDocsCollector (e.g., TDC.maxScore()). It will be the responsibility of the implementation (with TDC providing a base for all) to ensure that if the largest element is extracted, maxScore is set (so that we don't lose it). The reason I'm thinking about it is that because maxScore is probably used only for normalization, and if a certain application never uses it, why should we compute it?\n\nWhat do you think?",
            "date": "2009-04-08T15:32:18.865+0000",
            "id": 108
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. maxScore is only tracked in TopFieldCollector, which indeed adds several instructions to each collect, including a call to Math.max.\n\nRight, but most costly is the call to Scorer.score().  If you didn't\ncare to know the maxScore, but still wanted the raw score for each hit,\nit'd be much cheaper to only call Scorer.score() if the hit is\ncompetitive (ie, is inserted into the pq).\n\nbq. Were you thinking along those lines? If so, I can remove maxScore tracking from TFC and compute it in topDocs only.\n\nWe can't do that because that's a change in behavior (ie, maxScore\nincludes max across *all* hits, not just the final topN).\n\nbq. The reason I'm thinking about it is that because maxScore is probably used only for normalization, and if a certain application never uses it, why should we compute it?\n\nRight that's my reasoning too.  So I think we should be able to separately turn off \"track max score\".\n",
            "date": "2009-04-08T16:22:09.636+0000",
            "id": 109
        },
        {
            "author": "Shai Erera",
            "body": "Right ... so basically we're talking about changes in just TFC:\n# Add another parameter to TFC.create - trackMaxScore (default to true in IndexSearcher, but change to false in 3.0).\n# Keeping the non-scoring collectors as they are.\n# Create a ScoringNoMaxScoreCollector, which extends NonScoringCollector. It will compute score only if the doc has a chance to enter the queue. This means implementing collect() entirely, and not relying on super.collect.\n# Rename ScoringCollector to ScoringMaxScoreCollector and leave it as is: track both scores and maxScore and call super.collect().\n\nI figure there's no reason to create a NonScoringTrackMaxScore, since if we're calling score() to track maxScore, there's no reason not to set it on the docs that enter the pq.\n\nThis does not affect TopScoreDocCollector.",
            "date": "2009-04-08T17:53:17.970+0000",
            "id": 110
        },
        {
            "author": "Michael McCandless",
            "body": "Sounds right!  Wanna update the patch w/ that?",
            "date": "2009-04-08T17:57:29.024+0000",
            "id": 111
        },
        {
            "author": "Shai Erera",
            "body": "of course !",
            "date": "2009-04-08T18:11:00.550+0000",
            "id": 112
        },
        {
            "author": "Michael McCandless",
            "body": "I came across another simple search optimization I think we should do:\nif IndexSearcher can guarantee it visits docs in docID order (ie, turn\noff the \"sort by numDocs()\" now done by default, that was added in\nLUCENE-1483), then in TopFieldCollector and TopScoreDocCollector we\ncan remove careful tie-breaking in the \"equals\" cases.\n\nThis means in TopScoreDocCollector we could change this:\n{code}\n} else if (score >= reusableSD.score) {\n{code}\n(inside collect) to:\n{code}\n} else if (score > reusableSD.score) {\n{code}\n\nbecause if the new hit's score is == it can't possibly compete since\nits docID is guaranteed to be greater than what's in the queue.\n\nLikewise, in TopFieldCollector.OneComparatorNonScoringCollector, we\ncan change:\n\n{code}\nif (cmp < 0 || (cmp == 0 && doc + docBase > bottom.docID)) {\n  return;\n}\n{code}\n\nto:\n\n{code}\nif (cmp <= 0) {\n  return;\n}\n{code}\n\nand in MultiComparatorNonScoringCollector, we can change:\n{code}\n} else if (i == comparators.length - 1) {\n  // This is the equals case.\n  if (doc + docBase > bottom.docID) {\n    // Definitely not competitive\n    return;\n  }\n  break;\n}\n{code}\n\nto:\n\n{code}\n} else if (i == comparators.length - 1) {\n  return;\n}\n{code}\n\n(or possibly refactor this one to return if c == 0 on exiting the\nloop).\n\nWe do still need the \"break tie\" logic in the lessThan methods of\nHitQueue/FieldValueHitQueue, since PQ uses this to up/down heap itself\nafter we insert/delete.\n",
            "date": "2009-04-08T19:05:02.011+0000",
            "id": 113
        },
        {
            "author": "Shai Erera",
            "body": "Hey Mike. I actually planned to open another issue about optimizations to TSDC. Besides what you propose, I think that if we pre-fill pq with dummy objects (with score = Float.NEGATIVE_INF), we can skip the checks of \"if (reusableSD == null)\" and always assume the queue is full. We can even optimize it further by not calling pq.insertWithOverflow, but instead change top() and call adjustTop().\n\nI do think though that these belong in a different issue, since this one is about the refactoring. What do you say that I'll to this one the changes about maxScore we agreed, and then open a new one \"optimizations to TSDC and TFC\" and include those changes? I also want to do perf. measurements, ensure the unit tests cover everything etc. That issue already contains many changes.",
            "date": "2009-04-09T08:51:18.175+0000",
            "id": 114
        },
        {
            "author": "Michael McCandless",
            "body": "OK that sounds like a good plan.\n\nI'd be interested to see if pre-filling with sentinels gives better performance, too.\n\nHere's another one to add: some methods in Sort explicitly add SortField.FIELD_DOC as a \"tie breaker\" for the last SortField.  But, doing so should not be necessary (since we already break ties by docID), and is in fact less efficient (once the above optimization is in).",
            "date": "2009-04-09T08:59:22.181+0000",
            "id": 115
        },
        {
            "author": "Shai Erera",
            "body": "Does HitQueue favor documents with smaller ids? In case the score is equal I mean ... if not (I was under the impression larger doc Ids are favored), then if the scores are equal shouldn't the new doc replace the current doc? Isn't that's why the >= exists there now (othrewise it could be just >)? Or was it added as part of 1483, since it wasn't guaranteed documents are returned in increasing order?",
            "date": "2009-04-09T09:09:53.559+0000",
            "id": 116
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Does HitQueue favor documents with smaller ids?\n\nYes.  In it's lessThan method, when the scores are equal, it does this:\n{code}\nreturn hitA.doc > hitB.doc; \n{code}\n\nEg, hitB is \"favored\" (lessThan returns false) if it has a smaller doc than hitA.\n\nbq.  Or was it added as part of 1483, since it wasn't guaranteed documents are returned in increasing order?\n\nThis was added as part of LUCENE-1483, but can be optimized away once we go back to in-order docID processing from IndexSearcher.",
            "date": "2009-04-09T09:48:07.185+0000",
            "id": 117
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Eg, hitB is \"favored\" (lessThan returns false) if it has a smaller doc than hitA.\n\nDuh, I got that backwards.  This stuff is hard to think about!\n\nCorrected: if hitB has smaller doc than hitA, lessThan returns true, meaning hitB is preferred.",
            "date": "2009-04-09T10:05:26.737+0000",
            "id": 118
        },
        {
            "author": "Shai Erera",
            "body": "* Adds the ScoringNoMaxScore collectors\n* Adds some tests to TestSort in order to test that functionality\n* Fixes a bug which existed since this issue - when maxScore is set to Float.NaN, Math.max always returns NaN. Therefore I set the ScoringMaxScore collectors to set it to NEG_INF (to accommodate scorers which assign negative scores to documents).\n* Added \"nomaxscore\" property to SearchWithSortTask.\n* Changed ReadTask.doLogic() to always use search(Query, Collector). The reason is that currently if scoring is set to true, it uses the default search method, however in 3.0 that method will be changed to not compute scores, and we might forget to change the logic in ReadTask.\n\nBTW, I wonder if we can replace the call to Math.max with just 'if (score > maxScore)'? Looking at Math.max, it checks if the fist parameter is NaN (which I assume Scorer.score() will not return), and then if both equal 0.0f returns their sum, otherwise returns the biggest. Calling this method is quite expensive, and I think we will be safe with replacing it with 'if', however now that maxScore is decoupled as well, only the trackMaxScore collectors will suffer from it ....\nAnyway, I added a TODO in case we don't want to change it now.",
            "date": "2009-04-10T04:53:54.367+0000",
            "id": 119
        },
        {
            "author": "Shai Erera",
            "body": "added another test case to TestSort",
            "date": "2009-04-10T05:45:15.486+0000",
            "id": 120
        },
        {
            "author": "Michael McCandless",
            "body": "bq. BTW, I wonder if we can replace the call to Math.max with just 'if (score > maxScore)'?\n\nI would just go ahead and do that.\n\nLooks great!  All tests pass.  Here's all I found... can you make a\nnew patch Shai?:\n\n  * Patch is missing the fix to contrib/spatial\n\n  * Did you add a test case verifying maxScore is correct (so that the\n    Float.NaN issue would trip the test)?\n\n  * Javadoc of searchWithSort needs to describe nomaxscore param\n\n  * HitCollector isn't deprecated\n",
            "date": "2009-04-10T11:18:17.901+0000",
            "id": 121
        },
        {
            "author": "Shai Erera",
            "body": "bq. Did you add a test case verifying maxScore is correct (so that the Float.NaN issue would trip the test)?\n\nI added to following tests:\n* testSortWithoutScoreTracking - asserts that ScoreDoc.score is set to Float.NaN as well as maxScore.\n* testSortWithScoreNoMaxScoreTracking - asserts that ScoreDoc.score is not Float.NaN, but maxScore is.\n* testSortWithScoreAndMaxScoreTracking - asserts that both ScoreDoc.score and maxScore are not set to NaN.\n* testSortWithScoreAndMaxScoreTrackingNoResults - asserts that in case of a maxScore tracking collector with 0 results, maxScore is set to Float.NaN, rather than NEG_INF.\n\nbq. HitCollector isn't deprecated\n\nSomehow when I applied your patch, this change wasn't taken in. Anyway, I noticed its javadocs also referenced MRHC, so I fixed it also.\n\n",
            "date": "2009-04-10T18:46:16.520+0000",
            "id": 122
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I added to following tests:\n\nGreat, I'll have a look, thanks!\n\nbq. Somehow when I applied your patch, this change wasn't taken in. Anyway, I noticed its javadocs also referenced MRHC, so I fixed it also.\n\nMost likely the hunk was rejected, due to the $Id$ keyword in that javadoc.  \"svn patch\" should fix this.",
            "date": "2009-04-10T18:53:20.178+0000",
            "id": 123
        },
        {
            "author": "Michael McCandless",
            "body": "Patch looks good, and all tests patch.  I'll commit in a day or two, barring any more sudden improvements ;)",
            "date": "2009-04-10T19:33:50.636+0000",
            "id": 124
        },
        {
            "author": "Michael McCandless",
            "body": "Thanks Shai!",
            "date": "2009-04-13T18:35:53.883+0000",
            "id": 125
        },
        {
            "author": "Uwe Schindler",
            "body": "Hi,\nShalin found a backwards-incompatible change in the Searcher abstract class, I noticed this from his SVN comment for SOLR-940 (where he updated to Lucene trunk):\n{code}\nabstract public void search(Weight weight, Filter filter, Collector results) throws IOException;\n{code}\nThis should not be abstract for backwards compatibility, but instead throw an UnsupportedOperationException or have a default implementation that somehow wraps the Collector using an old HitCollector (not very nice, I do not know how to fix this in any other way). Before 3.0, where this change would be ok,  the Javadocs should note, that the deprecated HitCollector API will be removed and the Collector part will be made abstract.\nIf this method stays abstract, you cannot compile old code or replace lucene jars (this is seldom, as almost nobody creates private implementations of Searcher, but Solr does...",
            "date": "2009-04-24T18:07:26.205+0000",
            "id": 126
        },
        {
            "author": "Shai Erera",
            "body": "If you read somewhere above, you'll see that we've discussed this change. It seems that whatever we do, anyone who upgrades to 2.9 will need to touch his code. If you extend Searcher, you'll need to override that new method, regardless of what we choose to do. That's because if it's abstract, you need to implement it, and it it's concrete (throwing UOE), you'll need to override it since all the Searcher methods call this one at the end.\n\nI'm not sure wrapping a Collector with HitCollector will work, because all of the other classes now expect Collector, and their HitCollector variants call the Collector one with a HitCollectorWrapper (which is also deprecated).\n\nWe agreed that making it abstract is the lesser of all evils, since you'll spot it at compile time, rather than at runtime, when you'll hit a UOE.",
            "date": "2009-04-24T19:04:09.295+0000",
            "id": 127
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Shalin found a backwards-incompatible change in the Searcher abstract class\n\nWe could go either way on this... the evils were strong with either choice, and we struggled and eventually went with adding abstract method today, for the reasons Shai enumerated.\n",
            "date": "2009-04-24T19:52:56.950+0000",
            "id": 128
        },
        {
            "author": "Uwe Schindler",
            "body": "OK, I resolve the issue again. I was just wondering and wanted to be sure. I also missed the first entry in CHANGES.txt, which explains it.\nIt is the same like with the Fieldable interface in the past, it is seldom implemented/overwritten and so the \"normal\" user will not be affected. And those who implement Fieldable or extend Searcher must implement it.",
            "date": "2009-04-24T20:04:27.326+0000",
            "id": 129
        }
    ],
    "component": "core/search",
    "description": "This issue is a result of a recent discussion we've had on the mailing list. You can read the thread [here|http://www.nabble.com/Is-TopDocCollector%27s-collect()-implementation-correct--td22557419.html].\n\nWe have agreed to do the following refactoring:\n* Rename MultiReaderHitCollector to Collector, with the purpose that it will be the base class for all Collector implementations.\n* Deprecate HitCollector in favor of the new Collector.\n* Introduce new methods in IndexSearcher that accept Collector, and deprecate those that accept HitCollector.\n** Create a final class HitCollectorWrapper, and use it in the deprecated methods in IndexSearcher, wrapping the given HitCollector.\n** HitCollectorWrapper will be marked deprecated, so we can remove it in 3.0, when we remove HitCollector.\n** It will remove any instanceof checks that currently exist in IndexSearcher code.\n* Create a new (abstract) TopDocsCollector, which will:\n** Leave collect and setNextReader unimplemented.\n** Introduce protected members PriorityQueue and totalHits.\n** Introduce a single protected constructor which accepts a PriorityQueue.\n** Implement topDocs() and getTotalHits() using the PQ and totalHits members. These can be used as-are by extending classes, as well as be overridden.\n** Introduce a new topDocs(start, howMany) method which will be used a convenience method when implementing a search application which allows paging through search results. It will also attempt to improve the memory allocation, by allocating a ScoreDoc[] of the requested size only.\n* Change TopScoreDocCollector to extend TopDocsCollector, use the topDocs() and getTotalHits() implementations as they are from TopDocsCollector. The class will also be made final.\n* Change TopFieldCollector to extend TopDocsCollector, and make the class final. Also implement topDocs(start, howMany).\n* Change TopFieldDocCollector (deprecated) to extend TopDocsCollector, instead of TopScoreDocCollector. Implement topDocs(start, howMany)\n* Review other places where HitCollector is used, such as in Scorer, deprecate those places and use Collector instead.\n\nAdditionally, the following proposal was made w.r.t. decoupling score from collect():\n* Change collect to accecpt only a doc Id (unbased).\n* Introduce a setScorer(Scorer) method.\n* If during collect the implementation needs the score, it can call scorer.score().\nIf we do this, then we need to review all places in the code where collect(doc, score) is called, and assert whether Scorer can be passed. Also this raises few questions:\n* What if during collect() Scorer is null? (i.e., not set) - is it even possible?\n* I noticed that many (if not all) of the collect() implementations discard the document if its score is not greater than 0. Doesn't it mean that score is needed in collect() always?\n\nOpen issues:\n* The name for Collector\n* TopDocsCollector was mentioned on the thread as TopResultsCollector, but that was when we thought to call Colletor ResultsColletor. Since we decided (so far) on Collector, I think TopDocsCollector makes sense, because of its TopDocs output.\n* Decoupling score from collect().\n\nI will post a patch a bit later, as this is expected to be a very large patch. I will split it into 2: (1) code patch (2) test cases (moving to use Collector instead of HitCollector, as well as testing the new topDocs(start, howMany) method.\nThere might be even a 3rd patch which handles the setScorer thing in Collector (maybe even a different issue?)",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1575",
    "issuetypeClassified": "REFACTORING",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Refactoring Lucene collectors (HitCollector and extensions)",
    "systemSpecification": true,
    "version": ""
}