{
    "comments": [
        {
            "author": "Shai Erera",
            "body": "All tests pass.",
            "date": "2009-07-02T14:14:35.265+0000",
            "id": 0
        },
        {
            "author": "Mark Miller",
            "body": "nice catch",
            "date": "2009-07-02T14:15:31.686+0000",
            "id": 1
        },
        {
            "author": "Shai Erera",
            "body": "Thanks. Took me a while to think in that direction (I was sure UTF-8 is what's used in the code :) ).",
            "date": "2009-07-02T14:18:21.463+0000",
            "id": 2
        },
        {
            "author": "Mark Miller",
            "body": "I think that it makes sense to make the default the encoding the one that trec typically/always uses, but we should probably make this configurable from the alg file. We don't want to be locked down to one input encoding. Could be done in another issue though. Should allow that for the other contentsources as well.",
            "date": "2009-07-02T14:52:21.265+0000",
            "id": 3
        },
        {
            "author": "Shai Erera",
            "body": "I don't understand - the change is in TrecContentSource (only), which reads the TREC collection, which is encoded in ISO-8859-1. Why should it be configurable? Only if someone will read it and write it back in, say, UTF-8, it would make sense to make it configurable, right? Or am I missing something?",
            "date": "2009-07-02T17:56:57.901+0000",
            "id": 4
        },
        {
            "author": "Mark Miller",
            "body": "The trec data you are using is ISO-8859-1. Wouldn't it be conceivable that they might change the encoding to UTF-8 at some point? Or that someone else has created trec compatible data in another encoding? Or trec has data in different encodings? If something reads a source of files, and the files could technically be in any encoding, I would expect things to be configurable so that I can specify what encoding the files are in. It just seems like a good standard feature with something that reads what are essentially pluggable files. The format is going to be consistent, but why would the encoding necessarily be consistent?",
            "date": "2009-07-02T18:13:53.160+0000",
            "id": 5
        },
        {
            "author": "Shai Erera",
            "body": "You're right, I didn't think in that direction. I'll make it configurable, shouldn't be a problem. And if it makes sense (and I think it is), I'll put the config parameter on ContentSource.\n\nWill post a second patch soon.",
            "date": "2009-07-02T18:36:42.867+0000",
            "id": 6
        },
        {
            "author": "Robert Muir",
            "body": "I'd like this to be configurable. I used this package to test LUCENE-1628. \n\n(For this I actually ran it with -Dfile.encoding=UTF-8 to prevent this problem), so its \"configurable\" already...but not obvious.\n",
            "date": "2009-07-02T21:16:31.675+0000",
            "id": 7
        },
        {
            "author": "Shai Erera",
            "body": "Added content..source.encoding to ContentSource (default=null) and set it to ISO-8859-1 in TrecContentSource and UTF-8 in LineDocSource (in case someone wants to use a line file that wasn't created w/ WriteLineDocTask), unless a different encoding is specified.\n\nUpdated CHANGES and Javadocs.\n\nAll tests pass.",
            "date": "2009-07-03T10:52:16.016+0000",
            "id": 8
        },
        {
            "author": "Shai Erera",
            "body": "Any volunteers to help me get it in? I think it's ready for commit.",
            "date": "2009-07-05T20:02:45.916+0000",
            "id": 9
        },
        {
            "author": "Mark Miller",
            "body": "I havn't patched the code in, but looking at the patch, it looks like you are setting the default *after* specifying the encoding to the InputStream?\n\n+        GZIPInputStream zis = new GZIPInputStream(new FileInputStream(f), BUFFER_SIZE);\n+        reader = new BufferedReader(new InputStreamReader(zis, encoding), BUFFER_SIZE);\n\n...\n\n+    if (encoding == null) {\n+      encoding = \"ISO-8859-1\";\n+    }",
            "date": "2009-07-06T13:44:51.342+0000",
            "id": 10
        },
        {
            "author": "Shai Erera",
            "body": "if (encoding == null) happens in setConfig and the other one in openNextFile() (forgot the exact method name). The patch includes just the changes, w/o the method names, so it may not be obvious just by looking at it.",
            "date": "2009-07-06T13:51:41.809+0000",
            "id": 11
        },
        {
            "author": "Mark Miller",
            "body": "Okay, cool. I'll patch it in, run the tests, and commit later today.\n\nThanks Shai.",
            "date": "2009-07-06T13:54:57.581+0000",
            "id": 12
        }
    ],
    "component": "modules/benchmark",
    "description": "TrecContentSource opens InputStreamReader w/o a fixed encoding. On Windows, this means CP1252 (at least on my machine) which is ok. However, when I opened it on a Linux machine w/ a default of UTF-8, it failed to read the files. The patch changes it to use ISO-8859-1, which seems to be the right one (and http://mg4j.dsi.unimi.it/man/manual/ch01s04.html mentions this encoding in its example of a script which reads the data).\n\nPatch to follow shortly.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-1730",
    "issuetypeClassified": "BUG",
    "issuetypeTracker": "BUG",
    "priority": "Major",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "TrecContentSource should use a fixed encoding, rather than system dependent",
    "systemSpecification": true,
    "version": ""
}