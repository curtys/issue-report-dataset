{
    "comments": [
        {
            "author": "Andrzej Bialecki ",
            "body": "This is just a starting point for discussion - it's a pretty old file I found lying around, so it may not even compile with modern Lucene. Requires commons-compress.",
            "date": "2006-09-21T05:18:24.000+0000",
            "id": 0
        },
        {
            "author": "Paul Smith",
            "body": "If you're looking for freely available text in bulk, what about:\n\nhttp://www.gutenberg.org/wiki/Main_Page",
            "date": "2006-09-21T05:43:49.000+0000",
            "id": 1
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "Yes, that could be a good additional source. However, IMHO the primary corpus should be widely known and standardized, hence my proposal of the Reuters.\n\n(I mistakenly copy&paste-d the urls in the comment above - of course the corpus they're pointing at is the \"20 Newsgroups\", not the Reuters one. Correct url for the Reuters corpus is  http://www.daviddlewis.com/resources/testcollections/reuters21578/ ).",
            "date": "2006-09-21T06:31:20.000+0000",
            "id": 2
        },
        {
            "author": "Paul Smith",
            "body": "From a strict performance point of view, a standard set of important, but don't forget other languages.\n\nFrom a tokenization point of view (seperate to this issues), perhaps the Gutenberg project would be useful to test correctness of the analysis phase.",
            "date": "2006-09-21T06:34:09.000+0000",
            "id": 3
        },
        {
            "author": "Karl Wettin",
            "body": "It is also interesting to know how much time is consumed to assemble an instance of Document from the storage. According to my own tests this is the major reason to why InstantiatedIndex is so much faster than a FS/RAMDirectory. I also presume it to be the bottleneck of any RDBMS-, RMI- or any other \"proxy\"-based storage. ",
            "date": "2006-09-21T11:07:36.000+0000",
            "id": 4
        },
        {
            "author": "Grant Ingersoll",
            "body": "Since this has dependencies, do you think we should put it under contrib?  I would be for a Performance directory and we could then organize it from there.  Perhaps into packages for quantitative and qualitative performance.  ",
            "date": "2006-09-21T12:23:42.000+0000",
            "id": 5
        },
        {
            "author": "Andrzej Bialecki ",
            "body": "The dependency on commons-compress could be avoided - I used this just to be able to unpack tar.gz files, we can use Ant for that. If you meant the dependency on the corpus - can't Ant download this too as a dependency?\n\nRe: Project Gutenberg - good point, this is a good source for multi-lingual documents. The \"Europarl\" collection is another, although a bit more hefty, so that could be suitable for running large-scale benchmarks, and texts from Project Gutenberg for running small-scale tests.",
            "date": "2006-09-21T12:37:11.000+0000",
            "id": 6
        },
        {
            "author": "Grant Ingersoll",
            "body": "Yeah, ANT can do this, I think.  Take a look at the DB contrib package, it downloads.  I think I can setup the necessary stuff in contrib, if people think that is a good idea.  First contribution will be this file and then we can go from there.  I think Otis has run some perf. stuff too, but I am not sure if it can be contributed.  I think someone else has really studied query perf. so it would be cool if that was added too.",
            "date": "2006-09-21T12:49:06.000+0000",
            "id": 7
        },
        {
            "author": "Otis Gospodnetic",
            "body": "I still haven't gotten my employer to sign and fax the CCLA, so I'm stuck and can't contribute my search benchmark.\n\nI have a suggestion for a name for this - Lube, for Lucene Benchmark - contrib/lube.",
            "date": "2006-09-21T17:31:37.000+0000",
            "id": 8
        },
        {
            "author": "Michael McCandless",
            "body": "I think this is an incredibly important initiative: with every\nnon-trivial change to Lucene (eg lock-less commits) we must verify\nperformance did not get worse.  But, as things stand now, it's an\nad-hoc thing that each developer needs to do.\n\nSo (as a consumer of this), I would love to have a ready-to-use\nstandard test that I could run to check if I've slowed things down\nwith lock-less commits.\n\nIn the mean time I've been using Europarl for my testing.\n\nAlso important to realize is there are many dimensions to test.  With\nlock-less I'm focusing entirely on \"wall clock time to open readers\nand writers\" in different use cases like pure indexing, pure\nsearching, highly interactive mixed indexing/searching, etc.  And this\nis actually hard to test cleanly because in certain cases (highly\ninteractive case, or many readers case), the current Lucene hits many\n\"commit lock\" retries and/or timeouts (whereas lock-less doesn't).  So\nwhat's a \"fair\" comparison in this case?\n\nIn addition to standardizing on the corpus I think we ideallly need\nstandardized hardware / OS / software configuration as well, so the\nnumbers are easily comparable across time.  Even the test process\nitself is important, eg details like \"you should reboot the box before\neach run\" and \"discard results from first run then take average of\nnext 3 runs as your result\", are important.  It would be wonderful if\nwe could get this into a nightly automated regression test so we could\ntrack over time how the performance has changed (and, for example,\nquickly detect accidental regressions).  We should probably open this\nas a separate issue which depends first on this issue being complete.\n",
            "date": "2006-09-22T13:47:34.000+0000",
            "id": 9
        },
        {
            "author": "Mike Klaas",
            "body": "A few notes on benchmarks:\n\nFirst, it is important to realize that no benchmark will ever fully-capture all aspects of lucene performance, particularly since so many real-world data distributions are so varied.  That said, they are useful tools, especially if they are componentized to measure various aspects of lucene performance (the narrower the goal of the benchmark it, the better a benchmark can be created).\n\nIt is rather unrealistic to expect to standardize hardware / os ... better to compare before/after numbers on a single configuration, rather than comparing the numbers among configurations.  The test process _is_ important, but anything crucial should be built into the test (like the number of iterations; taking the average, etc).  Concerning the specifics of this: Requiring reboots is onerous and not an important criterion (at least for unix systems--I'm not sufficiently familiar with windows to comment).  Better to stipulate a relatively quiscient machine.  Or perhaps not--it might be useful to see how the machine load affects lucene performance.  Also, the arithmetic mean is a terrible way of combining results due to its emphasis on outliers.  Better is the average over minimum times of small sets of runs.  \n\nOf course, any scheme has its problems.  In general, the most important thing when using benchmarks is being aware of the limitations of the benchmark and methodology used.",
            "date": "2006-09-22T17:31:13.000+0000",
            "id": 10
        },
        {
            "author": "Grant Ingersoll",
            "body": "My comments are marked by GSI\n-----------\n\nIn the mean time I've been using Europarl for my testing.\n\nGSI: perhaps you can contribute once this is setup\n\nAlso important to realize is there are many dimensions to test. With\nlock-less I'm focusing entirely on \"wall clock time to open readers\nand writers\" in different use cases like pure indexing, pure\nsearching, highly interactive mixed indexing/searching, etc. And this\nis actually hard to test cleanly because in certain cases (highly\ninteractive case, or many readers case), the current Lucene hits many\n\"commit lock\" retries and/or timeouts (whereas lock-less doesn't). So\nwhat's a \"fair\" comparison in this case?\n\nGSI:  I am planning on taking Andrzej contribution and refactoring it into components that can be reused, as well as creating a \"standard\" benchmark which will be easy to run through a simple ant task, i.e. ant run-baseline\n\nGSI: From here, anybody can contribute their own (I will provide interfaces to facilitate this) benchmarks which others can choose to run. \n\n\nIn addition to standardizing on the corpus I think we ideallly need\nstandardized hardware / OS / software configuration as well, so the\nnumbers are easily comparable across time. \n\nGSI: Not really feasible unless you are proposing to buy us machines :-)  I think more important is the ability to do a before and after evaluation (that runs each test several times) as you make changes.  Anybody should be able to do the same.  Run the benchmark, apply the patch and then rerun the benchmark.\n",
            "date": "2006-09-22T18:14:08.000+0000",
            "id": 11
        },
        {
            "author": "Dawid Weiss",
            "body": "First -- I think it's a good initiative. Grant, when you're thinking about the infrastructure, it would be pretty neat to have a way of logging performance in a way so that one could draw charts from them. You know, for the visual folks :)\n\nAnyway, my other idea is that benchmarking Lucene can be performed on two levels: one is the user level, where the entire operation counts (such as indexing, searching etc). Another aspect is measurement of atomic parts _within_ the big operation so that you know how much of the whole thing each subpart takes. I wrote an interesting piece of code once that allows measuring times for named operation (per-thread) in a recursive way. Looks something like this:\n\nperfLogger.start(\"indexing\");\ntry {\n  .. code (with recursion etc)  ...\n  perfLogger.start(\"subpart\");\n  try { \n\n  } finally {\n     perfLogger.stop();\n  }\n} finally {\n  perfLogger.stop();\n}\n\nin the output you get something like this:\n\nindexing: 5 seconds;\n   ->subpart: : 2 seconds;\n   -> ...\n\nOf course everything comes at a price and the above logging costs some CPU cycles (my implementation stored a nesting stack in ThreadLocals).\n\nOne can always put that code in 'if' clauses attached to final variables and enable logging only for benchmarking targets (the compiler will get rid of logging statements then).\n\nIf folks are interested I can dig out that performance logger and maybe adopt it to what Grant comes up with.",
            "date": "2006-09-22T18:55:08.000+0000",
            "id": 12
        },
        {
            "author": "Michael McCandless",
            "body": "I agree: a simple ant-accessible benchmark to enable \"before and\nafter\" runs is an awesome step forward.  And that a standardized HW/SW\ntesting environment is not really realistic now.\n\n> GSI: perhaps you can contribute once this is setup \n\nI will try!",
            "date": "2006-09-22T19:17:08.000+0000",
            "id": 13
        },
        {
            "author": "Doron Cohen",
            "body": "Few things that would be nice to have in this performance package/framework - \n\n() indexing only overall time.\n() indexing only time changes as the index grows (might be the case that indexing performance starts to misbehave from a certain size or so).\n() search single user while indexing\n() search only single user\n() search only concurrent users\n() short queries\n() long queries\n() wild card queries\n() range queries\n() queries with rare words\n() queries with common words\n() tokenization/analysis only (above indexing measurements include tokenization, but it would be important to be able to \"prove\" to oneself that tokenization/analysis time is not hurt by  a recent change).\n\n() parametric control over:\n() () location of test input data.\n() () location of output index.\n() () location of output log/results.\n() ()  total collection size (total number of bytes/characters read from collection)\n() () document (average) size (bytes/chars) - test can break input data and recompose it into documents of desired size.\n() () \"implicit iteration size\" - merge-factor, max-buffered-docs\n() () \"explicit iteration size\" - how often the perf test calls\n() () long queries text\n() () short queries text\n() () which parts of the test framework capabilities to run\n() () number of users / threads.\n() () queries pace - how many queries are fired in, say, a minute.\n\nAdditional points:\n() Would help if all test run parameters are maintained in a properties (or xml config) file, so one can easily modify the test input/output without having to recompile the code.\n() Output to allow easy creation of graphs or so - perhaps best would be to have an result object, so others can easily extend with additional output formats.\n() index size as part of output.\n() number of index files as part of output (?)\n() indexing input module that can loop over the input collection. This allows to test indexing of a collection larger than the actual input collection being used. \n\n",
            "date": "2006-09-22T19:19:06.000+0000",
            "id": 14
        },
        {
            "author": "Grant Ingersoll",
            "body": "OK, I have a preliminary implementation based on adapting Andrzej's approach.  The interesting thing about this approach, is it is easy to adapt to be more or less exhaustive (i.e. how many of the parameters does one wish to have the system alter as it runs)  Thus, you can have it change the merge factors, max buffered docs, number of documents indexed, number of different queries run, etc.  The tradeoff, of course, is the length of time it takes to run these.\n\nSo my question to those interested, is what is a good baseline running time for testing in a standard way?  My initial thought is to have something that takes between 15-30 minutes to run, but I am not sure on this.  Another approach would be to have three \"baselines\":  1. quick validation (5 minutes to run...) 2. standard (15-45) 3. exhaustive (1-10 hours).  \n\nI know several others have built benchmarking suites for their internal use, what has been your strategy? \n\nThoughts, ideas, insights?\n\nThanks,\nGrant",
            "date": "2006-10-09T22:09:28.000+0000",
            "id": 15
        },
        {
            "author": "Marvin Humphrey",
            "body": "The indexing benchmarking apps I wrote take command line arguments for how many docs and how many reps.  My standard test is to do 1000 docs and 6 reps.  Within a couple seconds the first rep is done and the app is printing out results.  For rapid development, having something that speedy is really handy.",
            "date": "2006-10-09T22:28:07.000+0000",
            "id": 16
        },
        {
            "author": "Doug Cutting",
            "body": "As Marvin points out, quick micro-benchmarks are great to have.  But other effects only show up when things get very large.  So I think we need at least two baselines: micro and macro.",
            "date": "2006-10-10T20:54:20.000+0000",
            "id": 17
        },
        {
            "author": "Marvin Humphrey",
            "body": "Grant had asked me if he could reuse some code from the indexer benchmarks I wrote. Here are the relevant files, contributed with the expectation they will be cannibalized, not included verbatim. ",
            "date": "2006-10-24T00:33:43.000+0000",
            "id": 18
        },
        {
            "author": "Marvin Humphrey",
            "body": "One more file...",
            "date": "2006-10-24T00:35:33.000+0000",
            "id": 19
        },
        {
            "author": "Grant Ingersoll",
            "body": "OK, here is a first crack at a standard benchmark contribution based on Andrzej original contribution and some updates/changes by me.  I wasn't nearly as ambitious  as some of the comments attached here, but I think most of them are good things to strive for and will greatly benefit Lucene.\n\nI checked in the basic contrib directory structure, plus some library dependencies, as I wasn't sure how svn diff handles those.  I am posting this in patch format to solicit comments first instead of just committing and accepting patches.  My thoughts are I'll take a round of comments and make updates as warranted and then make an initial commit.  \n\nI am particularly interested in the interface/Driver specification and whether people think this approach is useful or not.  My thoughts behind it were it might be nice to have a standard way of creating/running benchmarks that could be driven by XML configuration files (some examples are in the conf directory).  I am not 100% sold on this and am open to compelling arguments why we should just have each benchmark have it's own main() method.\n\nAs for the actual Benchmarker, I have created a \"standard\" version, which runs off the Reuters collection that is downloaded automatically by the ANT task.  There are two ANT targets for the two benchmarks: run-micro-standard and run-standard.  The micro version takes a few minutes to run on my machine (it indexes 2000 docs), the other one takes a lot longer.\n\nThere are several support classes in the stats and util packages.  The stats package supports building and maintaining information about benchmarks.  The utils package contains one class for extracting information out of the Reuters documents for indexing.\n\nThe ReutersQueries class contains a set of Queries I created by looking at some of the docs in the collection and are a myriad of term, phrase, span, wildcard and other types of queries.  They aren't exhaustive by any means.\n\nIt should be stressed that these benchmarks are best used in gathering before and after numbers.  Furthermore, these aren't the be all end all of benchmarking for Lucene.  I hope the interface nature will encourage others to submit benchmarks for specific areas of Lucene not covered by this version.\n\nThanks to all who contributed their code/thoughts.  Patch to follow",
            "date": "2006-11-06T03:23:14.000+0000",
            "id": 20
        },
        {
            "author": "Grant Ingersoll",
            "body": "Initial Benchmark code based on Andrzej original contribution plus some changes by me to use the Reuters \"standard\" collection maintained at http://www.daviddlewis.com/resources/testcollections/reuters21578/reuters21578.tar.gz\n\n",
            "date": "2006-11-06T03:25:14.000+0000",
            "id": 21
        },
        {
            "author": "Grant Ingersoll",
            "body": "To run, checkout contrib/benchmark and then apply the benchmark.patch in the contrib/benchmark directory.",
            "date": "2006-11-06T03:26:49.000+0000",
            "id": 22
        },
        {
            "author": "Doron Cohen",
            "body": "I tried it and it is working nice! - \n1st run downloaded the documents from the Web before starting to index. \n2nd run started right off - as input docs are already in place - great. \n\nSeems the only output is what is printed to stdout, right? \n\nI got something like this: \n----------------------------\n     [echo] Working Directory: work\n     [java] Testing 4 different permutations.\n     [java] #-- ID: td-00_10_10, Sun Nov 05 22:40:49 PST 2006, heap=1065484288 --\n     [java] # source=work\\reuters-out, directory=org.apache.lucene.store.FSDirectory@D:\\devoss\\lucene\\java\\trunk\\contrib\\benchmark\\work\\index\n     [java] # maxBufferedDocs=10, mergeFactor=10, compound=true, optimize=true\n     [java] # Query data: R-reopen, W-warmup, T-retrieve, N-no\n     [java] # qd-0110 R W NT [body:salomon]\n     [java] # qd-0111 R W T [body:salomon]\n     [java] # qd-0100 R NW NT [body:salomon]\n...\n     [java] # qd-14011 NR W T [body:fo*]\n     [java] # qd-14000 NR NW NT [body:fo*]\n     [java] # qd-14001 NR NW T [body:fo*]\n\n     [java] Start Time: Sun Nov 05 22:41:38 PST 2006\n     [java]  - processed 500, run id=0\n     [java]  - processed 1000, run id=0\n     [java]  - processed 1500, run id=0\n     [java]  - processed 2000, run id=0\n     [java] End Time: Sun Nov 05 22:41:48 PST 2006\n     [java] warm = Warm Index Reader\n     [java] srch = Search Index\n     [java] trav = Traverse Hits list, optionally retrieving document\n\n     [java] # testData id\toperation\trunCnt\trecCnt\trec/s\tavgFreeMem\tavgTotalMem\n     [java] td-00_100_100\taddDocument\t1\t2000\t472.0321\t4493681\t22611558\n     [java] td-00_100_100\toptimize\t1\t1\t2.857143\t4229488\t22716416\n     [java] td-00_100_100\tqd-0110-warm\t1\t2000\t40000.0\t4250992\t22716416\n     [java] td-00_100_100\tqd-0110-srch\t1\t1\tInfinity\t4221288\t22716416\n...\n     [java] td-00_100_100\tqd-4110-srch\t1\t1\tInfinity\t3993624\t22716416\n     [java] td-00_100_100\tqd-4110-trav\t1\t0\tNaN\t3993624\t22716416\n     [java] td-00_100_100\tqd-4111-warm\t1\t2000\t50000.0\t3853192\t22716416\n...\nBUILD SUCCESSFUL\nTotal time: 1 minute 0 seconds\n----------------------------\n\nI think the \"infinity\" and \"NAN\" are caused by op time too short for divide-by-sec.\nThis can be avoided by modifying getRate() in TimeData:\n  public double getRate() {\n    double rps = (double) count * 1000.0 / (double) (elapsed>0 ? elapsed : 1);\n    return rps;\n  }\n\nI like much the logic of loading test data from the Web, and the scaleUp and maximumDocumentsToIndex params are handy. \n\nIt seems that all the test logic and some of its data (queries) are java coded. I initially thought of a setting where we define tasks/jobs that are parameterized, like:\n\n- createIndex(params)\n- writeToIndex(params):\n  - addDocs()\n  - optimize()\n- readFromIndex(params):\n  - searchIndex()\n  - fetchData()\n\n..and compose a test by an XML that says which of these simple jobs to run, with what params, in which order, serial/parallel, how long/often etc. \nThen creating a different test is as easy as creating a different XML that configures that test. \n\nOn the other hand, chances are, I know, that most useful cases would be those already defined here - standard and micro-standard, so can ask \"why bothering changing to define these building blocks\". I am not sure here, but thought I'll bring it up. \n\nAbout Using the driver - seems nice and clean to me. I don't know the Digester but it seems to read the config from the XML correctly.\n\nOther comments:\n1. I think there is a redundant call to params.showRunData(params.getId()) in runBenchmark(File,Options);\n2. Seems that rec/sec would be a bit more accurately computed by aggregating elapsed times (instead of rate) in showRunData()\n3. If TimeData not found (only memData) I think additional 0.0 should be printed\n4. columns allignments with tabs and floats is imperfect.:-)\n5. It would be nice I think to also get a summary of the results by \"task\" - e.g. srch, optimize, something like:\n     [java] # testData id     operation           runCnt     recCnt          rec/s       avgFreeMem      avgTotalMem\n     [java]                   warm                    60       2000       42,628.8        8,235,758       23,048,192\n     [java]                   srch                   120          1          571.4        8,300,613       23,048,192\n     [java]                   optimize                 1          1            2.9        9,375,732       23,048,192\n     [java]                   trav                   120        107       30,517.8        8,326,046       23,048,192\n     [java]                   addDocument              1       2000          441.8        7,310,929       22,206,872\n\nAttached timedata.zip has modifies TimeData.java and TestData.java for [1 to 5] above, and for the NAN/inifinite. ",
            "date": "2006-11-07T10:50:05.000+0000",
            "id": 23
        },
        {
            "author": "Grant Ingersoll",
            "body": "1st run downloaded the documents from the Web before starting to index. \n2nd run started right off - as input docs are already in place - great. \n\nSeems the only output is what is printed to stdout, right? \n\n\nGSI: The Benchmarker interface does return the TimeData, so other implementations, etc. could use the results programmatically.\n\n\n\nI like much the logic of loading test data from the Web, and the scaleUp and maximumDocumentsToIndex params are handy. \n\nIt seems that all the test logic and some of its data (queries) are java coded. I initially thought of a setting where we define tasks/jobs that are parameterized, like:\n\n- createIndex(params)\n- writeToIndex(params):\n  - addDocs()\n  - optimize()\n- readFromIndex(params):\n  - searchIndex()\n  - fetchData()\n\n\nGSI: I definitely agree that we want a more flexible one to meet people's benchmarking needs.  I wanted at least one test that is \"standard\" in that you can't change the parameters and test cases, so that we can all be on the same page on a run.  Then, when people are having discussions on performance they can say \"I ran the standard benchmark before and after and here are the results\" and we all know what they are talking about.  I think all the components are there for a parameterized version, all it takes is someone to extend the Standard one or implement there own that reads in a config file.  I will try to put in a fully parameterized version soon.  \n\n\nGSI: Thanks for the fixes, I will incorporate into my version and post another patch soon.",
            "date": "2006-11-07T13:04:19.000+0000",
            "id": 24
        },
        {
            "author": "Doron Cohen",
            "body": "I looked at extending the benchmark with:\n- different test \"scenarios\", i.e. other sequences of operations.\n- multithreaded tests, e.g. several queries in parallel.\n- rate of events, e.g. \"2 queries arriving per second\", or \"one query per second in parallel with 20 new documents in a minute\".\n- different data sources (input documents, queries).\n\nFor this I made lots of changes to the benchmark code, using parts of it and rewriting other parts. \nI would like to submit this code in a few days - it is running already but some functionality is missing.\n\nI would like to describe how it works to hopefully get early feedback. \n\nThere are several \"basic tasks\" defined - all extending an (abstract) class PerfTask:\n- AddDocTask\n- OptimizeTask\n- CreateIndexTask\netc. \n\nTo further extend the benchmark 'framework', new tasks can be added. Each task must implement the abstract method: doLogic(). For instance, in AddDocTask this method (doLogic) would call indexWriter.addDocument().\nThere are also setup() and tearDown() methods for performing work that should not be timed for that task. \n\nA special TaskSequence task contains other tasks. It is either parallel or sequential, which tells if it executes its child tasks serially or in parallel. \nTaskSequence also supports \"rate\": the pace in which its child tasks are \"fired\" can be controlled.\n\nWith these tasks, it is possible to describe a performance test 'algorithm' in a simple syntax.\n('algorithm' may be too big a word for this...?)\n\nA test invocation takes two parameters: \n- test.properties - file with various config properties.\n- test.alg               - file with the algorithm.\n\nBy convention, for each task class  \"OpNameTask\",  the command  \"OpName\"  is valid in test.alg.\n\nAdding a single document is done by:\n    AddDoc\n\nAdding 3 documents:\n   AddDoc\n   AddDoc\n   AddDoc\n\nOr, alternatively:\n   { AddDoc } : 3\n\nSo, '{' and '}' indicate a serial sequence of (child) tasks. \n\nTo fire 100 queries in a row:\n  { Search } : 100\n\nTo fire 100 queries in parallel:\n  [ Search ] : 100\n\nSo, '[' and ']' indicate a parallel group of tasks. \n\nTo fire 100 queries in a row, 2 queries per second (120 per minute):\n  { Search } : 100 : 120\n\nSimilar, but in parallel:\n  [ Search ] : 100 : 120\n\nA sequence task can be named for identifying it in reports:\n  { \"QueriesA\" Search } : 100 : 120\n\nAnd there are tasks that create reports. \n\nThere are more tasks, and more to tell on the alg syntax, but this post is already long..\n\nI find this quite powerful for perf testing.\nWhat do you (and you) think?\n\n- Doron\n",
            "date": "2006-11-12T10:01:34.000+0000",
            "id": 25
        },
        {
            "author": "Doron Cohen",
            "body": "I am attaching a sample tiny.* - the .alg and .properties files I currently use - I think they may help to understand how this works.",
            "date": "2006-11-12T10:12:27.000+0000",
            "id": 26
        },
        {
            "author": "Grant Ingersoll",
            "body": "OK, how about I commit my changes, then you can add a patch that shows your ideas?\n",
            "date": "2006-11-13T16:09:50.000+0000",
            "id": 27
        },
        {
            "author": "Doron Cohen",
            "body": "Sounds good.\n\nIn this case I will add my stuff under a new package: org.apache.lucene.benchmark2. (this package would have no dependencies in org.apache.lucene.benchmark.). I will also add tarkets in buid.xml, and add .alg, and .alg files under conf.\nMakes sense?\n\nDo you already know when you are going to commit it?",
            "date": "2006-11-13T17:04:38.000+0000",
            "id": 28
        },
        {
            "author": "Grant Ingersoll",
            "body": "I'm not a big fan of tacking a number on to the end of Java names, as it doesn't let you know much about what's in the file or package.  How about ConfigurableBenchmarker or PropertyBasedBenchmarker or something along those lines, since what you are proposing is a property based one.  I think it can just go in the benchmark package or you could make a sub package under there that is more descriptive.\n\nI will try to commit tonight or tomorrow morning.",
            "date": "2006-11-13T19:06:30.000+0000",
            "id": 29
        },
        {
            "author": "Doron Cohen",
            "body": "Good point on names with numbers - I'm renaming the package to taskBenchmark, as I think of it as \"task sequence\" based, more than as propetries based. \n",
            "date": "2006-11-14T20:43:24.000+0000",
            "id": 30
        },
        {
            "author": "Doron Cohen",
            "body": "Would be nice to get some feedback on what I already have at this point for the \"task based benchmark framework for Lucene\".  \n\nSo I am packing it as a zip file. I would probably resubmit as a patch when Grant commits the current benchmark code.\nSee attached taskBenchmark.zip.\n\nTo try out taskBenchmark, unzip under contrib/benchmark, on top of Grant's benchmark.patch.\nThis would do 3 changes:\n\n1. replace build.xml - only change there is adding two targets: run-task-standard and run-task-micro-standard.\n\n2. add 4 new files under conf:\n - task-standard.properties\n - task-standard.alg\n - task-micro-standard.properties\n - task-micro-standard.alg\n\n3. add a src package 'taskBenchmark' side by side with current 'benchmark' package.\n\nTo try it out, go to contrib/benchmark and try 'ant run-task-standard' or 'ant run-task-micro-standard'. \n\nSee inside the .alg files for how a test is specified.\n\nThe algorithm syntax and the entire package is documented in the package javadoc for taskBenchmark (package.html). \n\nRegards,\nDoron",
            "date": "2006-11-15T09:12:16.000+0000",
            "id": 31
        },
        {
            "author": "Doron Cohen",
            "body": "Attached taskBenchmark.zip as described earlier.",
            "date": "2006-11-15T09:15:26.000+0000",
            "id": 32
        },
        {
            "author": "Grant Ingersoll",
            "body": "Committed the benchmark patch plus Doron's update to TestData and TimeData",
            "date": "2006-11-15T14:30:44.000+0000",
            "id": 33
        },
        {
            "author": "Doron Cohen",
            "body": "I am attaching benchmark.byTask.patch - to be applied in the contrib/benchmark directory. \n\nRoot package of byTask classes was modified to org.apache.lucene.benchmark.byTask, in the lines of Grant's suggestion - seems better cause it keeps all benchmark classes under \nlucene.benchmark.\n\nI added one a sample .alg under conf and added some documentation. \n\nEntry point - documentation wise - is the package doc for org.apache.lucene.benchmark.byTask.\n\nThanks for any comments on this!\n\nPS. Before submitting the patch file, I tried to apply it myself on a clean version of the code, just to make sure that it works. But I got errors like this -- Could not retrieve revision 0 of \"...\\byTask\\..\" -- for every file under a new folder. So I am not sure if it is just my (Windows) svn patch applying utility, or is it really impossible to apply a patch that creates files in (yet) nonexistent directories.  I searched Lucene mailing lists and SVN mailing lists and went again through the SVN book again but nowhere could I find what is the expected behavior for applying a patch containing new directories. In fact, \"svn diff\" would not even show you files that are new (again, this is the Windows svn 1.4.2 version). (I used Tortoise SVN to create the patch). This is rather annoying and I might be misunderstanding something basic about SVN, but I thought it'd be better to share this experience here - might save some time for others trying to apply this patch or other patches...",
            "date": "2006-11-16T20:17:16.000+0000",
            "id": 34
        },
        {
            "author": "Grant Ingersoll",
            "body": "Doron,\n\nWhen I apply your patch, I am getting strange errors.  It seems to go through cleanly, but then the new files (for instance, byTask.stats.Report.java) has the whole file occurring twice in each file, thus causing duplicate class exceptions.  This happens for all the files in the byTask package.  The changes in the other files apply cleanly.\n\nI applied the patch as: patch -p0 -i <patch file> as I always do on a clean version.\n\nI suspect that your last comment may be at the root of the issue. Can you try applying this again to a clean version and see if you still have issues or whether it is something I am missing?  Can you regenerate this patch, perhaps using a command line tool?  Looking at the patch file, I am not sure what the issue is.  \n\nOtherwise, based on the documentation, this sounds really interesting and useful.  Based on some of your other patches, I assume you are using this to do benchmarking, no?\n\nThanks,\nGrant",
            "date": "2007-01-04T02:39:17.813+0000",
            "id": 35
        },
        {
            "author": "Doron Cohen",
            "body": "Grant, thanks for trying this out - I will update the patch shortly. \nI am using this for benchmarking - quite easy to add new stuff - and in fact I added some stuff lately but did not update here because wasn't sure if others are interested. \nI will verify what I have with svn head and pack it here as an updated patch.\nRegards,\nDoron",
            "date": "2007-01-04T19:03:42.534+0000",
            "id": 36
        },
        {
            "author": "Doron Cohen",
            "body": "This update of the byTask package includes:\n- allowing to tailor a perf test \"programmically\" (without an .alg file).\n- maintaining both the \"algorithm\" and the run-properties in a single .alg file - this is easier to maintain in my opinion.\n- some code cleanup.\n- build.xml has a single \"task related\" target now: run-task. an ant property is used to invoke other .alg files.\n- documentation updated (package docs under byTask).\n\nTo apply the patch from the trunk dir:   patch -p0 -i <byTask.2.patch.txt>\nTo test it, cd to contrib/benchmark and type:  ant run-task\n\nGrant, I noticed that the patch file contains EOL characters - Unix/DOS thing I guess.\nBut 'patch' works cleanly for me either with these characters or without them, so I am leaving these characters there.\nI hope this patch applies cleanly for you.\n",
            "date": "2007-01-05T04:41:49.735+0000",
            "id": 37
        },
        {
            "author": "Grant Ingersoll",
            "body": "Hey Doron, \n\nYour patch uses JDK 1.5.  I am assuming it is safe to use Class.getName in place of Class.getSimpleName, right?  I think once I do that plus change the String.contains calls to String.indexOf it should all be fine, right?  I have it compiling and running, so that is a good sign.  I will look to commit soon.\n\n-Grant",
            "date": "2007-01-11T03:21:38.435+0000",
            "id": 38
        },
        {
            "author": "Doron Cohen",
            "body": "Oops... I had the impression that compiling with compliance level 1.4 is sufficient to prevent this, but guess I need to read again what that compliance level setting guarantees exactly. \n\nAnyhow there are a 3 things that require 1.5:\n - Boolean.parseBoolean() --> Boolean.valueOf().booleanValue()\n - String.contains() --> indexOf()\n - Class.getSimpleName() --> ?\n\nModifying Class.getSimpleName() to Class.getName() would not be very nice - queries prints and task names prints would be quite ugly. To fix that I added a method simpleName(Class) to byTask.util.Format. I am attaching an updated patch - byTask.jre1.4.patch.txt - that includes this method and removes the Java 1.5  dependency.\n\nThanks for catching this!\nDoron",
            "date": "2007-01-11T07:47:42.230+0000",
            "id": 39
        },
        {
            "author": "Grant Ingersoll",
            "body": "Doron, \n\nI have committed your additions.  This truly is great stuff.  Thank you so much for contributing.  The documentation (code and package level) is well done, the output is very readable.  The alg language is a bit cryptic and takes a little deciphering, but you do document it quite nicely.   I like the extendability factor and I think it will make it easier for people to contribute benchmarking capabilities.\n\nI would love to see someone mod the reporting mechanism in the future to allow for printing info to something other than System.out, as I know people have expressed interest in being able to slurp the output into Excel or similar number crunching tools.   This could also lead to the possibility of running some of the algorithms nightly and then integrating with JUnitPerf or some other performance unit testing approach.\n\nWe may want to consider deprecating the other benchmarking stuff, although, I suppose it can't hurt to have multiple opinions in this area.\n\nAt any rate, this is very much appreciated.  I would encourage everyone who is interested in benchmarking to take a look and provide feedback.  I'm going to mark this bug as finished for now as I think we have a good baseline for benchmarking at this point.\n\nThanks again,\nGrant\n\n\n",
            "date": "2007-01-13T04:15:18.225+0000",
            "id": 40
        },
        {
            "author": "Grant Ingersoll",
            "body": "Have committed a baseline benchmarking suite thanks to Doron and Andrzej.   Bugs can now be opened specific to the code in the contrib area.",
            "date": "2007-01-13T04:16:33.248+0000",
            "id": 41
        },
        {
            "author": "Grant Ingersoll",
            "body": "This has been committed and is available for use.  New issues can be opened on specific problems.",
            "date": "2007-01-20T13:20:43.013+0000",
            "id": 42
        },
        {
            "author": "Marvin Humphrey",
            "body": "During the course of a recent IP audit, I determined that two out of three\nfiles I contributed to LUCENE-675 back in 2006 were in fact based on an\noriginal written by Murray Walker: LuceneIndexer.java and\nBenchmarkingIndexer.pm.   (The third file, \"extract_reuters.plx\", was my own\nwork as advertised.)\n\nMurray has graciously expressed a willingness to license his work to Apache,\nbut since the files in question were not used, the consensus opinion is that\nit would be best to delete them.  For further reference, see the\nlegal-discuss@a.o archives: <http://markmail.org/message/4esu3owjxft5n2f7>.\n\nI feel very fortunate that the problematic contributions were not integrated\ninto Lucene and that it was the work of an eminently reasonable solo author\nwhose work was inadvertently contributed without permission.  I apologize to\nMurray and to the Lucene community for my errors.\n",
            "date": "2010-08-21T17:15:46.233+0000",
            "id": 43
        }
    ],
    "component": "",
    "description": "We need an objective way to measure the performance of Lucene, both indexing and querying, on a known corpus. This issue is intended to collect comments and patches implementing a suite of such benchmarking tests.\n\nRegarding the corpus: one of the widely used and freely available corpora is the original Reuters collection, available from http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz or http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz. I propose to use this corpus as a base for benchmarks. The benchmarking suite could automatically retrieve it from known locations, and cache it locally.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-675",
    "issuetypeClassified": "OTHER",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Lucene benchmark: objective performance test for Lucene",
    "systemSpecification": true,
    "version": ""
}