{
    "comments": [
        {
            "author": "Michael Busch",
            "body": "Here is an interesting article about allocation/deallocation on modern JVMs:\nhttp://www.ibm.com/developerworks/java/library/j-jtp09275.html\n\nAnd here is a snippet that mentions how pooling is generally not faster anymore:\n\n----\nAllocation in JVMs was not always so fast -- early JVMs indeed had poor allocation and garbage collection performance, which is almost certainly where this myth got started. In the very early days, we saw a lot of \"allocation is slow\" advice -- because it was, along with everything else in early JVMs -- and performance gurus advocated various tricks to avoid allocation, such as object pooling. (Public service announcement: Object pooling is now a serious performance loss for all but the most heavyweight of objects, and even then it is tricky to get right without introducing concurrency bottlenecks.) However, a lot has happened since the JDK 1.0 days; the introduction of generational collectors in JDK 1.2 has enabled a much simpler approach to allocation, greatly improving performance. \n----\n\n",
            "date": "2010-03-15T06:33:31.548+0000",
            "id": 0
        },
        {
            "author": "Michael McCandless",
            "body": "Sounds great -- let's test it in practice.",
            "date": "2010-03-15T09:47:41.896+0000",
            "id": 1
        },
        {
            "author": "Michael Busch",
            "body": "Reply to Mike's comment on LUCENE-2293: https://issues.apache.org/jira/browse/LUCENE-2293?focusedCommentId=12845263&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12845263\n\n\n{quote}\nI think we can do even better, ie, that class wastes RAM for the single posting case (intStart, byteStart, lastDocID, docFreq, lastDocCode, lastDocPosition are not needed).\n\nEG we could have a separate class dedicated to the singleton case. When term is first encountered it's enrolled there. We'd probably need a separate hash to store these (though not necessarily?). If it's seen again it's switched to the full posting.\n{quote}\n\nHmm I think we'd need a separate hash.  Otherwise you have to subclass PostingList for the different cases (freq. vs. non-frequent terms) and do instanceof checks? Or with the parallel arrays idea maybe we could encode more information in the dense ID? E.g. use one bit to indicate if that term occurred more than once. \n\n{quote}\nI mean instead of allocating an instance per unique term, we assign an integer ID (dense, ie, 0, 1, 2...).\n\nAnd then we have an array for each member now in FreqProxTermsWriter.PostingList, ie int[] docFreqs, int [] lastDocIDs, etc. Then to look up say the lastDocID for a given postingID you just get lastDocIDs[postingID]. If we're worried about oversize allocation overhead, we can make these arrays paged... but that'd slow down each access.\n{quote}\n\nYeah I like that idea. I've done something similar for representing trees - I had a very compact Node class with no data but such a dense ID, and arrays that stored the associated data.  Very easy to add another data type with no RAM overhead (you only use the amount of RAM the data needs).\n\nThough, the price you pay is for dereferencing multiple times for each array?  \nAnd how much RAM would we safe? The pointer for the PostingList object (4-8 bytes), plus the size of the object header - how much is that in Java? \n\nSeems ilke it's 8 bytes: http://www.codeinstructions.com/2008/12/java-objects-memory-structure.html\n\nSo in a 32Bit JVM we would safe 4 bytes (pointer) + 8 bytes (header) - 4 bytes (ID) = 8 bytes.  For fields with tons of unique terms that might be worth it?  ",
            "date": "2010-03-15T16:33:50.062+0000",
            "id": 2
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nSounds great - let's test it in practice.\n{quote}\n\nI have to admit that I need to catch up a bit on the flex branch.  I was wondering if it makes sense to make these kinds of experiments (pooling vs. non-pooling) with the flex code? Is it as fast as trunk already, or are there related nocommits left that affect indexing performance?  I would think not much of the flex changes should affect the in-memory indexing performance (in TermsHash*).\n",
            "date": "2010-03-15T16:43:31.867+0000",
            "id": 3
        },
        {
            "author": "Earwin Burrfoot",
            "body": "> Seems ilke it's 8 bytes\nObject header is two words, so that's 16bytes for 64bit arch. (probably 12 for 64bit+CompressedOops?)\n\nAlso, GC time is (roughly) linear in number of objects on heap, so replacing single huge array of objects with few huge primitive arrays for their fields does miracles to your GC delays.",
            "date": "2010-03-15T17:05:05.850+0000",
            "id": 4
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Hmm I think we'd need a separate hash. Otherwise you have to subclass PostingList for the different cases (freq. vs. non-frequent terms) and do instanceof checks? Or with the parallel arrays idea maybe we could encode more information in the dense ID? E.g. use one bit to indicate if that term occurred more than once.\n\nOr 2 sets of parallel arrays (one for the singletons).... or, something.\n\nbq. So in a 32Bit JVM we would safe 4 bytes (pointer) + 8 bytes (header) - 4 bytes (ID) = 8 bytes. For fields with tons of unique terms that might be worth it?\n\nAnd also the GC cost.\n\nBut it seems like specializing singleton fields will be the bigger win.\n\nbq. I was wondering if it makes sense to make these kinds of experiments (pooling vs. non-pooling) with the flex code?\n\nLast I tested (a while back now) indexing perf was the same -- need to\ntest again w/ recent changes (eg terms index is switching to packed\nints).  For pooling vs not I'd just do the experiment on trunk?\n\nAnd most of this change (changing how postings data is buffered in\nRAM) is \"above\" flex I expect.\n\nBut if for some reason you need to start changing index postings\nformat then you should probably do that on flex.\n",
            "date": "2010-03-15T17:52:25.477+0000",
            "id": 5
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nbq. Seems ilke it's 8 bytes\n\nObject header is two words, so that's 16bytes for 64bit arch. (probably 12 for 64bit+CompressedOops?)\n{quote}\n\nRight, and the pointer'd also be 8 bytes (but compact int stays at 4\nbytes) so net/net on 64bit JRE savings would be 16-20 bytes per term.\n\nAnother thing we could do if we cutover to parallel arrays is to\nswitch to packed ints.  Many of these fields are horribly wasteful as\nints, eg docFreq or lastPosition.\n",
            "date": "2010-03-15T17:57:50.626+0000",
            "id": 6
        },
        {
            "author": "Jason Rutherglen",
            "body": "Carrying over from LUCENE-2312.  I'm proposing we for starters have a byte slice writer, lock, move or copy(?) the bytes from the writable byte pool/writer to a read only byte block pool, unlock.  This sounds like a fairly self-contained thing that can be unit tested at a low level.\n\nMike, can you add a bit as to how this could work?  Also, what is the IntBlockPool used for?  ",
            "date": "2010-03-16T17:50:16.134+0000",
            "id": 7
        },
        {
            "author": "Jason Rutherglen",
            "body": "Are there going to be issues with the char array buffers as well (ie, will we need to also flush them for concurrency?)",
            "date": "2010-03-16T17:59:51.264+0000",
            "id": 8
        },
        {
            "author": "Michael Busch",
            "body": "Shall we not first try to remove the downstream *PerThread classes and make the DocumentsWriter single-threaded without locking.  Then we add a PerThreadDocumentsWriter and DocumentsWriterThreadBinder, which talks to the PerThreadDWs and IW talks to DWTB.  We can pick other names :)\n\nWhen that's done we can think about what kind of locking/synchronization/volatile stuff we need for LUCENE-2312.",
            "date": "2010-03-16T19:29:17.841+0000",
            "id": 9
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael,\n\nFor LUCENE-2312, I think the searching isn't going to be an\nissue, I've got basic per thread doc writers working (though not\nthoroughly tested). I didn't see a great need to rework all the\nclasses, which even if we did, I'm not sure helps with the byte\narray read write issues? I'd prefer to get a proof of concept\nmore or less working, then refine it from there. I think there's\ntwo main design/implementation issues before we can roll\nsomething out:\n\n1) A new skip list implementation that at specific intervals\nwrites a new skip (ie, single level). Right now in trunk we have\na multilevel skiplist that requires ahead of time the number of\ndocs.\n\n2) Figure out the low -> high levels of byte/char/int array\nvisibility to reader threads. The main challenge here is the\nfact that the DW related code that utilizes this is really hard\nfor me to understand enough to know what can be changed, without\nthe side effect being bunches of other broken stuff. If there\nwas a Directory like class abstraction we could simply override\nand reimplement, we could do that, and maybe there is one, I'm\nnot sure yet. \n\nHowever if reworking the PerThread classes somehow makes the tie\ninto the IO (eg, the byte array pooling) system abstracted and\neasier, then I'm all for it.",
            "date": "2010-03-16T20:18:22.601+0000",
            "id": 10
        },
        {
            "author": "Jason Rutherglen",
            "body": "NormsWriterPerField has a growing norm byte array, we'd need a way to read/write lock it... \n\nI think we have concurrency issues in the TermsHash table?  Maybe it'd need to be rewritten to use ConcurrentHashMap?",
            "date": "2010-03-16T20:45:41.402+0000",
            "id": 11
        },
        {
            "author": "Jason Rutherglen",
            "body": "Actually TermsHashField doesn't need to be concurrent, it's only being written to and the terms concurrent skiplist (was a btree) holds the reference to the posting list.  So I think we're good there because terms enum never accesses the terms hash.  Nice!\n\n",
            "date": "2010-03-16T20:50:09.353+0000",
            "id": 12
        },
        {
            "author": "Michael Busch",
            "body": "I think we all agree that we want to have a single writer thread, multi reader thread model.  Only then the thread-safety problems in LUCENE-2312 can be reduced to visibility (no write-locking).  So I think making this change first makes most sense.  It involves a bit boring refactoring work unfortunately. ",
            "date": "2010-03-16T21:13:17.070+0000",
            "id": 13
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael, Agreed, can you outline how you think we should proceed then?",
            "date": "2010-03-17T00:04:24.868+0000",
            "id": 14
        },
        {
            "author": "Michael Busch",
            "body": "bq. Michael, Agreed, can you outline how you think we should proceed then?\n\nSorry for not responding earlier...\n\nI'm currently working on removing the PostingList object pooling, because it makes TermsHash and TermsHashPerThread much easier.  Have written the patch and all tests pass, though I haven't done performance testing yet.  Making TermsHash and TermsHashPerThread smaller will also make the patch here easier which will remove them. I'll post the patch soon. \n\nNext steps I think here are to make everything downstream of DocumentsWriter single-threaded (removal of *PerThread) classes.  Then we need to write the DocumentsWriterThreadBinder and have to think about how to apply deletes, commits and rollbacks to all DocumentsWriter instances.  ",
            "date": "2010-03-17T21:06:57.640+0000",
            "id": 15
        },
        {
            "author": "Michael Busch",
            "body": "All tests pass but I have to review if with the changes the memory consumption calculation still works correctly. Not sure if the junits test that?\n\nAlso haven't done any performance testing yet.  ",
            "date": "2010-03-17T21:12:28.949+0000",
            "id": 16
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael, I'm guessing this patch needs to be updated as per LUCENE-2329?  ",
            "date": "2010-03-25T17:56:22.173+0000",
            "id": 17
        },
        {
            "author": "Jason Rutherglen",
            "body": "Actually, I just browsed the patch again, I don't think it implements private doc writers as of yet?  \n\nI think you're right, we can get this issue completed.  LUCENE-2312's path looks clear at this point.  Shall I take a whack at it?",
            "date": "2010-03-25T18:03:30.964+0000",
            "id": 18
        },
        {
            "author": "Michael Busch",
            "body": "Hey Jason,\n\nDisregard my patch here.  I just experimented with removal of pooling, but then did LUCENE-2329 instead.  TermsHash and TermsHashPerThread are now much simpler, because all the pooling code is gone after 2329 was committed.  Should make it a little easier to get this patch done.\n\nSure it'd be awesome if you could provide a patch here.  I can help you, we should just frequently post patches here so that we don't both work on the same areas.\n\n",
            "date": "2010-03-25T18:27:36.018+0000",
            "id": 19
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael, I'm working on a patch and will post one (hopefully) shortly.",
            "date": "2010-03-25T19:01:15.862+0000",
            "id": 20
        },
        {
            "author": "Michael Busch",
            "body": "Awesome!",
            "date": "2010-03-25T21:00:30.437+0000",
            "id": 21
        },
        {
            "author": "Jason Rutherglen",
            "body": "I'm a little confused in the flushedDocCount, remap deletes conversion portions of DocWriter.  flushedDocCount is used as a global counter, however when we move to per thread doc writers, it won't be global anymore.  Is there a different (easier) way to perform remap deletes?  ",
            "date": "2010-03-26T00:47:08.211+0000",
            "id": 22
        },
        {
            "author": "Michael McCandless",
            "body": "Good question...\n\nWhen we buffer delete Term/Query we record the current docID as of when that delete had arrived (so that interleaved delete/adds are resolved properly).  The docID we record is \"absolute\" (ie, adds in the base flushedDocCount), so that we can decouple when deletes are materialized (moved into the deletedDocs BitVectors) from when new segments are flushed.\n\nI think we have a couple options.\n\nOption 1 is to use a relative (within the current segment) docID when the deleted Term/Query/docID is first buffered, but then make it absolute only when the segment is finally flushed.\n\nOption 2 is to use a relative docID, but do away with the decoupling, ie force deletions to always flush at the same time the segment is flushed.\n\nI think I like option 1 the best -- I suspect the decoupling gains us performance as it allows us to batch up more deletions (doing deletions in batch gets better locality, and also means opening/closing readers left often, in the non-pooling case).",
            "date": "2010-03-26T09:47:46.007+0000",
            "id": 23
        },
        {
            "author": "Jason Rutherglen",
            "body": "Mike, lets do option 1.  I think the process of making the doc id absolute is simply adding up the previous segments num docs to be the base?  \n\nOption 2 would use reader cloning?\n\n",
            "date": "2010-03-26T16:54:32.110+0000",
            "id": 24
        },
        {
            "author": "Michael McCandless",
            "body": "bq.  I think the process of making the doc id absolute is simply adding up the previous segments num docs to be the base?\n\nRight.\n\nbq. Option 2 would use reader cloning?\n\nI don't think so -- I think it'd have to pull a SegmentReader for every segment every time we flush a new segment, to resolve the deletions.  In the non-pooled case that'd be a newly opened SegmentReader for every segment in the index every time a new segment is flushed.",
            "date": "2010-03-26T16:59:58.774+0000",
            "id": 25
        },
        {
            "author": "Jason Rutherglen",
            "body": "Currently the doc writer manages the ram buffer size, however\nthis needs to be implemented across doc writers for this issue\nto be complete. IW addDoc returns doFlush from DW. I don't think\ndoFlush will be useful anymore?\n\nA slightly different memory management needs to be designed.\nRight now we allow the user to set the max ram buffer size and\nwhen the doc writer's buffers exceed the ram limit, the buffer\nis flushed and the process is complete.\n\nWith this issue, the flush logic probably needs to be bumped up\ninto IW, and flushing becomes a multi-docwriter ram usage\nexamination. For starters, if the aggregate ram usage of all doc\nwriters exceeds the IWC defined ram buffer size, we need to\nschedule flushing the doc writer with the greatest ram usage? I\nwonder if there's something I'm missing here in regards to\nsynchronization issues with DW?",
            "date": "2010-03-26T17:15:24.543+0000",
            "id": 26
        },
        {
            "author": "Jason Rutherglen",
            "body": "Following up on the previous comment, if the current thread (the one calling add doc) is also the one that needs to do the flushing, then only the thread attached to the doc writer with the greatest ram usage can/should do the flushing?",
            "date": "2010-03-26T17:26:04.535+0000",
            "id": 27
        },
        {
            "author": "Michael Busch",
            "body": "The easiest would be if each DocumentsWriterPerThread had a fixed buffer size, then they can flush fully independently and you don't need to manage RAM globally across threads.\n\nOf course then you'd need two config parameters: number of concurrent threads and buffer size per thread.\n",
            "date": "2010-03-26T17:34:16.345+0000",
            "id": 28
        },
        {
            "author": "Michael McCandless",
            "body": "But if 1 thread tends to index lots of biggish docs... don't we want to allow it to use up more than 1/nth?\n\nIe we don't want to flush unless total RAM usage has hit the limit?",
            "date": "2010-03-26T17:41:41.932+0000",
            "id": 29
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. Of course then you'd need two config parameters: number of\nconcurrent threads and buffer size per thread.\n\nI'm not sure how we'd enforce the number of threads? Or we'd\nhave to re-implement the wait system implemented in DW? In\npractice, each thread's DW will probably have roughly the same\nram buffer size so if they each have the same max size, that'd\nbe ok. I don't think we can limit the number of threads though,\nbecause we'd need to then implement interleaving doc ids? ",
            "date": "2010-03-26T17:52:22.387+0000",
            "id": 30
        },
        {
            "author": "Jason Rutherglen",
            "body": "Also, I think we can remove the sync on doFlushInternal?",
            "date": "2010-03-26T18:01:19.521+0000",
            "id": 31
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nBut if 1 thread tends to index lots of biggish docs... don't we want to allow it to use up more than 1/nth?\nIe we don't want to flush unless total RAM usage has hit the limit?\n{quote}\n\nSure that'd be the disadvantage.  But is that a realistic scenario?  That the \"avg. document size per thread\" differ significantly in an application?  ",
            "date": "2010-03-26T18:02:50.094+0000",
            "id": 32
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI'm not sure how we'd enforce the number of threads? Or we'd\nhave to re-implement the wait system implemented in DW? \n{quote}\n\nI was thinking we were going to do that... having a fixed number of DocumentsWriterPerThread instances, and a ThreadBinder that let's a thread wait if the perthread is not available.  You don't need to interleave docIds then?  \n",
            "date": "2010-03-26T18:06:33.524+0000",
            "id": 33
        },
        {
            "author": "Michael McCandless",
            "body": "bq.  But is that a realistic scenario? That the \"avg. document size per thread\" differ significantly in an application?\n\nI think this could happen, eg if an app uses different threads for indexing different sources of docs.  Not all apps index only 140 character docs from all threads ;)\n\nI think for this same reason the ThreadBinder should have affinity, ie, try to schedule the same thread to the same DW, assuming it's free.  If it's not free and another DW is free you should use the other one.\n",
            "date": "2010-03-26T18:35:43.046+0000",
            "id": 34
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I was thinking we were going to do that... having a fixed number of DocumentsWriterPerThread instances, and a ThreadBinder that let's a thread wait if the perthread is not available. You don't need to interleave docIds then?\n\nI think we'd have a fixed max?  And then we'd create a new DW when all existing ones are in-use and we're not yet at the max?\n\nEg if it's a thread pool of size 50 that's indexing, but, the rate of docs is very slow such that in practice only one of these 50 threads is indexing at once, we'd only use one DW.  And we'd flush when that DW hits the RAM limit.",
            "date": "2010-03-26T18:38:11.516+0000",
            "id": 35
        },
        {
            "author": "Michael McCandless",
            "body": "Yes, doFlushInternal should no longer be sync'd.  It should have a small sync'd at the end where it 1) inserts the new SegmentInfo into the in-memory segments, and 2) computes the new base for remapping the deletes.  I think it can then release the sync while it does the remapping of buffered deletes?",
            "date": "2010-03-26T18:42:11.441+0000",
            "id": 36
        },
        {
            "author": "Michael Busch",
            "body": "bq. Not all apps index only 140 character docs from all threads \n\nWhat a luxury! :)\n\n{quote}\nI think for this same reason the ThreadBinder should have affinity, ie, try to schedule the same thread to the same DW, assuming it's free. If it's not free and another DW is free you should use the other one.\n{quote}\n\nIf you didn't have such an affinity but use a random assignment of DWs to threads, would that balance the RAM usage across DWs without a global RAM management?",
            "date": "2010-03-26T19:20:54.732+0000",
            "id": 37
        },
        {
            "author": "Jason Rutherglen",
            "body": "The patch is not committable.  \n\nThe basics are here.  We can worry about max threads later.  For now there's a doc writer per thread.  I still don't fully understand remap deletes, so it's commented out.  There's a flushedDocCount per DW, and a global one in IW.  \n\nIWC is accessed directly where possible.\n\nWhen running TestIndexWriter, the DW resumeAllThreads assertion fails.\n\nIf the total ram buffer size is too large, and the current thread's DW is the largest, it's flushed.  The next test case can be in regards to the memory management.  ",
            "date": "2010-03-26T21:32:41.027+0000",
            "id": 38
        },
        {
            "author": "Jason Rutherglen",
            "body": "In regards to remap deletes. I looked at buffered deletes in\ndepth when implementing LUCENE-2047. I think deletes are fairly\nsimple, though there's all this logic to manage them because\nthey're buffered. I'm liking deleting in the foreground and\nqueuing the deleted doc ids per segment as a way to avoid\nremapping absolute doc ids in commit merge. Hopefully this\nsimplifies things?",
            "date": "2010-03-27T01:09:53.463+0000",
            "id": 39
        },
        {
            "author": "Jason Rutherglen",
            "body": "Mike, can you describe the difference between DW's deletesInRAM and deletesFlushed?  I have some idea, however maybe some clarification will help.  ",
            "date": "2010-03-27T01:41:33.085+0000",
            "id": 40
        },
        {
            "author": "Michael McCandless",
            "body": "I don't think we should delete in FG -- I suspect this'll give net/net worse performance, due to loss of locality.  It also means you must always keep readers available, which is an unnecessary cost for non-NRT apps.\n\ndeletesInRAM are those deletes done during the current segment.  deletesFlushed absorbs deletesInRAM on successful segment flush.  We have to segregate the two for proper recovery if we fail to flush the RAM buffer, eg say you hit a disk full while flushing a new segment, and then you close your IW successfully.  We have to make sure in that case that deletesInRAM are discarded.",
            "date": "2010-03-27T09:44:35.976+0000",
            "id": 41
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}I don't think we should delete in FG - I suspect this'll\ngive net/net worse performance, due to loss of locality. It also\nmeans you must always keep readers available, which is an\nunnecessary cost for non-NRT apps. {quote}\n\nRight, I agree it's best to buffer the deleted terms/queries...\nThe comments below touch on why this came up as a thought.\n\n{quote}deletesInRAM are those deletes done during the current\nsegment. deletesFlushed absorbs deletesInRAM on successful\nsegment flush. We have to segregate the two for proper recovery\nif we fail to flush the RAM buffer, eg say you hit a disk full\nwhile flushing a new segment, and then you close your IW\nsuccessfully. We have to make sure in that case that\ndeletesInRAM are discarded. {quote}\n\nOk, this helps... \n\nThe design issue I'm running into is the buffered deletes \"num\nlimit\" which seems to be the highest doc id of DW at the time\nthe delete docs method is called? Then when apply deletes is\ncalled, deletes are made only up to \"num limit\"? This is to\ninsure that documents (with for example a del term) that are\nadded after the delete docs call, are not deleted when apply\ndeletes is called, unless another call to del docs is made again\n(at which point the \"num limit\" is set to the current DW max doc\nid).\n\nIf the above is true, in order to not remap deletes on each\nmerge, would we need to maintain this \"num limit\" variable per\nDW? I don't think the global remap is useful with per thread DWs\nbecause a DW could fail, and we wouldn't know the order in\nsegment infos it could/would've been placed? Whereas with a\nsingle DW, we know the new segment will be placed last in\nsegment infos.\n\nWe could maintain a global deleted terms/queries queue, but then\nwe'd also need to maintain a term -> \"num limit\" map per DW? It\nseems a bit redundant, but maybe it's ok?\n\n",
            "date": "2010-03-28T23:31:53.192+0000",
            "id": 42
        },
        {
            "author": "Michael Busch",
            "body": "Yes, we would need to buffer terms/queries per DW and also per DW the BufferedDeletes.Num.  The docID spaces in two DWs will be completely independent of each other after this change.\n\n\nOne potential problem that we (I think) even today have is the following: If you index with multiple threads, and then call e.g. deleteDocuments(Term) with one of the indexer threads while you keep adding documents with the other threads, it's not clear to the caller when exactly the deleteDocuments(Term) will happen.  It depends on the thread scheduling. \n\nGoing back to the idea I mentioned here:\nhttps://issues.apache.org/jira/browse/LUCENE-2293?focusedCommentId=12841407&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12841407\n\nI mentioned the idea of having a sequence ID, that gets incremented on add, delete, update.  What if we had even with separate DWs a global sequence ID?  The sequence ID would tell you unambiguously which action happened when.  The add/update/delete methods could return the sequenceID that was assigned to that particular action.  \n\nThen we could e.g. track the delete terms globally together with the sequenceID of the corresponding delete call, while we still apply deletes during flush.  Since sequenceIDs enforce a strict ordering we can figure out to how many docs per DW we need to apply the delete terms.\n\nLater when we switch to real-time deletes (when the RAM is searchable) we will simply store the sequenceIDs in the deletes int[] array which I mentioned in my comment on LUCENE-2293.\n\nDoes this make sense?",
            "date": "2010-03-29T00:34:37.905+0000",
            "id": 43
        },
        {
            "author": "Michael Busch",
            "body": "bq. I think for this same reason the ThreadBinder should have affinity\n\nMike, can you explain what the advantages of this kind of thread affinity are?  I was always wondering why the DocumentsWriter code currently makes efforts to assign a ThreadState always to the same Thread?  Is that being done for performance reasons?  ",
            "date": "2010-03-29T00:58:40.054+0000",
            "id": 44
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}I mentioned the idea of having a sequence ID, that gets\nincremented on add, delete, update. What if we had even with\nseparate DWs a global sequence ID? The sequence ID would tell\nyou unambiguously which action happened when. The\nadd/update/delete methods could return the sequenceID that was\nassigned to that particular action.{quote}\n\nI think adding the global sequence id makes sense and would be\nsimple to add (eg, AtomicLong). However, in the apply deletes\nmethod how would we know which doc to stop deleting at? How\nwould the seq id map to a DW's doc id?\n\nIf we have independent buffered deletes per DW-thread, then how\nwill we keep track of the memory usage? eg, we'll be flushing\nDWs independently, but will not want to double count the same\nterm/query's size for memory tracking? I'm not sure how we'd do\nthat without having a global deletes manager that individual DWs\ninteract with (maybe have a ref count per term/query?). The deletes\nmanager would have the size/ram usage method, not individual\nDWs. The DWs would need to keep a hash map of term ->\nBufferedDeletes.Num.",
            "date": "2010-03-29T02:35:20.280+0000",
            "id": 45
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nHowever, in the apply deletes\nmethod how would we know which doc to stop deleting at? How\nwould the seq id map to a DW's doc id?\n{quote}\n\nWe could have a global deletes-map that stores seqID -> DeleteAction.  DeleteAction either contains a Term or a Query, and in addition an int \"flushCount\" (I'll explain in a bit what flushCount is used for.)\n\nEach DocumentsWriterPerThread would have a growing array that contains each seqID that \"affected\" that DWPT, i.e. the seqIDs of *all* deletes, plus the seqIDs of the adds/updates performed by that particular DWPT.  One bit of a seqID in that array can indicate if it's a delete or add/update.\n\nWhen it's time to flush we sort the array by increasing seqID and then loop a single time through it to find the seqIDs of all DeleteActions.  During the loop we count the number of adds/updates to determine the number of docs the DeleteActions affect.  After applying the deletes the DWPT makes a synchronized call to the global deletes-map and increments the flushCount int for each applied DeleteAction.  If flushCount==numThreadStates (== number of DWPT instances) the corresponding DeleteAction entry can be removed, because it was applied to all DWPT.\n\nI think this should work?  Or is there a simpler solution?\n",
            "date": "2010-03-29T04:48:48.150+0000",
            "id": 46
        },
        {
            "author": "Michael McCandless",
            "body": "Yeah I think we're gonna need the global sequenceID in some form -- my Options 1 or 2 can't work because the interleaving issue (as seen/required by the app) is a global thing.",
            "date": "2010-03-29T09:40:23.522+0000",
            "id": 47
        },
        {
            "author": "Michael McCandless",
            "body": "\nbq. Mike, can you explain what the advantages of this kind of thread affinity are? I was always wondering why the DocumentsWriter code currently makes efforts to assign a ThreadState always to the same Thread? Is that being done for performance reasons?\n\nIt's for performance. I expect there are apps where a given\nthread/pool indexes certain kind of docs, ie, the app threads\nthemselves have \"affinity\" for docs with similar term distributions.\nIn which case, it's best (most RAM efficient) if those docs w/\npresumably similar term stats are sent back to the same DW.  If you\nmix in different term stats into one buffer you get worse RAM\nefficiency.\n\nAlso, for better RAM efficiency you want *fewer* DWs... because we get\nmore RAM efficiency the higher the freq of the terms... but of course\nyou want more DWs for better CPU efficiency whenever that many threads\nare running at once.\n\nNet/net CPU efficiency should trump RAM efficiency, I think, so if\nthere is a conflict we should favor CPU efficiency.\n\nThough, thread affinity doesn't seem that CPU costly to implement?\nLookup the DW your thread first used... if it's free, seize it.  If\nit's not, fallback to any DW that's free.\n",
            "date": "2010-03-29T09:53:47.066+0000",
            "id": 48
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael B.: What you're talking about here:\nhttps://issues.apache.org/jira/browse/LUCENE-2324?focusedCommentI\nd=12850792page=com.atlassian.jira.plugin.system.issuetabpanels%3A\ncomment-tabpanel#action_12850792 is a transaction log?\n\nI'm not sure we need that level of complexity just yet? How\nwould we make the transaction log memory efficient? Are there\nother uses you foresee? Maybe there's a simpler solution for the\nBufferedDeletes.Num per DW problem that could make use of global\nsequence ids? I'd prefer to continue to use the per term/query\nmax doc id. There aren't performance issues with concurrently\naccessing and updating maps, so a global sync lock as the DW map\nvalues are updated should be OK?",
            "date": "2010-03-29T17:41:11.370+0000",
            "id": 49
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI'm not sure we need that level of complexity just yet? How\nwould we make the transaction log memory efficient?\n{quote}\n\nIs that really so complex?  You only need one additional int per doc in the DWPTs, and the global map for the delete terms.  You don't need to buffer the actual terms per DWPT.  I thought that's quite efficient?  But I'm totally open to other ideas.\n\nI can try tonight to code a prototype of this - I don't think it would be very complex actually.  But of course there might be complications I haven't thought of.\n\nbq.  Are there other uses you foresee?\n\nNot really for the \"transaction log\" as you called it.  I'd remove that log once we switch to deletes in the FG (when the RAM buffer is searchable).  But a nice thing would be for add/update/delete to return the seqID, and also the if RAMReader in the future had an API to check up to which seqID it's able to \"see\".  Then it's very clear to user of the API where a given reader is at.  \nFor this to work we have to assign the seqID at the *end* of a call.  E.g. when adding a large document, which takes a long time to process, it should get the seqID assigned after the \"work\" is done and right before the addDocument() call returns.  \n\n",
            "date": "2010-03-29T20:35:26.002+0000",
            "id": 50
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}You only need one additional int per doc in the DWPTs, and the global map for the delete terms.{quote}\n\nOk, lets give it a try, it'll be more clear with the prototype.  \n\nThe clarify, the apply deletes doc id up to will be the flushed doc count saved per term/query per DW, though it won't be saved, it'll be derived from the sequence id int array where the action has been encoded into the seq id int?\n\n",
            "date": "2010-03-29T21:12:17.122+0000",
            "id": 51
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nThe clarify, the apply deletes doc id up to will be the flushed doc count saved per term/query per DW, though it won't be saved, it'll be derived from the sequence id int array where the action has been encoded into the seq id int?\n{quote}\n\nYeah, that's the idea.  Let's see if it works :)",
            "date": "2010-03-29T22:32:19.028+0000",
            "id": 52
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael B, I  was going to wait for your prototype before continuing... :)",
            "date": "2010-04-05T23:23:48.373+0000",
            "id": 53
        },
        {
            "author": "Michael Busch",
            "body": "Sorry, Jason, I got sidetracked with LUCENE-2329 and other things at work.  I'll try to write the sequence ID stuff asap.  However, there's more we need to do here that is sort of independent of the deleted docs problem.  E.g. removing all the downstream perThread classes.   \n\nWe should work with the flex code from now on, as the flex branch will be merged into trunk soon.",
            "date": "2010-04-06T05:14:52.153+0000",
            "id": 54
        },
        {
            "author": "Michael Busch",
            "body": "The patch removes all *PerThread classes downstream of DocumentsWriter.\n\nThis simplifies a lot of the flushing logic in the different consumers.  The patch also removes FreqProxMergeState, because we don't have to interleave posting lists from different threads anymore of course.  I really like these simplifications!\n\nThere is still a lot to do:  The changes in DocumentsWriter and IndexWriter are currently just experimental to make everything compile.  Next I will introduce DocumentsWriterPerThread and implement the sequenceID logic (which was discussed here in earlier comments) and the new RAM management.  I also want to go through the indexing chain once again - there are probably a few more things to clean up or simplify.\n\nThe patch compiles and actually a surprising amount of tests pass.  Only multi-threaded tests seem to fail,\nwhich is not very surprising, considering I removed all thread-handling logic from DocumentsWriter. :) \n\nSo this patch isn't working yet - just wanted to post my current progress.  ",
            "date": "2010-04-14T21:14:14.105+0000",
            "id": 55
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael, nice!  I guess I should've spent more time removing the PerThread classes, but now we're pretty much there.  Indeed the simplification should make things a lot better.  I guess I'll wait for the next patch to work on something.",
            "date": "2010-04-14T21:46:57.987+0000",
            "id": 56
        },
        {
            "author": "Michael McCandless",
            "body": "This is awesome Michael!  Much simpler... nor more FreqProxMergeState, nor the logic to interleave/synchronize writing to the doc stores.  I like it!",
            "date": "2010-04-14T22:25:58.399+0000",
            "id": 57
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nIt's for performance. I expect there are apps where a given\nthread/pool indexes certain kind of docs, ie, the app threads\nthemselves have \"affinity\" for docs with similar term distributions.\nIn which case, it's best (most RAM efficient) if those docs w/\npresumably similar term stats are sent back to the same DW. If you\nmix in different term stats into one buffer you get worse RAM\nefficiency.\n{quote}\n\nI do see your point, but I feel like we shouldn't optimize/make compromises for this use case.  Mainly, because I think apps with such an affinity that you describe are very rare?  The usual design is a queued ingestion pipeline, where a pool of indexer threads take docs out of a queue and feed them to an IndexWriter, I think?  In such a world the threads wouldn't have an affinity for similar docs.\n\nAnd if a user really has so different docs, maybe the right answer would be to have more than one single index?  Even if today an app utilizes the thread affinity, this only results in maybe somewhat faster indexing performance, but the benefits would be lost after flusing/merging.  \n\nIf we assign docs randomly to available DocumentsWriterPerThreads, then we should on average make good use of the overall memory?  Alternatively we could also select the DWPT from the pool of available DWPTs that has the highest amount of free memory?  \n\nHaving a fully decoupled memory management is compelling I think, mainly because it makes everything so much simpler.  A DWPT could decide itself when it's time to flush, and the other ones can keep going independently.  \n\nIf you do have a global RAM management, how would the flushing work?  E.g. when a global flush is triggered because all RAM is consumed, and we pick the DWPT with the highest amount of allocated memory for flushing, what will the other DWPTs do during that flush?  Wouldn't we have to pause the other DWPTs to make sure we don't exceed the maxRAMBufferSize?\nOf course we could say \"always flush when 90% of the overall memory is consumed\", but how would we know that the remaining 10% won't fill up during the time the flush takes?  ",
            "date": "2010-04-15T00:52:09.247+0000",
            "id": 58
        },
        {
            "author": "Michael McCandless",
            "body": "bq. The usual design is a queued ingestion pipeline, where a pool of indexer threads take docs out of a queue and feed them to an IndexWriter, I think?\n\nbq. Mainly, because I think apps with such an affinity that you describe are very rare?\n\nHmm I suspect it's not that rare....  yes one design is a single\nindexing queue w/ dedicated thread pool only for indexing, but a push\nmodel is equal valid, where your app already has separate threads (or\nthread pools) servicing different content sources, so when a doc\narrives to one of those source-specific threads, it's that thread that\nindexes it, rather than handing off to a separately pool.\n\nLucene is used in a very wide variety of apps -- we shouldn't optimize\nthe indexer on such hard app specific assumptions.\n\nbq. And if a user really has so different docs, maybe the right answer would be to have more than one single index?\n\nHmm but the app shouldn't have to resort to this... (it doesn't have\nto today).\n\nBut... could we allow an add/updateDocument call to express this\naffinity, explicitly?  If you index homogenous docs you wouldn't use\nit, but, if you index drastically different docs that fall into clear\n\"categories\", expressing the affinity can get you a good gain in\nindexing throughput.\n\nThis may be the best solution, since then one could pass the affinity\neven through a thread pool, and then we would fallback to thread\nbinding if the document class wasn't declared?\n\nI mean this is virtually identical to \"having more than one index\",\nsince the DW is like its own index.  It just saves some of the\ncopy-back/merge cost of addIndexes...\n\nbq. Even if today an app utilizes the thread affinity, this only results in maybe somewhat faster indexing performance, but the benefits would be lost after flusing/merging.\n\nYes this optimization is only about the initial flush, but, it's\npotentially sizable.  Merging matters less since typically it's not\nthe bottleneck (happens in the BG, quickly enough).\n\nOn the right apps, thread affinity can make a huge difference.  EG if\nyou allow up to 8 thread states, and the threads are indexing content\nw/ highly divergent terms (eg, one language per thread, or, docs w/\nvery different field names), in the worst case you'll be up to 1/8 as\nefficient since each term must now be copied in up to 8 places\ninstead of one.  We have a high per-term RAM cost (reduced thanks to\nthe parallel arrays, but, still high).\n\nbq. If we assign docs randomly to available DocumentsWriterPerThreads, then we should on average make good use of the overall memory?\n\nIt really depends on the app -- if the term space is highly thread\ndependent (above examples) you an end up flush much more frequently for\na given RAM buffer.\n\nbq. Alternatively we could also select the DWPT from the pool of available DWPTs that has the highest amount of free memory?\n\nHmm... this would be kinda costly binder?  You'd need a pqueue?\nThread affinity (or the explicit affinity) is a single\nmap/array/member lookup.  But it's an interesting idea...\n\nbq. If you do have a global RAM management, how would the flushing work? E.g. when a global flush is triggered because all RAM is consumed, and we pick the DWPT with the highest amount of allocated memory for flushing, what will the other DWPTs do during that flush? Wouldn't we have to pause the other DWPTs to make sure we don't exceed the maxRAMBufferSize?\n\nThe other DWs would keep indexing :)  That's the beauty of this\napproach... a flush of one DW doesn't stop all other DWs from\nindexing, unliked today.\n\nAnd you want to serialize the flushing right?  Ie, only one DW flushes\nat a time (the others keep indexing).\n\nHmm I suppose flushing more than one should be allowed (OS/IO have\nalot of concurrency, esp since IO goes into write cache)... perhaps\nthat's the best way to balance index vs flush time?  EG we pick one to\nflush @ 90%, if we cross 95% we pick another to flush, another at\n100%, etc.\n\nbq. Of course we could say \"always flush when 90% of the overall memory is consumed\", but how would we know that the remaining 10% won't fill up during the time the flush takes?\n\nRegardless of the approach for document -> DW binding, this is an\nissue (ie it's non-differentiating here)?  Ie the other DWs continue\nto consume RAM while one DW is flushing.  I think the low/high water\nmark is an OK solution here?  Or the tiered flushing (I think I like\nthat better :) ).\n\nbq. Having a fully decoupled memory management is compelling I think, mainly because it makes everything so much simpler. A DWPT could decide itself when it's time to flush, and the other ones can keep going independently.\n\nI'm all for simplifying things, which you've already nicely done here,\nbut not of it's at the cost of a non-trivial potential indexing perf\nloss.  We're already taking a perf hit here, since the doc stores\ncan't be shared... I think that case is justifiable (good\nsimplification).\n",
            "date": "2010-04-15T16:08:31.169+0000",
            "id": 59
        },
        {
            "author": "Tim Smith",
            "body": "bq. But... could we allow an add/updateDocument call to express this affinity, explicitly?\n\ni would love to be able to explicitly define a \"segment\" affinity for documents i'm feeding\n\nthis would then allow me to say: \nall docs from table a has affinity 1\nall docs from table b has affinity 2\n\nthis would ideally result in indexing documents from each table into a different segment (obviously, i would then need to be able to have segment merging be affinity aware so optimize/merging would only merge segments that share an affinity)",
            "date": "2010-04-15T16:13:03.123+0000",
            "id": 60
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. only one DW flushes at a time (the others keep indexing).\n\nI think it's best to simply flush at 90% for now. We already\nexceed the ram buffer size because of over allocation? Perhaps\nwe can view the ram buffer size as a rough guideline not a hard\nand fast limit because, lets face it, we're using Java which is\nabout as inexact when it comes to RAM consumption as it gets?\nAlso, hopefully it would move the patch along faster and more\ncomplex algorithms could easily be added later. ",
            "date": "2010-04-15T16:17:31.389+0000",
            "id": 61
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\ni would love to be able to explicitly define a \"segment\" affinity for documents i'm feeding\n\nthis would then allow me to say: \nall docs from table a has affinity 1\nall docs from table b has affinity 2\n{quote}\n\nRight, this is exactly what affinity would be good for -- so IW would\ntry to send \"table a\" docs their own DW(s) and \"table b\" docs to their\nown DW(s), which should give faster indexing than randomly binding to\nDWs.\n\nBut:\n\nbq. this would ideally result in indexing documents from each table into a different segment (obviously, i would then need to be able to have segment merging be affinity aware so optimize/merging would only merge segments that share an affinity)\n\nThis part I was not proposing :)\n\nThe affinity would just be an optimization hint in creating the\ninitial flushed segments, so IW can speed up indexing.\n\nProbably if you really want to keep the segments segregated like that,\nyou should in fact index to separate indices?\n",
            "date": "2010-04-15T16:20:33.839+0000",
            "id": 62
        },
        {
            "author": "Tim Smith",
            "body": "bq. Probably if you really want to keep the segments segregated like that, you should in fact index to separate indices?\n\nThats what i'm currently thinking i'll have to do\n\nhowever it would be ideal if i could either subclass IndexWriter or use IndexWriter directly with this affinity concept (potentially writing my own segment merger that is affinity aware)\nthat makes it so i can easily use near real time indexing, as only one IndexWriter will be in the mix, as well as make managing deletes and a whole other host of issues with multiple indexes disappear\nAlso makes it so i can configure memory settings across all \"affinity groups\" instead of having to dynamically create them, each with their own memory bounds",
            "date": "2010-04-15T16:27:02.839+0000",
            "id": 63
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nBut... could we allow an add/updateDocument call to express this\naffinity, explicitly? If you index homogenous docs you wouldn't use\nit, but, if you index drastically different docs that fall into clear\n\"categories\", expressing the affinity can get you a good gain in\nindexing throughput.\n\nThis may be the best solution, since then one could pass the affinity\neven through a thread pool, and then we would fallback to thread\nbinding if the document class wasn't declared?\n{quote}\n\nI would like this if we then also added an API that can be used to specify the\nper-DWPT RAM size.  E.g. if someone has such an app where different threads\nindex docs of different sizes, then the DW that indexes big docs can be given\nmore memory?\n\nWhat I'm mainly trying to avoid is synchronization points between the\ndifferent DWPTs.  For example, currently the same ByteBlockAllocator is shared\nbetween the different threads, so all its methods need to be synchronized.\n\n\n{quote}\nThe other DWs would keep indexing  That's the beauty of this\napproach... a flush of one DW doesn't stop all other DWs from\nindexing, unliked today.\n\nAnd you want to serialize the flushing right? Ie, only one DW flushes\nat a time (the others keep indexing).\n\nHmm I suppose flushing more than one should be allowed (OS/IO have\nalot of concurrency, esp since IO goes into write cache)... perhaps\nthat's the best way to balance index vs flush time? EG we pick one to\nflush @ 90%, if we cross 95% we pick another to flush, another at\n100%, etc.\n{quote}\n\nOh I don't want to disallow flushing in parallel!  I think it makes perfect\nsense to allow more than one DW to flush at the same time.  If each DWPT has a\nprivate max buffer size, then it can decide on its own when it's time to\nflush.\n\n{quote}\nHmm I suppose flushing more than one should be allowed (OS/IO have\nalot of concurrency, esp since IO goes into write cache)... perhaps\nthat's the best way to balance index vs flush time? EG we pick one to\nflush @ 90%, if we cross 95% we pick another to flush, another at\n100%, etc.\n{quote}\n\nIf we allow flushing in parallel and also allow specifying the max RAM per\nDWPT, then there doesn't even have to be any cross-thread RAM tracking?  Each\nDWPT could just flush when its own buffer is full?\n\nSo let's summarize: \n # Expose a ThreadBinder API for controlling number of DWPT instances and\nthread affinity of DWPTs explicitly. (We can later decide if we want to also\nsupport such an affinity after a segment was flushed, as Tim is asking for.\nBut that should IMO not be part of this patch.)\n # Also expose an API for specifying the RAM buffer size per DWPT. \n # Allow flushing in parallel (multiple DWPTs can flush at the same time). A\nDWPT flushes when its buffer is full, independent of what the other DWPTs are\ndoing.\n # The default implementation of the ThreadBinder API assigns threads to DWPT\nrandomly and gives each DWPT 1/n-th of the overall memory.\n # The DWPT RAM value must be updateable.  E.g. when you first start indexing \nonly one DWPT should be created with the max RAM.  Then when multiple threads\nare used for adding documents another DWPT should be added and the RAM\nvalue of the already existing one should be reduced, and possibly a flush of that\nDWPT needs to be triggered.\n\nHow does this sound?",
            "date": "2010-04-19T16:48:51.737+0000",
            "id": 64
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael B, sounds good.  The approach outlined is straightforward and covers the edge cases.  ",
            "date": "2010-04-19T18:28:31.729+0000",
            "id": 65
        },
        {
            "author": "Michael McCandless",
            "body": "I still think this \"zero sync'd code\" at the cost of perf loss /\nexposing per-DWPT details is taking things too far.  You're cutting\ninto the bone...\n\nI don't think we should apps to be setting per-DWPT RAM limits, or,\neven expose to apps how IW manages threads (this is an impl. detail).\n\nI think we should keep the approach we have today -- you set the\noverall RAM limit and IW internally manages flushing when that\nallotted RAM is full.\n\n{quote}\nE.g. if someone has such an app where different threads\nindex docs of different sizes, then the DW that indexes big docs can be given\nmore memory?\n{quote}\n\nHmm this isn't really fair -- the app in general can't predict how\nmany docs of each type will come in, how IW allocates RAM for\ndifferent kinds of docs (this is an impl detail), etc.\n\n{quote}\nWhat I'm mainly trying to avoid is synchronization points between the\ndifferent DWPTs. For example, currently the same ByteBlockAllocator is shared\nbetween the different threads, so all its methods need to be synchronized.\n{quote}\n\nI understand the motivation, but...\n\nIs this sync really so bad?  First, we should move all\nallocators/pools to per-DWPT, so they don't need to be sync'd.\n\nThen, all that needs to be sync'd is the tracking of net RAM used (a\nsingle long), and then the logic to pick the DWPT(s) to flush?  So\nthen each DWPT would allocate its own RAM (unsync'd), track its own\nRAM used (unsync'd), and update the total (in tiny sync block) after\nthe update (add/del) is serviced?\n\nWe're still gonna need sync'd code, anyway (global sequence ID,\ngrabbing a DWPT), right?  We can put this \"go flush a DWPT\" logic in\nthe same block if we really have to?  It feels like we're going to\ngreat lengths (cutting into the bone) to avoid a trivial cost (the\nminor complexity of managing flushing based on aggregate RAM used).\n\n{quote}\n# Expose a ThreadBinder API for controlling number of DWPT instances and\nthread affinity of DWPTs explicitly. (We can later decide if we want to also\nsupport such an affinity after a segment was flushed, as Tim is asking for.\nBut that should IMO not be part of this patch.)\n# Also expose an API for specifying the RAM buffer size per DWPT.\n{quote}\n\nI don't think we should expose so much.\n\nI think, instead, we should add an optional method to Document (eg\nset/getSourceID or something), that'd reference which \"source\" this\ndoc comes from.  The app would set it, optionally, as a \"hint\" to IW.\n\nThe source ID should not be publically tied to DWPT -- how IW\noptimizes based on this \"hint\" from the app is really an impl detail.\nYes, today we'll use it for DWPT affinity; tomorrow, who knows.  EG,\nthat source ID need not be less than the max DWPTs.\n\nWhen source ID isn't provided we'd fallback to the same \"best guess\"\nwe have today (same thread = same source ID).\n\nThe javadoc would be something like \"as a hint to IW, to possibly\nimprove its indexing performance, if you have docs from difference\nsources you should set the source ID on your Document\".  And\nhow/whether IW makes use of this information is \"under the hood\"...\n\nWe can do this as a separate issue... it's fairly orthogonal.\n\nbq. Allow flushing in parallel (multiple DWPTs can flush at the same time). \n\n+1\n\nThis would be a natural way to protect against too much RAM usage\nwhile flush(es) are happening.  Start one flush going, but keep\nindexing docs into the other DWTPs... if RAM usage grows too much\nbeyond your first trigger and before that first flush has finished,\nstart a 2nd DWPT flushing, etc.  This is naturally self-regulating,\nsince the \"mutator\" threads are tied up doing the flushing...\n\n{quote}\nThe DWPT RAM value must be updateable. E.g. when you first start indexing\nonly one DWPT should be created with the max RAM. Then when multiple threads\nare used for adding documents another DWPT should be added and the RAM\nvalue of the already existing one should be reduced, and possibly a flush of that\nDWPT needs to be triggered.\n{quote}\n\nThis isn't great... I mean it's weird that on adding say a 3rd\nindexing thread I suddenly see a flush triggered even though I'm\nnowehere near the RAM limit.  Then, later, if I cut back to using only\n2 threads, I still only ever use up to 2/3rd of my RAM buffer.  IW's\nAPI really shouldn't have such \"surprising\" behavior where how many /\nwhich threads come through it so drastically affect it's flushing\nbehavior.\n",
            "date": "2010-04-20T11:14:24.030+0000",
            "id": 66
        },
        {
            "author": "Jason Rutherglen",
            "body": "I get Michael B's motivation however Mike M's global ram limit\nis probably better from a user perspective and it would be a\nsync only on a long RAM used variable, so we should be good? \n\nSounds like we use the current thread affinity system (ie, a\nhash map), that when the max threads is reached, new threads get\nkind of round robined onto existing DWPTs? \n\n{quote}if RAM usage grows too much beyond your first trigger and\nbefore that first flush has finished, start a 2nd DWPT flushing,\netc. {quote}\n\nWhat's the definition of the \"too much\" portion of the above\nstatement?",
            "date": "2010-04-20T15:50:55.412+0000",
            "id": 67
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI still think this \"zero sync'd code\" at the cost of perf loss /\nexposing per-DWPT details is taking things too far. You're cutting\ninto the bone... \n{quote}\n\nNo worries - I haven't started implementing the RAM management part yet. :)\n\n\n{quote}\nI don't think we should apps to be setting per-DWPT RAM limits, or,\neven expose to apps how IW manages threads (this is an impl. detail).\n\nI think we should keep the approach we have today - you set the\noverall RAM limit and IW internally manages flushing when that\nallotted RAM is full.\n{quote}\n\nI think the reason why we have two different APIs in mind (you: sourceID, I:\nexpert thread binder API) is that we're having different goals with them? You\nwant to make the out-of-the-box indexing performance as good as possible, and\nusers should have to set a minimum amount of easy-to-understand parameters\n(such as buffer size in MB). I think that's the right thing to do of course.\n(though that doesn't prevent us from adding an expert API in addition, as we\nalways have)\n\nI'm thinking a lot about real-time indexing and the searchable RAM buffer\nthese days, so the thread-binder API could help you to have more control over\nwhere your docs will actually end up and which reader will see them. But I\nthink too that this API would be very \"expert\" and not many people would use\nit.\n\nbq. We can do this as a separate issue... it's fairly orthogonal.\n\nYeah I was just thinking the same - I agree.\n\n{quote}\nIs this sync really so bad? First, we should move all\nallocators/pools to per-DWPT, so they don't need to be sync'd.\n{quote}\n\nOK cool that we agree on that. I was worried you wanted to have global pools\ntoo, if it's only the single long it's not very complicated, I agree.\n\n{quote}\nWe're still gonna need sync'd code, anyway (global sequence ID,\ngrabbing a DWPT), right? We can put this \"go flush a DWPT\" logic in\nthe same block if we really have to? It feels like we're going to\ngreat lengths (cutting into the bone) to avoid a trivial cost (the\nminor complexity of managing flushing based on aggregate RAM used).\n{quote}\n\nSorry if I'm being annoying :) Yeah sure, there will be several sync'd spots.\nIf we don't share any data structures between threads that hold indexed (and\nin the future searchable) data I'm happy.\n\nI haven't spent as much time as you thinking about the current RAM management\nyet and the current code that ensures thread safety - still learning some\nparts of the code. I do appreciate all your patient feedback!\n\n{quote}\nThis isn't great... I mean it's weird that on adding say a 3rd\nindexing thread I suddenly see a flush triggered even though I'm\nnowehere near the RAM limit. Then, later, if I cut back to using only\n2 threads, I still only ever use up to 2/3rd of my RAM buffer. IW's\nAPI really shouldn't have such \"surprising\" behavior where how many /\nwhich threads come through it so drastically affect it's flushing\nbehavior.\n{quote}\n\nYeah I don't really like that either. Let's not do that. I had first not\nthought about that disadvantage, added this point later to the list, and never\nreally liked it. (and knew you would complain about it :) )\n\nMy goal is to have a default indexing chain that isn't slower than the one we\nhave today, but searchable and that very fast. That's not trivial, but I think\nwe can do it!\n\nI'll implement the global flush trigger and make all pools DWPT-local. The\nexplicit thread-binder or sourceID APIs we can worry about later, as we agreed\nabove.\n\n",
            "date": "2010-04-20T15:51:22.163+0000",
            "id": 68
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nI think the reason why we have two different APIs in mind (you: sourceID, I:\nexpert thread binder API) is that we're having different goals with\nthem?\n{quote}\n\nYes, I think you're right!\n\n{quote}\nYou want to make the out-of-the-box indexing performance as good as\npossible, and users should have to set a minimum amount of\neasy-to-understand parameters (such as buffer size in MB). I think\nthat's the right thing to do of course.  (though that doesn't prevent\nus from adding an expert API in addition, as we always have)\n{quote}\n\nRight -- simple things should be simple and complex things should be\npossible.\n\n{quote}\nI'm thinking a lot about real-time indexing and the searchable RAM buffer\nthese days, so the thread-binder API could help you to have more control over\nwhere your docs will actually end up and which reader will see them. But I\nthink too that this API would be very \"expert\" and not many people would use\nit.\n{quote}\n\nIn fact now I want both :)\n\nIe, make it possible (optional) to declare the sourceID, and IW\noptimizes based on this hint.\n\nBut, also, letting advanced apps directly control individual DWPTs.\n(I think a new experimental Indexer interface can work well here...).\n\n{quote}\nbq. We can do this as a separate issue... it's fairly orthogonal.\n\nYeah I was just thinking the same - I agree.\n{quote}\n\nOK I'll open this...\n\n\n{quote}\nbq. Is this sync really so bad? First, we should move all allocators/pools to per-DWPT, so they don't need to be sync'd.\n\nOK cool that we agree on that. I was worried you wanted to have global pools\ntoo, if it's only the single long it's not very complicated, I agree.\n{quote}\n\nYeah let's not do global pools (anymore!)...\n\nbq. Sorry if I'm being annoying :)\n\nNo, you're not!  You're asking good questions (as usual)!\n\n{quote}\nMy goal is to have a default indexing chain that isn't slower than the one we\nhave today, but searchable and that very fast. That's not trivial, but I think\nwe can do it!\n{quote}\n\nThis is an awesome goal, and I agree very reachable.  Though the\ndeletes/sequence ID/merging/NRT interaction is going to be fun...\n\n{quote}\nI'll implement the global flush trigger and make all pools DWPT-local. The\nexplicit thread-binder or sourceID APIs we can worry about later, as we agreed\nabove.\n{quote}\n\nOK thanks.\n",
            "date": "2010-04-21T14:42:12.071+0000",
            "id": 69
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nSounds like we use the current thread affinity system (ie, a\nhash map), that when the max threads is reached, new threads get\nkind of round robined onto existing DWPTs?\n{quote}\n\nYeah something along those lines... and clearing out all mappings for\na given DWPT when it flushes.\n\n{quote}\nbq. if RAM usage grows too much beyond your first trigger and before that first flush has finished, start a 2nd DWPT flushing, etc.\n\nWhat's the definition of the \"too much\" portion of the above\nstatement?\n{quote}\n\nWe could do something simple, eg, at 90% RAM used, you flush your\nfirst DWPT.  At 110% RAM used, you flush all DWPTs.  And take linear\nsteps in between?\n\nEG if I have 5 DWPTs, I'd flush first one at 90%, 2nd at 95%, 3rd at\n100%, 4th at 105% and 5th at 110%.\n\nOf course, if flushing is fast, then RAM is quickly freed up, then we\nonly flush 1 DWPT at a time... we only need these tiers to\nself-regulate RAM consumed from ongoing indexing vs time it takes to\ndo the flush.\n",
            "date": "2010-04-21T15:11:55.803+0000",
            "id": 70
        },
        {
            "author": "Michael Busch",
            "body": "How shall we actually handle flushing by maxBufferedDocs?  It'd be the easiest to just make this a per-DWPT flush trigger.  Of course that'd be a change in runtime behavior, but I think that's acceptable?\n\nAnd flushing by maxBufferedDeleteTerms is also tricky, because that needs to be a flush across all DWPTs?  With searchable RAM buffers and deletes in the foreground this trigger should actually not be necessary anymore?  We would probably apply deletes when the realtime IndexReader is (re)opened?",
            "date": "2010-04-21T19:11:44.821+0000",
            "id": 71
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}flushing by maxBufferedDeleteTerms is also tricky,\nbecause that needs to be a flush across all DWPTs{quote}\n\nI think we simply keep track of maxBufferedDocs and\nmaxBufferedDeleteTerms globally. We can't deprecate because\nthat'd be a pain for users. \n\n{quote}With searchable RAM buffers and deletes in the foreground\nthis trigger should actually not be necessary anymore?{quote}\n\nThe deletes aren't entirely in the foreground, only the RAM\nbuffer deletes. Deletes to existing segments would use the\nexisting clone and delete mechanism. I asked about foreground\ndeletes to existing segments before, and we agreed that it's not\na good idea due to locality of terms/postings.\n\n{quote}We would probably apply deletes when the realtime\nIndexReader is (re)opened?{quote}\n\nRight, that's how the existing apply deletes basically works.\nThat'd be the same. Oh ok, I see, perhaps the global deletes\nmanager could cache the deletes for the existing segments, the\nDWPTs can keep track of their deletes separately. We probably\nwon't apply segment or DWPT deletes in the foreground? We'd\ncache them separately, and update ram consumption and unique\nterm/query counts globally. ",
            "date": "2010-04-21T20:16:06.510+0000",
            "id": 72
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nThe deletes aren't entirely in the foreground, only the RAM\nbuffer deletes. Deletes to existing segments would use the\nexisting clone and delete mechanism.\n{quote}\n\nI think deletes should all be based on the new sequenceIDs. Today the buffered\ndeletes depend on a value that can change (numDocs changes when segments are\nmerged). But with sequenceIDs they're absolute: each delete operation will\nhave a sequenceID assigned, and the ordering of all write operations (add,\nupdate, delete) is unambiguously defined by the sequenceIDs. Remember that\naddDocument/updateDocument/deleteDocuments will all return the sequenceID.\n\nThis means that we don't have to remap deletes anymore. Also the pooled\nSegmentReaders that read the flushed segments can use arrays of sequenceIDs\ninstead of BitSets? Of course that needs more memory, but even if you add 1M\ndocs without ever calling IW.commit/close you only need 8MB - I think that's\nacceptable. And this size is independent on how many times you call\nreopen/clone on the realtime readers, because they can all share the same\ndeletes array.\n\nWe can also modify IW.commit/close to return the latest sequenceID. This would\nbe very nice for document tracking. E.g. when you hit an aborting exception\nafter you flushed already some segments, then even though you must discard\neverything in the DW's buffer you can still call commit/close to commit\neverything that doesn't have to be discarded. IW.commit/close would in that\ncase return the sequenceID of the latest write operation that was successfully\ncommitted, i.e. that would be visible to an IndexReader. Though we have to be\ncareful here: multiple segments can have interleaving sequenceIDs, so we must\ndiscard every segment that has one or more sequenceIDs greater than the lowest\none in the DW. So we still need the push-deletes logic, that keeps RAM deletes\nseparate from the flushed ones until flushing was successful.\n\nDW/IW need to keep track of the largest sequenceID that is *safe*, i.e. that\ncould be committed even if DW hits an aborting exception. Some invariants: \n * All deletes with sequenceID smaller or equal to safeSeqID have already been\napplied to the deletes arrays of the flushed segments. \n * All deletes with sequenceID greater than safeSeqID are in a deletesInRAM \nbuffer. \n * safeSeqID is always smaller than any buffered doc or buffered delete in DW \nor DWPT.\n * safeSeqID is always equal to the maximum sequenceID of one, and only one,\nflushed segment.\n\nWhen IW.close/commit is called then safeSeqID is returned. If no aborting\nexception occurred it equals the highest sequenceID ever assigned (during that\nIW \"session\"). In any case it's always the sequenceID of the latest write\noperation an IndexReader will \"see\" that you open after IW.close/commit\nfinished.\n\nBut this would be nice for apps to track which docs made it successfully into\nthe index. Apps can then externally keep a log to figure out what they have to\nreply in case of exceptions.\n\nDoes all this make sense? :) This is very complex stuff, I wouldn't be\nsurprised if there's something I didn't think about.",
            "date": "2010-04-21T23:59:51.917+0000",
            "id": 73
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI think we simply keep track of maxBufferedDocs and\nmaxBufferedDeleteTerms globally. We can't deprecate because\nthat'd be a pain for users. \n{quote}\n\nThe problem with maxBufferedDocs is that you can only enforce it by\n\"stopping the world\", i.e. preventing other DWPTs from adding docs while\none DWPT is flushing.  Unless we also introduce the 90%-95% etc flush\ntriggers for maxBufferedDocs.\n\nThe other option would be to have maxBufferedDocs per DWPT.  Then\nthe theoretical total limit of docs in RAM would be \nmaxNumberThreadStates * maxBufferedDocs.",
            "date": "2010-04-22T08:17:06.283+0000",
            "id": 74
        },
        {
            "author": "Michael McCandless",
            "body": "bq. The problem with maxBufferedDocs is that you can only enforce it by \"stopping the world\"\n\nWhy not just stop the world for maxBufferedDocs/maxBufferedDelDocs?\nIt should be pretty simple to implement?\n\nObviously you'll take a perf hit if you flush by these counts, so, you\nreally should flush by RAM instead... But some apps need this (eg\nsetting up indices for ParallelReader, until we get SliceWriter\nworking...).\n",
            "date": "2010-04-22T15:46:43.206+0000",
            "id": 75
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nbq. The problem with maxBufferedDocs is that you can only enforce it by \"stopping the world\"\n\nWhy not just stop the world for maxBufferedDocs/maxBufferedDelDocs?\nIt should be pretty simple to implement?\n\nObviously you'll take a perf hit if you flush by these counts, so, you\nreally should flush by RAM instead... But some apps need this (eg\nsetting up indices for ParallelReader, until we get SliceWriter\nworking...).\n{quote}\n\nI'm not sure I understand how this would help for ParallelReader?\nI think you can't use multi-threaded indexing even today, because you\nhave no control over the order in which the docs will make it into the\nindex.  \n\nSo having maxBufferedDocs per DWPT seems tempting to me.  Then you know\nthat each written segment will have exactly a size of maxBufferedDocs,\nso this is much more predictable.  And if you index with a single \nthread only the behavior is identical to a \"global\" maxBufferedDocs\nflush trigger.",
            "date": "2010-04-22T17:01:00.346+0000",
            "id": 76
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nI'm not sure I understand how this would help for ParallelReader?\nI think you can't use multi-threaded indexing even today, because you\nhave no control over the order in which the docs will make it into the\nindex.\n{quote}\n\nWell, to set up indexes for PR today, you have to run IndexWriter in a very degraded state -- flush by doc count, use a single thread, turn off concurrent merging (use SMS), use LogDocMergePolicy.\n\n{quote}\nSo having maxBufferedDocs per DWPT seems tempting to me. Then you know\nthat each written segment will have exactly a size of maxBufferedDocs,\nso this is much more predictable. And if you index with a single \nthread only the behavior is identical to a \"global\" maxBufferedDocs\nflush trigger.\n{quote}\nYeah, maybe that'd be sufficient...?  It'd sort of \"match\" the current behaviour, in that you get segments flushed to the index with that many docs.",
            "date": "2010-04-22T17:08:41.005+0000",
            "id": 77
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. It'd sort of \"match\" the current behaviour, in that you get segments flushed to the index with that many docs.\n\nIt seems like the consensus is, most users don't use the maxBufferedDocs feature, however for the ones that do, we can stop the world and flush all segments?  That way we can focus on the RAM consumed use case?  ",
            "date": "2010-04-22T17:18:09.806+0000",
            "id": 78
        },
        {
            "author": "Michael Busch",
            "body": "No, I think the consensus is to have a DWPT maxBufferedDocs flush trigger, not a global one.",
            "date": "2010-04-22T18:24:27.260+0000",
            "id": 79
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. I think the consensus is to have a DWPT maxBufferedDocs flush trigger, not a global one. \n\nOk, so we'll need to implement dynamic resizing of the maxBufferedDocs per DWPT in the case where the maxNumberThreadStates changes?",
            "date": "2010-04-22T18:29:14.307+0000",
            "id": 80
        },
        {
            "author": "Michael Busch",
            "body": "bq. No, I think the consensus is to have a DWPT maxBufferedDocs flush trigger, not a global one. \n\nI think we should just keep it simple.  If you set maxBufferedDocs to 1000, then every thread will flush at 1000 docs.\nIf you set maxThreadStates to 5, then you could at the same time in theory have 5000 docs in memory.\n\nWe just have to explain this in the javadocs.  It's a change that we should also mention in the backwards-compatibility section.",
            "date": "2010-04-22T18:53:18.336+0000",
            "id": 81
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}This means that we don't have to remap deletes\nanymore.{quote}\n\nGood.\n\n{quote}the pooled SegmentReaders that read the flushed segments\ncan use arrays of sequenceIDs instead of BitSets?{quote}\n\nOk, that'd be an improvement. In order to understand all the\nlogic, I'd need to see a prototype, I can't really regurgitate\nwhat I've read thus far. Are there any concurrency issues with\nthe seq arrays? Like say if a reader is iterating a posting\nlist? I guess not because we're always incrementing? \n\n{quote}safeSeqID is always smaller than any buffered doc or\nbuffered delete in DW or DWPT.{quote}\n\nCan you elaborate on this? Wouldn't safeSeqID be greater than\nbuffered doc or deleted doc due to add\ndocs and interleaving?\n\nAlso can we modify the terminology, perhaps committed seq id is\nbetter? To me that's a little more clear. Maybe it'd be good to\nstart a wiki on this so we can have a master doc we're all\nreferring to (kind of a Solr thing that because the projects are\nmerged we can try for?) ;)\n\n",
            "date": "2010-04-22T19:03:56.068+0000",
            "id": 82
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}We just have to explain this in the javadocs. It's a change that we should also mention in the backwards-compatibility section.{quote}\n\nWe're punting eh?",
            "date": "2010-04-22T19:16:36.733+0000",
            "id": 83
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael B, what's the status?  I'm thinking of working on it.",
            "date": "2010-06-08T16:25:00.536+0000",
            "id": 84
        },
        {
            "author": "Michael Busch",
            "body": "I just created a new branch for this (and related RT features) here:\nhttps://svn.apache.org/repos/asf/lucene/dev/branches/realtime_search\n\nI'll commit the latest version of my patch soon (have to update to trunk and make it compile)...\n",
            "date": "2010-06-10T22:15:22.700+0000",
            "id": 85
        },
        {
            "author": "Michael Busch",
            "body": "Finally a new version of the patch! (Sorry for keeping you guys waiting...)\n\nIt's not done yet, but it compiles (against realtime branch!) and >95% of the core test cases pass.\n\nWork done in addition to last patch:\n\n- Added DocumentsWriterPerThread\n- Reimplemented big parts of DocumentsWriter\n- Added DocumentsWriterThreadPool which is an extension point for different pool implementation.  The default impl is\n  the ThreadAffinityDocumentsWriterThreadPool, which does what the old code did (try to assign a DWPT always to \n  the same thread).  It should be easy now to add Document#getSourceID() and another pool that can assign threads\n  based on the sourceID.\n- Initial implementation of sequenceIDs.  Currently they're only used to keep track of deletes and not for\n  e.g. NRT readers yet.\n- Lots of other changes here and there.\n\nTODOs:\n\n- Implement flush-by-ram logic\n- Implement logic to discard deletes from the deletes buffer\n- Finish sequenceID handling: IW#commit() and IW#close() should return ID of last flushed sequenceID\n- Maybe change delete logic:  currently deletes are applied when a segment is flushed.  Maybe we can keep it this way\n  in the realtime-branch though, because that's most likely what we want to do once the RAM buffer is searchable and\n  deletes are cheaper as they can then be done in-memory before flush\n- Fix unit tests (mostly exception handling and thread safety)\n- New test cases, e.g. for sequenceID testing\n- Simplify code:  In some places I copied code around, which can probably be further simplified\n- I started removing some of the old setters/getters in IW which are not in IndexWriterConfig - need to finish that,\n  or revert those changes and use a different patch\n- Fix nocommits\n- Performance testing\n\nI'm planning to commit this soon to the realtime branch, even though it's obviously not done yet.  But it's a big \npatch and changes will be easier to track with an svn history.",
            "date": "2010-07-21T10:22:14.973+0000",
            "id": 86
        },
        {
            "author": "Michael Busch",
            "body": "OK, I committed to the branch.  I'll try tomorrow to merge trunk into the branch.  I was already warned that there will most likely be lots of conflicts - so help is welcome! :)  ",
            "date": "2010-07-21T10:32:46.318+0000",
            "id": 87
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael, thanks for posting and committing the patch.  I'll be taking a look.\n\nBefore I/we forget, maybe we can describe what we discussed about RT features such as the terms dictionary (the AtomicIntArray linked list, possible usage of a btree), the multi-level skip list (static levels), and other such features at: https://issues.apache.org/jira/browse/LUCENE-2312",
            "date": "2010-07-21T17:34:30.512+0000",
            "id": 88
        },
        {
            "author": "Jason Rutherglen",
            "body": "We need to update the indexing chain comment in DocumentsWriterPerThread",
            "date": "2010-07-21T17:37:28.549+0000",
            "id": 89
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nWe need to update the indexing chain comment in DocumentsWriterPerThread \n{quote}\n\nThere's a lot of code cleanup to do.  I just wanted to checkpoint what I have so far.\n\n\n{quote}\nBefore I/we forget, maybe we can describe what we discussed about RT features such as the terms dictionary (the AtomicIntArray linked list, possible usage of a btree), the multi-level skip list (static levels), and other such features at: https://issues.apache.org/jira/browse/LUCENE-2312\n{quote}\n\nYeah, I will.  But first I need to catch up on sleep :)",
            "date": "2010-07-21T18:28:51.938+0000",
            "id": 90
        },
        {
            "author": "Jason Rutherglen",
            "body": "Looks like we're not using MergeDocIDRemapper anymore?",
            "date": "2010-07-21T21:02:26.042+0000",
            "id": 91
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}Implement logic to discard deletes from the deletes\nbuffer{quote}\n\nMichael, where in the code is this supposed to occur?\n\n{quote}Implement flush-by-ram logic{quote}\n\nI'll make a go of this.\n\n{quote}Maybe change delete logic: currently deletes are applied\nwhen a segment is flushed. Maybe we can keep it this way in the\nrealtime-branch though, because that's most likely what we want\nto do once the RAM buffer is searchable and deletes are cheaper\nas they can then be done in-memory before flush{quote}\n\nI think we'll keep things this way for this issue (ie, per\nthread document writers), however for LUCENE-2312 I think we'll\nwant to implement foreground deletes (eg, updating the deleted\ndocs sequences int[]).",
            "date": "2010-07-21T21:22:26.689+0000",
            "id": 92
        },
        {
            "author": "Michael McCandless",
            "body": "This is looking awesome Michael!  I love the removal of *PerThread --\nthey are all logically absorbed into DWPT, so everything is now per\nthread.\n\nI still see usage of docStoreOffset, but aren't we doing away with\nshared doc stores with the cutover to DWPT?\n\nI think you can further simplify DocumentsWriterPerThread.DocWriter;\nin fact I think you can remove it & all subclasses in consumers!  The\nconsumers can simply directly write their files.  The only reason this\nclass was created was because we have to interleave docs when writing\nthe doc stores; this is no longer needed since doc stores are again\nprivate to the segment.  I think we don't need PerDocBuffer, either.\nAnd this also simplifies RAM usage tracking!\n\nAlso, we don't need separate closeDocStore; it should just be closed\nduring flush.\n\nI like the ThreadAffinityDocumentsWriterThreadPool; it's the default\nright (I see some tests explicitly setting in on IWC; not sure why)?\n\nWe should make the in-RAM deletes impl somehow pluggable?\n",
            "date": "2010-07-22T10:27:35.580+0000",
            "id": 93
        },
        {
            "author": "Michael Busch",
            "body": "Thanks, Mike - great feedback! (as always)\n\n{quote}\nI still see usage of docStoreOffset, but aren't we doing away with\nshared doc stores with the cutover to DWPT?\n{quote}\n\nDo we want all segments that one DWPT writes to share the same\ndoc store, i.e. one doc store per DWPT, or remove doc stores \nentirely?\n\n\n{quote}\nI think you can further simplify DocumentsWriterPerThread.DocWriter;\nin fact I think you can remove it & all subclasses in consumers!\n{quote}\n\nI agree!  Now that a high number of testcases pass it's less scary\nto modify even more code :)  - will do this next.\n\n\n{quote}\nAlso, we don't need separate closeDocStore; it should just be closed\nduring flush.\n{quote}\n\nOK sounds good.\n\n\n{quote}\nI like the ThreadAffinityDocumentsWriterThreadPool; it's the default\nright (I see some tests explicitly setting in on IWC; not sure why)?\n{quote}\n\nIt's actually only TestStressIndexing2 and it sets it to use a different \nnumber of max thread states than the default.\n\n\n{quote}\nWe should make the in-RAM deletes impl somehow pluggable?\n{quote}\n\nDo you mean so that it's customizable how deletes are handled? \nE.g. doing live deletes vs. lazy deletes on flush?\nI think that's a good idea.  E.g. at Twitter we'll do live deletes always\nto get the lowest latency (and we don't have too many deletes),\nbut that's probably not the best default for everyone.\nSo I agree that making this customizable is a good idea.\n\nIt'd also be nice to have a more efficient data structure to buffer the\ndeletes.  With many buffered deletes the java hashmap approach\nwill not be very efficient.  Terms could be written into a byte pool,\nbut what should we do with queries?",
            "date": "2010-07-22T16:19:20.450+0000",
            "id": 94
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. It'd also be nice to have a more efficient data structure to buffer the deletes. With many buffered deletes the java hashmap approach will not be very efficient. Terms could be written into a byte pool, but what should we do with queries?\n\nIMO, terms are an order of magnitude more important than queries.  Most deletes will be by some sort of unique id, and will be in the same field.\n\nPerhaps a single byte[] with length prefixes (like the field cache has).  A single int could then represent a term (it would just be an offset into the byte[], which is field-specific, so no need to store the field each time).\n\nWe could then build a treemap or hashmap that natively used an int[]... but that may not be necessary (depending on how deletes are applied).  Perhaps a sort could be done right before applying, and duplicate terms could be handled at that time.\n\nAnyway, I'm only casually following this issue, but I'ts looking like really cool stuff!",
            "date": "2010-07-22T16:56:05.498+0000",
            "id": 95
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nbq. I still see usage of docStoreOffset, but aren't we doing away with shared doc stores with the cutover to DWPT?\n\nDo we want all segments that one DWPT writes to share the same\ndoc store, i.e. one doc store per DWPT, or remove doc stores \nentirely?\n{quote}\n\nOh good question... a single DWPT can in fact continue to share doc\nstore across the segments it flushes.\n\nHmm, but... this opto only helps in that we don't have to merge the\ndoc stores if we merge segments that already share their doc stores.\nBut if (say) I have 2 threads indexing, and I'm indexing lots of docs\nand each DWPT has written 5 segments, we will then merge these 10\nsegments, and must merge the doc stores at that point.  So the sharing\nisn't really buying us much (just not closing old files & opening new\nones, which is presumably negligible)?\n\n{quote}\nbq. I think you can further simplify DocumentsWriterPerThread.DocWriter; in fact I think you can remove it & all subclasses in consumers!\n\nI agree! Now that a high number of testcases pass it's less scary\nto modify even more code  - will do this next.\n\nbq. Also, we don't need separate closeDocStore; it should just be closed during flush.\n\nOK sounds good.\n{quote}\n\nSuper :)\n\n{quote}\nbq. I like the ThreadAffinityDocumentsWriterThreadPool; it's the default right (I see some tests explicitly setting in on IWC; not sure why)?\n\nIt's actually only TestStressIndexing2 and it sets it to use a different \nnumber of max thread states than the default.\n{quote}\n\nAhh OK great.\n\n{quote}\nbq. We should make the in-RAM deletes impl somehow pluggable?\n\nDo you mean so that it's customizable how deletes are handled? \n{quote}\n\nActually I was worried about the long[] sequenceIDs (adding 8 bytes\nRAM per buffered doc) -- this could be a biggish hit to RAM efficiency\nfor small docs.\n\n{quote} E.g. doing live deletes vs. lazy deletes on flush?\nI think that's a good idea. E.g. at Twitter we'll do live deletes always\nto get the lowest latency (and we don't have too many deletes),\nbut that's probably not the best default for everyone.\nSo I agree that making this customizable is a good idea.\n{quote}\n\nYeah, this too :)\n\nActually deletions today are not applied on flush -- they continue to\nbe buffered beyond flush, and then get applied just before a merge\nkicks off.  I think we should keep this (as an option and probably as\nthe default) -- it's important for apps w/ large indices that don't use\nNRT (and don't pool readers) because it's costly to open readers.\n\nSo it sounds like we should support \"lazy\" (apply-before-merge like\ntoday) and \"live\" (live means resolve deleted Term/Query -> docID(s)\nsynchronously inside deleteDocuments, right?).\n\nLive should also be less performant because of less temporal locality\n(vs lazy).\n\n{quote}\nIt'd also be nice to have a more efficient data structure to buffer the\ndeletes. With many buffered deletes the java hashmap approach\nwill not be very efficient. Terms could be written into a byte pool,\nbut what should we do with queries?\n{quote}\n\nI agree w/ Yonik: let's worry only about delete by Term (not Query)\nfor now.\n\nMaybe we could reuse (factor out) TermsHashPerField's custom hash\nhere, for the buffered Terms?  It efficiently maps a BytesRef --> int.\n\nAnother thing: it looks like finishFlushedSegment is sync'd on the IW\ninstance, but, it need not be sync'd for all of that?  EG\nreaderPool.get(), applyDeletes, building the CFS, may not need to be\ninside the sync block?\n",
            "date": "2010-07-22T17:30:21.711+0000",
            "id": 96
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nPerhaps a single byte[] with length prefixes (like the field cache has).  A single int could then represent a term (it would just be an offset into the byte[], which is field-specific, so no need to store the field each time).\n{quote}\n\nYeah that's pretty much how TermsHashPerField works.  I agree with Mike, \nlet's reuse that code.\n\n\n{quote}\nHmm, but... this opto only helps in that we don't have to merge the\ndoc stores if we merge segments that already share their doc stores.\nBut if (say) I have 2 threads indexing, and I'm indexing lots of docs\nand each DWPT has written 5 segments, we will then merge these 10\nsegments, and must merge the doc stores at that point. So the sharing\nisn't really buying us much (just not closing old files & opening new\nones, which is presumably negligible)?\n{quote}\n\nYeah that's true.  I agree it won't help much. I think we should just \nremove the doc stores, great simplification (which should also make \nparallel indexing a bit easier :) ).  \n\n\n{quote}\nAnother thing: it looks like finishFlushedSegment is sync'd on the IW\ninstance, but, it need not be sync'd for all of that? EG\nreaderPool.get(), applyDeletes, building the CFS, may not need to be\ninside the sync block?\n{quote}\n\nThanks for the hint.  I need to carefully go over all the synchronization, \nthere are likely more problems.  ",
            "date": "2010-07-22T17:51:15.682+0000",
            "id": 97
        },
        {
            "author": "Yonik Seeley",
            "body": "bq. Yeah that's pretty much how TermsHashPerField works. I agree with Mike, let's reuse that code.\n\nDo we even need to maintain a hash over it though, or can we simply keep a list (and allow dup terms until it's time to apply them)?",
            "date": "2010-07-22T18:00:44.799+0000",
            "id": 98
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}I think we should just remove the doc stores{quote}\n\nRight, I think we should remove sharing doc stores between\nsegments. And in general, RT apps will likely not want to use\ndoc stores if they are performing numerous updates and/or\ndeletes. We can explicitly state this in the javadocs.\n\nI'm thinking we could explore efficient deleted docs as sequence\nids in a different issue, specifically storing them in a short[]\nand wrapping around.  ",
            "date": "2010-07-22T21:06:14.005+0000",
            "id": 99
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI think we don't need PerDocBuffer, either.\nAnd this also simplifies RAM usage tracking!\n{quote}\n\nThe nice thing about that buffer is that on\nnon-aborting exceptions we can simply skip\nthe whole document, because nothing gets\nwritten to the stores until finishDocument \nis called.",
            "date": "2010-07-23T16:20:17.362+0000",
            "id": 100
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\n\nbq. I think we don't need PerDocBuffer, either.  And this also simplifies RAM usage tracking!\n\nThe nice thing about that buffer is that on\nnon-aborting exceptions we can simply skip\nthe whole document, because nothing gets\nwritten to the stores until finishDocument \nis called.\n{quote}\n\nWell, though, if we did write it \"live\", since the doc is marked deleted, it would be \"harmless\" right?\n\nOnly difference is it took up some disk space (which should be minor given that it's not \"normal\" to have lots of docs hitting non-aborting exceptions).\n\nAlso... when the doc is large, the fact that we double buffer (write first to RAMFile then to the store) can be costly.  Ie we consume 1X the stored-field size in transient RAM, vs writing directly to disk, I think?",
            "date": "2010-07-23T16:28:59.827+0000",
            "id": 101
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nWell, though, if we did write it \"live\", since the doc is marked deleted, it would be \"harmless\" right?\n\nOnly difference is it took up some disk space (which should be minor given that it's not \"normal\" to have lots of docs hitting non-aborting exceptions).\n\nAlso... when the doc is large, the fact that we double buffer (write first to RAMFile then to the store) can be costly.  Ie we consume 1X the stored-field size in transient RAM, vs writing directly to disk, I think?\n{quote]\n\nYeah I totally agree that this would be a nice performance win - I got first excited too about removing that extra buffering layer.  But we probably have to handle exceptions a bit differently then?  Because now all exceptions thrown in processDocument() are considered non-aborting, because they nothing is actually written to disk in processDocument().\n\nWe only abort when we encounter an exception in finishDocument(), because then the files might be corrupted.\n\nSo we probably just have to catch the IOExceptions somewhere else to figure out if it was e.g. a (non-aborting) exception from the TokenStream vs. e.g. a disk full (aborting) exception from StoredFieldsWriter.",
            "date": "2010-07-23T16:35:26.252+0000",
            "id": 102
        },
        {
            "author": "Michael McCandless",
            "body": "Yes, you're right!  So, after this change, an exception while processing stored fields / term vectors must handled as an aborting exception.",
            "date": "2010-07-24T14:49:34.778+0000",
            "id": 103
        },
        {
            "author": "Shai Erera",
            "body": "Is it possible that as part of this issue (or this effort), you'll think of opening PTDW for easier extensions (such as Parallel Indexing)? If it can easily be done by marking some methods as protected instead of (package-)private, then I think it's worth it. We can mark them as lucene.internal or something ... And perhaps even allow setting the DW on IW, for really expert use?\n\nIf you think that should be part of the overhaul refactoring you've once suggested (Michael B.), then I'm ok with that as well. Just thinking that it's better to hit the iron while it's still hot :).",
            "date": "2010-07-26T05:13:43.395+0000",
            "id": 104
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nIs it possible that as part of this issue (or this effort), you'll think of opening PTDW for easier extensions (such as Parallel Indexing)?\n{quote}\n\nYeah I'd like to make some progress on parallel indexing too.  I think now that DWPT is roughly working I can start thinking about what further changes are necessary in the indexer.",
            "date": "2010-07-27T20:55:53.655+0000",
            "id": 105
        },
        {
            "author": "Michael McCandless",
            "body": "BTW, randomly, LUCENE-2186 has already roughly factored out the BytesRef hash from TermsHashPerField... so if we want also to reuse that here, we can share.",
            "date": "2010-08-09T12:23:54.881+0000",
            "id": 106
        },
        {
            "author": "Michael McCandless",
            "body": "Is this near-comittable?  Ie \"just\" the DWPT cutover?  This part seems separable from making each DWPT's buffer searchable?\n\nI'm running some tests w/ 20 indexing threads and I think the sync'd flush is a big bottleneck...",
            "date": "2010-09-16T18:49:40.092+0000",
            "id": 107
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. I think the sync'd flush is a big bottleneck\n\nIs this because indexing stops while the DWPT segment is being flushed to disk or are you referring to a different sync?",
            "date": "2010-09-16T19:01:45.840+0000",
            "id": 108
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Is this because indexing stops while the DWPT segment is being flushed to disk or are you referring to a different sync?\n\nI'm talking about Lucene trunk today (ie before this patch).\n\nYes, because indexing of all 20 threads is blocked while a single thread moves the RAM buffer to disk.  But, with this patch, each thread will privately move its own RAM buffer to disk, not blocking the rest.\n\nWith 20 threads I'm seeing ~4 seconds of concurrent indexing and then 6-8 seconds to flush (w/ 256 MB RAM buffer).",
            "date": "2010-09-16T19:24:28.256+0000",
            "id": 109
        },
        {
            "author": "Michael Busch",
            "body": "bq. Is this near-comittable?\n\nI think we need to:\n  * merge trunk and make tests pass\n  * finish flushing by RAM\n  * make deletes work again\n\nThen it should be ready to commit.  Sorry, was so busy the last weeks that I couldn't make much progress.",
            "date": "2010-09-17T06:57:50.168+0000",
            "id": 110
        },
        {
            "author": "Michael McCandless",
            "body": "I just posted some details about the concurrency bottleneck of flush at http://chbits.blogspot.com/2010/09/lucenes-indexing-is-fast.html\n\nIt's pretty bad -- w/ only 6 threads, I see flush taking 54% of the time, ie for 54% of the time all threads are blocked on indexing while the flush runs.\n",
            "date": "2010-09-17T10:28:37.681+0000",
            "id": 111
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. BTW, randomly, LUCENE-2186 has already roughly factored out the BytesRef hash from TermsHashPerField... so if we want also to reuse that here, we can share.\ndo you guys still need the BytesHash being factored out since I currently work on splitting stand alone parts out of LUCENE-2186. If so that would be the next on the list... let me know\n\nbq. I just posted some details about the concurrency bottleneck of flush at http://chbits.blogspot.com/2010/09/lucenes-indexing-is-fast.html\n\nnice post mike :)\n\n",
            "date": "2010-09-17T11:02:04.564+0000",
            "id": 112
        },
        {
            "author": "Jason Rutherglen",
            "body": "Simon, I think the BytesHash being factored is useful, though not a must have for committing the flush by DWPT code.\n\nMike, I need to finish the unit tests for LUCENE-2573.\n\nMichael, what is the issue with deletes?  We don't need deletes to use sequence ids yet?  Maybe we should open a separate issue to make deletes work for the realtime/DWPT branch?",
            "date": "2010-09-19T01:07:07.753+0000",
            "id": 113
        },
        {
            "author": "Jason Rutherglen",
            "body": "Simon, on second thought, lets go ahead and factor out BytesHash, do you want to submit a patch for the realtime branch and post it here or should I?",
            "date": "2010-09-20T02:46:46.799+0000",
            "id": 114
        },
        {
            "author": "Jason Rutherglen",
            "body": "I opened an issue for the deletes LUCENE-2655",
            "date": "2010-09-20T02:50:19.248+0000",
            "id": 115
        },
        {
            "author": "Jason Rutherglen",
            "body": "Now that LUCENE-2680 is implemented we can get deletes working properly in the\nrealtime, which is really the DWPT branch at this point. Given the significant\nchanges made since it's creation, rather than continue with a realtime branch,\nI propose we patch into trunk the DWPT changes. ",
            "date": "2010-12-09T18:12:11.829+0000",
            "id": 116
        },
        {
            "author": "Michael Busch",
            "body": "Ideally we should merge trunk into realtime after LUCENE-2680 is committed, get everything working there, and then merge realtime back into trunk?\n\nI agree that it totally makes sense to get DWPT into trunk as soon as possible (ie. not wait until all realtime stuff is done).",
            "date": "2010-12-09T18:30:58.938+0000",
            "id": 117
        },
        {
            "author": "Jason Rutherglen",
            "body": "> and then merge realtime back into trunk\n\nI'd prefer to simply create a patch of the DWPT changes, which granted'll be large, however we're through most of the hurdles and at this point, the patch'll be straightforward to implement and test using the existing unit tests?",
            "date": "2010-12-09T18:37:39.800+0000",
            "id": 118
        },
        {
            "author": "Michael Busch",
            "body": "Not sure if that's much easier though, because what you said is true:  the realtime branch currently is basically the DWPT branch.\n",
            "date": "2010-12-09T18:58:42.972+0000",
            "id": 119
        },
        {
            "author": "Jason Rutherglen",
            "body": "Michael,\n\nI don't have commit access to the realtime branch. A patch is probably easier\nfor other people to work on and look at than the branch? Also, I have some time\nto work on integrating DWPT and the changes since the branch creation are\nfairly significant and probably require mostly manual merging. ",
            "date": "2010-12-10T18:46:54.540+0000",
            "id": 120
        },
        {
            "author": "Michael Busch",
            "body": "I started merging yesterday the latest trunk into realtime.  The merge is rather hard, as you might imagine :)  \nBut I'm down from 600 compile errors to ~100.  I can try to finish it this weekend.\n\nBut I don't want to block you, if you want to go the patch route and have time now don't wait for me.",
            "date": "2010-12-10T19:25:24.812+0000",
            "id": 121
        },
        {
            "author": "Michael Busch",
            "body": "bq. I started merging yesterday the latest trunk into realtime.\n\nAs part of this I want to clean up the branch a bit and remove unnecessary changes (like refactorings) to make the merge back into trunk less difficult.  When I'm done with the merge we should patch LUCENE-2680 into realtime.  (or commit to trunk and merge trunk into realtime again)",
            "date": "2010-12-10T19:31:14.519+0000",
            "id": 122
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. The merge is rather hard, as you might imagine\n\nI'd hold off until LUCENE-2680 is committed to trunk, otherwise the merge work may be redundant.  \n\nbq. commit to trunk and merge trunk into realtime again\n\nI think it's pretty much ready for trunk as the tests pass.  \n\nMerging realtime/DWPT branch back to trunk should be easier if we try to wrap up DWPT in a short period of time, eg, plow ahead until it's merge-able.\n\n",
            "date": "2010-12-10T23:49:52.552+0000",
            "id": 123
        },
        {
            "author": "Jason Rutherglen",
            "body": "As per Michael B's email, I'll start on integrating deletes and flush-by-RAM aka LUCENE-2573, along the way the deadlock issue could be uncovered, which test is it occurring on?",
            "date": "2010-12-21T16:20:16.128+0000",
            "id": 124
        },
        {
            "author": "Jason Rutherglen",
            "body": "Also, it'd be great if we could summarize the changes trunk -> DWPT branch.  ",
            "date": "2010-12-23T04:29:58.336+0000",
            "id": 125
        },
        {
            "author": "Jason Rutherglen",
            "body": "Here's ant test-core output.  Looks like it's deadlocking in TestIndexWriter?  There are some IR.reopen failures, a null pointer, and a delete count I'll look at.",
            "date": "2010-12-23T04:54:41.257+0000",
            "id": 126
        },
        {
            "author": "Jason Rutherglen",
            "body": "Small patch fixing the num deletes test null pointer. \n\nThe TestIndexReaderReopen failure seems to have something to do with flushing deletes.",
            "date": "2010-12-23T05:33:29.196+0000",
            "id": 127
        },
        {
            "author": "Jason Rutherglen",
            "body": "Looks like the problem with TestIndexReaderReopen is the test opens an IW, tried to queue deletes, however because there isn't an existing DWPT (at least when flush is called), the deletes haven't been recorded and so they're not applied to the index.  Perhaps we need to init DW with at least 1 DWPT?",
            "date": "2011-01-04T20:15:53.369+0000",
            "id": 128
        },
        {
            "author": "Jason Rutherglen",
            "body": "Also I think the deadlock is happening in DW update doc where we're calling delete term across all DWPTs. How will we guarantee point-in-timeness when RT is turned on? I guess with the sequence ids?",
            "date": "2011-01-04T21:20:38.184+0000",
            "id": 129
        },
        {
            "author": "Jason Rutherglen",
            "body": "I added an assertion showing the lack of DWPTs when delete is called.\n\ndeleteTermNoWait (which skips flush control) is called in update doc and now deadlock doesn't occur when executing TestIndexWriter.",
            "date": "2011-01-04T21:53:43.016+0000",
            "id": 130
        },
        {
            "author": "Jason Rutherglen",
            "body": "Taking a step back, I'm not sure flush control should be global, as flushing is\nentirely per thread now? If we're adding a delete term for every DWPT, if one\nis flushing do we wait or do we simply queue it up? I don't think we can wait\nin the delete call for a DWPT to completely flush?\n\nSo we'll likely need to delete in a PerThreadTask that's executed on each\nexisting DWPT. How we guarantee concurrency seems a little odd here as what if\na new DWPT is spun up while we're deleting in another thread? Perhaps we should\nsimply spin up on DW init, the max thread state number of DWPTs? This way we\nalways have > 0 available, and we can perhaps lock on seq id when adding\ndeletes to all DWPTs (it's a fast call). We may want to simply skip any\nflushing DWPTs when adding deletes?\n\nIn browsing the code, FlushControl isn't used in very many places. This'll get\na little bit more fleshed out when we integrate LUCENE-2573.",
            "date": "2011-01-05T00:50:09.312+0000",
            "id": 131
        },
        {
            "author": "Jason Rutherglen",
            "body": "I think we can get DWPTs working sans LUCENE-2573 for now.  We can do this by setting a hard max buffer size, deletes, etc per DWPT.  The values will be from IWC divided by the max thread states.  ",
            "date": "2011-01-05T01:52:56.526+0000",
            "id": 132
        },
        {
            "author": "Jason Rutherglen",
            "body": "Perhaps it's best to place the RAM tracking into FlushControl where the RAM\nconsumed by deleted query, terms, and added documents can be recorded, so that\nthe proper flush decision may be made in it, a central global object. To get this idea\nworking we'd need to implement LUCENE-2573 in FlushControl. I'll likely get\nstarted on this.",
            "date": "2011-01-05T16:49:17.018+0000",
            "id": 133
        },
        {
            "author": "Jason Rutherglen",
            "body": "Another model we could implement is a straight queuing. This'd give us total\nordering on all IW calls. Documents, deletes, and flushes would be queued up\nand executed asynchronously. For example in today's DWPT code we will still\nblock document additions while flushing because we're tying a thread to a given\nDWPT. If a thread's DWPT is flushing, wouldn't we want to simply assign the doc\nadd to a different non-flushing DWPT to gain full efficiency? This seems more\neasily doable with a queuing model. If we want synchronous flushing then we'd\nplace a flush event in the queue and wait for it to complete executing. How\ndoes this sound? \n\n",
            "date": "2011-01-05T18:32:07.283+0000",
            "id": 134
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nTaking a step back, I'm not sure flush control should be global, as flushing is\nentirely per thread now?\n{quote}\n\nI think flush control must be global?  Ie when we've used too much RAM we start flushing?\n\n{quote}\nIf we're adding a delete term for every DWPT, if one\nis flushing do we wait or do we simply queue it up? I don't think we can wait\nin the delete call for a DWPT to completely flush?\n{quote}\n\nI believe we can drop the delete in that case.  We only need to buffer into DWPTs that have at least 1 doc.",
            "date": "2011-01-06T00:31:15.747+0000",
            "id": 135
        },
        {
            "author": "Michael McCandless",
            "body": "{queue}\nAnother model we could implement is a straight queuing. This'd give us total\nordering on all IW calls. Documents, deletes, and flushes would be queued up\nand executed asynchronously. For example in today's DWPT code we will still\nblock document additions while flushing because we're tying a thread to a given\nDWPT. If a thread's DWPT is flushing, wouldn't we want to simply assign the doc\nadd to a different non-flushing DWPT to gain full efficiency? This seems more\neasily doable with a queuing model. If we want synchronous flushing then we'd\nplace a flush event in the queue and wait for it to complete executing. How\ndoes this sound?\n{queue}\nI think we should have to add queueing to all incoming ops...\n\nIf a given DWPT is flushing then we pick another?  Ie the binding logic would naturally avoid DWPTs that are not available -- either because another thread has it, or it's flushing.  But it would prefer to use the same DWPT it used last time, if possible (affinity).",
            "date": "2011-01-06T00:32:37.179+0000",
            "id": 136
        },
        {
            "author": "Jason Rutherglen",
            "body": "We're going to great lengths it seems to emulate a producer consumer queue (eg,\nordering of calls with sequence ids, thread pooling) without actually\nimplementing one. A fixed size blocking queue would simply block threads as\nneeded and would probably look cleaner in code. We could still implement thread\naffinities though I simply can't see most applications requiring affinity, so\nperhaps we can avoid it for now and put it back in later? \n\n{quote}I think flush control must be global? Ie when we've used too much RAM we\nstart flushing?{quote}\n\nRight, it should. I'm just not sure we still need FC's global waiting during\nflush, that'd seem to go away because the RAM usage tracking is in DW. If we\nrecord the new incremental RAM used (which I think we do) per add/update/delete\nthen we can enable a pluggable user defined flush policy. \n\n{quote} If a given DWPT is flushing then we pick another? Ie the binding logic\nwould naturally avoid DWPTs that are not available - either because another\nthread has it, or it's flushing. But it would prefer to use the same DWPT it\nused last time, if possible (affinity). {quote}\n\nHowever once the affinity DWPT flush completed, we'd need logic to revert back\nto the original?\n\nI think the 5% model of LUCENE-2573 may typically yield flushing that occurs in\nnear intervals of each other, ie, it's going to slow down the aggregate\nindexing if they're flushing on top of each other. Maybe we should start at 60%\nthen the multiple of 40% divided by maxthreadstate - 1? Ideally we'd\nstatistically optimize the flush interval per machine, eg, SSDs and RAM disks\nwill likely require only a small flush percentage interval.\n\n\n\n",
            "date": "2011-01-06T01:00:33.555+0000",
            "id": 137
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}I believe we can drop the delete in that case. We only need to buffer\ninto DWPTs that have at least 1 doc.{quote}\n\nRight if the DWPT's flushing we can skip it. In the queue model we'd consume and locate\nthe existing DWPTs, adding the delete to each DWPT not flushing. However in the\nzero DWPT case we still need to record a delete somewhere, most likely we'd\nneed to create a zero doc DWPT? Oh wait, we need to add the delete to the last\nsegment? Ah, I can fix that in the existing code (eg, fix the reopen test case\nfailures).",
            "date": "2011-01-06T01:49:39.622+0000",
            "id": 138
        },
        {
            "author": "Jason Rutherglen",
            "body": "Same as the last patch, however default deletes is added to DW to which deletes are added to when there are no available DWPTs.  On flush all threads, default deletes is applied to the last segment with no doc limit.  \n\nTestIndexReaderReopen now passes.",
            "date": "2011-01-06T05:04:12.988+0000",
            "id": 139
        },
        {
            "author": "Jason Rutherglen",
            "body": "Here's a new test.out, I'll look at TestCheckIndex which should probably work.  \n\n\"IndexFileDeleter doesn't know about file\" seems odd.  We're OOMing in TestIndexWriter because we're not flushing by RAM (eg, it currently defaults to return false).",
            "date": "2011-01-06T05:20:10.987+0000",
            "id": 140
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nWe're going to great lengths it seems to emulate a producer consumer queue (eg,\nordering of calls with sequence ids, thread pooling) without actually\nimplementing one. A fixed size blocking queue would simply block threads as\nneeded and would probably look cleaner in code. We could still implement thread\naffinities though I simply can't see most applications requiring affinity, so\nperhaps we can avoid it for now and put it back in later?\n{quote}\n\nI'm not sure we should queue.  I wonder how much this'd slow down the single threaded case?\n\nAlso: I thought we don't have sequence IDs anymore?  (At least, for landing DWPT; after that (for \"true RT\") we need something like sequence IDs?).\n\nI think thread/doc-class affinity is fairly important.  Docs compress better if they are indexed together with similar docs.\n\nbq. I'm just not sure we still need FC's global waiting during flush, that'd seem to go away because the RAM usage tracking is in DW.\n\nWe shouldn't do global waiting anymore -- this is what's great about DWPT.\n\nbq. However once the affinity DWPT flush completed, we'd need logic to revert back to the original?\n\nI don't think so?  I mean a DWPT post-flush is a clean slate.  Some other thread/doc-class can stick to it.\n\n{quote}\nI think the 5% model of LUCENE-2573 may typically yield flushing that occurs in\nnear intervals of each other, ie, it's going to slow down the aggregate\nindexing if they're flushing on top of each other. Maybe we should start at 60%\nthen the multiple of 40% divided by maxthreadstate - 1? Ideally we'd\nstatistically optimize the flush interval per machine, eg, SSDs and RAM disks\nwill likely require only a small flush percentage interval.\n{quote}\n\nYeah we'll have to run tests to try to gauge the best \"default\" policy.  And you're right that it'll depend on the relative strength of IO vs CPU on the machine.  Fast IO system means we can flush \"later\".",
            "date": "2011-01-06T16:35:38.104+0000",
            "id": 141
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nRight if the DWPT's flushing we can skip it. In the queue model we'd consume and locate\nthe existing DWPTs, adding the delete to each DWPT not flushing.\n{quote}\n\nWe have an issue open for the app to state that the delete will not apply to any docs indexed in the current session... once we do that, then, the deletes don't need to be buffered on any DWPTs; just on the \"latest\" already flushed segment in the index.\n\n{quote}\nHowever in the\nzero DWPT case we still need to record a delete somewhere, most likely we'd\nneed to create a zero doc DWPT? Oh wait, we need to add the delete to the last\nsegment? Ah, I can fix that in the existing code (eg, fix the reopen test case\nfailures).\n{quote}\n\nYes, to the last flushed segment.",
            "date": "2011-01-06T16:37:26.484+0000",
            "id": 142
        },
        {
            "author": "Jason Rutherglen",
            "body": "I added a FlushPolicy class, deletes record the ram used to DWPT and DW.  Recording to DW is for global ram used.  The TIW OOM is still occurring however.  The delete calls to FlushControl are gone, I'm not sure what's going to be left of it.\n\nbq. I'm not sure we should queue. I wonder how much this'd slow down the single threaded case?\n\nYes, that's a good point.\n\nbq. We shouldn't do global waiting anymore - this is what's great about DWPT.\n\nHowever we'll have global waiting for the flush all threads case.  I think that can move down to DW though.  Or should it simply be a sync in/on IW?\n\nbq. I thought we don't have sequence IDs anymore?\n\nThe seqid lock was there, however it was removed in the last update.  I think we need to clearly document the various locks and sync points as right now it's not clear enough to prevent deadlock situations.",
            "date": "2011-01-06T16:56:35.347+0000",
            "id": 143
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI believe we can drop the delete in that case. We only need to buffer into DWPTs that have at least 1 doc.\n{quote}\n\nYeah sounds right.\n\n{quote}\nIf a given DWPT is flushing then we pick another? Ie the binding logic would naturally avoid DWPTs that are not available - either because another thread has it, or it's flushing. But it would prefer to use the same DWPT it used last time, if possible (affinity).\n{quote}\n\nThis is actually what should be happening currently if the (default) ThreadAffinityThreadPool is used.  I've to check the code again and maybe write a test specifically for that.\n\nbq. Also: I thought we don't have sequence IDs anymore? (At least, for landing DWPT; after that (for \"true RT\") we need something like sequence IDs?).\n\nTrue, sequenceIDs are gone since the last merge.  And yes, I still think we'll need them for RT.  Even for the non-RT case sequenceIDs would have nice benefits.  If methods like addDocument(), deleteDocuments(), etc. return the sequenceID they'd define a strict ordering on those operations and make it transparent for the application, which would be beneficial for document tracking and log replay.\n\nBut anyway, let's add seqIDs back after the DWPT changes are done and in trunk.\n\n\n{quote}\nbq. We shouldn't do global waiting anymore - this is what's great about DWPT.\n\nHowever we'll have global waiting for the flush all threads case. I think that can move down to DW though. Or should it simply be a sync in/on IW?\n{quote}\n\nTrue, the only global lock that locks all thread states happens when flushAllThreads is called.  This is called when IW explicitly triggers a flush, e.g. on close/commit.  \nHowever, maybe this is not the right approach?  I guess we don't really need the global lock.  A thread performing the \"global flush\" could still acquire each thread state before it starts flushing, but return a threadState to the pool once that particular threadState is done flushing?\n\nA related question is: Do we want to piggyback on multiple threads when a global flush happens?  Eg. Thread 1 called commit, Thread 2 shortly afterwards addDocument().  When should addDocument() happen? \na) After all DWPTs finished flushing? \nb) After at least one DWPT finished flushing and is available again?\nc) Or should Thread 2 be used to help flushing DWPTs in parallel with Thread 1?  \n\na) is currently implemented, but I think not really what we want.\nb) is probably best for RT, because it means the lowest indexing latency for the new document to be added.\nc) probably means the best overall throughput (depending even on hardware like disk speed, etc)\n\nFor whatever option we pick, we'll have to carefully think about error handling.  It's quite straightforward for a) (just commit all flushed segments to SegmentInfos when the global flush completed succesfully).  But for b) and c) it's unclear what should happen if a DWPT flush fails after some completed already successfully before.\n",
            "date": "2011-01-06T19:16:28.211+0000",
            "id": 144
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}actually what should be happening currently if the (default)\nThreadAffinityThreadPool is used. I've to check the code again and maybe write\na test specifically for that.{quote}\n\nLets try to test it, though I'm not immediately sure how the test case'd look. \n\nbq. let's add seqIDs back after the DWPT changes are done and in trunk.\n\nRight.\n\n{quote}True, the only global lock that locks all thread states happens when\nflushAllThreads is called. This is called when IW explicitly triggers a flush,\ne.g. on close/commit. However, maybe this is not the right approach?{quote}\n\nI think this is fine for the DWPT branch as flush, commit, and close are\nexplicitly blocked commands issued by the user. If we implemented something\nmore complex now, it wouldn't carry over to RT because the DWPTs don't require\nflushing to search on them. Which leads to the main drawback probably being for\nNRT, eg, get reader. Hmm... In that case a stop the world flush does affect\noverall indexing performance. Perhaps we can add flush and not block all DWPTs\nin a separate issue after the DWPT branch is merged to trunk, if there's user\nneed?  Or perhaps it's easy to implement, I'm still trying to get a feel for the\nlock progression in the branch. \n\nIn the indexing many documents case, the DWPTs'll be flushed by the tiered RAM\nsystem. It's the bulk add case where we don't want to block all threads/DWPTs\nat once, eg, I think our main goal is to fix Mike's performance test, with NRT\nbeing secondary or even a distraction.\n\n{quote}But for b) and c) it's unclear what should happen if a DWPT flush fails\nafter some completed already successfully before.{quote}\n\nRight, all that'd be solved if we bulk moved IW to a Scala-like asynchronous\nqueuing model. However it's probably a bit too much to do right now. Perhaps in\nthe bulk add-many-docs case we'll need a callback for errors? No because the\nadd doc method call that triggers the flush will report any exception(s).",
            "date": "2011-01-06T21:00:34.218+0000",
            "id": 145
        },
        {
            "author": "Jason Rutherglen",
            "body": "When we're OOMing the ram used reported by DW is 0.  We're probably not adding to the RAM used value somewhere.",
            "date": "2011-01-06T21:25:05.214+0000",
            "id": 146
        },
        {
            "author": "Jason Rutherglen",
            "body": "RAM accounting is slightly improved, we had two variables in DW keeping track of it.  The extraneous is removed, however we're still OOMing in: ant test-core -Dtestcase=TestIndexWriter -Dtestmethod=testIndexingThenDeleting\n",
            "date": "2011-01-06T23:11:18.810+0000",
            "id": 147
        },
        {
            "author": "Michael McCandless",
            "body": "bq. I guess we don't really need the global lock. A thread performing the \"global flush\" could still acquire each thread state before it starts flushing, but return a threadState to the pool once that particular threadState is done flushing?\n\nGood question... we could (in theory) also flush them concurrently?  But, since we don't \"own\" the threads in IW, we can't easily do that, so I think no global lock, go through all DWPTs w/ current thread and flush, sequentially?  So all that's guaranteed after the global flush() returns is that all state present prior to when flush() is invoked, is moved to disk.  Ie if addDocs are still happening concurrently then the DWPTs will start filling up again even while the \"global flush\" runs.  That's fine.\n\n{quote}\n\nA related question is: Do we want to piggyback on multiple threads when a global flush happens? Eg. Thread 1 called commit, Thread 2 shortly afterwards addDocument(). When should addDocument() happen? \na) After all DWPTs finished flushing? \nb) After at least one DWPT finished flushing and is available again?\nc) Or should Thread 2 be used to help flushing DWPTs in parallel with Thread 1?\n\na) is currently implemented, but I think not really what we want.\nb) is probably best for RT, because it means the lowest indexing latency for the new document to be added.\nc) probably means the best overall throughput (depending even on hardware like disk speed, etc)\n{quote}\n\nI think start simple -- the addDocument always happens?  Ie it's never coordinated w/ the ongoing flush.  It picks a free DWPT like normal, and since flush is single threaded, there should always be a free DWPT?\n\nLonger term c) would be great, or, if IW has an ES then it'd send multiple flush jobs to the ES.\n\n{quote}\nFor whatever option we pick, we'll have to carefully think about error handling. It's quite straightforward for a) (just commit all flushed segments to SegmentInfos when the global flush completed succesfully). But for b) and c) it's unclear what should happen if a DWPT flush fails after some completed already successfully before.\n{quote}\n\nI think we should continue what we do today?  Ie, if it's an 'aborting' exception, then the entire segment held by that DWPT is discarded?  And we then throw this exc back to caller (and don't try to flush any other segments)?",
            "date": "2011-01-08T14:00:14.054+0000",
            "id": 148
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}So all that's guaranteed after the global flush() returns is that all\nstate present prior to when flush() is invoked, is moved to disk. Ie if addDocs\nare still happening concurrently then the DWPTs will start filling up again\neven while the \"global flush\" runs. That's fine.{quote}\n\nWhat if the user wants a guaranteed hard flush of all state up to the point of\nthe flush call (won't they want this sometimes with getReader)? If we're\nflushing sequentially (without pausing all threads) we're removing that? Maybe\nwe'll need to give the option of global lock/stop or sequential flush?\n\nAlso I think we need to clear the thread bindings of a DWPT just prior to the\nflush of the DWPT? Otherwise (when multiple threads are mapped to a single\nDWPT) the other threads will wait on the [main] DWPT flush when they should be\nspinning up a new DWPT? \n\nThen, what happens to reusing the DWPT if we're flushing it, and we spin a new\nDWPT (effectively replacing the old DWPT), eg, we're going to lose the byte[]\nrecycling? Maybe we need to and share and sync the byte[] pooling between DWPTs\nor will that noticeably affect indexing performance? ",
            "date": "2011-01-08T15:32:17.259+0000",
            "id": 149
        },
        {
            "author": "Jason Rutherglen",
            "body": "Also, don't we need the global lock for commit/close?",
            "date": "2011-01-08T15:35:14.985+0000",
            "id": 150
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nWhat if the user wants a guaranteed hard flush of all state up to the point of\nthe flush call (won't they want this sometimes with getReader)? If we're\nflushing sequentially (without pausing all threads) we're removing that? Maybe\nwe'll need to give the option of global lock/stop or sequential flush?\n{quote}\n\nWhat's a \"hard flush\"?\n\nWith the proposed approach, all docs added (or in the process of being added) will make it into the flushed segments once the flush returns; newly added docs after the flush call started may or not make it.  But this is fine?  I mean, if the app has stronger requirements then it should externally sync?\n\nbq. Also I think we need to clear the thread bindings of a DWPT just prior to the flush of the DWPT? \n\nRight.\n\nAs soon as a DWPT is pulled from production for flushing, it loses all thread affinity and becomes unavailable until its flush finishes.  When a thread needs a DWPT, it tries to pick the one it last had (affinity) but if that one's busy, it picks a new one.  If none are available but we are below our max DWPT count, it spins up a new one?\n\n{quote}\nThen, what happens to reusing the DWPT if we're flushing it, and we spin a new\nDWPT (effectively replacing the old DWPT), eg, we're going to lose the byte[]\nrecycling?\n{quote}\n\nWhy would we lose them?  Wouldn't that DWPT just go back into rotation once the flush is done?",
            "date": "2011-01-08T16:17:38.473+0000",
            "id": 151
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}As soon as a DWPT is pulled from production for flushing, it loses all thread affinity and becomes unavailable until its flush finishes. When a thread needs a DWPT, it tries to pick the one it last had (affinity) but if that one's busy, it picks a new one. If none are available but we are below our max DWPT count, it spins up a new one?{quote}\n\nRight.\n\n{quote}With the proposed approach, all docs added (or in the process of being added) will make it into the flushed segments once the flush returns; newly added docs after the flush call started may or not make it. But this is fine? I mean, if the app has stronger requirements then it should externally sync?{quote}\n\nOk.  The proposed change is simply the thread calling add doc will flush it's DWPT if needed, take it offline while doing so, and return it when completed.  I think the risk is a new DWPT likely will have been created during flush, which'd make the returning DWPT inutile?\n\n{quote}Why would we lose them? Wouldn't that DWPT just go back into rotation once the flush is done?{quote}\n\nYes, we just need to change the existing code a bit then.\n\nHowever I think we may still need the global lock for close, eg, today we're preventing the user from adding docs during close, after this issue is merged that behavior would change?  ",
            "date": "2011-01-08T16:36:46.092+0000",
            "id": 152
        },
        {
            "author": "Jason Rutherglen",
            "body": "And there's the case of the thread calling flush doesn't yet have a DWPT, it's going to need to get one assigned to it, however the one assigned may not be the max ram consumer.  What'll we do then?  If the user explicitly called flush we can a) do nothing b) flush (the max ram consumer) thread's DWPT, however that gets hairy with wait notifies (almost like the global lock?).",
            "date": "2011-01-08T17:35:18.274+0000",
            "id": 153
        },
        {
            "author": "Michael McCandless",
            "body": "bq. The proposed change is simply the thread calling add doc will flush it's DWPT if needed, take it offline while doing so, and return it when completed.\n\nWait -- this is the \"addDocument\" case right?  (I thought we were still talking about the \"flush the world\" case...).\n\nbq.  I think the risk is a new DWPT likely will have been created during flush, which'd make the returning DWPT inutile?\n\nA new DWPT will have been created only if more than one thread is indexing docs right?  In which case this is fine?  Ie the old DWPT (just flushed) will just go back into rotation, and when another thread comes in it can take it?\n\nBut, you're right: maybe we should sometimes \"prune\" DWPTs.  Or simply stop recycling any RAM, so that a just-flushed DWPT is an empty shell.\n\nbq. However I think we may still need the global lock for close, eg, today we're preventing the user from adding docs during close, after this issue is merged that behavior would change?\n\nWell, the threads still adding docs will hit AlreadyClosedException?  (But, that's just \"best effort\").  The behavior of calling IW.close while other threads are still adding docs has never been defined (and, shouldn't be) except that we won't corrupt your index, and we'll get all docs indexed before .close was called, committed.  So I think even for this case we don't need a global lock.",
            "date": "2011-01-08T19:45:02.317+0000",
            "id": 154
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nAnd there's the case of the thread calling flush doesn't yet have a DWPT, it's going to need to get one assigned to it, however the one assigned may not be the max ram consumer. What'll we do then? If the user explicitly called flush we can a) do nothing b) flush (the max ram consumer) thread's DWPT, however that gets hairy with wait notifies (almost like the global lock?).\n{quote}\n\nWait -- why would the thread calling flush need to have a DWPT assigned to it?  You're talking about the \"flush the world\" case?  (Ie the app calls IW.commit or IW.getReader).  In this case the thread just one by one pulls all DWPTs that have any indexed docs out of production, flushes them, clears them, and returns them to production?",
            "date": "2011-01-08T19:46:47.192+0000",
            "id": 155
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}the \"flush the world\" case? (Ie the app calls IW.commit or\nIW.getReader). In this case the thread just one by one pulls all DWPTs that\nhave any indexed docs out of production, flushes them, clears them, and returns\nthem to production?{quote}\n\nThe 2 cases are: A) Flush every DWPT sequentually (aka flush the world) and \nB) flush by RAM usage when adding docs or deleting. A is clear! I think with B\nwe're saying even if the calling thread is bound to DWPT #1, if DWPT #2 is\ngreater in size and the aggregate RAM usage exceeds the max, using the calling\nthread, we take DWPT #2 out of production, flush, and return it?\n\n{quote}The behavior of calling IW.close while other threads are still adding\ndocs has never been defined (and, shouldn't be) except that we won't corrupt\nyour index, and we'll get all docs indexed before .close was called, committed.\nSo I think even for this case we don't need a global lock.{quote}\n\nGreat, that simplifies and clarifies that we do not require a global lock.\n\n{quote}But, you're right: maybe we should sometimes \"prune\" DWPTs. Or simply\nstop recycling any RAM, so that a just-flushed DWPT is an empty shell.{quote}\n\nI'm not sure how we'd prune, typically object pools have a separate eviction\nthread, I think that's going overboard? Maybe we can simply throw out the DWPT\nand put recycling byte[]s and/or pooling DWPTs back in later if it's necessary?\n\n",
            "date": "2011-01-08T23:00:39.165+0000",
            "id": 156
        },
        {
            "author": "Jason Rutherglen",
            "body": "To further clarify, we also no longer have global aborts?  Each abort only applies to an individual DWPT?  ",
            "date": "2011-01-09T00:53:45.331+0000",
            "id": 157
        },
        {
            "author": "Michael Busch",
            "body": "bq. I think the risk is a new DWPT likely will have been created during flush, which'd make the returning DWPT inutile.\n\nThe DWPT will not be removed from the pool, just marked as busy during flush, like as its state is busy (or currently called \"non-idle\" in the code) during addDocumentI().  So no new DWPT would be created during flush if the maxThreadState limit was already reached.\n\n\n",
            "date": "2011-01-09T01:22:58.405+0000",
            "id": 158
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI think start simple - the addDocument always happens? Ie it's never coordinated w/ the ongoing flush. It picks a free DWPT like normal, and since flush is single threaded, there should always be a free DWPT?\n{quote}\n\nYeah I agree.  The change I'll make then is to not have the global lock and return a DWPT immediately to the pool and set it to 'idle' after its flush completed.\n\n{quote}\nI think we should continue what we do today? Ie, if it's an 'aborting' exception, then the entire segment held by that DWPT is discarded? And we then throw this exc back to caller (and don't try to flush any other segments)?\n{quote}\n\nWhat I meant was the following situation: Suppose we have two DWPTs and IW.commit() is called.  The first DWPT finishes flushing successfully, is returned to the pool and idle again.  The second DWPT flush fails with an aborting exception.  Should the segment of the first DWPT make it into the index or not?  I think segment 1 shouldn't be committed, ie. a global flush should be all or nothing.  This means we would have to delay the commit of the segments until all DWPTs flushed successfully.",
            "date": "2011-01-09T01:31:25.409+0000",
            "id": 159
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}I think segment 1 shouldn't be committed, ie. a global flush should be all or nothing. This means we would have to delay the commit of the segments until all DWPTs flushed successfully.{quote}\n\nIf a DWPT aborts during flush, we simply throw an exception, however we still keep the successfully flushed segment(s).  If there's an abort on any DWPT during commit then we throw away any successfully flushed segments as well.  I think that makes sense, eg, all or nothing.",
            "date": "2011-01-09T02:09:21.528+0000",
            "id": 160
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nI think with B\nwe're saying even if the calling thread is bound to DWPT #1, if DWPT #2 is\ngreater in size and the aggregate RAM usage exceeds the max, using the calling\nthread, we take DWPT #2 out of production, flush, and return it?\n{quote}\nRight -- the thread affinity has nothing to do with which thread gets to flush which DWPT.  Once flush is triggered, the thread doing the flushing is free to flush any DWPT.\n\n{quote}\nMaybe we can simply throw out the DWPT\nand put recycling byte[]s and/or pooling DWPTs back in later if it's necessary?\n{quote}\n\nOK let's start there and put back re-use only if we see a real perf issue?\n\nbq. What I meant was the following situation: Suppose we have two DWPTs and IW.commit() is called. The first DWPT finishes flushing successfully, is returned to the pool and idle again. The second DWPT flush fails with an aborting exception. \n\nHmm, tricky.  I think I'd lean towards keeping segment 1.  Discarding it would be inconsistent w/ aborts hit during the \"flushed by RAM\" case?  EG if seg 1 was flushed due to RAM usage, succeeds, and then later seg 2 is flushed due to RAM usage, but aborts.  In this case we would still keep seg 1?\n\nI think aborting a flush should only lose the docs in that one DWPT (as it is today).\n\nRemember, a call to commit may succeed in flushing seg 1 to disk, and updating the in-memory segment infos, but on hitting the aborting exc to seg 2, will throw that to the caller, not having committed *any* change to the index.  Exceptions thrown during the prepareCommit (phase 1) part of commit mean nothing is changed in the index.\n\nAlternatively... we could abort the entire IW session (as eg we handle OOME today) if ever an aborting exception was hit?  This might be cleaner?  But it's really a \"nuke the world\" option which scares me.  EG it could be a looong indexing session (app doesn't call commit() until the end) and we could be throwing away *alot* of progress.",
            "date": "2011-01-09T12:00:57.471+0000",
            "id": 161
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. Once flush is triggered, the thread doing the flushing is free to flush any DWPT.\n\nOK.\n\nbq. OK let's start there and put back re-use only if we see a real perf issue?\n\nI think that's best.  Balancing RAM isn't implemented in the branch, we can't predict the future usage of DWPT(s) (which could languish consuming RAM with byte[]s well after they're flushed due to a sudden drop in the number of calling threads external to IW).\n\n{quote}But it's really a \"nuke the world\" option which scares me. EG it could be a looong indexing session (app doesn't call commit() until the end) and we could be throwing away alot of progress.{quote}\n\nRight.  Another option is to on commit try to flush all segments, meaning even if one DWPT/segment aborts, continue on with the other DWPTs (ie, a best effort).  Then perhaps throw an exception with a report of which segment flushes succeeded, or simply return a report object detailing what happened during commit (somewhat expert usage though).  Either way I think we need to give a few options to the user, then choose a default and see if it sticks.  The default should probably be \"best effort\".\n\n",
            "date": "2011-01-09T18:01:07.678+0000",
            "id": 162
        },
        {
            "author": "Michael McCandless",
            "body": "I think on commit if we hit an aborting exception flushing a given DWPT, we throw it then & there.\n\nAny segs already flushed remain flushed (but not committed).  Any segs not yet flushed remain not yet flushed...",
            "date": "2011-01-10T12:06:07.254+0000",
            "id": 163
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. Any segs already flushed remain flushed (but not committed). Any segs not yet flushed remain not yet flushed...\n\nIf the segment are flushed, then they will be deleted?  Or they will be made available in a subsequent and completely successful commit?",
            "date": "2011-01-10T16:26:12.169+0000",
            "id": 164
        },
        {
            "author": "Michael Busch",
            "body": "bq. Longer term c) would be great, or, if IW has an ES then it'd send multiple flush jobs to the ES.\n\nLost in abbreviations :) - Can you remind me what 'ES' is?\n\nbq. But, you're right: maybe we should sometimes \"prune\" DWPTs. Or simply stop recycling any RAM, so that a just-flushed DWPT is an empty shell.\n\nI'm not sure I understand what the problem here with recycling RAM is.  Could someone elaborate?\n",
            "date": "2011-01-10T17:10:15.654+0000",
            "id": 165
        },
        {
            "author": "Michael Busch",
            "body": "bq. I think aborting a flush should only lose the docs in that one DWPT (as it is today).\n\nYeah I'm convinced now I don't want the \"nuke the world\" approach.  Btw, Mike, you're very good with giving things intuitive names :)\n\n\nbq. I think on commit if we hit an aborting exception flushing a given DWPT, we throw it then & there.\n\nYes sounds good.\n\n\n{quote}\nbq. Any segs already flushed remain flushed (but not committed). Any segs not yet flushed remain not yet flushed...\n\nIf the segment are flushed, then they will be deleted? Or they will be made available in a subsequent and completely successful commit?\n{quote}\n\nThe aborting exception might be thrown due to a disk-full situation.  This can be fixed and commit() called again, which then would flush the remaining DWPTs and commit all flushed segments.\nOtherwise, those flushed segments will be orphaned and deleted sometime later by a different IW because they don't belong to any SegmentInfos.",
            "date": "2011-01-10T17:16:16.965+0000",
            "id": 166
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. Lost in abbreviations  - Can you remind me what 'ES' is?\n\nI read it as ExecutorService, ie, a thread pool.\n\nbq. I'm not sure I understand what the problem here with recycling RAM is. Could someone elaborate?\n\nMainly that we could have DWPT(s) lying around unused, consuming [recycled] RAM, eg, from a sudden drop in the number of incoming threads after a flush.  This is a drop the code, and put it back in if that was a bad idea solution.\n\n",
            "date": "2011-01-10T17:17:21.874+0000",
            "id": 167
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nMainly that we could have DWPT(s) lying around unused, consuming [recycled] RAM, eg, from a sudden drop in the number of incoming threads after a flush. This is a drop the code, and put it back in if that was a bad idea solution.\n{quote}\n\nAh thanks, got it.  \n\n\nbq. Or simply stop recycling any RAM, so that a just-flushed DWPT is an empty shell.\n\n+1",
            "date": "2011-01-10T18:16:29.027+0000",
            "id": 168
        },
        {
            "author": "Michael McCandless",
            "body": "{quote}\nbq. Lost in abbreviations - Can you remind me what 'ES' is?\n\nI read it as ExecutorService, ie, a thread pool.\n{quote}\n\nYes, sorry that's what I meant.\n\nIe someday IW can take an ES too and farm things out to it when it could make use of concurrency (like flush the world).  But that's for later </dream>.",
            "date": "2011-01-10T19:50:11.282+0000",
            "id": 169
        },
        {
            "author": "Michael Busch",
            "body": "I made some progress with the concurrency model, especially removing the need for various locks to make everything easier.\n\n- DocumentsWriterPerThreadPool.ThreadState now extends ReentrantLock, which means that standard methods like lock() and unlock() can be used to reserve a DWPT for a task.\n- The max. number of DWPTs allowed (config.maxThreadStates) is instantiated up-front.  Creating a DWPT is cheap, so this is not a performance concern; this makes it easier to push config changes to the DWPTs without synchronizing on the pool and without having to worry about newly created DWPTs getting the same config settings.\n- DocumentsWriterPerThreadPool.getActivePerThreadsIterator() gives the caller a static snapshot of the active DWPTs at the time the iterator was acquired, e.g. for flushAllThreads() or DW.abort().  Here synchronizing on the pool isn't necessary either.\n- deletes are now pushed to DW.pendingDeletes() if no active DWPTs are present.\n\nTODOs:\n- fix remaining testcases that still fail\n- fix RAM tracking and flush-by-RAM\n- write new testcases to test thread pool, thread assignment, etc\n- review if all cases that were discussed in the recent comments here work as expected (likely not :) )\n- performance testing and code cleanup ",
            "date": "2011-01-13T09:33:36.522+0000",
            "id": 170
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}DocumentsWriterPerThreadPool.ThreadState now extends ReentrantLock, which means that standard methods like lock() and unlock() can be used to reserve a DWPT for a task.{quote}\n\nReally?  That makes synchronized seem simpler?\n\nbq. the max. number of DWPTs allowed (config.maxThreadStates) is instantiated up-front. \n\nWhat about the memory used, eg, the non-use of byte[] recycling?  I guess it'll be cleared on flush.\n\nbq. fix RAM tracking and flush-by-RAM\n\nI created a BytesUsed object that cascades the changes to parent BytesUsed objects, this allows each individual SD, DWPT, DW, etc to keep track of their bytes used, while also propagating the changes to the higher level objects, eg, SD -> DWPT, DWPT -> DW.",
            "date": "2011-01-13T16:06:23.584+0000",
            "id": 171
        },
        {
            "author": "Michael Busch",
            "body": "bq. Really?  That makes synchronized seem simpler?\n\nWell look at ThreadAffinityDocumentsWriterThreadPool.  There I'm able to use things like tryLock() and getQueueLength().\nAlso DocumentsWriterPerThreadPool has a getAndLock() method, that can be used by DW for addDocument(), whereas DW.flush(), which needs to iterate the DWPTs, can lock the individual DWPTs directly.  I think it's simpler, but I'm open to other suggestions of course :)\n\n\nbq. What about the memory used, eg, the non-use of byte[] recycling? I guess it'll be cleared on flush.\n\nYeah, sure.  That is independent on whether they're all created upfront or not.  But yeah, after flush or abort we need to clear the DWPT's state to make sure they're not consuming unused RAM (as you described in your earlier comment).",
            "date": "2011-01-13T17:21:22.110+0000",
            "id": 172
        },
        {
            "author": "Earwin Burrfoot",
            "body": "Maan, this comment list is infinite.\nHow do I currently get the ..er.. current version? Latest branch + latest Jason's patch?\n\nRegardless of everything else, I'd ask you not to extend random things :) at least if you can't say is-a about them.\nDocumentsWriterPerThreadPool.ThreadState IS A ReentrantLock? No. So you're better off encapsulating it rather than extending.\nSame can be applied to SegmentInfos that extends Vector :/",
            "date": "2011-01-13T17:36:46.599+0000",
            "id": 173
        },
        {
            "author": "Michael Busch",
            "body": "bq.  How do I currently get the ..er.. current version?\n\nJust do 'svn up' on the RT branch.\n\n\nbq. Regardless of everything else, I'd ask you not to extend random things\n\nThis was a conscious decision, not random.  Extending ReentrantLock is not an uncommon pattern, e.g. ConcurrentHashMap.Segment does exactly that.  ThreadState basically is nothing but a lock that has a reference to the corresponding DWPT it protects.\n\nI encourage you to look at the code.",
            "date": "2011-01-13T17:55:53.078+0000",
            "id": 174
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. look at ThreadAffinityDocumentsWriterThreadPool. There I'm able to use things like tryLock() and getQueueLength().\n\nMakes sense, I had only read the DocumentsWriterPerThreadPool part.\n\n* DWPT.perDocAllocator and freeLevel can be removed?\n\n* DWPT's RecyclingByteBlockAllocator -> DirectAllocator?\n\n* Looks like the deletes handling is updated to the patch\n\n* I don't think we need FlushControl anymore as the RAM tracking should occur in DW and there's no need for IW to [globally] wait for flushes.\n\n* The locking is more clear now, I can see DW.updateDocument locks the threadstate as does flushAllThreads.\n\nI'll reincorporate the RAM tracking, and then will try the unit tests again.  I'm curious if the file not found errors are gone.",
            "date": "2011-01-13T22:33:30.655+0000",
            "id": 175
        },
        {
            "author": "Michael Busch",
            "body": "bq. DWPT.perDocAllocator and freeLevel can be removed?\n\ndone.\n\nbq. DWPT's RecyclingByteBlockAllocator -> DirectAllocator?\n\ndone. Also removed more recycling code.\n\nbq.  I don't think we need FlushControl anymore as the RAM tracking should occur in DW and there's no need for IW to [globally] wait for flushes.\n\nI removed flushControl from DW.\n\nbq. I'm curious if the file not found errors are gone.\n\nI think there's something wrong with TermVectors - several related test cases fail. We need to investigate more.",
            "date": "2011-01-13T23:14:54.071+0000",
            "id": 176
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. I think there's something wrong with TermVectors - several related test cases fail. We need to investigate more.\n\nYes, there are far more than the last revision!  I don't think it's just TV however.",
            "date": "2011-01-13T23:54:40.432+0000",
            "id": 177
        },
        {
            "author": "Jason Rutherglen",
            "body": "Here's the latest test.out.  There's a lot of these:\n\n{code}\n[junit] junit.framework.AssertionFailedError: IndexFileDeleter doesn't know about file _5.fdt\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1156)\n    [junit] \tat org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1088)\n    [junit] \tat org.apache.lucene.index.IndexWriter.filesExist(IndexWriter.java:3273)\n    [junit] \tat org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:3321)\n    [junit] \tat org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2339)\n    [junit] \tat org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2410)\n    [junit] \tat org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1083)\n    [junit] \tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1027)\n    [junit] \tat org.apache.lucene.index.IndexWriter.close(IndexWriter.java:991)\n    [junit] \tat org.apache.lucene.index.TestAddIndexes.testMergeAfterCopy(TestAddIndexes.java:432)\n{code}",
            "date": "2011-01-14T00:40:51.081+0000",
            "id": 178
        },
        {
            "author": "Jason Rutherglen",
            "body": "I'm taking a guess here, however the ThreadAffinityDocumentsWriterThreadPool.getAndLock method looks a little suspicious as we're iterating on ThreadStates and on a non-concurrent hashmap calling put while not in a lock?  ",
            "date": "2011-01-14T17:06:23.202+0000",
            "id": 179
        },
        {
            "author": "Jason Rutherglen",
            "body": "Also multiple threads can call DocumentsWriterPerThread.addDocument and that's resulting in this:\n\n{code}[junit] java.lang.AssertionError: omitTermFreqAndPositions:false postings.docFreqs[termID]:0\n    [junit]     at org.apache.lucene.index.FreqProxTermsWriterPerField.addTerm(FreqProxTermsWriterPerField.java:143)\n    [junit]     at org.apache.lucene.index.TermsHashPerField.add(TermsHashPerField.java:234)\n    [junit]     at org.apache.lucene.index.DocInverterPerField.processFields(DocInverterPerField.java:91)\n    [junit]     at org.apache.lucene.index.DocFieldProcessor.processDocument(DocFieldProcessor.java:274)\n    [junit]     at org.apache.lucene.index.DocumentsWriterPerThread.addDocument(DocumentsWriterPerThread.java:184)\n    [junit]     at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:374)\n    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1403)\n    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1375)\n{code}",
            "date": "2011-01-14T17:12:29.852+0000",
            "id": 180
        },
        {
            "author": "Michael Busch",
            "body": "bq. as we're iterating on ThreadStates and on a non-concurrent hashmap calling put while not in a lock? \n\nThe threadBindings hashmap is a ConcurrentHashMap and the getActivePerThreadsIterator() is threadsafe I believe.",
            "date": "2011-01-14T17:20:38.533+0000",
            "id": 181
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. The threadBindings hashmap is a ConcurrentHashMap and the getActivePerThreadsIterator() is threadsafe I believe.\n\nSorry yes CHM is used, it all looks thread safe, but there must be multiple threads accessing a single DWPT at the same time for some of these errors to be occurring.  ",
            "date": "2011-01-14T17:34:58.087+0000",
            "id": 182
        },
        {
            "author": "Jason Rutherglen",
            "body": "Also, why are we always (well, likely) assigning the DWPT to a different thread state if tryLock returns false?  If there's a lot of contention (eg, far more incoming threads than DWPTs), then won't the thread assignation code become a hotspot?\n\nIn ThreadAffinityDocumentsWriterThreadPool.clearThreadBindings(ThreadState perThread) we're actually clearing the entire map.  When this's called in IW.flush (which is unsynced on IW), if there are multiple concurrent flushes, then perhaps a single DWPT is in use by multiple threads.  To safeguard against this and perhaps more easily add an assertion, maybe we should lock on the DWPT rather than ThreadState?",
            "date": "2011-01-14T19:39:32.463+0000",
            "id": 183
        },
        {
            "author": "Michael Busch",
            "body": "I just committed fixes for some failing tests.  \nEg. the addIndexes() problem is now fixed.  The problem was that I had accidentally removed the following line in DW.addIndexes():\n\n{code}\n // Update SI appropriately\n info.setDocStore(info.getDocStoreOffset(), newDsName, info.getDocStoreIsCompoundFile());\n{code}\n\ninfo.setDocStore() calls clearFiles(), which empties a SegmentInfo-local cache of all filenames that belong to the corresponding segment.  Since addIndexes() changes the segment name, it is important to refill that cache with the new file names.\n\nThis was a sneaky bug.  We should probably call clearFiles() explicitly there in addIndexes().  For now I added a comment.",
            "date": "2011-01-16T02:30:22.451+0000",
            "id": 184
        },
        {
            "author": "Jason Rutherglen",
            "body": "Are you also merging trunk in as svn up yields a lot of updates.\n\nThere are new test failures in: TestSnapshotDeletionPolicy",
            "date": "2011-01-16T02:55:07.645+0000",
            "id": 185
        },
        {
            "author": "Jason Rutherglen",
            "body": "The TestStressIndexing2 errors remind me of what I saw when working on LUCENE-2680.  I'll take a look.  They weren't there in the previous revisions of this branch.",
            "date": "2011-01-16T03:01:03.687+0000",
            "id": 186
        },
        {
            "author": "Jason Rutherglen",
            "body": "In DW.flushAllThreads we're accessing indexWriter.segmentInfos while we're not synced on IW, so the segment infos vector may be changing as we're accessing it.  I'm not sure how we can reasonably solve this, I don't think cloning segment infos will work.  In trunk, doFlush is sync'ed on IW and so doesn't run into these problems.  Perhaps for the flush all threads case we should simply sync on IW?",
            "date": "2011-01-16T03:41:02.662+0000",
            "id": 187
        },
        {
            "author": "Jason Rutherglen",
            "body": "DW.deleteTerms iterates on DWPTs without acquiring the ThreadState.lock, instead DWPT.deleteTerms is synced (on DWPT).  I think if a flush is occurring then deletes can get in at the same time?  I don't think BufferedDeletes supports that?",
            "date": "2011-01-16T04:39:15.229+0000",
            "id": 188
        },
        {
            "author": "Jason Rutherglen",
            "body": "I removed ThreadState and DWPT now extends ReentrantLock.  \n\nI added assertions such as delete terms assert numDocsInRAM > 0; which fails.  I think we should focus on cleaning up the locking, ie, do we need to use synchronized in DWPT?  Perhaps everything should use the RL?",
            "date": "2011-01-16T18:53:17.705+0000",
            "id": 189
        },
        {
            "author": "Jason Rutherglen",
            "body": "I just noticed this.  I think this line outside of any locking is probably not good for the concurrency of updateDoc.  Meaning all kinds of things can sneak in like flushes, before we get to adding the delete to all DWPTs?  This's part of what was tricky with LUCENE-2680, we had to keep the locking on DW for updateDoc.  Maybe to test this's an issue we can assert the count of the deletes, ala, FlushControl (which was added I think to ensure concurrency correctness)?\n\n{code}\n  // delete term from other DWPTs later, so that this thread\n  // doesn't have to lock multiple DWPTs at the same time\n  if (delTerm != null) {\n    deleteTerm(delTerm, perThread);\n  }\n{code}",
            "date": "2011-01-16T19:09:19.932+0000",
            "id": 190
        },
        {
            "author": "Jason Rutherglen",
            "body": "Maybe getAndLock should accept a delTerm, and lock on every non-flushing, non-aborting, numDocs > 0, DWPT, and add the delTerm to them, then unlock each locked DWPT?  This is analogous to how trunk adds the delTerm in the synced DW.getThreadState method?",
            "date": "2011-01-16T19:33:13.077+0000",
            "id": 191
        },
        {
            "author": "Jason Rutherglen",
            "body": "Here's a potential plan for the locking.  I think we still need a global lock for flush status, abort, and deletes.  DocumentsWriterPerThreadPool has the getAndLock method so maybe we should add a global lock on it.\n\n1) Add DWPTP.setFlushStatus(DWPT)\n2) Add DWPTP.setAbortStatus(DWPT)\n3) Add DWPTP.getFlushStatus(DWPT)\n4) Add DWPTP.getAbortStatus(DWPT)\n5) Add DWPTP.getAndLock(Thread requestingThread, DocumentsWriter documentsWriter, Document doc, Term delTerm)\n6) Add DWPTP.[deleteTerm,deleteQuery]\n\nWhere each of these methods acquires a lock on DWPTP.  \n\nI think it'll look somewhat like FlushControl, however the common parameter will be an idx to the appropriate DWPT, eg, getFlushPending(int idx).  Alternatively instead of placing this in DWPTP, DW is another possible candidate.",
            "date": "2011-01-16T20:14:57.838+0000",
            "id": 192
        },
        {
            "author": "Michael Busch",
            "body": "My last commit yesterday made almost all test cases pass.\n\nThe ones that test flush-by-ram are still failing.  Also TestStressIndexing2 still fails.  The reason has to do with how deletes are pushed into bufferedDeletes.  E.g. if I call addDocument() instead of updateDocument() in TestStressIndexing.IndexerThread then the test passes. \n\nI need to look more into that problem, but otherwise it's looking good and we're pretty close!",
            "date": "2011-01-17T22:23:33.885+0000",
            "id": 193
        },
        {
            "author": "Jason Rutherglen",
            "body": "Very nice!  Looks like we needed all kinds of IW syncs?  I noticed that in addition to TestStressIndexing2, TestNRTThreads was also failing.  The attached patch fixes both by adding a sync on DW for deletes (and the update doc delete term).  Time to add the RAM usage?",
            "date": "2011-01-17T23:33:44.152+0000",
            "id": 194
        },
        {
            "author": "Jason Rutherglen",
            "body": "Looks like TestNRTThreads is still sometimes failing, if I moved the sync around then it passes and TestStressIndexing2 fails.  ",
            "date": "2011-01-17T23:57:27.267+0000",
            "id": 195
        },
        {
            "author": "Jason Rutherglen",
            "body": "Ok, TestNRTThreads works after 10+ iterations.  TestStressIndexing2 works most of the time however with enough iterations, eg, \"ant test-core -Dtestcase=TestStressIndexing2 -Dtests.iter=30\" it fails.  I think that deletes are sneaking in because we're not sync'ed on DW as we're flushing the DWPT.  Ideally some assertions would pick this up.",
            "date": "2011-01-18T07:05:01.816+0000",
            "id": 196
        },
        {
            "author": "Michael McCandless",
            "body": "I ran a quick perf test here: I built the 10M Wikipedia index,\nStandard codec, using 6 threads.  Trunk took 541.6 sec; RT took 518.2\nsec (only a bit faster), but the test wasn't really fair because it\nflushed @ docCount=12870.\n\nBut I can't test flush by RAM -- that's not working yet on RT right?\n\n(The search results matched, which is nice!)\n\nThen I ran a single-threaded test.  Trunk took 1097.1 sec and RT took\n1040.5 sec -- a bit faster!  Presumably in the noise (we don't expect\na speedup?), but excellent that it's not slower...\n\nI think we lost infoStream output on the details of flushing?  I can't\nsee when which DWPTs are flushing...\n",
            "date": "2011-01-18T11:30:59.825+0000",
            "id": 197
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. I can't test flush by RAM - that's not working yet on RT right?\n\nRight, we're only flushing by doc count, so we could be flushing segments that are too small?  However we can see some of the concurrency gains by not sync'ing on IW and allowing documents updates to continue while flushing.",
            "date": "2011-01-18T14:45:41.563+0000",
            "id": 198
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nI ran a quick perf test here: I built the 10M Wikipedia index,\nStandard codec, using 6 threads. Trunk took 541.6 sec; RT took 518.2\nsec (only a bit faster), but the test wasn't really fair because it\nflushed @ docCount=12870.\n{quote}\n\nThanks for running the tests!\nHmm that's a bit disappointing - we were hoping for more speedup.  \nFlushing by docCount is currently per DWPT, so every initial segment\nin your test had 12870 docs. I guess there's a lot of merging happening.\n\nMaybe you could rerun with higher docCount?\n\nbq. But I can't test flush by RAM - that's not working yet on RT right?\n\nTrue.  I'm going to add that soonish.  There's one thread-safety bug \nrelated to deletes that needs to be fixed too.\n\n{quote}\nThen I ran a single-threaded test. Trunk took 1097.1 sec and RT took\n1040.5 sec - a bit faster! Presumably in the noise (we don't expect\na speedup?), but excellent that it's not slower...\n{quote}\n\nYeah I didn't expect much speedup - cool! :)  Maybe because some \ncode is gone, like the WaitQueue, not sure how much overhead that \nadded in the single-threaded case.\n\n{quote}\nI think we lost infoStream output on the details of flushing? I can't\nsee when which DWPTs are flushing...\n{quote}\n\nOh yeah, good point, I'll add some infoStream messages to DWPT!",
            "date": "2011-01-18T16:19:53.194+0000",
            "id": 199
        },
        {
            "author": "Michael McCandless",
            "body": "\nThe branch is looking very nice!!  Very clean :)\n\nRandom comments:\n\nWhy does DW.anyDeletions need to be sync'd?\n\nMissing headers on at least DocumentsWriterPerThreadPool,\nThreadAffinityDWTP.\n\nIWC.setIndexerThreadPool's javadoc is stale.\n\nOn ThreadAffinityDWTP... it may be better if we had a single queue,\nwhere threads wait in line, if no DWPT is available?  And when a DWPT\nfinishes it then notifies any waiting threads?  (Ie, instead of queue-per-DWPT).\n\nI see the fieldInfos.update(dwpt.getFieldInfos()) (in\nDW.updateDocument) -- is there a risk that two threads bring a new\nfield into existence at the same time, but w/ different config?  Eg\none doc omitsTFAP and the other doesn't?  Or, on flush, does each DWPT\nuse its private FieldInfos to correctly flush the segment?  (Hmm: do\nwe seed each DWPT w/ the original FieldInfos created by IW on init?).\n\nHow are we handling the case of open IW, do delete-by-term but no\nadded docs?\n\nDoes DW.pushDeletes really need to sync on IW?  BufferedDeletes is\nsync'd already.\n\nDW.substractFlushedDocs is mis-spelled (not sure it's used though).\n\nIn DW.deleteTerms... shouldn't we skip a DWPT if it has no buffered\ndocs?\n",
            "date": "2011-01-18T18:35:40.664+0000",
            "id": 200
        },
        {
            "author": "Michael Busch",
            "body": "bq. Why does DW.anyDeletions need to be sync'd?\n\nHmm good point.  Actually only the call to DW.pendingDeletes.any() needs to be synced, but not the loop that calls the DWPTs.\n\n{quote}\nIn ThreadAffinityDWTP... it may be better if we had a single queue,\nwhere threads wait in line, if no DWPT is available? And when a DWPT\nfinishes it then notifies any waiting threads? (Ie, instead of queue-per-DWPT).\n{quote}\n\nWhole foods instead of safeway? :)\nYeah that would be fairer.  A large doc (= a full cart) wouldn't block unlucky other docs.  I'll make that change, good idea!\n\n{quote}\nI see the fieldInfos.update(dwpt.getFieldInfos()) (in\nDW.updateDocument) - is there a risk that two threads bring a new\nfield into existence at the same time, but w/ different config? Eg\none doc omitsTFAP and the other doesn't? Or, on flush, does each DWPT\nuse its private FieldInfos to correctly flush the segment? (Hmm: do\nwe seed each DWPT w/ the original FieldInfos created by IW on init?).\n{quote}\n\nEvery DWPT has its own private FieldInfos.  When a segment is flushed the DWPT uses its private FI and then it updates the original DW.fieldInfos (from IW), which is a synchronized call.  \n\nThe only consumer of DW.getFieldInfos() is SegmentMerger in IW.  Hmm, given that IW.flush() isn't synchronized anymore I assume this can lead into a problem?  E.g. the SegmentMerger gets a FieldInfos that's \"newer\" than the list of segments it's trying to flush?\n\nbq. How are we handling the case of open IW, do delete-by-term but no added docs?\n\nDW has a SegmentDeletes (pendingDeletes) which gets pushed to the last segment.  We only add delTerms to DW.pendingDeletes if we couldn't push it to any DWPT.  Btw. I think the whole pushDeletes business isn't working correctly yet, I'm looking into it.  I need to understand the code that coalesces the deletes better. \n\nbq. In DW.deleteTerms... shouldn't we skip a DWPT if it has no buffered docs?\n\nYeah, I did that already, but not committed yet.",
            "date": "2011-01-18T19:31:20.568+0000",
            "id": 201
        },
        {
            "author": "Michael Busch",
            "body": "So I'm wondering about the following problem with deletes:\n\nSuppose we open a new IW on an existing index with 2 segments _1 and _2. IW is set to maxBufferedDocs=1000.  The app starts indexing with two threads, so two DWPTs are created.  DWPT1 starts working on _3, DWPT2 on _4.  Both \"remember\" that they must apply their deletes only to segments _1 and _2.  After adding 500 docs thread 2 stops indexing for an hour, but thread 1 keeps working.  While thread 2 is sleeping several segment flushes (_3, _5, _6, etc) happen.\n\nNow thread 2 wakes up again and adds another 500 docs, and also some deletes, so DWPT2 has to flush finally.  How can it figure out to which docs the deletes to apply to?  _1 and _2 are probably gone a long time ago.  If we apply the deletes to all of _3 this would be a mistake too.\n\nI'm starting to think there's no way around sequenceIds?  Even without RT.",
            "date": "2011-01-19T08:38:33.835+0000",
            "id": 202
        },
        {
            "author": "Michael McCandless",
            "body": "I like the grocery store analogy!  Yes, just like that :) \n\nbq. Every DWPT has its own private FieldInfos. When a segment is flushed the DWPT uses its private FI and then it updates the original DW.fieldInfos (from IW), which is a synchronized call.\n\nOK that sounds good.\n\nbq. Hmm, given that IW.flush() isn't synchronized anymore I assume this can lead into a problem? E.g. the SegmentMerger gets a FieldInfos that's \"newer\" than the list of segments it's trying to flush?\n\nYes... or, the FieldInfos changes (due to flush) while SegmentMerger is still merging.  Probably SR should make a deep copy of the FieldInfos when it starts?\n\nbq. DW has a SegmentDeletes (pendingDeletes) which gets pushed to the last segment. We only add delTerms to DW.pendingDeletes if we couldn't push it to any DWPT. Btw. I think the whole pushDeletes business isn't working correctly yet, I'm looking into it. I need to understand the code that coalesces the deletes better.\n\nOK :)\n\nbq. How can it figure out to which docs the deletes to apply to? _1 and _2 are probably gone a long time ago. If we apply the deletes to all of _3 this would be a mistake too.\n\nHmmm... I think you're right (this is a problem).\n\nI think we also have problems w/ updateDocument?  That call is\nsupposed be atomic (ie the del & addDoc can never be separately\ncommitted), but, I think if one DWPT (holding the del term) gets\nflushed but another (holding the del term and the added doc) aborts\nand then you commit, you could see the del \"make it\" but not the\naddDocument?\n\nFinally, we are wanting to allow out-of-order-merges soon... so\nthat eg BSMP becomes much simpler to implement & bring to core,\nbut, these buffered deletes also make that more complicated.\n\nGonna have to mull on this...\n",
            "date": "2011-01-19T14:21:26.017+0000",
            "id": 203
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. we are wanting to allow out-of-order-merges soon\n\nThen the coalescing of deletes won't work.  \n\nbq. I'm starting to think there's no way around sequenceIds? Even without RT.\n\nThis sounds about right, however would the sequence-ids be for all segments (instead of only for DW)?",
            "date": "2011-01-19T15:25:27.325+0000",
            "id": 204
        },
        {
            "author": "Michael McCandless",
            "body": "OK I think Michael's example can be solved, with a small change to the delete buffering.\n\nWhen a delete arrives, we should buffer in each DWPT, but also buffer into the \"global\" deletes pool (held in DocumentsWriter).\n\nWhenever any DWPT is flushed, that global pool is pushed.\n\nThen, the buffered deletes against each DWPT are carried (as usual) along w/ the segment that's flushed from that DWPT, but those buffered deletes *only* apply to the docs in that one segment.\n\nThe pushed deletes from the global pool apply to all prior segments (ie, they \"coalesce\").\n\nThis way, the deletes that will be applied to the already flushed segments are aggressively pushed.\n\nSeparately, I think we should relax the error semantics for updateDocument: if an aborting exception occurs (eg disk full while flushing a segment), then it's possible that the \"delete\" from an updateDocument will have applied but the \"add\" did not.  Outside of error cases, of course, updateDocument will continue to be atomic (ie a commit() can never split the delete & add).  Then the updateDocument case is handled as just an [atomic wrt flush] add & delete.",
            "date": "2011-01-20T17:10:20.716+0000",
            "id": 205
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. When a delete arrives, we should buffer in each DWPT, but also buffer into the \"global\" deletes pool (held in DocumentsWriter).\n\nThis'll work, however it seems like it's going to be a temporary solution if we implement sequence-ids properly and/or implement non-sequential merges.  In fact, with shared doc-store gone, what's holding up non-sequential merging?",
            "date": "2011-01-20T17:26:56.005+0000",
            "id": 206
        },
        {
            "author": "Michael McCandless",
            "body": "bq. In fact, with shared doc-store gone, what's holding up non-sequential merging?\n\nNothing really!  We could/should go do it right now... I think it should be trivial.  Then, we should fixup our default MP to behave more like BSMP!!  Immense segments are merged only pair wise, and no inadvertent optimizing...\n\nI think the buffered deletes will work fine for non-sequential merging -- we'd do the same coalescing we do now, only applying deletes on-demand to the to-be-merged segs, etc.\n\nWe just have to make sure the merged segment is appended to the end of the index (well, what was the end as of when the merge kicked off); this way I think we can continue w/ the invariant that buffered deletes apply to all segments to their \"left\"?",
            "date": "2011-01-20T19:21:25.671+0000",
            "id": 207
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}We could/should go do it right now{quote}\n\nNice!\n\n{quote}I think the buffered deletes will work fine for non-sequential merging - we'd do the same coalescing we do now, only applying deletes on-demand to the to-be-merged segs, etc.{quote}\n\nI think this is going to make IW deletes even more hairy and hard to understand!  Though if we keep the option of using a BV for deletes then there's probably no choice.  If we implemented sequence-id deletes using a short[], then we're only increasing the RAM usage by 16 times, though we then do not need to clone which can generate excessive garbage (in a high flush [N]RT enviro).  ",
            "date": "2011-01-20T20:25:36.884+0000",
            "id": 208
        },
        {
            "author": "Michael McCandless",
            "body": "bq. If we implemented sequence-id deletes using a short[], then we're only increasing the RAM usage by 16 times, though we then do not need to clone which can generate excessive garbage (in a high flush [N]RT enviro).\n\nHow would we handle wraparound (in a concurrent way)?  Also, 16 fold increase in RAM usage is not cheap!\n\nI think, instead, we should recycle the bit vectors?  See, what we do now is really quite silly:  we drop the BV, let GC recycle it, allocate a new BV (same size), copy in nearly the same bits that we just discarded, set a few more bits.\n\nIf instead we had a pool that'd hold recently freed BVs (for a given segment), but, also tracked their \"state\" ie what \"delete gen\" they were at, and then when we need a new BV for that same segment, we pull a free one, catch it up (replay the deletes that had arrived since it was created), and use it, that's very fast?  Ie the cost is about as good as it can be -- incremental to the number of deletes actually recorded.  And the added RAM is \"a few bits\" per doc, where exactly how many \"a few\" is is decided by your app, ie, how many in-flight readers it \"tends\" to keep open.",
            "date": "2011-01-21T13:54:14.634+0000",
            "id": 209
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. How would we handle wraparound (in a concurrent way)? Also, 16 fold increase in RAM usage is not cheap!\n\nInstantiate a new array, and the next reader's seqid is set to 0, while a second value is incremented to guarantee uniqueness of the reader.\n\nA short's 8 bytes * 500,000 docs (in the RAM buffer, is that a lot?) = ~4 MB?  Right, it'll eat into the RAM buffer but it's not extreme (or is it?!).\n\nbq. we drop the BV, let GC recycle it, allocate a new BV (same size), copy in nearly the same bits that we just discarded, set a few more bits.\n\nRight, that's probably our best option for DF, BV, norms, and any other similar array.  I did propose that a while back, and I'm not sure why, but I don't think you were a big fan:  LUCENE-1574\n\nWould this also be used for DW's deletes?  At 16 times the minimum size of the sequence-ids then the pooled approach would allow the equivalent of 16 BVs!  The paged approach I think'll have issues in a low reader latency enviro, ie, create overhead from all the changes.  Whereas an array is fast to change, and fast to copy.\n\nbq. also tracked their \"state\" ie what \"delete gen\" they were at, and then when we need a new BV for that same segment, we pull a free one, catch it up\n\nCouldn't we simply use System.arraycopy and be done?\n\n\n\n\n\n",
            "date": "2011-01-21T15:55:20.527+0000",
            "id": 210
        },
        {
            "author": "Jason Rutherglen",
            "body": "Sorry, pre-coffee math, a short is 2 bytes! * 500,000 = ~1 MB.  That's a little less painful.",
            "date": "2011-01-21T15:58:30.469+0000",
            "id": 211
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Right, it'll eat into the RAM buffer but it's not extreme (or is it?!).\n\nI think this could be acceptable (2 bytes per doc) as long as it's only for the docIDs in iW's RAM buffer, and not for docs in flushed segments?\n\nbq. I did propose that a while back, and I'm not sure why, but I don't think you were a big fan: LUCENE-1574\n\nUgh, you're right!  Back then I wasn't a fan.... but, back then I didn't realize we could also reuse the contents of the bit vector (not just the allocated RAM), using a replay log.\n\nbq. Would this also be used for DW's deletes?\n\nIt's tempting -- but let's first see how it works out for the flushed segments.\n\nbq. The paged approach I think'll have issues in a low reader latency enviro, ie, create overhead from all the changes. Whereas an array is fast to change, and fast to copy.\n\nYou mean paged BV right?  I think that, and more generally any transactional data structure (eg like Zoie's wrapped bloom filter / HashSet approach) is too much added cost for searching.  Using RT/NRT shouldn't slow down searching, ie I prefer the cost be front loaded into the reopen than backloaded into all searches.\n\nbq. Couldn't we simply use System.arraycopy and be done?\n\nWell... System.arraycopy, while fast, is still O(N).  Yes, it has a small constant in front, but for a large index that cost will start to dominate.  Vs the cost of replaying the log, assuming the log is \"smallish\", is linear in the number of deletes since this BV's last reader.  Still I expect we'll need a hybrid approach -- if the number of deletes in the log is too many then we fallback to System.arraycopy.\n",
            "date": "2011-01-22T11:11:40.023+0000",
            "id": 212
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. the cost of replaying the log, assuming the log is \"smallish\"\n\nThis is recording and replaying the doc-ids?  How/when does a previous BV become 'free' to be used by the next reader?  What if they're open at the same time?  And if it's a previous previous reader that's been closed, won't that be quite a few docids to save?  Eg, a delete-by-query has removed thousands of docs, I guess we'd use System.arraycopy then.  The most usual case is updateDocument with [N]RT, which'd generate few doc-ids.  \n\nbq. System.arraycopy, while fast, is still O(N)\n\nRight, the larger segments will really adversely affect performance, as they do today, however the indexing is so much slower with NRT + clone that it's not noticeable.  \n\nbq. Using RT/NRT shouldn't slow down searching\n\nRight!  The cost needs to be in the indexing and/or reopen threads.",
            "date": "2011-01-22T16:11:45.113+0000",
            "id": 213
        },
        {
            "author": "Michael McCandless",
            "body": "One problem I realized in discussing stuff w/ Simon....\n\nThe DWPT-private FieldInfos we now make are dangerous since they break bulk merging of stored fields and term vectors, I think?\n\nSomehow, we have to let each DWPT have some privacy, but, the field name -> number binding should be \"global\".  I think Simon is going to open a separate issue to make something possible along these lines...\n\nI'll make a test case that asserts bulk merging is \"working\" w/ threaded indexing... would be good to know it's working properly today, too :)",
            "date": "2011-01-23T18:53:14.472+0000",
            "id": 214
        },
        {
            "author": "Michael McCandless",
            "body": "I'm getting compilation errors after update the RT branch -- eg, BlockTermState is missing?",
            "date": "2011-01-24T00:04:51.847+0000",
            "id": 215
        },
        {
            "author": "Simon Willnauer",
            "body": "{quote}\nSomehow, we have to let each DWPT have some privacy, but, the field name -> number binding should be \"global\". I think Simon is going to open a separate issue to make something possible along these lines...\n{quote}\nYep, I opened LUCENE-2881 for this",
            "date": "2011-01-24T07:33:25.975+0000",
            "id": 216
        },
        {
            "author": "Jason Rutherglen",
            "body": "The compilation errors are gone, TestNRTThreads and TestStressIndexing2 are still failing.  I think we need to implement Mike's idea: \n\nhttps://issues.apache.org/jira/browse/LUCENE-2324?focusedCommentId=12984285&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12984285\n\nthen retest.  \n\nIs a test deadlocking somewhere, ant hasn't returned.",
            "date": "2011-01-25T18:01:03.695+0000",
            "id": 217
        },
        {
            "author": "Jason Rutherglen",
            "body": "I think we have a logical issue with updateDoc in that we're adding the delTerm\nto *all* DWPTs, however we're flushing DWPTs un-synced. This means that a\ndelTerm would be applied to a segment prior to the DWPT with the new doc being\nflushed. This can easily happen because even though getReader syncs on flush,\ndoFlush is not synced, meaning another concurrent flush may begin, merges may\nbe performed, all without the new doc being made available. The solution is\nprobably as simple as treating updateDoc deletes as *special* and storing them\nin a Map<SegmentInfo/DWPT,<Term,Integer>> that belongs to the DWPT that\nreceived the new doc (instead of adding the delTerm to each DWPT).\n\nThis updateDocDeletesMap would be flushed only when the DWPT [with the updated\ndoc] is flushed, and if a DWPT no longer exists, we can add the\ndelTerm to the last segment. ",
            "date": "2011-01-27T23:29:56.694+0000",
            "id": 218
        },
        {
            "author": "Michael McCandless",
            "body": "Ahh, I see the problem now -- because we don't do a full sync on flushing all DWPTs for getReader (or, commit), we can see the delete part of an updateDocument sneak into a the \"commit\" without the corresponding add.\n\nI think this means we have to do a full stop on getReader or commit?\n\nJason I don't think special tracking of updateDocument deletes will work.  EG, the document I'm replacing could be in another DWPT?",
            "date": "2011-01-28T11:41:43.475+0000",
            "id": 219
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. Jason I don't think special tracking of updateDocument deletes will work. EG, the document I'm replacing could be in another DWPT?\n\nRight.  I think that's why we'd need to keep track of [unfortunately] the delTerms of all DWPTs per DWPT.  Then when a DWPT flushes it's deletes and documents, it'll flush delTerms to the other DWPTs.  Whereas today, in the updateDoc call we're flushing the delTerm to all DWPTs (this could be too early), which would logically seem to break atomicity.  \n\nbq. we have to do a full stop on getReader or commit?\n\nUpdate doc isn't sync'd so we'd need to stop it as well?  Where are we gaining flush concurrency then?  ",
            "date": "2011-01-28T13:30:18.877+0000",
            "id": 220
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Where are we gaining flush concurrency then?\n\nEven if we do full stop for commit and getReader, we've still gained concurrency on the batch indexing case.\n\nSo, I think we can actually avoid full stop for commit/getReader.\n\nThe current plan w/ deletes is that a delete gets buffered 1) into the global pool (stored in DW and pushed whenever any DWPT flushes), as well as 2) per DWPT.  The per-DWPT pools apply *only* to the segment flushed from that DWPT, while the global pool applies during coalescing (ie to all \"prior\" segments).\n\nTo avoid the full-stop, I think during the flush we can have two global delete pools.  We carefully sweep all DWPTs and flush each, in succession.  Any DWPT not yet flushed is free to continue indexing as normal, putting deletes into the first global pool, flushing as normal.  But, a DWPT that has been flushed by the \"sweeper\" must instead put deletes for an updateDocument carefully into the 2nd pool, and not buffer the delete into DWPTs not yet flushed.\n\nBasically, as the sweeper visits each DWPT, it's segregating them into \"pre commit point\" and \"post commit point\".\n\nWe also must then ensure that no \"post commit point\" DWPT is allowed to flush until all \"pre commit point\" DWPTs have been visited by the sweeper.",
            "date": "2011-01-28T13:42:21.526+0000",
            "id": 221
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. Even if we do full stop for commit and getReader, we've still gained concurrency on the batch indexing case.\n\nRight, I'm still worried this case has update doc concurrency issues.\n\n{quote}The current plan w/ deletes is that a delete gets buffered 1) into the global pool (stored in DW and pushed whenever any DWPT flushes), as well as 2) per DWPT. The per-DWPT pools apply only to the segment flushed from that DWPT, while the global pool applies during coalescing (ie to all \"prior\" segments).{quote}\n\nOk, this is very clear now.\n\nbq. We also must then ensure that no \"post commit point\" DWPT is allowed to flush until all \"pre commit point\" DWPTs have been visited by the sweeper.\n\nAh, this's the key that should solve the edge cases.",
            "date": "2011-01-28T14:36:18.132+0000",
            "id": 222
        },
        {
            "author": "Michael McCandless",
            "body": "Actually, instead of buffering the delete term against each DWPT, and pushing the buffered delete packets when we flush the DWPT...\n\nWhy don't we simply resolve the delete \"live\"?\n\nSee, TermsHashPerField holds all indexed terms in a hash table, so, lookup is very fast (much faster than what we do today, where segment first gets flushed, then we load an SR and use terms dict to seek to the right term).\n\nWe could eg add a parallel int[] array (ie a new int field, per unique term) that'd hold the docIDUpto.  On flush, we'd then skip any docIDs less than the docIDUpto for that term.\n\nAlternatively, we could keep a separate hash (as we have right now), but then, on flushing the segment, we apply the deletes as we flush.",
            "date": "2011-01-29T13:27:32.078+0000",
            "id": 223
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Alternatively, we could keep a separate hash (as we have right now), but then, on flushing the segment, we apply the deletes as we flush.\n\nI'll open a separate issue for this -- I think it's a low hanging fruit.",
            "date": "2011-01-29T14:12:47.227+0000",
            "id": 224
        },
        {
            "author": "Michael McCandless",
            "body": "OK, I opened LUCENE-2897.\n",
            "date": "2011-01-29T14:40:22.983+0000",
            "id": 225
        },
        {
            "author": "Jason Rutherglen",
            "body": "I had to read this a few times, yes it's very elegant as we're skipping the postings that otherwise would be deleted immediately after flush, and we're reusing the terms map already in DWPT.",
            "date": "2011-01-29T14:41:07.423+0000",
            "id": 226
        },
        {
            "author": "Simon Willnauer",
            "body": "Can anyone gimme a quick statement about what is left here or what the status of this issue is? I am at the point where I need to do some rather big changes to DocValues which I would not need if we have DWPT so I might rather help here before wasting time.",
            "date": "2011-02-24T14:48:58.856+0000",
            "id": 227
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nSomehow, we have to let each DWPT have some privacy, but, the field name -> number binding should be \"global\". I think Simon is going to open a separate issue to make something possible along these lines...\n{quote}\n\nThis is done now (LUCENE-2881) and merged into the RT branch.\n\n\n{quote}\nThe current plan w/ deletes is that a delete gets buffered 1) into the global pool (stored in DW and pushed whenever any DWPT flushes), as well as 2) per DWPT. The per-DWPT pools apply only to the segment flushed from that DWPT, while the global pool applies during coalescing (ie to all \"prior\" segments).\n{quote}\n\nI implemented and committed this approach.  It's looking pretty good - almost all tests pass.  Only TestStressIndexing2 is sometimes failing - but only when updateDocument() is called, not when I modify the test to only use add, delete-by-term and delete-by-query. \n\n{quote}\nTo avoid the full-stop, I think during the flush we can have two global delete pools. We carefully sweep all DWPTs and flush each, in succession. Any DWPT not yet flushed is free to continue indexing as normal, putting deletes into the first global pool, flushing as normal. But, a DWPT that has been flushed by the \"sweeper\" must instead put deletes for an updateDocument carefully into the 2nd pool, and not buffer the delete into DWPTs not yet flushed.\n{quote}\n\nI haven't done this yet - it might fix the failing test I described.",
            "date": "2011-02-25T07:27:17.881+0000",
            "id": 228
        },
        {
            "author": "Michael Busch",
            "body": "{quote}\nCan anyone gimme a quick statement about what is left here or what the status of this issue is? I am at the point where I need to do some rather big changes to DocValues which I would not need if we have DWPT so I might rather help here before wasting time.\n{quote}\n\nI think it's very close!  The new deletes approach is implemented, and various bugs are fixed.  Also the latest trunk is merged in (including LUCENE-2881).  \nOutstanding issues are to fix the updateDocument() problems, and finish flush-by-RAM (LUCENE-2573).\n\nOther than TestStressIndexing2 and TestNRTThreads (updateDocument problem) and a few tests that rely on flush-by-RAM, all core and contrib tests are passing now.",
            "date": "2011-02-25T07:31:50.844+0000",
            "id": 229
        },
        {
            "author": "Simon Willnauer",
            "body": "bq. Outstanding issues are to fix the updateDocument() problems, and finish flush-by-RAM (LUCENE-2573).\nSeems like LUCENE-2573 is more isolated than the updateDocument() issue so I think I can spend time on that one without interfering with what you are working on. I might need some time to get into what has been done so far, might come back here or on the list if I have questions. ",
            "date": "2011-02-25T19:50:32.396+0000",
            "id": 230
        },
        {
            "author": "Michael McCandless",
            "body": "One nice side effect of DWPT is it should allow us to get over the 2GB RAM buffer limit, assuming you use multiple threads.\n\nIe I can set my RAM buffer to 10 GB, and if I'm using 5 threads, it should work.\n\nNot sure it's really meaningful in practice, since in past tests I haven't seen any gains over ~128 or 256 MB buffer... but maybe that's changed now.",
            "date": "2011-03-10T16:53:42.193+0000",
            "id": 231
        },
        {
            "author": "Jason Rutherglen",
            "body": "Is the max optimal DWPT size related to the size of the terms hash, or is it likely something else?",
            "date": "2011-03-10T17:00:43.113+0000",
            "id": 232
        },
        {
            "author": "Michael McCandless",
            "body": "bq. Is the max optimal DWPT size related to the size of the terms hash, or is it likely something else?\n\nBigger really should be better I think.\n\nBecause 1) the RAM efficiency ought to scale up very well, as you see a given term in more and more docs (hmm, though, maybe not, because from Zipf's law, half your terms will be singletons no matter how many docs you index), and 2) less merging is required.\n\nI'm not sure why in the past perf seemd to taper off and maybe get worst after RAM buffer was over 256 MB... we should definitely re-test.",
            "date": "2011-03-10T17:16:34.830+0000",
            "id": 233
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}Because 1) the RAM efficiency ought to scale up very well, as you see a given term in more and more docs (hmm, though, maybe not, because from Zipf's law, half your terms will be singletons no matter how many docs you index), and 2) less merging is required.{quote}\n\nI'm not sure how we handled concurrency on the terms hash before, however with DWPTs there won't be contention regardless.  It'd be nice if we could build 1-2 GB segment's in RAM, I think that would greatly reduce the number merges that are required downstream.  Eg, then there's less need for merging by size, and most merges would be caused by the number/percentage of deletes.  If it turns out the low DF terms are causing the slowdown, maybe there is a different hashing system that could be used.",
            "date": "2011-03-10T17:25:16.474+0000",
            "id": 234
        },
        {
            "author": "Michael McCandless",
            "body": "Concurrency today is similar to DWPT in that we simply write into multiple segments in RAM.\n\nBut the, on flush, we do a merge (in RAM) of these segments and write a single on-disk segment.\n\nVs this change which instead writes N on-disk segments and lets \"normal\" merging merge them.\n\nI think making a different data structure to hold low-DF terms would actually be a big boost in RAM efficiency.  The RAM-per-unique-term is fairly high...",
            "date": "2011-03-10T17:29:36.633+0000",
            "id": 235
        },
        {
            "author": "Jason Rutherglen",
            "body": "bq. I think making a different data structure to hold low-DF terms would actually be a big boost in RAM efficiency. The RAM-per-unique-term is fairly high...\n\nHowever we're not sure why a largish 1+ GB RAM buffer seems to slow down?  If we're round robin indexing against the DWPTs I think they'll have a similar number of unique terms as today, even though each DWPT will be smaller in size total size from each containing 1/Nth docs.  ",
            "date": "2011-03-11T13:33:49.631+0000",
            "id": 236
        },
        {
            "author": "Michael McCandless",
            "body": "The slowdown could have been due to the merge sort by docID that we do today on flush.\n\nIe, if a given term X occurrs in 6 DWPTs (today) then we merge-sort the docIDs from the postings of that term, which is costly.  (The \"normal\" merge that will merge these DWPTs after this issue lands just append by docIDs).\n\nSo maybe after this lands we'll see only faster performance the larger the RAM buffer :)  That would be nice!",
            "date": "2011-03-11T14:11:22.987+0000",
            "id": 237
        },
        {
            "author": "Jason Rutherglen",
            "body": "{quote}Ie, if a given term X occurrs in 6 DWPTs (today) then we merge-sort the docIDs from the postings of that term, which is costly. (The \"normal\" merge that will merge these DWPTs after this issue lands just append by docIDs).{quote}\n\nRight, this is the same principal motivation behind implementing DWPTs for use with realtime search, eg, the doc-id interleaving is too expensive to be performed at query time.",
            "date": "2011-03-11T14:23:24.598+0000",
            "id": 238
        },
        {
            "author": "Simon Willnauer",
            "body": "guys I opened LUCENE-3023 to land on trunk! can I close this and we iterate on LUCENE-3023 from now on?\n\nsimon",
            "date": "2011-04-14T12:21:22.463+0000",
            "id": 239
        },
        {
            "author": "Simon Willnauer",
            "body": "we land this on trunk via LUCENE-3023 ",
            "date": "2011-04-28T15:19:55.711+0000",
            "id": 240
        }
    ],
    "component": "core/index",
    "description": "See LUCENE-2293 for motivation and more details.\n\nI'm copying here Mike's summary he posted on 2293:\n\nChange the approach for how we buffer in RAM to a more isolated\napproach, whereby IW has N fully independent RAM segments\nin-process and when a doc needs to be indexed it's added to one of\nthem. Each segment would also write its own doc stores and\n\"normal\" segment merging (not the inefficient merge we now do on\nflush) would merge them. This should be a good simplification in\nthe chain (eg maybe we can remove the *PerThread classes). The\nsegments can flush independently, letting us make much better\nconcurrent use of IO & CPU.",
    "hasPatch": true,
    "hasScreenshot": false,
    "id": "LUCENE-2324",
    "issuetypeClassified": "RFE",
    "issuetypeTracker": "IMPROVEMENT",
    "priority": "Minor",
    "product": "LUCENE",
    "project": "LUCENE",
    "summary": "Per thread DocumentsWriters that write their own private segments",
    "systemSpecification": true,
    "version": ""
}